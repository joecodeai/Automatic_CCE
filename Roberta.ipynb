{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model_id = \"FacebookAI/roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cce_df = pd.read_csv(\"/raid/deallab/CCE_Data/raw_data/finecite/full_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the DataFrame\n",
    "results = []\n",
    "for index, row in cce_df.iterrows():\n",
    "    # Clean the paragraph by replacing <ref> tags with '[TREF]'\n",
    "    clean_paragraph = re.sub(r'<ref.*?>.*?</ref>', '[TREF]', row[\"paragraph\"])\n",
    "\n",
    "    # Split the cleaned paragraph into words using ';' as the delimiter\n",
    "    words = clean_paragraph.split(';')\n",
    "\n",
    "    # Process the context_location1 list\n",
    "    context_location1 = eval(row[\"context_location1\"])\n",
    "\n",
    "    # Check if the lengths match, and map the context_location1 to the words\n",
    "    if len(context_location1) == len(words):\n",
    "        # Aggregate the mapped results for the current row\n",
    "        mapped_result = list(zip(context_location1, words))\n",
    "        \n",
    "        # Separate the numbers and words into separate lists\n",
    "        numbers = [str(item[0]) for item in mapped_result]  # Convert numbers to strings\n",
    "        mapped_words = [item[1].strip() for item in mapped_result]  # Strip extra spaces from words\n",
    "        \n",
    "        results.append({\n",
    "            \"Paragraph\": ' '.join(mapped_words),\n",
    "            \"Scope\": numbers\n",
    "        })\n",
    "    else:\n",
    "        results.append({\n",
    "            \"Paragraph\": \"Length of context_location1 and words don't match\",\n",
    "            \"Scope\": \"Mismatch\"\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Scope column elements to lists of integers\n",
    "def convert_scope_to_int(scope):\n",
    "    if isinstance(scope, str):\n",
    "        # Convert string representation of list to an actual list of integers\n",
    "        scope = eval(scope)\n",
    "    # Ensure all elements in the list are integers\n",
    "    return [int(i) for i in scope]\n",
    "\n",
    "# Apply the conversion to the Scope column\n",
    "df[\"Scope\"] = df[\"Scope\"].apply(convert_scope_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer for RoBERTa\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Check if the tokenizer has a pad_token and set it\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize and prepare dataset\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the paragraphs\n",
    "    tokens = tokenizer(examples[\"Paragraph\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    \n",
    "    # Process labels (Scope)\n",
    "    max_label_length = 512  # Same as max_length for consistency\n",
    "    padded_labels = []\n",
    "    \n",
    "    for label_list in examples[\"Scope\"]:\n",
    "        # Truncate if necessary\n",
    "        if len(label_list) > max_label_length:\n",
    "            label_list = label_list[:max_label_length]\n",
    "        \n",
    "        # Pad with -100\n",
    "        padded_label = label_list + [-100] * (max_label_length - len(label_list))\n",
    "        padded_labels.append(padded_label)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    tokens[\"labels\"] = torch.tensor(padded_labels, dtype=torch.long)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Convert the DataFrame to a Dataset\n",
    "dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1055/1055 [00:01<00:00, 1002.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply the tokenization function to the dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=dataset.column_names  # Remove all original columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model for token classification\n",
    "model = RobertaForTokenClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=4  # Adjust this number based on your specific task\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2640' max='2640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2640/2640 06:54, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.181800</td>\n",
       "      <td>1.177531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.147200</td>\n",
       "      <td>1.224751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.174800</td>\n",
       "      <td>1.091165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.073900</td>\n",
       "      <td>0.996247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.944900</td>\n",
       "      <td>0.893089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.992700</td>\n",
       "      <td>0.820209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.673700</td>\n",
       "      <td>0.645233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.720500</td>\n",
       "      <td>0.538606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.526400</td>\n",
       "      <td>0.457363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.453100</td>\n",
       "      <td>0.435886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/raid/deallab/anaconda3/envs/lasse_automatic_cce/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2640, training_loss=0.9061096646568992, metrics={'train_runtime': 417.2796, 'train_samples_per_second': 25.283, 'train_steps_per_second': 6.327, 'total_flos': 2756730629529600.0, 'train_loss': 0.9061096646568992, 'epoch': 10.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_Roberta/tokenizer_config.json',\n",
       " './trained_Roberta/special_tokens_map.json',\n",
       " './trained_Roberta/vocab.json',\n",
       " './trained_Roberta/merges.txt',\n",
       " './trained_Roberta/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./trained_Roberta')\n",
    "tokenizer.save_pretrained('./trained_Roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference on a new paragraph\n",
    "def classify_paragraph(paragraph):\n",
    "    # Clean and tokenize the input paragraph\n",
    "    clean_paragraph = re.sub(r'<ref.*?>.*?</ref>', '[TREF]', paragraph)\n",
    "    tokens = tokenizer(clean_paragraph, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokens = {key: value.to(model.device) for key, value in tokens.items()}\n",
    "    \n",
    "    # Run inference\n",
    "    outputs = model(**tokens)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Map predictions to scopes\n",
    "    predicted_scope = predictions[0].cpu().numpy().tolist()\n",
    "    \n",
    "    return predicted_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_scopes(true_scope, predicted_scope):\n",
    "    # Ensure both lists have the same length\n",
    "    max_length = max(len(true_scope), len(predicted_scope))\n",
    "    \n",
    "    # Pad shorter list with a placeholder for comparison\n",
    "    true_scope_padded = true_scope + [-1] * (max_length - len(true_scope))\n",
    "    predicted_scope_padded = predicted_scope + [-1] * (max_length - len(predicted_scope))\n",
    "    \n",
    "    # Compute the number of correct predictions\n",
    "    correct_predictions = sum(t == p for t, p in zip(true_scope_padded, predicted_scope_padded) if t != -1)\n",
    "    total_predictions = sum(1 for t in true_scope_padded if t != -1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0.0\n",
    "    \n",
    "    return correct_predictions, total_predictions, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Scope: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "True Scope: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Correct Predictions: 87/89\n",
      "Accuracy: 97.75%\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "new_paragraph = 'Neural Machine Translation (NMT) has opened several research directions to exploit as many and diverse data as possible. Massive multilingual NMT models, for instance, take advantage of several language-pair datasets in a single system [TREF] . This offers several advantages, such as a simple training process and enhanced performance of the language-pairs with little data (although sometimes detrimental to the high-resource language-pairs). However, massive models of dozens of languages are not necessarily the best outcome, as it is demonstrated that smaller clusters still offer the same benefits [TREF] .'\n",
    "\n",
    "# Get the predicted scope for the new paragraph\n",
    "predicted_scope = classify_paragraph(new_paragraph)\n",
    "print(\"Predicted Scope:\", predicted_scope)\n",
    "\n",
    "# df[\"Scope\"][0] is the true scope for comparison\n",
    "true_scope = df[\"Scope\"][0]\n",
    "print(\"True Scope:\", true_scope)\n",
    "\n",
    "# Compare scopes\n",
    "correct_predictions, total_predictions, accuracy = compare_scopes(true_scope, predicted_scope)\n",
    "print(f\"Correct Predictions: {correct_predictions}/{total_predictions}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
