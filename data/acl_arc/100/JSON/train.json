[
    {
        "gold": {
            "text": [
                "Semantic similarity is typically defined via the lexical relations of synonymy (automobile -car) and hypernymy (vehicle -car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be-tween two words (#REF). 3",
                "Dissimilar words can be semantically related, e.g.",
                "via functional relationships (night -dark) or when they are antonyms (high -low).",
                "Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Semantic similarity is typically defined via the lexical relations of synonymy (automobile -car) and hypernymy (vehicle -car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be-tween two words (#REF). 3 Dissimilar words can be semantically related, e.g. via functional relationships (night -dark) or when they are antonyms (high -low). Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #TARGET_REF ) .",
        "output": "{\"label\": [\"BACK\"]}"
    },
    {
        "gold": {
            "text": [
                "(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)",
                "We shall see that vague descriptions pose particular challenges to incrementality.",
                "One question emerges when the IA is combined with findings on word order and incremental interpretation.",
                "If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?",
                "This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).",
                "This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (#REF).",
                "A similar problem is discussed in the psycholinguistics of interpretation ( #TARGET_REF ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known .",
                "Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.",
                "Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.) We shall see that vague descriptions pose particular challenges to incrementality. One question emerges when the IA is combined with findings on word order and incremental interpretation. If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected? This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6). This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (#REF). A similar problem is discussed in the psycholinguistics of interpretation ( #TARGET_REF ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known . Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives. Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "10 Both algorithms are provided by Weka (http://www.cs.waikato.ac.nz/ml/weka/).",
                "provide complementary perspectives.",
                "While the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features.",
                "To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.",
                "Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.",
                "( Details of how the average-expert model performs can be found in our prior work ( #TARGET_REF ) . )"
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:10 Both algorithms are provided by Weka (http://www.cs.waikato.ac.nz/ml/weka/). provide complementary perspectives. While the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features. To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below. Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings. ( Details of how the average-expert model performs can be found in our prior work ( #TARGET_REF ) . )",
        "output": "{\"label\": [\"COMP\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting.",
                "We have presented a comparative evaluation of our approach on datasets from four different domains.",
                "In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets.",
                "Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.",
                "Our CRF-based approach also yields promising results in the crossdomain setting.",
                "The features we employ scale well across domains, given that the opinion target vocabularies are substantially different.",
                "For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #REF ; #TARGET_REF ) , perform in comparison to our approach .",
                "Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.",
                "We observe similar challenges as #REF regarding the analysis of complex sentences.",
                "Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting. We have presented a comparative evaluation of our approach on datasets from four different domains. In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets. Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction. Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #REF ; #TARGET_REF ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as #REF regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.",
        "output": "{\"label\": [\"MOT\"]}"
    },
    {
        "gold": {
            "text": [
                "The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages.",
                "Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment.",
                "Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF).",
                "But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #TARGET_REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #TARGET_REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) .",
        "output": "{\"label\": [\"EXT\"]}"
    },
    {
        "gold": {
            "text": [
                "In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #TARGET_REF ) and automatically trained transducers ( #REF ) with a larger range of positions ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #TARGET_REF ) and automatically trained transducers ( #REF ) with a larger range of positions .",
        "output": "{\"label\": [\"FUT\"]}"
    },
    {
        "gold": {
            "text": [
                "Several recent works suggest studying coreference jointly with other tasks.",
                "#REF model entity coreference and event coreference jointly ; #TARGET_REF consider joint coreference and entity-linking .",
                "The work closest to ours is that of #REF, which studies a joint anaphoricity detection and coreference resolution framework.",
                "While their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Several recent works suggest studying coreference jointly with other tasks. #REF model entity coreference and event coreference jointly ; #TARGET_REF consider joint coreference and entity-linking . The work closest to ours is that of #REF, which studies a joint anaphoricity detection and coreference resolution framework. While their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different.",
        "output": "{\"label\": [\"BACK\"]}"
    },
    {
        "gold": {
            "text": [
                "Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #REF ; #TARGET_REF ) .",
                "We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.",
                "Consider, for instance, the list of properties L = size > 3 cm, size < 9 cm .",
                "If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm).",
                "Clearly, sophisticated use of gradable adjectives requires a separation between CD and linguistic realization, unless one is willing to complicate linguistic realization considerably."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #REF ; #TARGET_REF ) . We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult. Consider, for instance, the list of properties L = size > 3 cm, size < 9 cm . If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm). Clearly, sophisticated use of gradable adjectives requires a separation between CD and linguistic realization, unless one is willing to complicate linguistic realization considerably.",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The features can be easily obtained by modifying the TAT extraction algorithm described in ( #TARGET_REF ) .",
                "When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees.",
                "Then we use the toolkit implemented by #REF to train MERS models for the ambiguous source syntactic trees separately.",
                "We set the iteration number to 100 and Gaussian prior to 1."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:The features can be easily obtained by modifying the TAT extraction algorithm described in ( #TARGET_REF ) . When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees. Then we use the toolkit implemented by #REF to train MERS models for the ambiguous source syntactic trees separately. We set the iteration number to 100 and Gaussian prior to 1.",
        "output": "{\"label\": [\"COMP\"]}"
    },
    {
        "gold": {
            "text": [
                "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) .",
        "output": "{\"label\": [\"MOT\"]}"
    },
    {
        "gold": {
            "text": [
                "The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages.",
                "Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment.",
                "Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF).",
                "But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #TARGET_REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #TARGET_REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) .",
        "output": "{\"label\": [\"EXT\"]}"
    },
    {
        "gold": {
            "text": [
                "Each set of translations is stored separately , and for each set the `` marker hypothesis '' ( #TARGET_REF ) is used to segment the phrasal lexicon into a `` marker lexicon . ''",
                "The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are \"marked\" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes.",
                "That is, a basic phrase-level segmentation of an input sentence can be achieved by exploiting a closed list of known marker words to signal the start and end of each segment."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:Each set of translations is stored separately , and for each set the `` marker hypothesis '' ( #TARGET_REF ) is used to segment the phrasal lexicon into a `` marker lexicon . '' The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are \"marked\" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes. That is, a basic phrase-level segmentation of an input sentence can be achieved by exploiting a closed list of known marker words to signal the start and end of each segment.",
        "output": "{\"label\": [\"FUT\"]}"
    },
    {
        "gold": {
            "text": [
                "A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language (#REFb).",
                "There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one language but not the other (e.g., English He walked and French Il est allé), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly.",
                "Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #TARGET_REF ) .",
                "The top row of Figure 1 shows two word alignments between an English-French sentence pair.",
                "We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language (#REFb). There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one language but not the other (e.g., English He walked and French Il est allé), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly. Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #TARGET_REF ) . The top row of Figure 1 shows two word alignments between an English-French sentence pair. We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns.",
        "output": "{\"label\": [\"BACK\"]}"
    },
    {
        "gold": {
            "text": [
                "With all its strong points, there are a number of restrictions to the proposed approach.",
                "First, in its present form it is suitable only for processing of reasonably \"wellbehaved\" texts that consistently use capitalization (mixed case) and do not contain much noisy data.",
                "Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts.",
                "We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.",
                "This is where robust syntactic systems like SATZ ( #REF ) or the POS tagger reported in #TARGET_REF , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:With all its strong points, there are a number of restrictions to the proposed approach. First, in its present form it is suitable only for processing of reasonably \"wellbehaved\" texts that consistently use capitalization (mixed case) and do not contain much noisy data. Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts. We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. This is where robust syntactic systems like SATZ ( #REF ) or the POS tagger reported in #TARGET_REF , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.",
                "This section , which elaborates on preliminary results reported in #TARGET_REF , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .",
                "For an example of a completely annotated abstract, see Figure 2.",
                "Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering. This section , which elaborates on preliminary results reported in #TARGET_REF , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence . For an example of a completely annotated abstract, see Figure 2. Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases.",
        "output": "{\"label\": [\"COMP\"]}"
    },
    {
        "gold": {
            "text": [
                "In the longer term, it is clear that neither the contents nor form of any existing published dictionary meet all the requirements of a natural language processing system.",
                "A substantial component of the research reported above has been devoted to restructuring LDOCE to make it more suitable for automatic analysis.",
                "However, even after this process much of the information in LDOCE remains difficult to access, essentially because it is aimed at a human reader, as opposed to a computer system.",
                "This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis.",
                "In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #TARGET_REF ) .",
                "However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:In the longer term, it is clear that neither the contents nor form of any existing published dictionary meet all the requirements of a natural language processing system. A substantial component of the research reported above has been devoted to restructuring LDOCE to make it more suitable for automatic analysis. However, even after this process much of the information in LDOCE remains difficult to access, essentially because it is aimed at a human reader, as opposed to a computer system. This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis. In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #TARGET_REF ) . However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly.",
        "output": "{\"label\": [\"MOT\"]}"
    },
    {
        "gold": {
            "text": [
                "These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #TARGET_REF ) .",
                "Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.",
                "An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #TARGET_REF ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.",
        "output": "{\"label\": [\"EXT\"]}"
    },
    {
        "gold": {
            "text": [
                "The idea behind the Doc-Pred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected.",
                "In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #TARGET_REF ; #REF ) .",
                "We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2).",
                "The input to Snob is a set of binary vectors, one vector per response document.",
                "The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency).",
                "The predictive model is a Decision Graph (#REF), which, like Snob, is based on the MML principle."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:The idea behind the Doc-Pred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #TARGET_REF ; #REF ) . We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency). The predictive model is a Decision Graph (#REF), which, like Snob, is based on the MML principle.",
        "output": "{\"label\": [\"FUT\"]}"
    },
    {
        "gold": {
            "text": [
                "We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface.",
                "AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface. AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #TARGET_REF ) .",
        "output": "{\"label\": [\"BACK\"]}"
    },
    {
        "gold": {
            "text": [
                "The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in Alshawi (1990, 1992), and implemented in SRI's Core Language Engine (CLE).",
                "In the CLE-QLF approach, as rationally reconstructed by #TARGET_REF and #REF , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.",
                "Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in Alshawi (1990, 1992), and implemented in SRI's Core Language Engine (CLE). In the CLE-QLF approach, as rationally reconstructed by #TARGET_REF and #REF , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules. Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.",
                "Kuhn and de #REF proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model.",
                "Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.",
                "The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.",
                "But unlike the cache model, it uses a multipass strategy.",
                "#TARGET_REF developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last",
                "In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.",
                "Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.",
                "For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition. Kuhn and de #REF proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model. Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage. The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. #TARGET_REF developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.",
        "output": "{\"label\": [\"COMP\"]}"
    },
    {
        "gold": {
            "text": [
                "There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #TARGET_REF ; #REF ) .",
                "Incorporating such techniques would deo crease the system developer workload.",
                "However, there has been no work on domain-independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which WIT handles well.",
                "Therefore incorporating those techniques remains as a future work."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #TARGET_REF ; #REF ) . Incorporating such techniques would deo crease the system developer workload. However, there has been no work on domain-independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which WIT handles well. Therefore incorporating those techniques remains as a future work.",
        "output": "{\"label\": [\"MOT\"]}"
    },
    {
        "gold": {
            "text": [
                "However, there are at least three arguments against iterating PT.",
                "First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time.",
                "Secondly , the cooperative principle of #TARGET_REF , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader .",
                "Finally, it has been shown by #REF that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by #REF strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.",
                "Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly , the cooperative principle of #TARGET_REF , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader . Finally, it has been shown by #REF that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by #REF strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph. Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.",
        "output": "{\"label\": [\"EXT\"]}"
    },
    {
        "gold": {
            "text": [
                "Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #TARGET_REF ) .",
                "Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.",
                "For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.",
                "In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.",
                "Differently,  designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment.",
                "We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.",
                "This subject will be one of our future work topics."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #TARGET_REF ) . Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment. For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler. Differently,  designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment. We believe that it is possible to investigate the space of both tree structure and alignment simultaneously. This subject will be one of our future work topics.",
        "output": "{\"label\": [\"FUT\"]}"
    },
    {
        "gold": {
            "text": [
                "Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #REF ; #REF ; #REF ; #TARGET_REFa ; #REFb ; #REF ) .",
                "Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the \"meaning of words is entirely given by other words\" (#REFb)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #REF ; #REF ; #REF ; #TARGET_REFa ; #REFb ; #REF ) . Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the \"meaning of words is entirely given by other words\" (#REFb).",
        "output": "{\"label\": [\"BACK\"]}"
    },
    {
        "gold": {
            "text": [
                "For each interaction definition we can therefore check which of the flame clauses are applicable and discard the non-applicable ones.",
                "We thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates.",
                "The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques (#REF).",
                "29 The unfolding transformation is also referred to as partial execution, for example, by #REF.",
                "Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.",
                "As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #TARGET_REF ) .",
                "Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section.",
                "To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates.",
                "3° The successive unfolding steps are schematically represented in Figure 20."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:For each interaction definition we can therefore check which of the flame clauses are applicable and discard the non-applicable ones. We thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates. The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques (#REF). 29 The unfolding transformation is also referred to as partial execution, for example, by #REF. Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time. As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #TARGET_REF ) . Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section. To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates. 3° The successive unfolding steps are schematically represented in Figure 20.",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Bridging or associative anaphora has been widely discussed in the linguistic literature (#REF;#REF;#REF;Löbner, 1998).",
                "#REF and #REF include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference).",
                "We follow our previous work ( #TARGET_REFb ) and restrict bridging to non-coreferential cases .",
                "We also exclude comparative anaphora (#REF)."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:Bridging or associative anaphora has been widely discussed in the linguistic literature (#REF;#REF;#REF;Löbner, 1998). #REF and #REF include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work ( #TARGET_REFb ) and restrict bridging to non-coreferential cases . We also exclude comparative anaphora (#REF).",
        "output": "{\"label\": [\"COMP\"]}"
    },
    {
        "gold": {
            "text": [
                "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #TARGET_REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #TARGET_REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) .",
        "output": "{\"label\": [\"MOT\"]}"
    },
    {
        "gold": {
            "text": [
                "The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages.",
                "Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment.",
                "Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF).",
                "But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #TARGET_REF ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #TARGET_REF ) .",
        "output": "{\"label\": [\"EXT\"]}"
    },
    {
        "gold": {
            "text": [
                "The RenTAL system is implemented in LiLFeS ( #TARGET_REF ) 2 .",
                "LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (#REF;.",
                "We applied our system to the XTAG English grammar (The XTAG Research #REF) 3 , which is a large-scale FB-LTAG grammar for English.",
                "The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (#REF) 6 (the average length is 6.32 words).",
                "This result empirically attested the strong equivalence of our algorithm."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The RenTAL system is implemented in LiLFeS ( #TARGET_REF ) 2 . LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (#REF;. We applied our system to the XTAG English grammar (The XTAG Research #REF) 3 , which is a large-scale FB-LTAG grammar for English. The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (#REF) 6 (the average length is 6.32 words). This result empirically attested the strong equivalence of our algorithm.",
        "output": "{\"label\": [\"FUT\"]}"
    },
    {
        "gold": {
            "text": [
                "Another important task of text normalization is sentence boundary disambiguation (SBD) or sentence splitting.",
                "Segmenting text into sentences is an important aspect in developing many applications: syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. Sentence splitting in most cases is a simple matter: a period, an exclamation mark, or a question mark usually signals a sentence boundary.",
                "In certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary.",
                "Furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end-of-sentence indicator (fullstop).",
                "A detailed introduction to the SBD problem can be found in #TARGET_REF ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Another important task of text normalization is sentence boundary disambiguation (SBD) or sentence splitting. Segmenting text into sentences is an important aspect in developing many applications: syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. Sentence splitting in most cases is a simple matter: a period, an exclamation mark, or a question mark usually signals a sentence boundary. In certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary. Furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end-of-sentence indicator (fullstop). A detailed introduction to the SBD problem can be found in #TARGET_REF .",
        "output": "{\"label\": [\"BACK\"]}"
    },
    {
        "gold": {
            "text": [
                "However, these methodologies have not used historical information at the dialogue level as described here.",
                "In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.",
                "The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.",
                "Thus, an error in this work has no pattern but occurs probabilistically.",
                "A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.",
                "The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors.",
                "However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.",
                "Another dialogue acquisition system has been developed by #TARGET_REF .",
                "However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.",
                "The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.",
                "It self activates to bias recognition toward historically observed patterns but is not otherwise observable."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #TARGET_REF . However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user. It self activates to bias recognition toward historically observed patterns but is not otherwise observable.",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "7 In ISNotes, 71% of NP antecedents occur in the same or up to two sentences prior to the anaphor.",
                "Initial experiments show that increasing the window size more than two sentences decreases the performance. 8",
                "To deal with data imbalance, the SVM light parameter is set according to the ratio between positive and negative instances in the training set. 9",
                "To compare the learning-based approach to the rulebased system described in Section 3 directly, we report the mlSystem ruleFeats We provide mlSystem ruleFeats with the same knowledge resources as the rule-based system.",
                "All rules from the rule-based system are incorporated into mlSystem ruleFeats as the features.",
                "mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( #REF ; #TARGET_REFa ; #REFb ) on bridging anaphora recognition and antecedent selection .",
                "Some of these features overlap with the atomic features used in the rule-based system."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:7 In ISNotes, 71% of NP antecedents occur in the same or up to two sentences prior to the anaphor. Initial experiments show that increasing the window size more than two sentences decreases the performance. 8 To deal with data imbalance, the SVM light parameter is set according to the ratio between positive and negative instances in the training set. 9 To compare the learning-based approach to the rulebased system described in Section 3 directly, we report the mlSystem ruleFeats We provide mlSystem ruleFeats with the same knowledge resources as the rule-based system. All rules from the rule-based system are incorporated into mlSystem ruleFeats as the features. mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( #REF ; #TARGET_REFa ; #REFb ) on bridging anaphora recognition and antecedent selection . Some of these features overlap with the atomic features used in the rule-based system.",
        "output": "{\"label\": [\"COMP\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #TARGET_REF , â¢ a `` conditioning '' facility as described by #REF , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #TARGET_REF , â¢ a `` conditioning '' facility as described by #REF , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns .",
        "output": "{\"label\": [\"MOT\"]}"
    },
    {
        "gold": {
            "text": [
                "Our motivation for generation of material for language education exists in work such as #REF and #TARGET_REF , which deal with automatic generation of classic fill in the blank questions .",
                "Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "related work:Our motivation for generation of material for language education exists in work such as #REF and #TARGET_REF , which deal with automatic generation of classic fill in the blank questions . Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences.",
        "output": "{\"label\": [\"EXT\"]}"
    },
    {
        "gold": {
            "text": [
                "With our typology of links, we aim to solve the framing problem as defined in Section 1.2.",
                "We want to model views as dynamic objects -the creation of context will be still arbitrary, but changes are very easily.",
                "We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:With our typology of links, we aim to solve the framing problem as defined in Section 1.2. We want to model views as dynamic objects -the creation of context will be still arbitrary, but changes are very easily. We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #TARGET_REF ) .",
        "output": "{\"label\": [\"FUT\"]}"
    },
    {
        "gold": {
            "text": [
                "Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.",
                "For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (#REF).",
                "Current state-of-the-art statistical parsers ( #TARGET_REF ; #REF ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and #REF ) .",
                "However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest.",
                "For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora.",
                "Because supervised training demands significant human involvement (e.g., annotating the syntactic structure of each sentence by hand), creating a new corpus is a labor-intensive and time-consuming endeavor.",
                "The goal of this work is to minimize a system's reliance on annotated training data."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data. For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (#REF). Current state-of-the-art statistical parsers ( #TARGET_REF ; #REF ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and #REF ) . However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest. For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora. Because supervised training demands significant human involvement (e.g., annotating the syntactic structure of each sentence by hand), creating a new corpus is a labor-intensive and time-consuming endeavor. The goal of this work is to minimize a system's reliance on annotated training data.",
        "output": "{\"label\": [\"BACK\"]}"
    },
    {
        "gold": {
            "text": [
                "To copy otherwise, or to republish, requires a fee and/or specific permission.",
                "0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (#REFb).",
                "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.",
                "#REF;#REF) consult relatively small lexicons, typically generated by hand.",
                "Two exceptions to this generalisation are the Linguistic String Project ( #REF ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( #REF ; #TARGET_REF ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .",
                "In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see #REF for details).",
                "However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.",
                "In this paper we provide an evaluation of the LDOCE grammar code system from this perspective."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (#REFb). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #REF;#REF) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( #REF ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( #REF ; #TARGET_REF ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see #REF for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In our previous work ( #TARGET_REF ) , we started an initial investigation on conversation entailment .",
                "We have collected a dataset of 875 instances.",
                "Each instance consists of a conversation segment and a hypothesis (as described in Section 1).",
                "The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions.",
                "We developed an approach that is motivated by previous work on textual entailment.",
                "We use clauses in the logic-based approaches as the underlying representation of our system.",
                "Based on this representation, we apply a two stage entailment process similar to #REF developed for textual entailment: an alignment stage followed by an entailment stage."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "related work:In our previous work ( #TARGET_REF ) , we started an initial investigation on conversation entailment . We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1). The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions. We developed an approach that is motivated by previous work on textual entailment. We use clauses in the logic-based approaches as the underlying representation of our system. Based on this representation, we apply a two stage entailment process similar to #REF developed for textual entailment: an alignment stage followed by an entailment stage.",
        "output": "{\"label\": [\"COMP\"]}"
    },
    {
        "gold": {
            "text": [
                "The function α(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task.",
                "Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #TARGET_REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:The function α(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task. Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #TARGET_REF ) .",
        "output": "{\"label\": [\"MOT\"]}"
    },
    {
        "gold": {
            "text": [
                "The DM features also include running tallies for the number of reprompts (num-reprompts), number of confirmation prompts (num.confirms), and number of subdialogue prompts (num-subdials), that had been played up to each point in the diMogue, as well as running percentages (percent-reprompts, percentconfirms, percent-subdials).",
                "The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #TARGET_REF ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:The DM features also include running tallies for the number of reprompts (num-reprompts), number of confirmation prompts (num.confirms), and number of subdialogue prompts (num-subdials), that had been played up to each point in the diMogue, as well as running percentages (percent-reprompts, percentconfirms, percent-subdials). The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #TARGET_REF ) .",
        "output": "{\"label\": [\"EXT\"]}"
    },
    {
        "gold": {
            "text": [
                "The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish.",
                "It consists of 1600 pairs derived from the RTE3 development and test sets (800+800).",
                "Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #TARGET_REF ) .",
                "The method relies on translation-validation cycles, defined as separate jobs routed to MTurk's workforce.",
                "Translation jobs return one Spanish version for each hypothesis.",
                "Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference.",
                "At each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job.",
                "Although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired CLTE corpus."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish. It consists of 1600 pairs derived from the RTE3 development and test sets (800+800). Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #TARGET_REF ) . The method relies on translation-validation cycles, defined as separate jobs routed to MTurk's workforce. Translation jobs return one Spanish version for each hypothesis. Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference. At each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job. Although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired CLTE corpus.",
        "output": "{\"label\": [\"FUT\"]}"
    },
    {
        "gold": {
            "text": [
                "Some generalizations of our method are fairly straightforward. For example, consider a relational description (cf., #REF) involving a gradable adjective, as in the dog in the large shed.",
                "CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #TARGET_REF , Section 8.6.2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed .",
                "Note that this is felicitous even if the shed is not the largest one in the domain, as is true for d2 in the following situation (contains-a=b means that a is contained by b):"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:Some generalizations of our method are fairly straightforward. For example, consider a relational description (cf., #REF) involving a gradable adjective, as in the dog in the large shed. CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #TARGET_REF , Section 8.6.2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed . Note that this is felicitous even if the shed is not the largest one in the domain, as is true for d2 in the following situation (contains-a=b means that a is contained by b):",
        "output": "{\"label\": [\"BACK\"]}"
    },
    {
        "gold": {
            "text": [
                "The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).",
                "The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.",
                "Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.",
                "The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1). The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( #REF ; #REF ; #TARGET_REF ) .",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "For both languages, we resolved coreference by using SWIZZLE,our implementation of a bilingual coreference resolver.",
                "SWIZZLE is a multilingual enhancement of COCKTAIL ( #TARGET_REF ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  .",
                "When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively.",
                "When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuris- tics for coreference.",
                "Our experiments show that SWIZZLEoutperformed COCKTAILon both English and Romanian test documents."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:For both languages, we resolved coreference by using SWIZZLE,our implementation of a bilingual coreference resolver. SWIZZLE is a multilingual enhancement of COCKTAIL ( #TARGET_REF ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  . When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively. When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuris- tics for coreference. Our experiments show that SWIZZLEoutperformed COCKTAILon both English and Romanian test documents.",
        "output": "{\"label\": [\"COMP\"]}"
    },
    {
        "gold": {
            "text": [
                "Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.",
                "However , the method we are currently using in the ATIS domain ( #TARGET_REF ) represents our most promising approach to this problem .",
                "We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).",
                "The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.",
                "Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to specifications based on its broad-class identity (as one of noun, noun-phrase, predicate, quantifier, etc.).",
                "For example, a [subject] is a noun-phrase node with the label \"topic.\"",
                "During the top-down cycle, it creates a blank frame and inserts it into a \"topic\" slot in the frame that was handed to it.",
                "It passes the blank frame to its children, who will then fill it appropriately, labeling it as a QSET or as a REFERENCE."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group. However , the method we are currently using in the ATIS domain ( #TARGET_REF ) represents our most promising approach to this problem . We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects). The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree. Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to specifications based on its broad-class identity (as one of noun, noun-phrase, predicate, quantifier, etc.). For example, a [subject] is a noun-phrase node with the label \"topic.\" During the top-down cycle, it creates a blank frame and inserts it into a \"topic\" slot in the frame that was handed to it. It passes the blank frame to its children, who will then fill it appropriately, labeling it as a QSET or as a REFERENCE.",
        "output": "{\"label\": [\"MOT\"]}"
    },
    {
        "gold": {
            "text": [
                "Note: In our translation from English to logic we are assuming that \"it\" is anaphoric (with the pronoun following the element that it refers to), not cataphoric (the other way around).",
                "This means that the \"it\" that brought the disease in P1 will not be considered to refer to the infection \"i\" or the death \"d\" in P3.",
                "This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #TARGET_REF , p. 329 ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:Note: In our translation from English to logic we are assuming that \"it\" is anaphoric (with the pronoun following the element that it refers to), not cataphoric (the other way around). This means that the \"it\" that brought the disease in P1 will not be considered to refer to the infection \"i\" or the death \"d\" in P3. This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #TARGET_REF , p. 329 ) .",
        "output": "{\"label\": [\"EXT\"]}"
    },
    {
        "gold": {
            "text": [
                "BilderNetle (\"little ImageNet\" in Swabian German) is our new data set of German noun-to-ImageNet synset mappings.",
                "ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #TARGET_REF ) .",
                "Multiple synsets exist for each meaning of a word.",
                "For example, Im-ageNet contains two different synsets for the word mouse: one contains images of the animal, while the other contains images of the computer peripheral.",
                "This BilderNetle data set provides mappings from German noun types to images of the nouns via ImageNet."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:BilderNetle (\"little ImageNet\" in Swabian German) is our new data set of German noun-to-ImageNet synset mappings. ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #TARGET_REF ) . Multiple synsets exist for each meaning of a word. For example, Im-ageNet contains two different synsets for the word mouse: one contains images of the animal, while the other contains images of the computer peripheral. This BilderNetle data set provides mappings from German noun types to images of the nouns via ImageNet.",
        "output": "{\"label\": [\"FUT\"]}"
    },
    {
        "gold": {
            "text": [
                "A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.",
                "Word frequency counts in internet search engines are inconsistent and unreliable ( #TARGET_REF ) .",
                "Tools based on static corpora do not suffer from this problem, e.g.",
                "BNCweb 7 , developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) are both based on the British National Corpus.",
                "Both BNCweb and View enable access to annotated corpora and facilitate searching on part-ofspeech tags.",
                "In addition, PIE 9 (Phrases in English), developed at USNA, which performs searches on n-grams (based on words, parts-ofspeech and characters), is currently restricted to the British National Corpus as well, although other static corpora are being added to its database.",
                "In contrast, little progress has been made toward annotating sizable sample corpora from the web."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies. Word frequency counts in internet search engines are inconsistent and unreliable ( #TARGET_REF ) . Tools based on static corpora do not suffer from this problem, e.g. BNCweb 7 , developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) are both based on the British National Corpus. Both BNCweb and View enable access to annotated corpora and facilitate searching on part-ofspeech tags. In addition, PIE 9 (Phrases in English), developed at USNA, which performs searches on n-grams (based on words, parts-ofspeech and characters), is currently restricted to the British National Corpus as well, although other static corpora are being added to its database. In contrast, little progress has been made toward annotating sizable sample corpora from the web.",
        "output": "{\"label\": [\"BACK\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .",
                "However, in our situation, there is not always one single winner (two methods can perform similarly well for a given request), and there are different ways to pick winners (for example, based on F-score or precision).",
                "Therefore, such an approach would require the utilization of subjective heuristics for creating labels, which would significantly influence what is being learned.",
                "Instead, we adopt an unsupervised approach that finds patterns in the data-confidence values coupled with performance scores (Section 6.1)-and then attempts to fit unseen data to these patterns (Section 6.2).",
                "Heuristics are still needed in order to decide which response-generation method to apply to an unseen case, but they are applied only after the learning is complete (Section 6.3).",
                "In other words, the subjective process of setting performance criteria (which should be conducted by the organization running the helpdesk) does not influence the machine learning process."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Following #TARGET_REF , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases . However, in our situation, there is not always one single winner (two methods can perform similarly well for a given request), and there are different ways to pick winners (for example, based on F-score or precision). Therefore, such an approach would require the utilization of subjective heuristics for creating labels, which would significantly influence what is being learned. Instead, we adopt an unsupervised approach that finds patterns in the data-confidence values coupled with performance scores (Section 6.1)-and then attempts to fit unseen data to these patterns (Section 6.2). Heuristics are still needed in order to decide which response-generation method to apply to an unseen case, but they are applied only after the learning is complete (Section 6.3). In other words, the subjective process of setting performance criteria (which should be conducted by the organization running the helpdesk) does not influence the machine learning process.",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Once the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing.",
                "The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #TARGET_REF , 1985 ) .",
                "The codes are doubly articulated; capital letters represent the grammatical relations which hold between a verb and its arguments and numbers represent subcategorisation frames which a verb can appear in.",
                "Most of the subcategorisation frames are specified by syntactic category, but some are very ill-specified; for instance, 9 is defined as \"needs a descriptive word or phrase\".",
                "In practice many adverbial and predicative complements will satisfy this code, when attached to a verb; for example, put [xg] where the code marks a locative adverbial prepositional phrase vs. make under sense 14 (hereafter written make( 14)) is coded IX9] where it marks a predicative noun phrase or prepositional phrase."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:Once the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing. The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #TARGET_REF , 1985 ) . The codes are doubly articulated; capital letters represent the grammatical relations which hold between a verb and its arguments and numbers represent subcategorisation frames which a verb can appear in. Most of the subcategorisation frames are specified by syntactic category, but some are very ill-specified; for instance, 9 is defined as \"needs a descriptive word or phrase\". In practice many adverbial and predicative complements will satisfy this code, when attached to a verb; for example, put [xg] where the code marks a locative adverbial prepositional phrase vs. make under sense 14 (hereafter written make( 14)) is coded IX9] where it marks a predicative noun phrase or prepositional phrase.",
        "output": "{\"label\": [\"COMP\"]}"
    },
    {
        "gold": {
            "text": [
                "Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.",
                "On one side, we plan to explore alternative ways to build phrase and paraphrase tables.",
                "One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #TARGET_REF ) .",
                "Another interesting direction is to investigate the potential of paraphrase patterns (i.e.",
                "patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (#REF).",
                "On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge.",
                "As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment.",
                "1343"
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE. On one side, we plan to explore alternative ways to build phrase and paraphrase tables. One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #TARGET_REF ) . Another interesting direction is to investigate the potential of paraphrase patterns (i.e. patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (#REF). On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge. As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment. 1343",
        "output": "{\"label\": [\"MOT\"]}"
    },
    {
        "gold": {
            "text": [
                "A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon.",
                "Most current HPSG analyses of Dutch, German, Italian, and French fall into that category.",
                "1 Furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run-time.",
                "Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry.",
                "This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned.",
                "As shown in #TARGET_REF this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon. Most current HPSG analyses of Dutch, German, Italian, and French fall into that category. 1 Furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run-time. Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry. This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned. As shown in #TARGET_REF this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .",
        "output": "{\"label\": [\"EXT\"]}"
    },
    {
        "gold": {
            "text": [
                "The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).",
                "The system is implemented based on (#REF) and ).",
                "In the system , we extract both the minimal GHKM rules ( #TARGET_REF ) , and the rules of SPMT Model 1 ( #REF ) with phrases up to length L = 5 on the source side ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (#REF) and ). In the system , we extract both the minimal GHKM rules ( #TARGET_REF ) , and the rules of SPMT Model 1 ( #REF ) with phrases up to length L = 5 on the source side .",
        "output": "{\"label\": [\"FUT\"]}"
    },
    {
        "gold": {
            "text": [
                "The main source of Novelle are wikis and blogs.",
                "While wikis have spread from a detailed design ( #TARGET_REF ) , unfortunately blogs have not been designed under a model .",
                "So we have tested and compared the most used tools available for blogging: Bloggers, WordPress, MovableType and LiveJournal."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:The main source of Novelle are wikis and blogs. While wikis have spread from a detailed design ( #TARGET_REF ) , unfortunately blogs have not been designed under a model . So we have tested and compared the most used tools available for blogging: Bloggers, WordPress, MovableType and LiveJournal.",
        "output": "{\"label\": [\"BACK\"]}"
    },
    {
        "gold": {
            "text": [
                "There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.",
                "This includes work on generalized expectation ( #REF ) , posterior regularization ( #REF ) and constraint driven learning ( #TARGET_REF ; #REF ) .",
                "The work of #REF on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.",
                "In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.",
                "For our setting this would mean using weak application specific signals to improve dependency parsing.",
                "Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation ( #REF ) , posterior regularization ( #REF ) and constraint driven learning ( #TARGET_REF ; #REF ) . The work of #REF on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics. For our setting this would mean using weak application specific signals to improve dependency parsing. Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training.",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations.",
                "Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #TARGET_REF ) .",
                "However, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans.",
                "In addition, because that work was based on features synthesized over the entire dialogues, the hypotheses that were learned could not be used for prediction during runtime."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "conclusion:The research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations. Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #TARGET_REF ) . However, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans. In addition, because that work was based on features synthesized over the entire dialogues, the hypotheses that were learned could not be used for prediction during runtime.",
        "output": "{\"label\": [\"COMP\"]}"
    },
    {
        "gold": {
            "text": [
                "• Site based corpus annotation -in which the user can specify a web site to annotate • Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate • Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger).",
                "Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #TARGET_REFb ) and shingling techniques described by #REF ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "method:• Site based corpus annotation -in which the user can specify a web site to annotate • Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate • Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger). Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #TARGET_REFb ) and shingling techniques described by #REF .",
        "output": "{\"label\": [\"MOT\"]}"
    },
    {
        "gold": {
            "text": [
                "A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.",
                "For instance , #TARGET_REF , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''",
                "The pairing of these two sentences may be said to create a small paragraph.",
                "Our point is that an acceptable structure can be assigned to the utterance \"Reagan thinks bananas\" only within the paragraph in which this utterance occurs.",
                "We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task.",
                "That is, we can ask in the first sentence of a paragraph about Kissinger's favorite fruit, elaborate the question and the circumstances in the next few sentences, and give the above answer at the end.",
                "We do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism (although it may be); rather, it has the grammatical role of providing functional structures that can be assigned to strings."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings. For instance , #TARGET_REF , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? '' The pairing of these two sentences may be said to create a small paragraph. Our point is that an acceptable structure can be assigned to the utterance \"Reagan thinks bananas\" only within the paragraph in which this utterance occurs. We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task. That is, we can ask in the first sentence of a paragraph about Kissinger's favorite fruit, elaborate the question and the circumstances in the next few sentences, and give the above answer at the end. We do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism (although it may be); rather, it has the grammatical role of providing functional structures that can be assigned to strings.",
        "output": "{\"label\": [\"EXT\"]}"
    },
    {
        "gold": {
            "text": [
                "This section describes our joint coreference resolution and mention head detection framework.",
                "Our work is inspired by the latent left-linking model in #TARGET_REF and the ILP formulation from #REF .",
                "The joint learning and inference model takes as input mention head candidates (Sec.",
                "3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.",
                "This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier.",
                "The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones.",
                "By learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from #REF.",
                "This joint framework aims to improve performance on both mention head detection and on coreference."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:This section describes our joint coreference resolution and mention head detection framework. Our work is inspired by the latent left-linking model in #TARGET_REF and the ILP formulation from #REF . The joint learning and inference model takes as input mention head candidates (Sec. 3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions. This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier. The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones. By learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from #REF. This joint framework aims to improve performance on both mention head detection and on coreference.",
        "output": "{\"label\": [\"FUT\"]}"
    },
    {
        "gold": {
            "text": [
                "Given that we were targeting all envisaged access routes from LDOCE to systems implemented in Lisp, and since the natural data structure for Lisp is the s-expression, we adopted the approach of converting the tape source into a set of list structures, one per entry.",
                "Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English ( #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Given that we were targeting all envisaged access routes from LDOCE to systems implemented in Lisp, and since the natural data structure for Lisp is the s-expression, we adopted the approach of converting the tape source into a set of list structures, one per entry. Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English ( #TARGET_REF ) .",
        "output": "{\"label\": [\"BACK\"]}"
    },
    {
        "gold": {
            "text": [
                "The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #REF ) and case-based reasoning ( #TARGET_REF ) .",
                "Such technologies require significant human input, and are difficult to create and maintain (#REF).",
                "In contrast, the techniques examined in this article are corpus-based and data-driven.",
                "The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #REF ) and case-based reasoning ( #TARGET_REF ) . Such technologies require significant human input, and are difficult to create and maintain (#REF). In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus.",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper we present a linguistically motivated framework for uniform lexicostructural processing.",
                "It has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation (NLG) and for transfer in machine translation (MT).",
                "Our work extends directions taken in systems such as Ariane ( #REF ) , FoG ( #REF ) , JOYCE ( Rambow and #TARGET_REF ) , and LFS ( #REF ) .",
                "Although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics-based approaches to MT."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:In this paper we present a linguistically motivated framework for uniform lexicostructural processing. It has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation (NLG) and for transfer in machine translation (MT). Our work extends directions taken in systems such as Ariane ( #REF ) , FoG ( #REF ) , JOYCE ( Rambow and #TARGET_REF ) , and LFS ( #REF ) . Although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics-based approaches to MT.",
        "output": "{\"label\": [\"COMP\"]}"
    },
    {
        "gold": {
            "text": [
                "Even better accuracy can be achieved with a more fine-grained link class structure.",
                "Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .",
                "Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #TARGET_REF ) .",
                "If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.",
                "In this manner, the model can account for a wider range of translation phenomena."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy . Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #TARGET_REF ) . If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena.",
        "output": "{\"label\": [\"MOT\"]}"
    },
    {
        "gold": {
            "text": [
                "Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) .",
                "Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) . Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.",
        "output": "{\"label\": [\"EXT\"]}"
    },
    {
        "gold": {
            "text": [
                "The shallow parser used is the SNoW-based CSCL parser (#REF;#REF).",
                "SNoW ( #TARGET_REF ; #REF ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .",
                "It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.",
                "Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.",
                "However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.",
                "Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).",
                "The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The shallow parser used is the SNoW-based CSCL parser (#REF;#REF). SNoW ( #TARGET_REF ; #REF ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example . It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes. However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference. Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.). The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.",
        "output": "{\"label\": [\"FUT\"]}"
    },
    {
        "gold": {
            "text": [
                "All EBMT systems, from the initial proposal by #REF to the recent collection of #REF, are premised on the availability of subsentential alignments derived from the input bitext.",
                "There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3",
                " Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.",
                "#TARGET_REF attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora .",
                "#REF replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.",
                "#REF use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.",
                "The respective lengths of the putative alignments in terms of characters is also an important factor.",
                "Ahrenberg, Andersson, and #REF observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:All EBMT systems, from the initial proposal by #REF to the recent collection of #REF, are premised on the availability of subsentential alignments derived from the input bitext. There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3  Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. #TARGET_REF attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . #REF replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. #REF use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and #REF observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.",
        "output": "{\"label\": [\"BACK\"]}"
    },
    {
        "gold": {
            "text": [
                "A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.",
                "However, these methodologies have not used historical information at the dialogue level as described here.",
                "In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.",
                "The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.",
                "Thus, an error in this work has no pattern but occurs probabilistically.",
                "A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.",
                "The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #TARGET_REF where program flowcharts were constructed from traces of their behaviors .",
                "However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.",
                "Another dialogue acquisition system has been developed by #REF.",
                "However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.",
                "The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #TARGET_REF where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #REF. However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In selecting features for Korean, we have to ac- count for relatively free word order (#REF).",
                "We follow our previous work ( #TARGET_REF ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also #REF ) .",
                "Each word is broken down into: stem, affixes, stem POS, and affixes POS.",
                "We also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties.",
                "Although these are relatively shallow features, they provide enough lexical and grammatical context to help select better or worse training data (section 3) and to provide a basis for a preliminary system (section 4)."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:In selecting features for Korean, we have to ac- count for relatively free word order (#REF). We follow our previous work ( #TARGET_REF ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also #REF ) . Each word is broken down into: stem, affixes, stem POS, and affixes POS. We also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties. Although these are relatively shallow features, they provide enough lexical and grammatical context to help select better or worse training data (section 3) and to provide a basis for a preliminary system (section 4).",
        "output": "{\"label\": [\"COMP\"]}"
    },
    {
        "gold": {
            "text": [
                "The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #TARGET_REFa ) .",
                "A fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge-poor methods are.",
                "In particular, more research should be carried out on the factors influencing the performance of these algorithms.",
                "One of the impediments to the evaluation or fuller utilization of machine learning techniques is the lack of widely available corpora annotated for anaphoric or coreferential links.",
                "More work toward the proposal of consistent and comprehensive evaluation is necessary; so too is work in multilingual contexts.",
                "Some of these challenges have been addressed in the papers published in this issue, but ongoing research will continue to address them in the near future."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #TARGET_REFa ) . A fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge-poor methods are. In particular, more research should be carried out on the factors influencing the performance of these algorithms. One of the impediments to the evaluation or fuller utilization of machine learning techniques is the lack of widely available corpora annotated for anaphoric or coreferential links. More work toward the proposal of consistent and comprehensive evaluation is necessary; so too is work in multilingual contexts. Some of these challenges have been addressed in the papers published in this issue, but ongoing research will continue to address them in the near future.",
        "output": "{\"label\": [\"MOT\"]}"
    },
    {
        "gold": {
            "text": [
                "is the probability of producing the target tree fragment frag.",
                "To generate frag,  used a geometric prior to decide how many child nodes to assign each node.",
                "Differently, we require that each multi-word non-terminal node must have two child nodes.",
                "This is because the binary structure has been verified to be very effective for tree-based translation ( #TARGET_REF ; #REFa ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "method:is the probability of producing the target tree fragment frag. To generate frag,  used a geometric prior to decide how many child nodes to assign each node. Differently, we require that each multi-word non-terminal node must have two child nodes. This is because the binary structure has been verified to be very effective for tree-based translation ( #TARGET_REF ; #REFa ) .",
        "output": "{\"label\": [\"EXT\"]}"
    },
    {
        "gold": {
            "text": [
                "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #TARGET_REF , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #TARGET_REF , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .",
        "output": "{\"label\": [\"FUT\"]}"
    },
    {
        "gold": {
            "text": [
                "As mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms.",
                "In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #TARGET_REF cases ) .",
                "#REF cases).",
                "Due to lack of space we will only brief on some of these non-standard cases without going into the details."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:As mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms. In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #TARGET_REF cases ) . #REF cases). Due to lack of space we will only brief on some of these non-standard cases without going into the details.",
        "output": "{\"label\": [\"BACK\"]}"
    },
    {
        "gold": {
            "text": [
                "Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.",
                "An alternative representation based on #REF is presented in #REF, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.",
                "Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #TARGET_REF use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience. An alternative representation based on #REF is presented in #REF, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree. Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #TARGET_REF use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing .",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In P2, on the other hand, we recast SC as a se- quence labeling task.",
                "In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance la- bel for each post in the input post sequence.",
                "This choice is motivated by an observation we made previously ( #TARGET_REFa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3"
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:In P2, on the other hand, we recast SC as a se- quence labeling task. In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance la- bel for each post in the input post sequence. This choice is motivated by an observation we made previously ( #TARGET_REFa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3",
        "output": "{\"label\": [\"COMP\"]}"
    },
    {
        "gold": {
            "text": [
                "We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.",
                "Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #REF ; #TARGET_REF ) for self training .",
                "Self-training should also benefit other discriminatively trained parsers with latent annotations (#REF), although training would be much slower compared to using generative models, as in our case."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #REF ; #TARGET_REF ) for self training . Self-training should also benefit other discriminatively trained parsers with latent annotations (#REF), although training would be much slower compared to using generative models, as in our case.",
        "output": "{\"label\": [\"MOT\"]}"
    },
    {
        "gold": {
            "text": [
                "We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.",
                "( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #TARGET_REFa ) ) .",
                "Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP.",
                "To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type. ( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #TARGET_REFa ) ) . Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP. To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.",
        "output": "{\"label\": [\"EXT\"]}"
    },
    {
        "gold": {
            "text": [
                "The shallow parser used is the SNoW-based CSCL parser (#REF;#REF).",
                "SNoW ( #REF ; #TARGET_REF ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .",
                "It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.",
                "Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.",
                "However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.",
                "Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).",
                "The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The shallow parser used is the SNoW-based CSCL parser (#REF;#REF). SNoW ( #REF ; #TARGET_REF ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example . It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes. However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference. Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.). The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.",
        "output": "{\"label\": [\"FUT\"]}"
    },
    {
        "gold": {
            "text": [
                "A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.",
                "#REF considers the second verb V2 as an aspectual complex comparable to the auxiliaries.",
                "#TARGET_REF argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .",
                "#REF tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\".",
                "Similar findings have been proposed by #REF that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.",
                "#REF tried to represent Bangla CVs in terms of HPSG formalism.",
                "She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.",
                "Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. #REF considers the second verb V2 as an aspectual complex comparable to the auxiliaries. #TARGET_REF argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure . #REF tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by #REF that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. #REF tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible. Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.",
        "output": "{\"label\": [\"BACK\"]}"
    },
    {
        "gold": {
            "text": [
                "Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing \"there exists\" by \"all\" in the above definitions (cf. #TARGET_REFb ).",
                "We will have, however, no need for \"strong\" notions in this paper.",
                "Also, in a practical system, \"satisfies\" should be probably replaced by \"violates fewest.\""
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing \"there exists\" by \"all\" in the above definitions (cf. #TARGET_REFb ). We will have, however, no need for \"strong\" notions in this paper. Also, in a practical system, \"satisfies\" should be probably replaced by \"violates fewest.\"",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The process study most similar to ours is that of Koehn (2009a), who compared scratch, post-edit, and simple interactive modes.",
                "However, he used undergraduate, non-professional subjects, and did not consider re-tuning.",
                "Our experimental design with professional bilingual translators follows our previous work #TARGET_REFa ) comparing scratch translation to post-edit .",
                "Many research translation UIs have been proposed including TransType (#REF), Caitra (#REFb), Thot (Ortiz-Martínez and #REF), TransCenter (#REFb), and CasmaCat (#REF).",
                "However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "related work:The process study most similar to ours is that of Koehn (2009a), who compared scratch, post-edit, and simple interactive modes. However, he used undergraduate, non-professional subjects, and did not consider re-tuning. Our experimental design with professional bilingual translators follows our previous work #TARGET_REFa ) comparing scratch translation to post-edit . Many research translation UIs have been proposed including TransType (#REF), Caitra (#REFb), Thot (Ortiz-Martínez and #REF), TransCenter (#REFb), and CasmaCat (#REF). However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature.",
        "output": "{\"label\": [\"COMP\"]}"
    },
    {
        "gold": {
            "text": [
                "Whereas previous finite-state-model-based toolkits place many severe restrictions on domain descriptions, WIT has enough descriptive power to build a variety of dialogue systems.",
                "Although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information.",
                "For example, it is possible to represent a discourse stack whose depth is limited.",
                "Recording some dialogue history is also possible.",
                "Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.",
                "For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #TARGET_REF ; #REF ) .",
                "The language generation module features Common Lisp functions, so there is no limitation on the description.",
                "Some of the systems we have developed feature a generation method based on hierarchical planning (#REF).",
                "It is also possible to build a simple finite-state-model-based dialogue system using WIT.",
                "States can be represented by dialogue phases in WIT."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Whereas previous finite-state-model-based toolkits place many severe restrictions on domain descriptions, WIT has enough descriptive power to build a variety of dialogue systems. Although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information. For example, it is possible to represent a discourse stack whose depth is limited. Recording some dialogue history is also possible. Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #TARGET_REF ; #REF ) . The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (#REF). It is also possible to build a simple finite-state-model-based dialogue system using WIT. States can be represented by dialogue phases in WIT.",
        "output": "{\"label\": [\"MOT\"]}"
    },
    {
        "gold": {
            "text": [
                "Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.",
                "As noted above , it is well documented ( #TARGET_REF ) that subcategorization frames ( and their frequencies ) vary across domains .",
                "We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ.",
                "For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.",
                "It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases.",
                "This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.",
                "Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data. As noted above , it is well documented ( #TARGET_REF ) that subcategorization frames ( and their frequencies ) vary across domains . We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ. For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon. It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases. This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.",
        "output": "{\"label\": [\"EXT\"]}"
    },
    {
        "gold": {
            "text": [
                "The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.",
                "To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (#REF).18",
                "This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender).",
                "We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #TARGET_REF ) .19",
                "The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input).",
                "The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values. To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (#REF).18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #TARGET_REF ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts.",
        "output": "{\"label\": [\"FUT\"]}"
    },
    {
        "gold": {
            "text": [
                "Surveys and articles on the topic include #REF , de #REF , and #TARGET_REF .",
                "Still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self-contained."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Surveys and articles on the topic include #REF , de #REF , and #TARGET_REF . Still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self-contained.",
        "output": "{\"label\": [\"BACK\"]}"
    },
    {
        "gold": {
            "text": [
                "Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #REF ; #TARGET_REF ; #REF ) .",
                "#REF;#REF;#REF).",
                "But we won't pursue this topic further here."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #REF ; #TARGET_REF ; #REF ) . #REF;#REF;#REF). But we won't pursue this topic further here.",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The reordering models we describe follow our previous work using function word models for translation ( #TARGET_REF ; #REF ) .",
                "The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.",
                "To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.",
                "This section provides a high level overview of our reordering model, which attempts to leverage this information."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:The reordering models we describe follow our previous work using function word models for translation ( #TARGET_REF ; #REF ) . The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them. To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases. This section provides a high level overview of our reordering model, which attempts to leverage this information.",
        "output": "{\"label\": [\"COMP\"]}"
    },
    {
        "gold": {
            "text": [
                "We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.",
                "Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (#REF;#REF) for self training.",
                "Self-training should also benefit other discriminatively trained parsers with latent annotations ( #TARGET_REF ) , although training would be much slower compared to using generative models , as in our case ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (#REF;#REF) for self training. Self-training should also benefit other discriminatively trained parsers with latent annotations ( #TARGET_REF ) , although training would be much slower compared to using generative models , as in our case .",
        "output": "{\"label\": [\"MOT\"]}"
    },
    {
        "gold": {
            "text": [
                "The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages.",
                "Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment.",
                "Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF).",
                "But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #TARGET_REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #TARGET_REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) .",
        "output": "{\"label\": [\"EXT\"]}"
    },
    {
        "gold": {
            "text": [
                "In a final processing stage , we generalize over the marker lexicon following a process found in #TARGET_REF .",
                "In Block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool.",
                "In a subsequent stage, chunk pairs are extracted, which are then generalized to produce a set of translation templates for each source, target segment."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:In a final processing stage , we generalize over the marker lexicon following a process found in #TARGET_REF . In Block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool. In a subsequent stage, chunk pairs are extracted, which are then generalized to produce a set of translation templates for each source, target segment.",
        "output": "{\"label\": [\"FUT\"]}"
    },
    {
        "gold": {
            "text": [
                "Semantic construction proceeds from the derived tree ( #TARGET_REF ) rather than -- as is more common in TAG -- from the derivation tree .",
                "This is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.",
                "The association between tree nodes and unification variables encodes the syntax/semantics interface -it specifies which node in the tree provides the value for which variable in the final semantic representation."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Semantic construction proceeds from the derived tree ( #TARGET_REF ) rather than -- as is more common in TAG -- from the derivation tree . This is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation. The association between tree nodes and unification variables encodes the syntax/semantics interface -it specifies which node in the tree provides the value for which variable in the final semantic representation.",
        "output": "{\"label\": [\"BACK\"]}"
    },
    {
        "gold": {
            "text": [
                "written as fully specified relations between words, rather, only what is supposed to be changed is specified.",
                "Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #TARGET_REF , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .",
                "This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 ° The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.",
                "The index that the subject bore in the input is assigned to an optional prepositional complement in the output."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:written as fully specified relations between words, rather, only what is supposed to be changed is specified. Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #TARGET_REF , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch . This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 ° The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output. The index that the subject bore in the input is assigned to an optional prepositional complement in the output.",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "1. Char-n-grams (G): We start with a character n-gram-based approach (#REF), which is most common and followed by many language identification researchers.",
                "Following the work of #REF, we select character n-grams (n=1 to 5) and the word as the features in our experiments.",
                "2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments.",
                "raw length value as a feature , we follow our previous work ( #REF ; #TARGET_REF ) and create multiple features for length using a decision tree ( J48 ) .",
                "We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.",
                "4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:1. Char-n-grams (G): We start with a character n-gram-based approach (#REF), which is most common and followed by many language identification researchers. Following the work of #REF, we select character n-grams (n=1 to 5) and the word as the features in our experiments. 2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. raw length value as a feature , we follow our previous work ( #REF ; #TARGET_REF ) and create multiple features for length using a decision tree ( J48 ) . We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.",
        "output": "{\"label\": [\"COMP\"]}"
    },
    {
        "gold": {
            "text": [
                "For shuffling paraphrases, french alternations are partially described in (#REF) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.",
                "For complementing this database and for converse constructions, the LADL tables (#REF) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.",
                "In particular , ( #TARGET_REF ) lists the converses of some 3 500 predicative nouns ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:For shuffling paraphrases, french alternations are partially described in (#REF) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs. For complementing this database and for converse constructions, the LADL tables (#REF) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions. In particular , ( #TARGET_REF ) lists the converses of some 3 500 predicative nouns .",
        "output": "{\"label\": [\"MOT\"]}"
    },
    {
        "gold": {
            "text": [
                "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.",
                "They proved to be useful in a number of NLP applications such as natural language generation ( #TARGET_REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) .",
                "One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF).",
                "With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.",
                "After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #TARGET_REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases.",
        "output": "{\"label\": [\"EXT\"]}"
    },
    {
        "gold": {
            "text": [
                "For our experiments we used the standard division of the WSJ ( #TARGET_REF ) , with sections 2 through 21 for training ( approx .",
                "40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.",
                "As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks.",
                "Without loss of generality, all trees were converted to binary branching (and were reconverted to n-ary trees after parsing).",
                "We employed the same unknown (category) word model as in #REF, based on statistics on word-endings, hyphenation and capitalization in combination with Good-Turing (#REF: 85 -87).",
                "We used \"evalb\" 4 to compute the standard PARSEVAL scores for our results (Manning & #REF).",
                "We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:For our experiments we used the standard division of the WSJ ( #TARGET_REF ) , with sections 2 through 21 for training ( approx . 40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set. As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks. Without loss of generality, all trees were converted to binary branching (and were reconverted to n-ary trees after parsing). We employed the same unknown (category) word model as in #REF, based on statistics on word-endings, hyphenation and capitalization in combination with Good-Turing (#REF: 85 -87). We used \"evalb\" 4 to compute the standard PARSEVAL scores for our results (Manning & #REF). We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems.",
        "output": "{\"label\": [\"FUT\"]}"
    },
    {
        "gold": {
            "text": [
                "from Section 23 of the Penn Treebank as described in Cahill, McCarthy, et al. (2004).",
                "For the full set of annotations they achieve precision of over 96.5% and recall of over 96.6%.",
                "There is, however, a risk of overfitting when evaluation is limited to a gold standard of this size.",
                "More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank ( #TARGET_REF ) , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .",
                "They report precision of over 88.5% and recall of over 86% (Table 2).",
                "The PARC 700 Dependency Bank differs substantially from both the DCU 105 f-structure bank and the automatically generated f-structures in regard to the style of linguistic analysis, feature nomenclature, and feature geometry.",
                "Some, but not all, of these differences are captured by automatic conversion software."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "method:from Section 23 of the Penn Treebank as described in Cahill, McCarthy, et al. (2004). For the full set of annotations they achieve precision of over 96.5% and recall of over 96.6%. There is, however, a risk of overfitting when evaluation is limited to a gold standard of this size. More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank ( #TARGET_REF ) , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators . They report precision of over 88.5% and recall of over 86% (Table 2). The PARC 700 Dependency Bank differs substantially from both the DCU 105 f-structure bank and the automatically generated f-structures in regard to the style of linguistic analysis, feature nomenclature, and feature geometry. Some, but not all, of these differences are captured by automatic conversion software.",
        "output": "{\"label\": [\"BACK\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ Only an automatic evaluation was performed , which relied on having model responses ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:â¢ Only an automatic evaluation was performed , which relied on having model responses ( #REF ; #TARGET_REF ) .",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The tutorial policy makes a high-level decision as to which strategy to use (for example, \"acknowledge the correct part and give a high specificity hint\") based on the answer analysis and dialogue context.",
                "At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy .",
                "Since the system accepts unrestricted input, interpretation errors are unavoidable.",
                "Our recovery policy is modeled on the TargetedHelp ( #TARGET_REF ) policy used in task-oriented dialogue .",
                "If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I'm sorry, I'm having a problem understanding.",
                "I don't know the word power.\"",
                "The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:The tutorial policy makes a high-level decision as to which strategy to use (for example, \"acknowledge the correct part and give a high specificity hint\") based on the answer analysis and dialogue context. At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp ( #TARGET_REF ) policy used in task-oriented dialogue . If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I'm sorry, I'm having a problem understanding. I don't know the word power.\" The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.",
        "output": "{\"label\": [\"COMP\"]}"
    },
    {
        "gold": {
            "text": [
                "Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.",
                "This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.",
                "Future research should apply the work of #TARGET_REF and #REF , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations. This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension. Future research should apply the work of #TARGET_REF and #REF , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .",
        "output": "{\"label\": [\"MOT\"]}"
    }
]