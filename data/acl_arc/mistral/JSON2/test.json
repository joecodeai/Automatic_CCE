[
    {
        "gold": {
            "text": [
                "A number of alignment techniques have been proposed , varying from statistical methods ( #REF ; #TARGET_REF ) to lexical methods ( #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A number of alignment techniques have been proposed , varying from statistical methods ( #REF ; #TARGET_REF ) to lexical methods ( #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Only a few such corpora exist , including the Hansard English-French corpus and the HKUST EnglishChinese corpus ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Only a few such corpora exist , including the Hansard English-French corpus and the HKUST EnglishChinese corpus ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A number of alignment techniques have been proposed , varying from statistical methods ( #REF ; #REF ) to lexical methods ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A number of alignment techniques have been proposed , varying from statistical methods ( #REF ; #REF ) to lexical methods ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "A number of alignment techniques have been proposed , varying from statistical methods ( #TARGET_REF ; #REF ) to lexical methods ( #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A number of alignment techniques have been proposed , varying from statistical methods ( #TARGET_REF ; #REF ) to lexical methods ( #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Robust natural language understanding in Atlas-Andes is provided by RosÃ© 's CARMEL system ( RosÃ© 2000 ) ; it uses the spelling correction algorithm devised by #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Robust natural language understanding in Atlas-Andes is provided by RosÃ© 's CARMEL system ( RosÃ© 2000 ) ; it uses the spelling correction algorithm devised by #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "2 See ( #TARGET_REF ) for how MIMIC 's dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 2 See ( #TARGET_REF ) for how MIMIC 's dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( #REF ; #TARGET_REF ) ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( #REF ; #TARGET_REF ) ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A companion paper describes the evaluation process and results in further detail ( #TARGET_REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: A companion paper describes the evaluation process and results in further detail ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #TARGET_REF ) , which we leave for future work ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: 5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #TARGET_REF ) , which we leave for future work .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Furthermore , a number of performance features , largely based on the PARADISE dialogue evaluation scheme ( #TARGET_REF ) , were automatically logged , derived , or manually annotated ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Furthermore , a number of performance features , largely based on the PARADISE dialogue evaluation scheme ( #TARGET_REF ) , were automatically logged , derived , or manually annotated .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The goal of the JAVOX toolkit is to speech-enable traditional desktop applications -- this is similar to the goals of the MELISSA project ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The goal of the JAVOX toolkit is to speech-enable traditional desktop applications -- this is similar to the goals of the MELISSA project ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "It is also possible to focus on non-compositional compounds , a key point in bilingual applications ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It is also possible to focus on non-compositional compounds , a key point in bilingual applications ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This method allows the efficient retrieval of arbitrary length n-grams ( #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This method allows the efficient retrieval of arbitrary length n-grams ( #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints ( #REF ; #TARGET_REF ; hua #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints ( #REF ; #TARGET_REF ; hua #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The speech and language processing architecture is based on that of the SRI CommandTalk system ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The speech and language processing architecture is based on that of the SRI CommandTalk system ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "CornmandTalk ( #TARGET_REF ) , Circuit Fix-It Shop ( #REF ) and TRAINS-96 ( #REF ; #REF ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: CornmandTalk ( #TARGET_REF ) , Circuit Fix-It Shop ( #REF ) and TRAINS-96 ( #REF ; #REF ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We do this with a first-order HMM part-ofspeech tagger ( Merialdo #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We do this with a first-order HMM part-ofspeech tagger ( Merialdo #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This alignment is done on the basis of both length ( Gale and Church #TARGET_REF ) and a notion of cognateness ( #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: This alignment is done on the basis of both length ( Gale and Church #TARGET_REF ) and a notion of cognateness ( #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ Before indexing the text , we process it with Textract ( #REF ; #TARGET_REF ) , which performs lemmatization , and discovers proper names and technical terms ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: â¢ Before indexing the text , we process it with Textract ( #REF ; #TARGET_REF ) , which performs lemmatization , and discovers proper names and technical terms .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "100000 word stems of German ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: 100000 word stems of German ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Linguistic preprocessing of text documents is carried out by re-using smes , an information extraction core system for real-world German text processing ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Linguistic preprocessing of text documents is carried out by re-using smes , an information extraction core system for real-world German text processing ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our implementation of the NP-based QA system uses the Empire noun phrase finder , which is described in detail in #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Our implementation of the NP-based QA system uses the Empire noun phrase finder , which is described in detail in #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #TARGET_REF ) , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task ( #REF ) :"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #TARGET_REF ) , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task ( #REF ) :\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "According to current tagger comparisons ( #REF ; #REF ) , and according to a comparsion of the results presented here with those in ( #TARGET_REF ) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: According to current tagger comparisons ( #REF ; #REF ) , and according to a comparsion of the results presented here with those in ( #TARGET_REF ) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This confirms that although Kozima 's approach ( #TARGET_REF ) is computationally expensive , it does produce more precise segmentation ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This confirms that although Kozima 's approach ( #TARGET_REF ) is computationally expensive , it does produce more precise segmentation .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "R98 ( , , , , â ) uses a variant of Kozima 's semantic similarity measure ( #TARGET_REF ) to compute block similarity ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: R98 ( , , , , â ) uses a variant of Kozima 's semantic similarity measure ( #TARGET_REF ) to compute block similarity .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #TARGET_REF , who report large speed-ups from the elimination of disjunction processing during unification ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #TARGET_REF , who report large speed-ups from the elimination of disjunction processing during unification .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( #TARGET_REF ) and ( #REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( #TARGET_REF ) and ( #REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( #REF ) and ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( #REF ) and ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Accuracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Accuracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Although this is only true in cases where y occurs in an upward monotone context ( #TARGET_REF ) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Although this is only true in cases where y occurs in an upward monotone context ( #TARGET_REF ) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "We parse each sentence with the Collins parser ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We parse each sentence with the Collins parser ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "It is based on the dataset of #TARGET_REF ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0 -- F9 ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: It is based on the dataset of #TARGET_REF ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0 -- F9 ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "We collect substring rationales for a sentiment classification task ( #TARGET_REF ) and use them to obtain significant accuracy improvements for each annotator ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We collect substring rationales for a sentiment classification task ( #TARGET_REF ) and use them to obtain significant accuracy improvements for each annotator .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the same set of binary features as in previous work on this dataset ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We use the same set of binary features as in previous work on this dataset ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We gather similar words using #TARGET_REFa ) , mining similar verbs from a comparable-sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use #REF 's approach to obtaining web-counts ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We gather similar words using #TARGET_REFa ) , mining similar verbs from a comparable-sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use #REF 's approach to obtaining web-counts .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We also made use of the person-name/instance pairs automatically extracted by #TARGET_REF .2This data provides counts for pairs such as \"Edwin Moses , hurdler\" and \"William Farley , industrialist."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We also made use of the person-name/instance pairs automatically extracted by #TARGET_REF .2This data provides counts for pairs such as \"Edwin Moses , hurdler\" and \"William Farley , industrialist.\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We study the cases where a 9Recall that even the #TARGET_REF system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We study the cases where a 9Recall that even the #TARGET_REF system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REFa ) 's similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ) , because these have similar subjects to eat ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REFa ) 's similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ) , because these have similar subjects to eat .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Also , the #TARGET_REF approach will be undefined if the pair is unobserved on the web ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Also , the #TARGET_REF approach will be undefined if the pair is unobserved on the web .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The advantage of tuning similarity to the application of interest has been shown previously by #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The advantage of tuning similarity to the application of interest has been shown previously by #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "MI was also recently used for inference-rule SPs by #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: MI was also recently used for inference-rule SPs by #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Usually , the classes are from WordNet ( #REF ) , although they can also be inferred from clustering ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Usually , the classes are from WordNet ( #REF ) , although they can also be inferred from clustering ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and #TARGET_REFa ) 's information-theoretic metric work best ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and #TARGET_REFa ) 's information-theoretic metric work best .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We parsed the 3 GB AQUAINT corpus ( #REF ) using Minipar ( #TARGET_REFb ) , and collected verb-object and verb-subject frequencies , building an empirical MI model from this data ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We parsed the 3 GB AQUAINT corpus ( #REF ) using Minipar ( #TARGET_REFb ) , and collected verb-object and verb-subject frequencies , building an empirical MI model from this data .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "We measure this association using pointwise Mutual Information ( MI ) ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We measure this association using pointwise Mutual Information ( MI ) ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #TARGET_REF 's conditional probability scores for pseudodisambiguation of ( v , n , n â² ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #TARGET_REF 's conditional probability scores for pseudodisambiguation of ( v , n , n â² ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "HOLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: HOLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "While many approaches have addressed this problem , our work is most closely related to that of ( #REF ; #TARGET_REF ; #REF ; #REF ) , which convert the inputs into logical forms and then attempt to ` prove ' H from T plus a set of axioms ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: While many approaches have addressed this problem , our work is most closely related to that of ( #REF ; #TARGET_REF ; #REF ; #REF ) , which convert the inputs into logical forms and then attempt to ` prove ' H from T plus a set of axioms .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The studies presented by #TARGET_REF and #REF differed in the number of states that they used ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The studies presented by #TARGET_REF and #REF differed in the number of states that they used .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The resulting training procedure is analogous to the one presented in ( #REF ) and ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The resulting training procedure is analogous to the one presented in ( #REF ) and ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Their kernel is also very time consuming and in their more general sparse setting it requires O ( mn3 ) time and O ( mn2 ) space , where m and n are the number of nodes of the two trees ( m > = n ) ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Their kernel is also very time consuming and in their more general sparse setting it requires O ( mn3 ) time and O ( mn2 ) space , where m and n are the number of nodes of the two trees ( m > = n ) ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Secondly , as ( #TARGET_REF ) show , marginalizing out the different segmentations during decoding leads to improved performance ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Secondly , as ( #TARGET_REF ) show , marginalizing out the different segmentations during decoding leads to improved performance .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "More recently , ( #REF ) has performed a good survey of document categorization ; recent works can also be found in ( #REF ) , ( #TARGET_REF ) , and ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: More recently , ( #REF ) has performed a good survey of document categorization ; recent works can also be found in ( #REF ) , ( #TARGET_REF ) , and ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Based on this advise ( Moore and #TARGET_REF ) exclude the latent segmentation variables and opt for a heuristic training procedure ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Based on this advise ( Moore and #TARGET_REF ) exclude the latent segmentation variables and opt for a heuristic training procedure .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Latent variables we wish to consider are an increased number of word classes ; more flexible regions -- see #TARGET_REF on learning a state transition diagram for acoustic regions in phone recognition -- and phonological features and syllable boundaries ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Latent variables we wish to consider are an increased number of word classes ; more flexible regions -- see #TARGET_REF on learning a state transition diagram for acoustic regions in phone recognition -- and phonological features and syllable boundaries .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We would like to use features that look at wide context on the input side , which is inexpensive ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: We would like to use features that look at wide context on the input side , which is inexpensive ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #TARGET_REF ; #REF ; #REF ) , to ranking models for Web search applications ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #TARGET_REF ; #REF ; #REF ) , to ranking models for Web search applications .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "As already mentioned in the literature , see for example ( #TARGET_REF ) , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As already mentioned in the literature , see for example ( #TARGET_REF ) , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( #TARGET_REF ) and products of latent variable grammars ( #REF ) , despite being a single generative PCFG ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( #TARGET_REF ) and products of latent variable grammars ( #REF ) , despite being a single generative PCFG .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( #REF ) and products of latent variable grammars ( #TARGET_REF ) , despite being a single generative PCFG ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( #REF ) and products of latent variable grammars ( #TARGET_REF ) , despite being a single generative PCFG .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In multi-party discussion people usually mention each other 's name for the purpose of disentanglement ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In multi-party discussion people usually mention each other 's name for the purpose of disentanglement ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We found that the oldest system ( #TARGET_REF ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We found that the oldest system ( #TARGET_REF ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Others include selectional preferences , transitivity ( #TARGET_REF ) , mutual exclusion , symmetry , etc. ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Others include selectional preferences , transitivity ( #TARGET_REF ) , mutual exclusion , symmetry , etc. .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally , we experiment with a method for combining phrase tables proposed in ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Finally , we experiment with a method for combining phrase tables proposed in ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , our previous work ( #TARGET_REF ; #REF ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For example , our previous work ( #TARGET_REF ; #REF ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "We found the same number using our previous approach ( #TARGET_REF ) , which is roughly equivalent to our core module ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: We found the same number using our previous approach ( #TARGET_REF ) , which is roughly equivalent to our core module .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "We take some core ideas from our previous work on mining script information ( #TARGET_REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: We take some core ideas from our previous work on mining script information ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Our own work ( #TARGET_REF ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Our own work ( #TARGET_REF ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Provided with the candidate fragment elements , we previously ( #TARGET_REF ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Provided with the candidate fragment elements , we previously ( #TARGET_REF ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Similar to ( #TARGET_REFa ) , our summarization system is , which consists of three key components : an initial sentence pre-selection module to select some important sentence candidates ; the above compression model to generate n-best compressions for each sentence ; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Similar to ( #TARGET_REFa ) , our summarization system is , which consists of three key components : an initial sentence pre-selection module to select some important sentence candidates ; the above compression model to generate n-best compressions for each sentence ; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #TARGET_REF ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #TARGET_REF ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "These automatic transformations are based on linguistic rules ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: These automatic transformations are based on linguistic rules ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We carried out two parallel experiments with two parsers available for Czech , parser I ( #REF ) and parser II ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We carried out two parallel experiments with two parsers available for Czech , parser I ( #REF ) and parser II ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "To make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic CzechEnglish dictionary by running GIZA + + training ( translation models 1-4 , see #TARGET_REF ) on the training part of the English-Czech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: To make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic CzechEnglish dictionary by running GIZA + + training ( translation models 1-4 , see #TARGET_REF ) on the training part of the English-Czech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary.\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "For the evaluation of the results we use the BLEU score ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For the evaluation of the results we use the BLEU score ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder ( #REF ; #TARGET_REF ; #REF ) , trained on the same parallel corpus ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder ( #REF ; #TARGET_REF ; #REF ) , trained on the same parallel corpus .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluated our translations with IBM 's BLEU evaluation metric ( #TARGET_REF ) , using the same evaluation method and reference retranslations that were used for evaluation at HLT #REF at CLSP ( Haji 6 et al. , 2002 ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We evaluated our translations with IBM 's BLEU evaluation metric ( #TARGET_REF ) , using the same evaluation method and reference retranslations that were used for evaluation at HLT #REF at CLSP ( Haji 6 et al. , 2002 ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We performed translation experiments with an implementation of the IBM-4 translation model ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We performed translation experiments with an implementation of the IBM-4 translation model ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The maximum entropy approach ( #TARGET_REF ) presents a powerful framework for the combination of several knowledge sources ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The maximum entropy approach ( #TARGET_REF ) presents a powerful framework for the combination of several knowledge sources .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "For descriptions of SMT systems see for example ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For descriptions of SMT systems see for example ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance ( #TARGET_REF ) or ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance ( #TARGET_REF ) or ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For descriptions of SMT systems see for example ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For descriptions of SMT systems see for example ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Due to using a global model like CRFs , our previous work in ( #REF ; #TARGET_REFc ) reported the best results over the evaluated corpora of Bakeoff-2 until now7 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Due to using a global model like CRFs , our previous work in ( #REF ; #TARGET_REFc ) reported the best results over the evaluated corpora of Bakeoff-2 until now7 .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "1 The representation in #TARGET_REF is even more compact than ours for grammars that are not self-embedding ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: 1 The representation in #TARGET_REF is even more compact than ours for grammars that are not self-embedding .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This idea was proposed by Krauwer and des #REF , #REF , and #REF , and was rediscovered by #REF and recently by #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This idea was proposed by Krauwer and des #REF , #REF , and #REF , and was rediscovered by #REF and recently by #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We rephrase the method of #TARGET_REF as follows : First , we construct the approximating finite automaton according to the unparameterized RTN method above ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We rephrase the method of #TARGET_REF as follows : First , we construct the approximating finite automaton according to the unparameterized RTN method above .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This method can be generalized , inspired by #TARGET_REF , who derive N-gram probabilities from stochastic context-free grammars ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This method can be generalized , inspired by #TARGET_REF , who derive N-gram probabilities from stochastic context-free grammars .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "See #TARGET_REF for a variant of this approximation that constructs finite transducers rather than finite automata ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: See #TARGET_REF for a variant of this approximation that constructs finite transducers rather than finite automata .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A very similar formulation , for another grammar transformation , is given in #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A very similar formulation , for another grammar transformation , is given in #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Typical letter-to-sound rule sets are those described by #REF , #REF , #REF , #REF , and #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Typical letter-to-sound rule sets are those described by #REF , #REF , #REF , #REF , and #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For instance , #TARGET_REF recently wrote : `` To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans '' ( p. 520 ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For instance , #TARGET_REF recently wrote : `` To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans '' ( p. 520 ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "See also the work of #TARGET_REF , which considers computer-based pronunciation by analogy but does not mention the possible application to text-to-speech synthesis ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: See also the work of #TARGET_REF , which considers computer-based pronunciation by analogy but does not mention the possible application to text-to-speech synthesis .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Consequently , fusion has been applied to a wide variety of pattern recognition and decision theoretic problems -- using a plethora of theories , techniques , and tools -- including some applications in computational linguistics ( e.g. , #TARGET_REF ; van Halteren , Zavrel , and #REF ) and speech technology ( e.g. , #REF ; #REF9 ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Consequently , fusion has been applied to a wide variety of pattern recognition and decision theoretic problems -- using a plethora of theories , techniques , and tools -- including some applications in computational linguistics ( e.g. , #TARGET_REF ; van Halteren , Zavrel , and #REF ) and speech technology ( e.g. , #REF ; #REF9 ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Many researchers have proposed greedy algorithms for estimating nonprobabilistic word-to-word translation models, also known as translation lexicons (e.g., Catizone, Russell, and #REF; #REF; #REF; #REF; #TARGET_REF 1995; #REF)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Many researchers have proposed greedy algorithms for estimating nonprobabilistic word-to-word translation models, also known as translation lexicons (e.g., Catizone, Russell, and #REF; #REF; #REF; #REF; #TARGET_REF 1995; #REF).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #TARGET_REFb ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #TARGET_REFb ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Unlike the models proposed by #TARGET_REFb ) , this model is symmetric , because both word bags are generated together from a joint probability distribution ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Unlike the models proposed by #TARGET_REFb ) , this model is symmetric , because both word bags are generated together from a joint probability distribution .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "If each word 's translation is treated as a sense tag ( #TARGET_REF ) , then `` translational '' collocations have the unique property that the collocate and the word sense are one and the same !"
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: If each word 's translation is treated as a sense tag ( #TARGET_REF ) , then `` translational '' collocations have the unique property that the collocate and the word sense are one and the same !\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In informal experiments described elsewhere ( #TARGET_REF ) , I found that the G2 statistic suggested by #REF slightly outperforms 02 ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: In informal experiments described elsewhere ( #TARGET_REF ) , I found that the G2 statistic suggested by #REF slightly outperforms 02 .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ cross-language information retrieval ( e.g. , #REF ) , â¢ multilingual document filtering ( e.g. , #REF ) , â¢ computer-assisted language learning ( e.g. , #TARGET_REF ) , â¢ certain machine-assisted translation tools ( e.g. , #REF ; #REFa ) , â¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and #REF ; #REF ) ,"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: â¢ cross-language information retrieval ( e.g. , #REF ) , â¢ multilingual document filtering ( e.g. , #REF ) , â¢ computer-assisted language learning ( e.g. , #TARGET_REF ) , â¢ certain machine-assisted translation tools ( e.g. , #REF ; #REFa ) , â¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and #REF ; #REF ) ,\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In this situation , #TARGET_REFb , 293 ) recommend `` evaluating the expectations using only a single , probable alignment . ''"
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: In this situation , #TARGET_REFb , 293 ) recommend `` evaluating the expectations using only a single , probable alignment . ''\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Just as easily , we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not ( cfXXX #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Just as easily , we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not ( cfXXX #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Until now , translation models have been evaluated either subjectively ( e.g. White and O'#REF ) or using relative metrics , such as perplexity with respect to other models ( #TARGET_REFb ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Until now , translation models have been evaluated either subjectively ( e.g. White and O'#REF ) or using relative metrics , such as perplexity with respect to other models ( #TARGET_REFb ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In informal experiments described elsewhere ( #REF ) , I found that the G2 statistic suggested by #TARGET_REF slightly outperforms 02 ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In informal experiments described elsewhere ( #REF ) , I found that the G2 statistic suggested by #TARGET_REF slightly outperforms 02 .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( #REF ; #REF ; #TARGET_REF ) , lexical conceptual structures ( #REF ) and WordNet synsets ( #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( #REF ; #REF ; #TARGET_REF ) , lexical conceptual structures ( #REF ) and WordNet synsets ( #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ cross-language information retrieval ( e.g. , #TARGET_REF ) , â¢ multilingual document filtering ( e.g. , #REF ) , â¢ computer-assisted language learning ( e.g. , #REF ) , â¢ certain machine-assisted translation tools ( e.g. , #REF ; #REFa ) , â¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and #REF ; #REF ) ,"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: â¢ cross-language information retrieval ( e.g. , #TARGET_REF ) , â¢ multilingual document filtering ( e.g. , #REF ) , â¢ computer-assisted language learning ( e.g. , #REF ) , â¢ certain machine-assisted translation tools ( e.g. , #REF ; #REFa ) , â¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and #REF ; #REF ) ,\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Many other such cases are described in Danlos 's book ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Many other such cases are described in Danlos 's book ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Despite these arguments , most applied NLG systems use a pipelined architecture ; indeed , a pipeline was used in every one of the systems surveyed by #TARGET_REF and #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Despite these arguments , most applied NLG systems use a pipelined architecture ; indeed , a pipeline was used in every one of the systems surveyed by #TARGET_REF and #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG , the Upper Model ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG , the Upper Model ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Shortly after the publication of The Sound Pattern of English ( #REF ) , Kornai points out , `` #REF demonstrated that the context-sensitive machinery of SPE ... [ could ] be replaced by a much simpler one , based on finite-state transducers ( FSTs ) ; the same conclusion was reached independently by Kaplan and Kay , whose work remained an underground classic until it was finally published in #TARGET_REF . ''"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Shortly after the publication of The Sound Pattern of English ( #REF ) , Kornai points out , `` #REF demonstrated that the context-sensitive machinery of SPE ... [ could ] be replaced by a much simpler one , based on finite-state transducers ( FSTs ) ; the same conclusion was reached independently by Kaplan and Kay , whose work remained an underground classic until it was finally published in #TARGET_REF . ''\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of \"simple constraints\" ( e.g. , #TARGET_REFb ) is of course an empirical question ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of \"simple constraints\" ( e.g. , #TARGET_REFb ) is of course an empirical question .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF report excellent part-of-speech tagging results using a handcrafted approach that is close to OT .3 More speculatively , imagine an OT grammar for stylistic revision of parsed sentences ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF report excellent part-of-speech tagging results using a handcrafted approach that is close to OT .3 More speculatively , imagine an OT grammar for stylistic revision of parsed sentences .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "But typical OT grammars offer much richer finite-state models of left context ( #TARGET_REFa ) than provided by the traditional HMM finite-state topologies ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: But typical OT grammars offer much richer finite-state models of left context ( #TARGET_REFa ) than provided by the traditional HMM finite-state topologies .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF use mutual information to identify collocations , a method they claim is reasonably effective for words with a frequency of not less than five ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF use mutual information to identify collocations , a method they claim is reasonably effective for words with a frequency of not less than five .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #TARGET_REF ) :"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #TARGET_REF ) :\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , #TARGET_REF proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao , for the number 5000000000000000005000000000000005000000000005000000005000 , are not context-free , which implies that Chinese is not a context-free language and thus might parse in exponential worst-case time ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , #TARGET_REF proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao , for the number 5000000000000000005000000000000005000000000005000000005000 , are not context-free , which implies that Chinese is not a context-free language and thus might parse in exponential worst-case time .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Discriminant analysis has been employed by researchers in automatic text genre detection ( #TARGET_REFb ; #REF ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Discriminant analysis has been employed by researchers in automatic text genre detection ( #TARGET_REFb ; #REF ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The algorithm we implemented is inspired by the work of #TARGET_REF on word sense disambiguation ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: The algorithm we implemented is inspired by the work of #TARGET_REF on word sense disambiguation .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "The changes made were inspired by those described in #TARGET_REF , page 75 ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: The changes made were inspired by those described in #TARGET_REF , page 75 ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "For the joint segmentation and POS-tagging task , we present a novel solution using the framework in this article , and show that it gives comparable accuracies to our previous work ( #TARGET_REFa ) , while being more than an order of magnitude faster ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For the joint segmentation and POS-tagging task , we present a novel solution using the framework in this article , and show that it gives comparable accuracies to our previous work ( #TARGET_REFa ) , while being more than an order of magnitude faster .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Previously ( #TARGET_REF ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Previously ( #TARGET_REF ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "This evaluation set-up is an improvement versus the one we previously reported ( #TARGET_REF ) , in which fixed partitions were used for training , development , and testing ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: This evaluation set-up is an improvement versus the one we previously reported ( #TARGET_REF ) , in which fixed partitions were used for training , development , and testing .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "CCGBank ( #TARGET_REF ) is used to train the model ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: CCGBank ( #TARGET_REF ) is used to train the model .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In our previous papers ( #TARGET_REF ; Zhang , Blackwood , and #REF ) , we applied a set of beams to this structure , which makes it similar to the data structure used for phrase-based MT decoding ( #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In our previous papers ( #TARGET_REF ; Zhang , Blackwood , and #REF ) , we applied a set of beams to this structure , which makes it similar to the data structure used for phrase-based MT decoding ( #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This was done by MERT optimization ( #TARGET_REF ) towards post-edits under the TER target metric ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: This was done by MERT optimization ( #TARGET_REF ) towards post-edits under the TER target metric .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "To prepare SMT outputs for post-editing , the creators of the corpus used their own WMT10 system ( #REF ) , based on the Moses phrase-based decoder ( #TARGET_REF ) with dense features ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: To prepare SMT outputs for post-editing , the creators of the corpus used their own WMT10 system ( #REF ) , based on the Moses phrase-based decoder ( #TARGET_REF ) with dense features .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In contrast , a single statistical model allows one to maintain a single table ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In contrast , a single statistical model allows one to maintain a single table ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "IGEN uses standard chart generation techniques ( #TARGET_REF ) in its base generator to efficiently produce generation candidates ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: IGEN uses standard chart generation techniques ( #TARGET_REF ) in its base generator to efficiently produce generation candidates .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Notice that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen , unless we know in advance all of the properties that can be attributed to a given object , as in the case of Jordan 's work on the COCONUT domain ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Notice that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen , unless we know in advance all of the properties that can be attributed to a given object , as in the case of Jordan 's work on the COCONUT domain ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( #REF ) ) ( #TARGET_REFa ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( #REF ) ) ( #TARGET_REFa ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Using WordNet , annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in ( #TARGET_REFb ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Using WordNet , annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in ( #TARGET_REFb ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 1 gives the interpretations of eight adjective-noun combinations discussed in #TARGET_REF and #REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Table 1 gives the interpretations of eight adjective-noun combinations discussed in #TARGET_REF and #REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Adjectives , more than other categories , are a striking example of regular polysemy since they are able to take on different meanings depending on their context , viz. , the noun or noun class they modify ( see #TARGET_REF and the references therein ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Adjectives , more than other categories , are a striking example of regular polysemy since they are able to take on different meanings depending on their context , viz. , the noun or noun class they modify ( see #TARGET_REF and the references therein ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We have presented an ensemble approach to word sense disambiguation ( #TARGET_REF ) where multiple Naive Bayesian classifiers , each based on co -- occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We have presented an ensemble approach to word sense disambiguation ( #TARGET_REF ) where multiple Naive Bayesian classifiers , each based on co -- occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "See ( #TARGET_REF ) for a discussion ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: See ( #TARGET_REF ) for a discussion .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Other definitions of predicates may be found in ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other definitions of predicates may be found in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The task we used to compare different generalisation techniques is similar to that used by #TARGET_REF and #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The task we used to compare different generalisation techniques is similar to that used by #TARGET_REF and #REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The problem with this approach is that any threshold is , to some extent , arbitrary , and there is evidence to suggest that , for some tasks , low counts are important ( #TARGET_REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: The problem with this approach is that any threshold is , to some extent , arbitrary , and there is evidence to suggest that , for some tasks , low counts are important ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "However , #TARGET_REF claims that the log-likelihood chisquared statistic ( G2 ) is more appropriate for corpus-based NLP ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: However , #TARGET_REF claims that the log-likelihood chisquared statistic ( G2 ) is more appropriate for corpus-based NLP .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "The task we used to compare different generalisation techniques is similar to that used by #REF and #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The task we used to compare different generalisation techniques is similar to that used by #REF and #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #TARGET_REF that the G2 statistic is better suited for use in corpus-based NLP ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #TARGET_REF that the G2 statistic is better suited for use in corpus-based NLP .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "These observations and this line of reasoning has not escaped the attention of theoretical linguists : #TARGET_REF propose that argument structure is , in fact , encoded syntactically ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: These observations and this line of reasoning has not escaped the attention of theoretical linguists : #TARGET_REF propose that argument structure is , in fact , encoded syntactically .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It projects a functional head , voice ( #TARGET_REF ) , whose specifier is the external argument ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It projects a functional head , voice ( #TARGET_REF ) , whose specifier is the external argument .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper , I present a computational implementation of Distributed Morphology ( #TARGET_REF ) , a non-lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: In this paper , I present a computational implementation of Distributed Morphology ( #TARGET_REF ) , a non-lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This approach has its roots in Fillmore 's Case #REF , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( #REF ) and PropBank ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This approach has its roots in Fillmore 's Case #REF , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( #REF ) and PropBank ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In ( #TARGET_REF ) , I present evidence from Mandarin Chinese that this analysis is on the right track ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: In ( #TARGET_REF ) , I present evidence from Mandarin Chinese that this analysis is on the right track .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Due to advances in statistical syntactic parsing techniques ( #REF ; #TARGET_REF ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Due to advances in statistical syntactic parsing techniques ( #REF ; #TARGET_REF ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #REF ; #REF ; #TARGET_REFb ; Rappaport #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #REF ; #REF ; #TARGET_REFb ; Rappaport #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "With a minimal set of features and a small number of lexical entries , #REF has successfully modeled many of the argument alternations described by #REF using a #TARGET_REF style analysis ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: With a minimal set of features and a small number of lexical entries , #REF has successfully modeled many of the argument alternations described by #REF using a #TARGET_REF style analysis .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF observes that accomplishments differ from achievements only in terms of event duration , which is often a question of granularity ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF observes that accomplishments differ from achievements only in terms of event duration , which is often a question of granularity .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "With a minimal set of features and a small number of lexical entries , #REF has successfully modeled many of the argument alternations described by #TARGET_REF using a #REF style analysis ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: With a minimal set of features and a small number of lexical entries , #REF has successfully modeled many of the argument alternations described by #TARGET_REF using a #REF style analysis .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This framework , where the `` semantic load '' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by #TARGET_REFa ) , among many others ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This framework , where the `` semantic load '' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by #TARGET_REFa ) , among many others .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Here , I adopt the model proposed by #TARGET_REF and decompose lexical verbs into verbalizing heads and verbal roots ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Here , I adopt the model proposed by #TARGET_REF and decompose lexical verbs into verbalizing heads and verbal roots .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This approach has its roots in Fillmore 's Case #REF , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( #TARGET_REF ) and PropBank ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This approach has its roots in Fillmore 's Case #REF , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( #TARGET_REF ) and PropBank ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #TARGET_REF ; #REF ; #REFb ; Rappaport #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #TARGET_REF ; #REF ; #REFb ; Rappaport #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Due to advances in statistical syntactic parsing techniques ( #TARGET_REF ; #REF ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Due to advances in statistical syntactic parsing techniques ( #TARGET_REF ; #REF ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #REF ; #TARGET_REF ; #REFb ; Rappaport #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #REF ; #TARGET_REF ; #REFb ; Rappaport #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: #TARGET_REF has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #TARGET_REF ) , possibly arranged in an inheritance hierarchy ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #TARGET_REF ) , possibly arranged in an inheritance hierarchy .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #REF ; #REF ; #REFb ; Rappaport #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #REF ; #REF ; #REFb ; Rappaport #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A more recent approach , advocated by Rappaport #REF , describes a basic set of event templates corresponding to Vendler 's event classes ( #TARGET_REF ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A more recent approach , advocated by Rappaport #REF , describes a basic set of event templates corresponding to Vendler 's event classes ( #TARGET_REF ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REFb ) and Topkara et al. ( 2006a ) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REFb ) and Topkara et al. ( 2006a ) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The first lexical substitution method was proposed by #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The first lexical substitution method was proposed by #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF , #REF , #REF , #TARGET_REF and Topkara et al. ( 2006a ) all belong to the syntactic transformation category ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF , #REF , #REF , #TARGET_REF and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Later works , such as #TARGET_REFa ) , #REF , #REF and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Later works , such as #TARGET_REFa ) , #REF , #REF and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Later works , such as Atallah et al. ( 2001a ) , #REF , #REF and #TARGET_REFb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Later works , such as Atallah et al. ( 2001a ) , #REF , #REF and #TARGET_REFb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF , #REF , #TARGET_REF , #REF and Topkara et al. ( 2006a ) all belong to the syntactic transformation category ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF , #REF , #TARGET_REF , #REF and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Later works , such as Atallah et al. ( 2001a ) , #TARGET_REF , #REF and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Later works , such as Atallah et al. ( 2001a ) , #TARGET_REF , #REF and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The Google n-gram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation ( #TARGET_REF ) , and contains English n-grams and their observed frequency counts , for counts of at least 40 ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The Google n-gram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation ( #TARGET_REF ) , and contains English n-grams and their observed frequency counts , for counts of at least 40 .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Steganography is concerned with hiding information in some cover medium , by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Steganography is concerned with hiding information in some cover medium , by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF , #REF , #REF , #REF and #TARGET_REFa ) all belong to the syntactic transformation category ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF , #REF , #REF , #REF and #TARGET_REFa ) all belong to the syntactic transformation category .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF aimed to embed information by exploiting the linguistic phenomenon of presupposition , with the idea that some presuppositional information can be removed without changing the meaning of a sentence ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF aimed to embed information by exploiting the linguistic phenomenon of presupposition , with the idea that some presuppositional information can be removed without changing the meaning of a sentence .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF also note that the applicability of paraphrases is strongly influenced by context ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF also note that the applicability of paraphrases is strongly influenced by context .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The paraphrase dictionary that we use was generated for us by Chris Callison-Burch , using the technique described in #TARGET_REF , which exploits a parallel corpus and methods developed for statistical machine translation ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The paraphrase dictionary that we use was generated for us by Chris Callison-Burch , using the technique described in #TARGET_REF , which exploits a parallel corpus and methods developed for statistical machine translation .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF , #REF , #REF , #REF and Topkara et al. ( 2006a ) all belong to the syntactic transformation category ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF , #REF , #REF , #REF and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that we are concerned with transformations which are linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Note that we are concerned with transformations which are linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our proposed method is based on the automatically acquired paraphrase dictionary described in #TARGET_REF , in which the application of paraphrases from the dictionary encodes secret bits ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Our proposed method is based on the automatically acquired paraphrase dictionary described in #TARGET_REF , in which the application of paraphrases from the dictionary encodes secret bits .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "However , the literature on Linguistic Steganography , in which linguistic properties of a text are modified to hide information , is small compared with other media ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However , the literature on Linguistic Steganography , in which linguistic properties of a text are modified to hide information , is small compared with other media ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the #TARGET_REF CCG parser to analyse the sentence before and after paraphrasing ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We use the #TARGET_REF CCG parser to analyse the sentence before and after paraphrasing .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF , #TARGET_REF , #REF , #REF and Topkara et al. ( 2006a ) all belong to the syntactic transformation category ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF , #TARGET_REF , #REF , #REF and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In our previous work ( #TARGET_REF ; #REF ) , we applied our approach to tokenized Arabic and our DA-MSA transfer component used feature transfer rules only ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In our previous work ( #TARGET_REF ; #REF ) , we applied our approach to tokenized Arabic and our DA-MSA transfer component used feature transfer rules only .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The parallel corpus is word-aligned using GIZA + + ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The parallel corpus is word-aligned using GIZA + + ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This is a similar conclusion to our previous work in #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This is a similar conclusion to our previous work in #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the open-source Moses toolkit ( #TARGET_REF ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We use the open-source Moses toolkit ( #TARGET_REF ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are Core & #REF ; #TARGET_REF ; Nakatani & #REF ; Shriberg , Bear , & #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are Core & #REF ; #TARGET_REF ; Nakatani & #REF ; Shriberg , Bear , & #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Similar to our previous work ( #REFb ) , we used the supervised WSD approach described in ( #TARGET_REF ) for our experiments , using the naive Bayes algorithm as our classifier ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Similar to our previous work ( #REFb ) , we used the supervised WSD approach described in ( #TARGET_REF ) for our experiments , using the naive Bayes algorithm as our classifier .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We chose to follow #TARGET_REF and split the sentences evenly to facilitate further comparison ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We chose to follow #TARGET_REF and split the sentences evenly to facilitate further comparison .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #TARGET_REF ) and the perceptron POS tagging model from #REF ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #TARGET_REF ) and the perceptron POS tagging model from #REF .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Our HDP extension is also inspired from the Bayesian model proposed by #TARGET_REF ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Our HDP extension is also inspired from the Bayesian model proposed by #TARGET_REF .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) : 1 ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) : 1 .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ) : 1 ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ) : 1 .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "More recently , an alignment selection approach was proposed in ( #TARGET_REF ) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: More recently , an alignment selection approach was proposed in ( #TARGET_REF ) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Inspired by ( #TARGET_REF ) , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Inspired by ( #TARGET_REF ) , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach ( #TARGET_REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous sentiment-analysis work in different domains has considered inter-document similarity (#REF; #REF; #REF) or explicit inter-document references in the form of hyperlinks ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Previous sentiment-analysis work in different domains has considered inter-document similarity (#REF; #REF; #REF) or explicit inter-document references in the form of hyperlinks ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There has also been work focused upon determining the political leaning (e.g., \"liberal\" vs. \"conservative\") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the \"unlabeled\" texts) ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There has also been work focused upon determining the political leaning (e.g., \"liberal\" vs. \"conservative\") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the \"unlabeled\" texts) ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There has also been work focused upon determining the political leaning (e.g., \"liberal\" vs. \"conservative\") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the \"unlabeled\" texts) ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There has also been work focused upon determining the political leaning (e.g., \"liberal\" vs. \"conservative\") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the \"unlabeled\" texts) ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Notable early papers on graph-based semisupervised learning include #REF , #REF , #REF , and #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Notable early papers on graph-based semisupervised learning include #REF , #REF , #REF , and #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "More sophisticated approaches have been proposed ( #TARGET_REF ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: More sophisticated approaches have been proposed ( #TARGET_REF ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Also relevant is work on the general problems of dialog-act tagging ( #REF ) , citation analysis ( #REF ) , and computational rhetorical analysis ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Also relevant is work on the general problems of dialog-act tagging ( #REF ) , citation analysis ( #REF ) , and computational rhetorical analysis ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, we may find textual4 evidence of a high likelihood of agreement be-tween two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings (see #REF but cfXXX #TARGET_REF."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example, we may find textual4 evidence of a high likelihood of agreement be-tween two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings (see #REF but cfXXX #TARGET_REF.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There has also been work focused upon determining the political leaning (e.g., \"liberal\" vs. \"conservative\") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the \"unlabeled\" texts) ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There has also been work focused upon determining the political leaning (e.g., \"liberal\" vs. \"conservative\") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the \"unlabeled\" texts) ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Fol- lowing standard practice in sentiment analysis ( #TARGET_REF ) , the input to SVMlight con- sisted of normalized presence-of-feature (rather than frequency-of-feature) vectors."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Fol- lowing standard practice in sentiment analysis ( #TARGET_REF ) , the input to SVMlight con- sisted of normalized presence-of-feature (rather than frequency-of-feature) vectors.\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( #REF ; #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( #REF ; #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous sentiment-analysis work in different domains has considered inter-document similarity ( #TARGET_REF ; #REF ; #REF ) or explicit inter-document references in the form of hyper- links (#REF)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Previous sentiment-analysis work in different domains has considered inter-document similarity ( #TARGET_REF ; #REF ; #REF ) or explicit inter-document references in the form of hyper- links (#REF).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Also relevant is work on the general problems of dialog-act tagging ( #REF ) , citation analysis ( #TARGET_REF ) , and computational rhetorical analysis ( #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Also relevant is work on the general problems of dialog-act tagging ( #REF ) , citation analysis ( #TARGET_REF ) , and computational rhetorical analysis ( #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A few others incorporate various measures of inter-document similarity between the texts to be labeled ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A few others incorporate various measures of inter-document similarity between the texts to be labeled ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A few others incorporate various measures of inter-document similarity between the texts to be labeled ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A few others incorporate various measures of inter-document similarity between the texts to be labeled ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A few others incorporate various measures of inter-document similarity between the texts to be labeled ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A few others incorporate various measures of inter-document similarity between the texts to be labeled ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "More sophisticated approaches have been proposed ( #REF ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: More sophisticated approaches have been proposed ( #REF ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "As has been previously observed and exploited in the NLP literature ( #TARGET_REF ; #REF ; #REF ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: As has been previously observed and exploited in the NLP literature ( #TARGET_REF ; #REF ; #REF ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In the United States , for example , governmental bodies are providing and soliciting political documents via the Internet , with lofty goals in mind : electronic rulemaking ( eRulemaking ) initiatives involving the `` electronic collection , distribution , synthesis , and analysis of public commentary in the regulatory rulemaking process '' , may `` [ alter ] the citizen-government relationship '' ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In the United States , for example , governmental bodies are providing and soliciting political documents via the Internet , with lofty goals in mind : electronic rulemaking ( eRulemaking ) initiatives involving the `` electronic collection , distribution , synthesis , and analysis of public commentary in the regulatory rulemaking process '' , may `` [ alter ] the citizen-government relationship '' ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular, since we treat each individual speech within a debate as a single \"document\", we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( #TARGET_REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In particular, since we treat each individual speech within a debate as a single \"document\", we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( #TARGET_REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF maintains a survey of this area ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF maintains a survey of this area .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous sentiment-analysis work in different domains has considered inter-document similarity ( #REF ; #TARGET_REF ; #REF ) or explicit inter-document references in the form of hyperlinks (#REF)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Previous sentiment-analysis work in different domains has considered inter-document similarity ( #REF ; #TARGET_REF ; #REF ) or explicit inter-document references in the form of hyperlinks (#REF).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes #REF , #REF , #TARGET_REF , and #REF ; see #REF for an active bibliography ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes #REF , #REF , #TARGET_REF , and #REF ; see #REF for an active bibliography ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "As has been previously observed and exploited in the NLP literature ( #REF ; #TARGET_REF ; #REF ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: As has been previously observed and exploited in the NLP literature ( #REF ; #TARGET_REF ; #REF ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our classification framework , directly inspired by #TARGET_REF , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Our classification framework , directly inspired by #TARGET_REF , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes #REF , #REF , #REF , and #TARGET_REF ; see #REF for an active bibliography ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes #REF , #REF , #REF , and #TARGET_REF ; see #REF for an active bibliography ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Notable early papers on graph-based semisupervised learning include #TARGET_REF , #REF , #REF , and #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Notable early papers on graph-based semisupervised learning include #TARGET_REF , #REF , #REF , and #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Relationships between the unlabeled items #TARGET_REF consider sequential relations between different types of emails ( e.g. , between requests and satisfactions thereof ) to classify messages , and thus also explicitly exploit the structure of conversations ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Relationships between the unlabeled items #TARGET_REF consider sequential relations between different types of emails ( e.g. , between requests and satisfactions thereof ) to classify messages , and thus also explicitly exploit the structure of conversations .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion , given the dense nature of legislative language and the fact that ( U.S. ) bills often reach several hundred pages in length ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion , given the dense nature of legislative language and the fact that ( U.S. ) bills often reach several hundred pages in length ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "As has been previously observed and exploited in the NLP literature ( #REF ; #REF ; #TARGET_REF ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: As has been previously observed and exploited in the NLP literature ( #REF ; #REF ; #TARGET_REF ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( #REF ; #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( #REF ; #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Notable early papers on graph-based semisupervised learning include #REF , #TARGET_REF , #REF , and #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Notable early papers on graph-based semisupervised learning include #REF , #TARGET_REF , #REF , and #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( #REF ; #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( #REF ; #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes #TARGET_REF , #REF , #REF , and #REF ; see #REF for an active bibliography ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes #TARGET_REF , #REF , #REF , and #REF ; see #REF for an active bibliography ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Notable early papers on graph-based semisupervised learning include #REF , #REF , #TARGET_REF , and #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Notable early papers on graph-based semisupervised learning include #REF , #REF , #TARGET_REF , and #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Also relevant is work on the general problems of dialog-act tagging ( #REF ) , citation analysis ( #REF ) , and computational rhetorical analysis ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Also relevant is work on the general problems of dialog-act tagging ( #REF ) , citation analysis ( #REF ) , and computational rhetorical analysis ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "An exception is #TARGET_REF , who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: An exception is #TARGET_REF , who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings ( see #TARGET_REF but cfXXX #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings ( see #TARGET_REF but cfXXX #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous sentiment-analysis work in different domains has considered inter-document similarity ( #REF ; #REF ; #TARGET_REF ) or explicit inter-document references in the form of hyperlinks (#REF)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Previous sentiment-analysis work in different domains has considered inter-document similarity ( #REF ; #REF ; #TARGET_REF ) or explicit inter-document references in the form of hyperlinks (#REF).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes #REF , #TARGET_REF , #REF , and #REF ; see #REF for an active bibliography ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes #REF , #TARGET_REF , #REF , and #REF ; see #REF for an active bibliography ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Our plan is to implement a windowed or moving-average version of BLEU as in ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Our plan is to implement a windowed or moving-average version of BLEU as in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "The flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g. ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g. ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This contrasts with the findings described in #TARGET_REF where significant improvements could be achieved by increasing the number of source languages ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This contrasts with the findings described in #TARGET_REF where significant improvements could be achieved by increasing the number of source languages .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "As suggested in #TARGET_REF this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: As suggested in #TARGET_REF this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Whereas #TARGET_REF dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Whereas #TARGET_REF dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in #TARGET_REF ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in #TARGET_REF .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "A possible future direction would be to compare the query string to retrieved results using a method similar to that of #TARGET_REF ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: A possible future direction would be to compare the query string to retrieved results using a method similar to that of #TARGET_REF .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "The article classifier is a discriminative model that draws on the state-of-the-art approach described in #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The article classifier is a discriminative model that draws on the state-of-the-art approach described in #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks ( #TARGET_REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Another line of research that is correlated with ours is recognition of agreement/disagreement ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) and classification of stances ( #REF ; #REF ) in online forums ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Another line of research that is correlated with ours is recognition of agreement/disagreement ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) and classification of stances ( #REF ; #REF ) in online forums .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Another line of research that is correlated with ours is recognition of agreement/disagreement ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ) and classification of stances ( #REF ; #REF ) in online forums ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Another line of research that is correlated with ours is recognition of agreement/disagreement ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ) and classification of stances ( #REF ; #REF ) in online forums .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    }
]