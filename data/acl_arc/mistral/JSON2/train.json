[
    {
        "gold": {
            "text": [
                "The framework was originally developed for the realization of deep-syntactic structures in NLG ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The framework was originally developed for the realization of deep-syntactic structures in NLG ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( #REF ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( #TARGET_REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( #REF ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "More details on how the structural divergences described in ( #TARGET_REF ) can be accounted for using our formalism can be found in ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: More details on how the structural divergences described in ( #TARGET_REF ) can be accounted for using our formalism can be found in ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Our work extends directions taken in systems such as Ariane ( #REF ) , FoG ( #REF ) , JOYCE ( Rambow and #TARGET_REF ) , and LFS ( #REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Our work extends directions taken in systems such as Ariane ( #REF ) , FoG ( #REF ) , JOYCE ( Rambow and #TARGET_REF ) , and LFS ( #REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG ( #REF ) , LFS ( #REF ) , and JOYCE ( Rambow and #TARGET_REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG ( #REF ) , LFS ( #REF ) , and JOYCE ( Rambow and #TARGET_REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger ( #TARGET_REF ) ) and parsers generally aim to produce a tree spanning each sentence ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger ( #TARGET_REF ) ) and parsers generally aim to produce a tree spanning each sentence .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "SWIZZLE is a multilingual enhancement of COCKTAIL ( #TARGET_REF ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: SWIZZLE is a multilingual enhancement of COCKTAIL ( #TARGET_REF ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #TARGET_REF ) , ( #REF ) ( #REF ) ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #TARGET_REF ) , ( #REF ) ( #REF ) ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Details of the top performing heuristics of COCKTAIL were reported in ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Details of the top performing heuristics of COCKTAIL were reported in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For this research , we used a coreference resolution system ( ( #TARGET_REF ) ) that implements different sets of heuristics corresponding to various forms of coreference ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For this research , we used a coreference resolution system ( ( #TARGET_REF ) ) that implements different sets of heuristics corresponding to various forms of coreference .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Each component will return a confidence measure of the reliability of its prediction , c.f. ( #TARGET_REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Each component will return a confidence measure of the reliability of its prediction , c.f. ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "We use an in-house statistical tagger ( based on ( #TARGET_REF ) ) to tag the text in which the unknown word occurs ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We use an in-house statistical tagger ( based on ( #TARGET_REF ) ) to tag the text in which the unknown word occurs .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Research that is more similar in goal to that outlined in this paper is Vosse ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Research that is more similar in goal to that outlined in this paper is Vosse ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Corpus frequency : ( #TARGET_REF ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Corpus frequency : ( #TARGET_REF ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #TARGET_REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #TARGET_REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the same set of binary features as in previous work on this dataset ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We use the same set of binary features as in previous work on this dataset ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( #REF ) , class n-grams ( #TARGET_REF ) , grammatical features ( #REF ) , etc ' ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( #REF ) , class n-grams ( #TARGET_REF ) , grammatical features ( #REF ) , etc ' .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Unfortunately , as shown in ( #TARGET_REF ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Unfortunately , as shown in ( #TARGET_REF ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "As shown in ( #TARGET_REF ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: As shown in ( #TARGET_REF ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #TARGET_REF ] , which is still considered one of the best smoothing methods for n-gram language models ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #TARGET_REF ] , which is still considered one of the best smoothing methods for n-gram language models .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The features can be easily obtained by modifying the TAT extraction algorithm described in ( #TARGET_REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: The features can be easily obtained by modifying the TAT extraction algorithm described in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #TARGET_REF ; #REF ) and history-based parsing ( #REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #TARGET_REF ; #REF ) and history-based parsing ( #REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #REF ; #TARGET_REF ) and history-based parsing ( #REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #REF ; #TARGET_REF ) and history-based parsing ( #REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #REF ; #REF ) and history-based parsing ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #REF ; #REF ) and history-based parsing ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "We could also introduce new variables , e.g. , nonterminal refinements ( #TARGET_REF ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( #REF ; #REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: We could also introduce new variables , e.g. , nonterminal refinements ( #TARGET_REF ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( #REF ; #REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( #REF ; #TARGET_REF ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( #REF ; #TARGET_REF ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "The first direct application of parse forest in translation is our previous work ( #TARGET_REF ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: The first direct application of parse forest in translation is our previous work ( #TARGET_REF ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #REF ) , Information Extraction ( #TARGET_REF ) , and Machine Translation ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #REF ) , Information Extraction ( #TARGET_REF ) , and Machine Translation ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "To prove that our method is effective , we also make a comparison between the performances of our system and #TARGET_REF , #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: To prove that our method is effective , we also make a comparison between the performances of our system and #TARGET_REF , #REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #TARGET_REF ) , Information Extraction ( #REF ) , and Machine Translation ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #TARGET_REF ) , Information Extraction ( #REF ) , and Machine Translation ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF did very encouraging work on the feature calibration of semantic role labeling ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF did very encouraging work on the feature calibration of semantic role labeling .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "After the PropBank ( #TARGET_REF ) was built , #REF and #REF have produced more complete and systematic research on Chinese SRL ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: After the PropBank ( #TARGET_REF ) was built , #REF and #REF have produced more complete and systematic research on Chinese SRL .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Experiments on Chinese SRL ( #TARGET_REF , #REF ) reassured these findings ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Experiments on Chinese SRL ( #TARGET_REF , #REF ) reassured these findings .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF has built a semantic role classifier exploiting the interdependence of semantic roles ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: #TARGET_REF has built a semantic role classifier exploiting the interdependence of semantic roles .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF has made the first attempt working on the single semantic role level to make further improvement ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF has made the first attempt working on the single semantic role level to make further improvement .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #TARGET_REF , #REF and #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #TARGET_REF , #REF and #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "To prove that our method is effective , we also make a comparison between the performances of our system and #REF , #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: To prove that our method is effective , we also make a comparison between the performances of our system and #REF , #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "be found in figure 2 , which is similar with that in #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: be found in figure 2 , which is similar with that in #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #REF ) , Information Extraction ( #REF ) , and Machine Translation ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #REF ) , Information Extraction ( #REF ) , and Machine Translation ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the same data setting with #TARGET_REF , however a bit different from #REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We use the same data setting with #TARGET_REF , however a bit different from #REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #REF , #REF and #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #REF , #REF and #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Experiments on Chinese SRL ( #REF , #TARGET_REF ) reassured these findings ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Experiments on Chinese SRL ( #REF , #TARGET_REF ) reassured these findings .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF has made some preliminary attempt on the idea of hierarchical semantic role labeling."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF has made some preliminary attempt on the idea of hierarchical semantic role labeling.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #REF , #TARGET_REF and #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #REF , #TARGET_REF and #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the same data setting with #REF , however a bit different from #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We use the same data setting with #REF , however a bit different from #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The candidate feature templates include : Voice from #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The candidate feature templates include : Voice from #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Semantic Role labeling ( SRL ) was first defined in #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Semantic Role labeling ( SRL ) was first defined in #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #TARGET_REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #TARGET_REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #REF ; #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #REF ; #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #REF ; #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #REF ; #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Other representations use the link structure ( #TARGET_REF ) or generate graph representations of the extracted features ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other representations use the link structure ( #TARGET_REF ) or generate graph representations of the extracted features ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some researchers ( #REF ; #TARGET_REF ) have explored the use of Wikipedia information to improve the disambiguation process ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some researchers ( #REF ; #TARGET_REF ) have explored the use of Wikipedia information to improve the disambiguation process .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "2The WePS-1 corpus includes data from the Web03 testbed ( #TARGET_REF ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: 2The WePS-1 corpus includes data from the Web03 testbed ( #TARGET_REF ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #REF ) and sometimes in combination with others see for instance ( #TARGET_REF ; #REF ) - ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #REF ) and sometimes in combination with others see for instance ( #TARGET_REF ; #REF ) - .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Other representations use the link structure ( #REF ) or generate graph representations of the extracted features ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other representations use the link structure ( #REF ) or generate graph representations of the extracted features ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF compared the performace of NEs versus BoW features ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF compared the performace of NEs versus BoW features .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It provides a fine grained NE recognition covering 100 different NE types ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: It provides a fine grained NE recognition covering 100 different NE types ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some researchers ( #TARGET_REF ; #REF ) have explored the use of Wikipedia information to improve the disambiguation process ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some researchers ( #TARGET_REF ; #REF ) have explored the use of Wikipedia information to improve the disambiguation process .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The Web People Search task , as defined in the first WePS evaluation campaign ( #TARGET_REF ) , consists of grouping search results for a given name according to the different people that share it ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The Web People Search task , as defined in the first WePS evaluation campaign ( #TARGET_REF ) , consists of grouping search results for a given name according to the different people that share it .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We have used the testbeds from WePS-1 ( #TARGET_REF , 2007)2 and WePS-2 (#REF) evaluation campaigns 3."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We have used the testbeds from WePS-1 ( #TARGET_REF , 2007)2 and WePS-2 (#REF) evaluation campaigns 3.\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #REF ; #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #REF ; #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( #REF ) and Cross-document Coreference ( CDC ) ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( #REF ) and Cross-document Coreference ( CDC ) ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF , the second WePS campaign showed similar trends regarding the use of NE features ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF , the second WePS campaign showed similar trends regarding the use of NE features ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #TARGET_REF ) and sometimes in combination with others see for instance ( #REF ; #REF ) - ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #TARGET_REF ) and sometimes in combination with others see for instance ( #REF ; #REF ) - .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #REF ) and sometimes in combination with others see for instance ( #REF ; #TARGET_REF ) - ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #REF ) and sometimes in combination with others see for instance ( #REF ; #TARGET_REF ) - .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: #REF have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: #REF have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #REF ; #TARGET_REF ) for self training ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #REF ; #TARGET_REF ) for self training .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #TARGET_REF ; #REF ) for self training ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #TARGET_REF ; #REF ) for self training .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Self-training should also benefit other discriminatively trained parsers with latent annotations ( #TARGET_REF ) , although training would be much slower compared to using generative models , as in our case ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Self-training should also benefit other discriminatively trained parsers with latent annotations ( #TARGET_REF ) , although training would be much slower compared to using generative models , as in our case .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #TARGET_REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #TARGET_REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( #TARGET_REF ) or a memory-efficient trie implementation based on a succinct data structure ( #REF ; #REF ) to reduce required memory usage ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( #TARGET_REF ) or a memory-efficient trie implementation based on a succinct data structure ( #REF ; #REF ) to reduce required memory usage .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "The reordering models we describe follow our previous work using function word models for translation ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: The reordering models we describe follow our previous work using function word models for translation ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "To model o ( Li , S â T ) , o ( Ri , S â T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: To model o ( Li , S â T ) , o ( Ri , S â T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The reordering models we describe follow our previous work using function word models for translation ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: The reordering models we describe follow our previous work using function word models for translation ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "To model d ( FWi â 1 , S â T ) , d ( FWi +1 , S â T ) , i.e. whether Li , S â T and Ri , S â T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: To model d ( FWi â 1 , S â T ) , d ( FWi +1 , S â T ) , i.e. whether Li , S â T and Ri , S â T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In our previous work ( #TARGET_REF ) , we started an initial investigation on conversation entailment ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: In our previous work ( #TARGET_REF ) , we started an initial investigation on conversation entailment .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Using the implicit modeling of argument consistency , we follow the same approach as in our previous work ( #TARGET_REF ) and trained a logistic regression model to predict verb alignment based on the features in Table 1 ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Using the implicit modeling of argument consistency , we follow the same approach as in our previous work ( #TARGET_REF ) and trained a logistic regression model to predict verb alignment based on the features in Table 1 .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that in our original work ( #TARGET_REF ) , only development data were used to show some initial observations."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Note that in our original work ( #TARGET_REF ) , only development data were used to show some initial observations.\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This alignment is obtained by following the same set of rules learned from the development dataset as in ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: This alignment is obtained by following the same set of rules learned from the development dataset as in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "To address this limitation , our previous work ( #TARGET_REF ) has initiated an investigation on the problem of conversation entailment ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: To address this limitation , our previous work ( #TARGET_REF ) has initiated an investigation on the problem of conversation entailment .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "In our previous work ( #TARGET_REF ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â§ ... â§ dm , and a hypothesis H represented by another set of clauses H = h1 â§ ... â§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: In our previous work ( #TARGET_REF ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â§ ... â§ dm , and a hypothesis H represented by another set of clauses H = h1 â§ ... â§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the structures previously used by #TARGET_REF , and propose one new structure ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We use the structures previously used by #TARGET_REF , and propose one new structure .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This revalidates the observation of #TARGET_REF that phrase structure representations and dependency representations add complimentary value to the learning task ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This revalidates the observation of #TARGET_REF that phrase structure representations and dependency representations add complimentary value to the learning task .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Here , the PET and GR kernel perform similar : this is different from the results of ( #TARGET_REF ) where GR performed much worse than PET for ACE data ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Here , the PET and GR kernel perform similar : this is different from the results of ( #TARGET_REF ) where GR performed much worse than PET for ACE data .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our results also confirm the insights gained by #TARGET_REF , who observed that in crossdomain polarity analysis adding more training data is not always beneficial ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our results also confirm the insights gained by #TARGET_REF , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #REF ; #TARGET_REF ) , perform in comparison to our approach ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #REF ; #TARGET_REF ) , perform in comparison to our approach .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #TARGET_REF ; #REF ) , perform in comparison to our approach ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #TARGET_REF ; #REF ) , perform in comparison to our approach .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "An implementation of the transition-based dependency parsing frame- work ( #TARGET_REF ) using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in #REF with a beam size of 8. Beams with varying sizes can be used to produce k-best lists."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: An implementation of the transition-based dependency parsing frame- work ( #TARGET_REF ) using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in #REF with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This includes work on generalized expectation ( #REF ) , posterior regularization ( #REF ) and constraint driven learning ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This includes work on generalized expectation ( #REF ) , posterior regularization ( #REF ) and constraint driven learning ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the non-projective k-best MST algorithm to generate k-best lists ( #TARGET_REF ) , where k = 8 for the experiments in this paper ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We use the non-projective k-best MST algorithm to generate k-best lists ( #TARGET_REF ) , where k = 8 for the experiments in this paper .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "1Our rules are similar to those from #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: 1Our rules are similar to those from #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This includes work on question answering ( #REF ) , sentiment analysis ( #TARGET_REF ) , MT reordering ( #REF ) , and many other tasks ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This includes work on question answering ( #REF ) , sentiment analysis ( #TARGET_REF ) , MT reordering ( #REF ) , and many other tasks .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "An implementation of graph- based parsing algorithms with an arc-factored parameterization ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: An implementation of graph- based parsing algorithms with an arc-factored parameterization ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The method is called targeted self-training as it is similar in vein to self-training ( #TARGET_REF ) , with the exception that the new parse data is targeted to produce accurate word reorderings ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The method is called targeted self-training as it is similar in vein to self-training ( #TARGET_REF ) , with the exception that the new parse data is targeted to produce accurate word reorderings .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( #TARGET_REF ) and is simpler to measure ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( #TARGET_REF ) and is simpler to measure .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "This includes work on question answering ( #REF ) , sentiment analysis ( #REF ) , MT reordering ( #TARGET_REF ) , and many other tasks ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This includes work on question answering ( #REF ) , sentiment analysis ( #REF ) , MT reordering ( #TARGET_REF ) , and many other tasks .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "• Transition-based: An implementation of the transition-based dependency parsing framework ( #REF ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #TARGET_REF with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: • Transition-based: An implementation of the transition-based dependency parsing framework ( #REF ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #TARGET_REF with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #REF ; #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #REF ; #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #REF ; #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #REF ; #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The work that is most similar to ours is that of #TARGET_REF , who introduced the Constraint Driven Learning algorithm ( CODL ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The work that is most similar to ours is that of #TARGET_REF , who introduced the Constraint Driven Learning algorithm ( CODL ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "A recent study by #TARGET_REF also investigates the task of training parsers to improve MT reordering ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A recent study by #TARGET_REF also investigates the task of training parsers to improve MT reordering .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (#REF) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction ( #TARGET_REF ) and textual entailment ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (#REF) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction ( #TARGET_REF ) and textual entailment ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Identical to the standard perceptron proof , e.g. , #TARGET_REF , by inserting in loss-separability for normal separability ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Identical to the standard perceptron proof , e.g. , #TARGET_REF , by inserting in loss-separability for normal separability .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #TARGET_REF ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (#REF) and textual entailment (#REF)."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #TARGET_REF ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (#REF) and textual entailment (#REF).\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #TARGET_REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #TARGET_REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "One obvious approach to this problem is to employ parser reranking ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One obvious approach to this problem is to employ parser reranking ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (#REF) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction ( #REF ) and textual entailment ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (#REF) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction ( #REF ) and textual entailment ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This includes work on generalized expectation ( #REF ) , posterior regularization ( #REF ) and constraint driven learning ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This includes work on generalized expectation ( #REF ) , posterior regularization ( #REF ) and constraint driven learning ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "An implementation of the transition-based dependency parsing framework ( #TARGET_REF ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #REF with a beam size of 8 ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: An implementation of the transition-based dependency parsing framework ( #TARGET_REF ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #REF with a beam size of 8 .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This includes work on generalized expectation ( #TARGET_REF ) , posterior regularization ( #REF ) and constraint driven learning ( #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This includes work on generalized expectation ( #TARGET_REF ) , posterior regularization ( #REF ) and constraint driven learning ( #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #REF ; #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #REF ; #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This includes work on question answering ( #TARGET_REF ) , sentiment analysis ( #REF ) , MT reordering ( #REF ) , and many other tasks ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This includes work on question answering ( #TARGET_REF ) , sentiment analysis ( #REF ) , MT reordering ( #REF ) , and many other tasks .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "criteria and data used in our experiments are based on the work of #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: criteria and data used in our experiments are based on the work of #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper , inspired by KNN-SVM ( #TARGET_REF ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: In this paper , inspired by KNN-SVM ( #TARGET_REF ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Motivated by (#TARGET_REF, 2003; #REF), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW\\u2212WbII2+ A \\ufffd j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[\\u03b1W \\u00b7 h(fj, e)] P\\u03b1(e|fj; W) = (7) Ee'Ec; exp[\\u03b1W \\u00b7 h(fj, e')], where \\u03b1 > 0 is a real number valued smoother."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Motivated by (#TARGET_REF, 2003; #REF), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW\\u2212WbII2+ A \\ufffd j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[\\u03b1W \\u00b7 h(fj, e)] P\\u03b1(e|fj; W) = (7) Ee'Ec; exp[\\u03b1W \\u00b7 h(fj, e')], where \\u03b1 > 0 is a real number valued smoother.\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Our method resorts to some translation examples , which is similar as example-based translation or translation memory ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our method resorts to some translation examples , which is similar as example-based translation or translation memory ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Actually , if we use LSH technique ( #TARGET_REF ) in retrieval process , the local method can be easily scaled to a larger training data ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Actually , if we use LSH technique ( #TARGET_REF ) in retrieval process , the local method can be easily scaled to a larger training data .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "(#REF; #REF; #REF; #TARGET_REF ) employed an evaluation metric as a loss function and directly optimized it."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: (#REF; #REF; #REF; #TARGET_REF ) employed an evaluation metric as a loss function and directly optimized it.\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The local training method ( #TARGET_REF ) is widely employed in computer vision ( #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The local training method ( #TARGET_REF ) is widely employed in computer vision ( #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some methods are based on likelihood ( #REF ; #REF ) , error rate ( #REF ; #TARGET_REF ; #REF ; #REF ) , margin ( #REF ; #REF ) and ranking ( #REF ) , and among which minimum error rate training ( MERT ) ( #REF ) is the most popular one ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some methods are based on likelihood ( #REF ; #REF ) , error rate ( #REF ; #TARGET_REF ; #REF ; #REF ) , margin ( #REF ; #REF ) and ranking ( #REF ) , and among which minimum error rate training ( MERT ) ( #REF ) is the most popular one .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #REF ) with modified Kneser-Ney smoothing ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #REF ) with modified Kneser-Ney smoothing ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #TARGET_REF ) with modified Kneser-Ney smoothing ( #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #TARGET_REF ) with modified Kneser-Ney smoothing ( #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Some methods are based on likelihood ( #REF ; #REF ) , error rate ( #REF ; #REF ; #REF ; #TARGET_REF ) , margin ( #REF ; #REF ) and ranking ( #REF ) , and among which minimum error rate training ( MERT ) ( #REF ) is the most popular one ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some methods are based on likelihood ( #REF ; #REF ) , error rate ( #REF ; #REF ; #REF ; #TARGET_REF ) , margin ( #REF ; #REF ) and ranking ( #REF ) , and among which minimum error rate training ( MERT ) ( #REF ) is the most popular one .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In the field of machine learning research , incremental training has been employed in the work ( #TARGET_REF ; #REF ) , but there is little work for tuning parameters of statistical machine translation ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In the field of machine learning research , incremental training has been employed in the work ( #TARGET_REF ; #REF ) , but there is little work for tuning parameters of statistical machine translation .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "( #REF ; #REF ; #TARGET_REF ; #REF ) employed an evaluation metric as a loss function and directly optimized it ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: ( #REF ; #REF ; #TARGET_REF ; #REF ) employed an evaluation metric as a loss function and directly optimized it .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "( #REF ; #TARGET_REF ; #REF ; #REF ) employed an evaluation metric as a loss function and directly optimized it ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: ( #REF ; #TARGET_REF ; #REF ; #REF ) employed an evaluation metric as a loss function and directly optimized it .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The significance testing is performed by paired bootstrap re-sampling ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The significance testing is performed by paired bootstrap re-sampling ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "(b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#TARGET_REF, 2011)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: (b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#TARGET_REF, 2011).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "( #TARGET_REF ; #REF ) used maximum likelihood estimation to learn weights for MT."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: ( #TARGET_REF ; #REF ) used maximum likelihood estimation to learn weights for MT.\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "To retrieve translation examples for a test sentence , ( #TARGET_REF ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :"
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: To retrieve translation examples for a test sentence , ( #TARGET_REF ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Some methods are based on likelihood ( #REF ; #REF ) , error rate ( #TARGET_REF ; #REF ; #REF ; #REF ) , margin ( #REF ; #REF ) and ranking ( #REF ) , and among which minimum error rate training ( MERT ) ( #REF ) is the most popular one ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some methods are based on likelihood ( #REF ; #REF ) , error rate ( #TARGET_REF ; #REF ; #REF ; #REF ) , margin ( #REF ; #REF ) and ranking ( #REF ) , and among which minimum error rate training ( MERT ) ( #REF ) is the most popular one .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We employ the idea of ultraconservative update ( #TARGET_REF ; #REF ) to propose two incremental methods for local training in Algorithm 2 as follows ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We employ the idea of ultraconservative update ( #TARGET_REF ; #REF ) to propose two incremental methods for local training in Algorithm 2 as follows .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "( #TARGET_REF ; #REF ; #REF ; #REF ) employed an evaluation metric as a loss function and directly optimized it ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: ( #TARGET_REF ; #REF ; #REF ; #REF ) employed an evaluation metric as a loss function and directly optimized it .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "We use an in-house developed hierarchical phrase-based translation ( #TARGET_REF ) as our baseline system , and we denote it as In-Hiero ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We use an in-house developed hierarchical phrase-based translation ( #TARGET_REF ) as our baseline system , and we denote it as In-Hiero .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We run GIZA + + ( #TARGET_REF ) on the training corpus in both directions ( #REF ) to obtain the word alignment for each sentence pair ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We run GIZA + + ( #TARGET_REF ) on the training corpus in both directions ( #REF ) to obtain the word alignment for each sentence pair .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "( #REF ; #REF ; #TARGET_REF ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: ( #REF ; #REF ; #TARGET_REF ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Some methods are based on likelihood ( #REF ; #REF ) , error rate ( #REF ; #REF ; #REF ; #REF ) , margin ( #REF ; #REF ) and ranking ( #TARGET_REF ) , and among which minimum error rate training ( MERT ) ( #REF ) is the most popular one ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some methods are based on likelihood ( #REF ; #REF ) , error rate ( #REF ; #REF ; #REF ; #REF ) , margin ( #REF ; #REF ) and ranking ( #TARGET_REF ) , and among which minimum error rate training ( MERT ) ( #REF ) is the most popular one .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Although evaluated on different data sets , this result is consistent with results from previous work ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Although evaluated on different data sets , this result is consistent with results from previous work ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #TARGET_REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "In a similar vein , #TARGET_REF showed that a different feature-topic model improved predictions on a fill-in-the-blank task ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In a similar vein , #TARGET_REF showed that a different feature-topic model improved predictions on a fill-in-the-blank task .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some works abstract perception via the usage of symbolic logic representations ( #TARGET_REF ; #REF ; #REF ; #REF ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some works abstract perception via the usage of symbolic logic representations ( #TARGET_REF ; #REF ; #REF ; #REF ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "They use a Bag of Visual Words ( BoVW ) model ( #TARGET_REF ) to create a bimodal vocabulary describing documents ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: They use a Bag of Visual Words ( BoVW ) model ( #TARGET_REF ) to create a bimodal vocabulary describing documents .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "More recently , #REF show that visual attribute classifiers , which have been immensely successful in object recognition ( #TARGET_REF ) , act as excellent substitutes for feature"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: More recently , #REF show that visual attribute classifiers , which have been immensely successful in object recognition ( #TARGET_REF ) , act as excellent substitutes for feature\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some efforts have tackled tasks such as automatic image caption generation ( #REFa ; #REF ) , text illustration ( #REF ) , or automatic location identification of Twitter users ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some efforts have tackled tasks such as automatic image caption generation ( #REFa ; #REF ) , text illustration ( #REF ) , or automatic location identification of Twitter users ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #TARGET_REF ; #REF ; #REFb ; #REF ; #REF ; #REF ; #REFa ; #REFb ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #TARGET_REF ; #REF ; #REFb ; #REF ; #REF ; #REF ; #REFa ; #REFb ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "That is, we simply take the original mLDA model of #TARGET_REF (2009) and generalize it in the same way they generalize LDA."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: That is, we simply take the original mLDA model of #TARGET_REF (2009) and generalize it in the same way they generalize LDA.\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF extend LDA to allow for the inference of document and topic distributions in a multimodal corpus ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering ( #TARGET_REF ) , and images are then quantized over the 5,000 codewords ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering ( #TARGET_REF ) , and images are then quantized over the 5,000 codewords .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This result is consistent with other works using this model with these features ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This result is consistent with other works using this model with these features ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. , #REF ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. , #REF ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( #TARGET_REF ) or robot commands ( #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( #TARGET_REF ) or robot commands ( #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some efforts have tackled tasks such as automatic image caption generation ( #REFa ; #TARGET_REF ) , text illustration ( #REF ) , or automatic location identification of Twitter users ( #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some efforts have tackled tasks such as automatic image caption generation ( #REFa ; #TARGET_REF ) , text illustration ( #REF ) , or automatic location identification of Twitter users ( #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #REF ; #REF ; #REF ; #REFa ; #REFb ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #REF ; #REF ; #REF ; #REFa ; #REFb ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the same method as #TARGET_REF for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We use the same method as #TARGET_REF for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von #REF ) , computing power , improved computer vision models ( #REF ; #REF ; #REF ; #TARGET_REF ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von #REF ) , computing power , improved computer vision models ( #REF ; #REF ; #REF ; #TARGET_REF ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This seems to provide additional evidence of #TARGET_REFb ) 's suggestion that something like a distributional hypothesis of images is plausible ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This seems to provide additional evidence of #TARGET_REFb ) 's suggestion that something like a distributional hypothesis of images is plausible .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "It is frequently used in tasks like scene identification , and #TARGET_REF shows that distance in GIST space correlates well with semantic distance in WordNet ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: It is frequently used in tasks like scene identification , and #TARGET_REF shows that distance in GIST space correlates well with semantic distance in WordNet .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REFa ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REFa ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some efforts have tackled tasks such as automatic image caption generation ( #TARGET_REFa ; #REF ) , text illustration ( #REF ) , or automatic location identification of Twitter users ( #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some efforts have tackled tasks such as automatic image caption generation ( #TARGET_REFa ; #REF ) , text illustration ( #REF ) , or automatic location identification of Twitter users ( #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group ( #TARGET_REF ) containing approximately 1.7 B word tokens ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group ( #TARGET_REF ) containing approximately 1.7 B word tokens .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "To name a few examples , #REF and #REF show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #TARGET_REF show that verb clusters can be used to improve activity recognition in videos ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: To name a few examples , #REF and #REF show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #TARGET_REF show that verb clusters can be used to improve activity recognition in videos .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , we measure association norm prediction as an average of percentile ranks ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Following #TARGET_REF , we measure association norm prediction as an average of percentile ranks .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We also compute GIST vectors ( #TARGET_REF ) for every image using LearGIST ( #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We also compute GIST vectors ( #TARGET_REF ) for every image using LearGIST ( #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Some efforts have tackled tasks such as automatic image caption generation ( #REFa ; #REF ) , text illustration ( #REF ) , or automatic location identification of Twitter users ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some efforts have tackled tasks such as automatic image caption generation ( #REFa ; #REF ) , text illustration ( #REF ) , or automatic location identification of Twitter users ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #REF ) in the prediction of association norms ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #REF ) in the prediction of association norms .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "To name a few examples , #REF and #TARGET_REF show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #REF show that verb clusters can be used to improve activity recognition in videos ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: To name a few examples , #REF and #TARGET_REF show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #REF show that verb clusters can be used to improve activity recognition in videos .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "More recently , #TARGET_REF show that visual attribute classifiers , which have been immensely successful in object recognition ( #REF ) , act as excellent substitutes for feature"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: More recently , #TARGET_REF show that visual attribute classifiers , which have been immensely successful in object recognition ( #REF ) , act as excellent substitutes for feature\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #REF ; #TARGET_REF ; #REF ; #REFa ; #REFb ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #REF ; #TARGET_REF ; #REF ; #REFa ; #REFb ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von #REF ) , computing power , improved computer vision models ( #REF ; #TARGET_REF ; #REF ; #REF ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von #REF ) , computing power , improved computer vision models ( #REF ; #TARGET_REF ; #REF ; #REF ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #TARGET_REF ; #REFb ; #REF ; #REF ; #REF ; #REFa ; #REFb ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #TARGET_REF ; #REFb ; #REF ; #REF ; #REF ; #REFa ; #REFb ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Association Norms ( AN ) is a collection of association norms collected by Schulte im #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Association Norms ( AN ) is a collection of association norms collected by Schulte im #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Latent Dirichlet Allocation ( #TARGET_REF ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Latent Dirichlet Allocation ( #TARGET_REF ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some works abstract perception via the usage of symbolic logic representations ( #REF ; #REF ; #TARGET_REF ; #REF ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some works abstract perception via the usage of symbolic logic representations ( #REF ; #REF ; #TARGET_REF ; #REF ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "To name a few examples , #TARGET_REF and #REF show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #REF show that verb clusters can be used to improve activity recognition in videos ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: To name a few examples , #TARGET_REF and #REF show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #REF show that verb clusters can be used to improve activity recognition in videos .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some works abstract perception via the usage of symbolic logic representations ( #REF ; #TARGET_REF ; #REF ; #REF ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some works abstract perception via the usage of symbolic logic representations ( #REF ; #TARGET_REF ; #REF ; #REF ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #TARGET_REF ; #REF ; #REF ; #REFa ; #REFb ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #TARGET_REF ; #REF ; #REF ; #REFa ; #REFb ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some efforts have tackled tasks such as automatic image caption generation ( #REFa ; #REF ) , text illustration ( #TARGET_REF ) , or automatic location identification of Twitter users ( #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some efforts have tackled tasks such as automatic image caption generation ( #REFa ; #REF ) , text illustration ( #TARGET_REF ) , or automatic location identification of Twitter users ( #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( #REF ) or robot commands ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( #REF ) or robot commands ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von #REF ) , computing power , improved computer vision models ( #REF ; #REF ; #REF ; #REF ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von #REF ) , computing power , improved computer vision models ( #REF ; #REF ; #REF ; #REF ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "To solve these scaling issues , we implement Online Variational Bayesian Inference ( #REF ; #TARGET_REF ) for our models ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: To solve these scaling issues , we implement Online Variational Bayesian Inference ( #REF ; #TARGET_REF ) for our models .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #REF ; #REF ; #REF ; #TARGET_REFa ; #REFb ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #REF ; #REF ; #REF ; #TARGET_REFa ; #REFb ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #REF ; #REF ; #TARGET_REF ; #REFa ; #REFb ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #REF ; #REF ; #TARGET_REF ; #REFa ; #REFb ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This result is consistent with other works using this model with these features ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This result is consistent with other works using this model with these features ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von #REF ) , computing power , improved computer vision models ( #TARGET_REF ; #REF ; #REF ; #REF ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von #REF ) , computing power , improved computer vision models ( #TARGET_REF ; #REF ; #REF ; #REF ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von #REF ) , computing power , improved computer vision models ( #REF ; #REF ; #TARGET_REF ; #REF ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von #REF ) , computing power , improved computer vision models ( #REF ; #REF ; #TARGET_REF ; #REF ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #TARGET_REF ) in the prediction of association norms ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #TARGET_REF ) in the prediction of association norms .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "To solve these scaling issues , we implement Online Variational Bayesian Inference ( #TARGET_REF ; #REF ) for our models ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: To solve these scaling issues , we implement Online Variational Bayesian Inference ( #TARGET_REF ; #REF ) for our models .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The first work to do this with topic models is #TARGET_REFb ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The first work to do this with topic models is #TARGET_REFb ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "This choice is motivated by an observation we made previously ( #TARGET_REFa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3"
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: This choice is motivated by an observation we made previously ( #TARGET_REFa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Following our previous work on stance classification ( #TARGET_REFc ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( #REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Following our previous work on stance classification ( #TARGET_REFc ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( #REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Our experimental design with professional bilingual translators follows our previous work #TARGET_REFa ) comparing scratch translation to post-edit ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Our experimental design with professional bilingual translators follows our previous work #TARGET_REFa ) comparing scratch translation to post-edit .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "This is in line with our previous findings from ( #TARGET_REF ) that candidates with higher power attempt to shift topics less often than others when responding to moderators ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This is in line with our previous findings from ( #TARGET_REF ) that candidates with higher power attempt to shift topics less often than others when responding to moderators .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "We follow our previous work ( #TARGET_REFb ) and restrict bridging to non-coreferential cases ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: We follow our previous work ( #TARGET_REFb ) and restrict bridging to non-coreferential cases .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( #REF ; #TARGET_REFa ; #REFb ) on bridging anaphora recognition and antecedent selection ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( #REF ; #TARGET_REFa ; #REFb ) on bridging anaphora recognition and antecedent selection .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "in history-based models ( #TARGET_REF ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: in history-based models ( #TARGET_REF ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "7A11 our results are computed with the evalb program following the now-standard criteria in ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: 7A11 our results are computed with the evalb program following the now-standard criteria in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Many statistical parsers ( #REF ; #REF ; #REF ) are based on a history-based probability model ( #TARGET_REF ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Many statistical parsers ( #REF ; #REF ; #REF ) are based on a history-based probability model ( #TARGET_REF ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #TARGET_REF ) , as has conditioning on the left-corner child ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #TARGET_REF ) , as has conditioning on the left-corner child ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We used a publicly available tagger ( #TARGET_REF ) to tag the words and then used these in the input to the system ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We used a publicly available tagger ( #TARGET_REF ) to tag the words and then used these in the input to the system .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Many statistical parsers ( #REF ; #TARGET_REF ; #REF ) are based on a history-based probability model ( #REF ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Many statistical parsers ( #REF ; #TARGET_REF ; #REF ) are based on a history-based probability model ( #REF ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #REF ) , as has conditioning on the left-corner child ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #REF ) , as has conditioning on the left-corner child ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This is roughly an 11 % relative reduction in error rate over #TARGET_REF and Bods PCFG-reduction reported in Table 1 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This is roughly an 11 % relative reduction in error rate over #TARGET_REF and Bods PCFG-reduction reported in Table 1 .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 's subtrees , reporting a 5.1 % relative reduction in error rate over the model in #REF on the WSJ ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 's subtrees , reporting a 5.1 % relative reduction in error rate over the model in #REF on the WSJ .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #REF and #REF , Bonnema et al. 's estimator performs worse and is comparable to #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #REF and #REF , Bonnema et al. 's estimator performs worse and is comparable to #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Compared to the reranking technique in #TARGET_REF , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Compared to the reranking technique in #TARGET_REF , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #REF and #TARGET_REF , Bonnema et al. 's estimator performs worse and is comparable to #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #REF and #TARGET_REF , Bonnema et al. 's estimator performs worse and is comparable to #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Most DOP models , such as in #REF , #REF , #TARGET_REF , Sima'an ( 2000 ) and Collins & #REF , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Most DOP models , such as in #REF , #REF , #TARGET_REF , Sima'an ( 2000 ) and Collins & #REF , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF , #REF , #REF and #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #REF , #REF , #REF and #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "For our experiments we used the standard division of the WSJ ( #TARGET_REF ) , with sections 2 through 21 for training ( approx ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For our experiments we used the standard division of the WSJ ( #TARGET_REF ) , with sections 2 through 21 for training ( approx .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF , #REF , #REF and #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF , #REF , #REF and #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #TARGET_REF and #REF , Bonnema et al. 's estimator performs worse and is comparable to #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #TARGET_REF and #REF , Bonnema et al. 's estimator performs worse and is comparable to #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "And #TARGET_REF argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #REF who use exactly the same set of ( all ) tree fragments as proposed in #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: And #TARGET_REF argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #REF who use exactly the same set of ( all ) tree fragments as proposed in #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Most DOP models , such as in #REF , #REF , #REF , Sima'an ( 2000 ) and #TARGET_REF , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Most DOP models , such as in #REF , #REF , #REF , Sima'an ( 2000 ) and #TARGET_REF , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Most DOP models , such as in #REF , #TARGET_REF , #REF , Sima'an ( 2000 ) and Collins & #REF , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Most DOP models , such as in #REF , #TARGET_REF , #REF , Sima'an ( 2000 ) and Collins & #REF , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Most DOP models , such as in #REF , #REF , #REF , #TARGET_REF and Collins & #REF , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Most DOP models , such as in #REF , #REF , #REF , #TARGET_REF and Collins & #REF , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by #TARGET_REF , 2002 ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by #TARGET_REF , 2002 ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This approach has now gained wide usage , as exemplified by the work of #TARGET_REF , 1999 ) , Charniak ( 1996 , 1997 ) , #REF , #REF , and many others ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: This approach has now gained wide usage , as exemplified by the work of #TARGET_REF , 1999 ) , Charniak ( 1996 , 1997 ) , #REF , #REF , and many others .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "The importance of including nonheadwords has become uncontroversial ( e.g. #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The importance of including nonheadwords has become uncontroversial ( e.g. #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "And #REF argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #TARGET_REF who use exactly the same set of ( all ) tree fragments as proposed in #REF ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: And #REF argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #TARGET_REF who use exactly the same set of ( all ) tree fragments as proposed in #REF .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Following the work of #TARGET_REF , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Following the work of #TARGET_REF , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "For compound splitting , we follow #TARGET_REF , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For compound splitting , we follow #TARGET_REF , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We follow #TARGET_REF , for compound merging ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We follow #TARGET_REF , for compound merging .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Other approaches use less deep linguistic resources ( e.g. , POS-tags #REF ) or are ( almost ) knowledge-free ( e.g. , #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Other approaches use less deep linguistic resources ( e.g. , POS-tags #REF ) or are ( almost ) knowledge-free ( e.g. , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF used unification in an SMT system to model some of the agreement phenomena that we model."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF used unification in an SMT system to model some of the agreement phenomena that we model.\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( #REF ) and the BitPar parser , which is a state-of-the-art parser of German ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( #REF ) and the BitPar parser , which is a state-of-the-art parser of German ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We prepare the training data by splitting compounds in two steps , following the technique of #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We prepare the training data by splitting compounds in two steps , following the technique of #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF , #REF , #REF , #TARGET_REF , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #REF , #REF , #REF , #TARGET_REF , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF introduced factored SMT ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF introduced factored SMT .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #REF , #TARGET_REF and others ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #REF , #TARGET_REF and others .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #TARGET_REF , #REF and others ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #TARGET_REF , #REF and others .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Other approaches use less deep linguistic resources ( e.g. , POS-tags #TARGET_REF ) or are ( almost ) knowledge-free ( e.g. , #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Other approaches use less deep linguistic resources ( e.g. , POS-tags #TARGET_REF ) or are ( almost ) knowledge-free ( e.g. , #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our approach to extract and classify social events builds on our previous work ( #TARGET_REF ) , which in turn builds on work from the relation extraction community ( #REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Our approach to extract and classify social events builds on our previous work ( #TARGET_REF ) , which in turn builds on work from the relation extraction community ( #REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "I A more detailed discussion of various aspects of the proposed parser can be found in ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: I A more detailed discussion of various aspects of the proposed parser can be found in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Proceedings of EACL '99 example , the ALE parser ( #TARGET_REF ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Proceedings of EACL '99 example , the ALE parser ( #TARGET_REF ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "As shown in ( #TARGET_REF ) â¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340 ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As shown in ( #TARGET_REF ) â¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340 .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #TARGET_REF )"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #TARGET_REF )\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "See , among others , ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: See , among others , ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The ConTroll grammar development system as described in ( #TARGET_REFb ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The ConTroll grammar development system as described in ( #TARGET_REFb ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "See also ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: See also ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #REF ) as discussed in ( #TARGET_REFa ) and ( #REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #REF ) as discussed in ( #TARGET_REFa ) and ( #REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "` See ( #TARGET_REF ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: ` See ( #TARGET_REF ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This contrasts with one of the traditional approaches ( e.g. , #TARGET_REF ; #REF ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This contrasts with one of the traditional approaches ( e.g. , #TARGET_REF ; #REF ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #TARGET_REF ) and automatically trained transducers ( #REF ) with a larger range of positions ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #TARGET_REF ) and automatically trained transducers ( #REF ) with a larger range of positions .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "At the same time , we believe our method has advantages over the approach developed initially at IBM ( #TARGET_REF ; #REF ) for training translation systems automatically ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: At the same time , we believe our method has advantages over the approach developed initially at IBM ( #TARGET_REF ; #REF ) for training translation systems automatically .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "1 Â° The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner '' ( #TARGET_REF , 203 ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 1 Â° The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner '' ( #TARGET_REF , 203 ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "McDonald has even argued for extending the model to a large number of components ( #REF ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: McDonald has even argued for extending the model to a large number of components ( #REF ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "McDonald has even argued for extending the model to a large number of components ( #REF ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: McDonald has even argued for extending the model to a large number of components ( #REF ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Something like this approach is in fact used in some systems ( e.g. , #REF ; #REF ; #TARGET_REFa ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Something like this approach is in fact used in some systems ( e.g. , #REF ; #REF ; #TARGET_REFa ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "These include devices such as interleaving the components ( #REF ; #REF ) , backtracking on failure ( #REF ; #REF ) , allowing the linguistic component to interrogate the planner ( #TARGET_REF ; #REF ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( #REFa , 1988c ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: These include devices such as interleaving the components ( #REF ; #REF ) , backtracking on failure ( #REF ; #REF ) , allowing the linguistic component to interrogate the planner ( #TARGET_REF ; #REF ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( #REFa , 1988c ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "These include devices such as interleaving the components ( #REF ; #TARGET_REF ) , backtracking on failure ( #REF ; #REF ) , allowing the linguistic component to interrogate the planner ( #REF ; #REF ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( #REFa , 1988c ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: These include devices such as interleaving the components ( #REF ; #TARGET_REF ) , backtracking on failure ( #REF ; #REF ) , allowing the linguistic component to interrogate the planner ( #REF ; #REF ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( #REFa , 1988c ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This approach has occasionally been taken , as in #REF and #REF and , at least implicitly , in #TARGET_REF and #REF ; however , under this approach , all of the flexibility and simplicity of modular design is lost ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This approach has occasionally been taken , as in #REF and #REF and , at least implicitly , in #TARGET_REF and #REF ; however , under this approach , all of the flexibility and simplicity of modular design is lost .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Hovy has described another text planner that builds similar plans ( #TARGET_REFb ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Hovy has described another text planner that builds similar plans ( #TARGET_REFb ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The names given to the components vary ; they have been called \"strategic\" and \"tactical\" components ( e.g. , #REF ; #REF ; #REF ) 1 , \"planning\" and \"realization\" ( e.g. , #REF ; #TARGET_REFa ) , or simply \"what to say\" versus \"how to say it\" ( e.g. , #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The names given to the components vary ; they have been called \"strategic\" and \"tactical\" components ( e.g. , #REF ; #REF ; #REF ) 1 , \"planning\" and \"realization\" ( e.g. , #REF ; #TARGET_REFa ) , or simply \"what to say\" versus \"how to say it\" ( e.g. , #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Surveys and articles on the topic include #REF , de #REF , and #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Surveys and articles on the topic include #REF , de #REF , and #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #TARGET_REF but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #TARGET_REF but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "One approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated ( #TARGET_REF , #REF , #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated ( #TARGET_REF , #REF , #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It is known that certain cue words and phrases ( #TARGET_REF ) can serve as explicit indicators of discourse structure ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: It is known that certain cue words and phrases ( #TARGET_REF ) can serve as explicit indicators of discourse structure .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and #REF is transformation-based learning ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and #REF is transformation-based learning ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and #REF ) and tagging ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and #REF ) and tagging ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ #TARGET_REF ] , or the labels of UDRS [ #REF ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ #TARGET_REF ] , or the labels of UDRS [ #REF ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In the CLE-QLF approach, as rationally reconstructed by #TARGET_REF and #REF , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In the CLE-QLF approach, as rationally reconstructed by #TARGET_REF and #REF , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "We then go on to compare the current approach with that of some other theories with similar aims : the `` standard  version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #TARGET_REF and #REF ; underspecified Discourse Representation Theory ( #REF ) ; and the `` glue language  approach of #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We then go on to compare the current approach with that of some other theories with similar aims : the `` standard  version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #TARGET_REF and #REF ; underspecified Discourse Representation Theory ( #REF ) ; and the `` glue language  approach of #REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #TARGET_REF , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #TARGET_REF , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any ( #TARGET_REF ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any ( #TARGET_REF ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( #TARGET_REF ) , with the resolution process as described here ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( #TARGET_REF ) , with the resolution process as described here .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "The version proposed here combines a basic insight from #REF with higher-order unification to give an analysis that has a strong resemblance to that proposed in #TARGET_REF , 1991 ) , with some differences that are commented on below ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The version proposed here combines a basic insight from #REF with higher-order unification to give an analysis that has a strong resemblance to that proposed in #TARGET_REF , 1991 ) , with some differences that are commented on below .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "It is interesting to compare this analysis with that described in Dalrymple , Shieber , and #REF and #TARGET_REF , 1991 ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: It is interesting to compare this analysis with that described in Dalrymple , Shieber , and #REF and #TARGET_REF , 1991 ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "only the available five relative scopings of the quantifiers are produced ( #TARGET_REF , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ; â¢ the equivalences are reversible , and thus the above sentences cart be generated from scoped logical forms ; â¢ partial scopings are permitted ( see Reyle [ 19961 ) â¢ scoping can be freely interleaved with other types of reference resolution ; â¢ unscoped or partially scoped forms are available for inference or for generation at every stage ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: only the available five relative scopings of the quantifiers are produced ( #TARGET_REF , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ; â¢ the equivalences are reversible , and thus the above sentences cart be generated from scoped logical forms ; â¢ partial scopings are permitted ( see Reyle [ 19961 ) â¢ scoping can be freely interleaved with other types of reference resolution ; â¢ unscoped or partially scoped forms are available for inference or for generation at every stage .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #TARGET_REF ) work to our own framework ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #TARGET_REF ) work to our own framework .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( #REF , 1991 ; #REF ; #REF ; #REF ; Williams , Harvey , and #REF ; #TARGET_REF ; #REF , 1998b ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( #REF , 1991 ; #REF ; #REF ; #REF ; Williams , Harvey , and #REF ; #TARGET_REF ; #REF , 1998b ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( #REF ; #REF ; Ge , Hale , and #REF ; #REF ; the continuing interest in centering , used either in original or in revised form ( #REF ; #REF ; #TARGET_REF ; #REF ) ; and proposals related to the evaluation methodology in anaphora resolution ( #REFa , 2001b ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( #REF ; #REF ; Ge , Hale , and #REF ; #REF ; the continuing interest in centering , used either in original or in revised form ( #REF ; #REF ; #TARGET_REF ; #REF ) ; and proposals related to the evaluation methodology in anaphora resolution ( #REFa , 2001b ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #REF ) , BFP ( Brennan , Friedman , and #REF ) , and Strube 's 5list approach ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #REF ) , BFP ( Brennan , Friedman , and #REF ) , and Strube 's 5list approach ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( #REF , 1991 ; #REF ; #REF ; #TARGET_REF ; Williams , Harvey , and #REF ; #REF ; #REF , 1998b ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( #REF , 1991 ; #REF ; #REF ; #TARGET_REF ; Williams , Harvey , and #REF ; #REF ; #REF , 1998b ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( #REF ; #REF ; Ge , Hale , and #REF ; #REF ; the continuing interest in centering , used either in original or in revised form ( #REF ; #TARGET_REF ; #REF ; #REF ) ; and proposals related to the evaluation methodology in anaphora resolution ( #REFa , 2001b ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( #REF ; #REF ; Ge , Hale , and #REF ; #REF ; the continuing interest in centering , used either in original or in revised form ( #REF ; #TARGET_REF ; #REF ; #REF ) ; and proposals related to the evaluation methodology in anaphora resolution ( #REFa , 2001b ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( #REF ; #TARGET_REF ; Ge , Hale , and #REF ; #REF ; the continuing interest in centering , used either in original or in revised form ( #REF ; #REF ; #REF ; #REF ) ; and proposals related to the evaluation methodology in anaphora resolution ( #REFa , 2001b ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( #REF ; #TARGET_REF ; Ge , Hale , and #REF ; #REF ; the continuing interest in centering , used either in original or in revised form ( #REF ; #REF ; #REF ; #REF ) ; and proposals related to the evaluation methodology in anaphora resolution ( #REFa , 2001b ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( #REF ; Azzam , Humphreys , and #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; Mitkov , Belguith , and #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( #REF ; Azzam , Humphreys , and #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; Mitkov , Belguith , and #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( #REF , 1991 ; #REF ; #TARGET_REF ; #REF ; Williams , Harvey , and #REF ; #REF ; #REF , 1998b ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( #REF , 1991 ; #REF ; #TARGET_REF ; #REF ; Williams , Harvey , and #REF ; #REF ; #REF , 1998b ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #REF ; #REF ; #TARGET_REF ; #REF ) , which was difficult both to represent and to process , and which required considerable human input ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #REF ; #REF ; #TARGET_REF ; #REF ) , which was difficult both to represent and to process , and which required considerable human input .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( #REF ; Azzam , Humphreys , and #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; Mitkov , Belguith , and #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( #REF ; Azzam , Humphreys , and #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; Mitkov , Belguith , and #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #TARGET_REFa ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #TARGET_REFa ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( #REF ; #REF ; Ge , Hale , and #REF ; #REF ; the continuing interest in centering , used either in original or in revised form ( #REF ; #REF ; #REF ; #TARGET_REF ) ; and proposals related to the evaluation methodology in anaphora resolution ( #REFa , 2001b ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( #REF ; #REF ; Ge , Hale , and #REF ; #REF ; the continuing interest in centering , used either in original or in revised form ( #REF ; #REF ; #REF ; #TARGET_REF ) ; and proposals related to the evaluation methodology in anaphora resolution ( #REFa , 2001b ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #REF ; #REF ; #REF ; #TARGET_REF ) , which was difficult both to represent and to process , and which required considerable human input ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #REF ; #REF ; #REF ; #TARGET_REF ) , which was difficult both to represent and to process , and which required considerable human input .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #TARGET_REF , #REF , and #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #TARGET_REF , #REF , and #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( #REF ; Azzam , Humphreys , and #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; Mitkov , Belguith , and #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( #REF ; Azzam , Humphreys , and #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; Mitkov , Belguith , and #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( #REF , 1991 ; #REF ; #REF ; #REF ; Williams , Harvey , and #REF ; #REF ; #TARGET_REF , 1998b ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( #REF , 1991 ; #REF ; #REF ; #REF ; Williams , Harvey , and #REF ; #REF ; #TARGET_REF , 1998b ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #TARGET_REF ) , BFP ( Brennan , Friedman , and #REF ) , and Strube 's 5list approach ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #TARGET_REF ) , BFP ( Brennan , Friedman , and #REF ) , and Strube 's 5list approach ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( #REF ; Azzam , Humphreys , and #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; Mitkov , Belguith , and #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( #REF ; Azzam , Humphreys , and #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; Mitkov , Belguith , and #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #REF , #REF , and #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #REF , #REF , and #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #TARGET_REF ; #REF ; #REF ; #REF ) , which was difficult both to represent and to process , and which required considerable human input ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #TARGET_REF ; #REF ; #REF ; #REF ) , which was difficult both to represent and to process , and which required considerable human input .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Proper names are the main concern of the named-entity recognition subtask ( #TARGET_REF 1998) of information extraction."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Proper names are the main concern of the named-entity recognition subtask ( #TARGET_REF 1998) of information extraction.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #TARGET_REF ] , Brill 's [ #REFa ] , and MaxEnt [ #REF ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #TARGET_REF ] , Brill 's [ #REFa ] , and MaxEnt [ #REF ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This is implemented as a cascade of simple strategies , which were briefly described in #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: This is implemented as a cascade of simple strategies , which were briefly described in #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus ( #TARGET_REF ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus ( #TARGET_REF ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Since then this idea has been applied to several tasks , including word sense disambiguation ( #REF ) and named-entity recognition ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Since then this idea has been applied to several tasks , including word sense disambiguation ( #REF ) and named-entity recognition ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #TARGET_REF ) , neural networks ( #REF ) , and maximum-entropy modeling ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #TARGET_REF ) , neural networks ( #REF ) , and maximum-entropy modeling ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #REF ) , neural networks ( #TARGET_REF ) , and maximum-entropy modeling ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #REF ) , neural networks ( #TARGET_REF ) , and maximum-entropy modeling ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #REF ] , Brill 's [ #TARGET_REFa ] , and MaxEnt [ #REF ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #REF ] , Brill 's [ #TARGET_REFa ] , and MaxEnt [ #REF ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #REF ] , Brill 's [ #REFa ] , and MaxEnt [ #TARGET_REF ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #REF ] , Brill 's [ #REFa ] , and MaxEnt [ #TARGET_REF ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #TARGET_REF ) with the Alembic system ( #REF ) : a 0.5 % error rate ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #TARGET_REF ) with the Alembic system ( #REF ) : a 0.5 % error rate .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "For instance , the Alembic workbench ( #TARGET_REF ) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For instance , the Alembic workbench ( #TARGET_REF ) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #TARGET_REF ( 0.44 % vs. 0.5 % error rate ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #TARGET_REF ( 0.44 % vs. 0.5 % error rate ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #REF ] or Brill 's [ #TARGET_REFb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #REF ] or Brill 's [ #TARGET_REFb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "As #TARGET_REF rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: As #TARGET_REF rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #TARGET_REF ] or Brill 's [ #REFb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #TARGET_REF ] or Brill 's [ #REFb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Before using the DCA method , we applied a Russian morphological processor ( #TARGET_REF ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Before using the DCA method , we applied a Russian morphological processor ( #TARGET_REF ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This is similar to \"one sense per collocation\" idea of #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This is similar to \"one sense per collocation\" idea of #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "A variety of such lists for many languages are already available ( e.g. , #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A variety of such lists for many languages are already available ( e.g. , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #TARGET_REF : 0.28 % vs. 0.20 % error rate ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #TARGET_REF : 0.28 % vs. 0.20 % error rate ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In some systems such dependencies are learned from labeled examples ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In some systems such dependencies are learned from labeled examples ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The best performance on the Brown corpus , a 0.2 % error rate , was reported by #TARGET_REF , who trained a decision tree classifier on a 25-million-word corpus ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The best performance on the Brown corpus , a 0.2 % error rate , was reported by #TARGET_REF , who trained a decision tree classifier on a 25-million-word corpus .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "One of the better-known approaches is described in #TARGET_REF , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One of the better-known approaches is described in #TARGET_REF , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Since then this idea has been applied to several tasks , including word sense disambiguation ( #TARGET_REF ) and named-entity recognition ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Since then this idea has been applied to several tasks , including word sense disambiguation ( #TARGET_REF ) and named-entity recognition ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A detailed introduction to the SBD problem can be found in #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A detailed introduction to the SBD problem can be found in #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This is where robust syntactic systems like SATZ ( #REF ) or the POS tagger reported in #TARGET_REF , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This is where robust syntactic systems like SATZ ( #REF ) or the POS tagger reported in #TARGET_REF , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: #TARGET_REF pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Unlike other POS taggers , this POS tagger ( #TARGET_REF ) was also trained to disambiguate sentence boundaries ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Unlike other POS taggers , this POS tagger ( #TARGET_REF ) was also trained to disambiguate sentence boundaries .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #REF ) with the Alembic system ( #TARGET_REF ) : a 0.5 % error rate ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #REF ) with the Alembic system ( #TARGET_REF ) : a 0.5 % error rate .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This is where robust syntactic systems like SATZ ( #TARGET_REF ) or the POS tagger reported in #REF , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This is where robust syntactic systems like SATZ ( #TARGET_REF ) or the POS tagger reported in #REF , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last"
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: #TARGET_REF developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "The description of the EAGLE workbench for linguistic engineering ( #TARGET_REF ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The description of the EAGLE workbench for linguistic engineering ( #TARGET_REF ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #REF ) , neural networks ( #REF ) , and maximum-entropy modeling ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #REF ) , neural networks ( #REF ) , and maximum-entropy modeling ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF recently described a hybrid method for finding abbreviations and their definitions ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF recently described a hybrid method for finding abbreviations and their definitions .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For instance , #TARGET_REF report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For instance , #TARGET_REF report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #REF ; #REF ; #TARGET_REF ) .7 As an example , consider the translation into French of the house collapsed ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #REF ; #REF ; #TARGET_REF ) .7 As an example , consider the translation into French of the house collapsed .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #TARGET_REF , #REF , and #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #TARGET_REF , #REF , and #REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ Learnability ( #TARGET_REF ) â¢ Text generation ( #REF ; Milosavljevic , Tulloch , and #REF ) â¢ Speech generation ( #REF ) â¢ Localization ( Sch Â¨ aler 1996 )"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: â¢ Learnability ( #TARGET_REF ) â¢ Text generation ( #REF ; Milosavljevic , Tulloch , and #REF ) â¢ Speech generation ( #REF ) â¢ Localization ( Sch Â¨ aler 1996 )\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #TARGET_REF , #REF , and #REF , inter alia ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #TARGET_REF , #REF , and #REF , inter alia .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ Learnability ( #REF ) â¢ Text generation ( #TARGET_REF ; Milosavljevic , Tulloch , and #REF ) â¢ Speech generation ( #REF ) â¢ Localization ( Sch Â¨ aler 1996 )"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: â¢ Learnability ( #REF ) â¢ Text generation ( #TARGET_REF ; Milosavljevic , Tulloch , and #REF ) â¢ Speech generation ( #REF ) â¢ Localization ( Sch Â¨ aler 1996 )\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #TARGET_REF ; #REF ; #REF ) .7"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #TARGET_REF ; #REF ; #REF ) .7\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ language learning ( #REF ; #REF ; Morgan , Meier , and #REF ) â¢ monolingual grammar induction ( #REF ) â¢ grammar optimization ( #REF ) â¢ insights into universal grammar ( #REF ) â¢ machine translation ( #REF , 1997 ; #TARGET_REF ; Gough , Way , and #REF )"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: â¢ language learning ( #REF ; #REF ; Morgan , Meier , and #REF ) â¢ monolingual grammar induction ( #REF ) â¢ grammar optimization ( #REF ) â¢ insights into universal grammar ( #REF ) â¢ machine translation ( #REF , 1997 ; #TARGET_REF ; Gough , Way , and #REF )\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ language learning ( #TARGET_REF ; #REF ; Morgan , Meier , and #REF ) â¢ monolingual grammar induction ( #REF ) â¢ grammar optimization ( #REF ) â¢ insights into universal grammar ( #REF ) â¢ machine translation ( #REF , 1997 ; #REF ; Gough , Way , and #REF )"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: â¢ language learning ( #TARGET_REF ; #REF ; Morgan , Meier , and #REF ) â¢ monolingual grammar induction ( #REF ) â¢ grammar optimization ( #REF ) â¢ insights into universal grammar ( #REF ) â¢ machine translation ( #REF , 1997 ; #REF ; Gough , Way , and #REF )\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #REF , #TARGET_REF , and #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #REF , #TARGET_REF , and #REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "More recently , #TARGET_REF have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and Carl ( 2003 , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: More recently , #TARGET_REF have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and Carl ( 2003 , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF , 1997 ) assumes that words ending in - ed are verbs ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF , 1997 ) assumes that words ending in - ed are verbs .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #REF ; #TARGET_REF ; #REF ) .7"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #REF ; #TARGET_REF ; #REF ) .7\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â > French and English â > Urdu ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â > French and English â > Urdu .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ language learning ( #REF ; #REF ; Morgan , Meier , and #REF ) â¢ monolingual grammar induction ( #REF ) â¢ grammar optimization ( #TARGET_REF ) â¢ insights into universal grammar ( #REF ) â¢ machine translation ( #REF , 1997 ; #REF ; Gough , Way , and #REF )"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: â¢ language learning ( #REF ; #REF ; Morgan , Meier , and #REF ) â¢ monolingual grammar induction ( #REF ) â¢ grammar optimization ( #TARGET_REF ) â¢ insights into universal grammar ( #REF ) â¢ machine translation ( #REF , 1997 ; #REF ; Gough , Way , and #REF )\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In a final processing stage , we generalize over the marker lexicon following a process found in #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: In a final processing stage , we generalize over the marker lexicon following a process found in #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #TARGET_REF to permit a limited form of insertion in the translation process ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #TARGET_REF to permit a limited form of insertion in the translation process .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In their Gaijin system , #TARGET_REF give a result of 63 % accurate translations obtained for English â > German on a test set of 791 sentences from CorelDRAW manuals ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In their Gaijin system , #TARGET_REF give a result of 63 % accurate translations obtained for English â > German on a test set of 791 sentences from CorelDRAW manuals .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ language learning ( #REF ; #TARGET_REF ; Morgan , Meier , and #REF ) â¢ monolingual grammar induction ( #REF ) â¢ grammar optimization ( #REF ) â¢ insights into universal grammar ( #REF ) â¢ machine translation ( #REF , 1997 ; #REF ; Gough , Way , and #REF )"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: â¢ language learning ( #REF ; #TARGET_REF ; Morgan , Meier , and #REF ) â¢ monolingual grammar induction ( #REF ) â¢ grammar optimization ( #REF ) â¢ insights into universal grammar ( #REF ) â¢ machine translation ( #REF , 1997 ; #REF ; Gough , Way , and #REF )\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ language learning ( #REF ; #REF ; Morgan , Meier , and #REF ) â¢ monolingual grammar induction ( #TARGET_REF ) â¢ grammar optimization ( #REF ) â¢ insights into universal grammar ( #REF ) â¢ machine translation ( #REF , 1997 ; #REF ; Gough , Way , and #REF )"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: â¢ language learning ( #REF ; #REF ; Morgan , Meier , and #REF ) â¢ monolingual grammar induction ( #TARGET_REF ) â¢ grammar optimization ( #REF ) â¢ insights into universal grammar ( #REF ) â¢ machine translation ( #REF , 1997 ; #REF ; Gough , Way , and #REF )\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For English â\\x88\\x92 > Urdu , #TARGET_REF , page 213 ) notes that \"the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For English â\\x88\\x92 > Urdu , #TARGET_REF , page 213 ) notes that \"the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #REF , #TARGET_REF , and #REF , inter alia ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #REF , #TARGET_REF , and #REF , inter alia .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This is then generalized , following a methodology based on #TARGET_REF , to generate the `` generalized marker lexicon . ''"
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: This is then generalized , following a methodology based on #TARGET_REF , to generate the `` generalized marker lexicon . ''\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ Learnability ( #REF ) â¢ Text generation ( #REF ; Milosavljevic , Tulloch , and #REF ) â¢ Speech generation ( #TARGET_REF ) â¢ Localization ( Sch Â¨ aler 1996 )"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: â¢ Learnability ( #REF ) â¢ Text generation ( #REF ; Milosavljevic , Tulloch , and #REF ) â¢ Speech generation ( #TARGET_REF ) â¢ Localization ( Sch Â¨ aler 1996 )\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Each set of translations is stored separately , and for each set the `` marker hypothesis '' ( #TARGET_REF ) is used to segment the phrasal lexicon into a `` marker lexicon . ''"
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Each set of translations is stored separately , and for each set the `` marker hypothesis '' ( #TARGET_REF ) is used to segment the phrasal lexicon into a `` marker lexicon . ''\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "That is, where #TARGET_REF substitutes variables for various words in his templates, we replace certain lexical items with their marker tag."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: That is, where #TARGET_REF substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Nevertheless , #TARGET_REF , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Nevertheless , #TARGET_REF , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #REF , #REF , and #TARGET_REF , inter alia ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #REF , #REF , and #TARGET_REF , inter alia .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "More specifically , the notion of the phrasal lexicon ( used first by #TARGET_REF ) has been used successfully in a number of areas :"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: More specifically , the notion of the phrasal lexicon ( used first by #TARGET_REF ) has been used successfully in a number of areas :\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "All EBMT systems , from the initial proposal by #TARGET_REF to the recent collection of #REF , are premised on the availability of subsentential alignments derived from the input bitext ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: All EBMT systems , from the initial proposal by #TARGET_REF to the recent collection of #REF , are premised on the availability of subsentential alignments derived from the input bitext .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF combines lexical and dependency mappings to form his generalizations ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF combines lexical and dependency mappings to form his generalizations .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The work of #TARGET_REF and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The work of #TARGET_REF and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some examples include text categorization ( #TARGET_REF ) , base noun phrase chunking ( #REF ) , part-of-speech tagging ( Engelson #REF ) , spelling confusion set disambiguation ( #REF ) , and word sense disambiguation ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some examples include text categorization ( #TARGET_REF ) , base noun phrase chunking ( #REF ) , part-of-speech tagging ( Engelson #REF ) , spelling confusion set disambiguation ( #REF ) , and word sense disambiguation ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Another technique for making better use of unlabeled data is cotraining ( #TARGET_REF ) , in which two sufficiently different learners help each other learn by labeling training data for one another ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Another technique for making better use of unlabeled data is cotraining ( #TARGET_REF ) , in which two sufficiently different learners help each other learn by labeling training data for one another .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and #REF ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #TARGET_REF ; #REF ) , and Collins 's Model 2 parser ( 1997 ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and #REF ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #TARGET_REF ; #REF ) , and Collins 's Model 2 parser ( 1997 ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Some well-known approaches include rule-based models ( #REF ) , backed-off models ( #TARGET_REF ) , and a maximumentropy model ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some well-known approaches include rule-based models ( #REF ) , backed-off models ( #TARGET_REF ) , and a maximumentropy model ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some examples include text categorization ( #REF ) , base noun phrase chunking ( #REF ) , part-of-speech tagging ( Engelson #REF ) , spelling confusion set disambiguation ( #TARGET_REF ) , and word sense disambiguation ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some examples include text categorization ( #REF ) , base noun phrase chunking ( #REF ) , part-of-speech tagging ( Engelson #REF ) , spelling confusion set disambiguation ( #TARGET_REF ) , and word sense disambiguation ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities ( #TARGET_REF ) , we can efficiently compute the probability of the sentence , P ( w | G ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities ( #TARGET_REF ) , we can efficiently compute the probability of the sentence , P ( w | G ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our algorithm is similar to the approach taken by #TARGET_REF for inducing PCFG parsers ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our algorithm is similar to the approach taken by #TARGET_REF for inducing PCFG parsers .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some examples include text categorization ( #REF ) , base noun phrase chunking ( #TARGET_REF ) , part-of-speech tagging ( Engelson #REF ) , spelling confusion set disambiguation ( #REF ) , and word sense disambiguation ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some examples include text categorization ( #REF ) , base noun phrase chunking ( #TARGET_REF ) , part-of-speech tagging ( Engelson #REF ) , spelling confusion set disambiguation ( #REF ) , and word sense disambiguation ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and #REF ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #REF ; #TARGET_REF ) , and Collins 's Model 2 parser ( 1997 ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and #REF ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #REF ; #TARGET_REF ) , and Collins 's Model 2 parser ( 1997 ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Current state-of-the-art statistical parsers ( #TARGET_REF ; #REF ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Current state-of-the-art statistical parsers ( #TARGET_REF ; #REF ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some well-known approaches include rule-based models ( #TARGET_REF ) , backed-off models ( #REF ) , and a maximumentropy model ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some well-known approaches include rule-based models ( #TARGET_REF ) , backed-off models ( #REF ) , and a maximumentropy model ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In the first experiment , we use an induction algorithm ( #TARGET_REFa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: In the first experiment , we use an induction algorithm ( #TARGET_REFa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Similar approaches are being explored for parsing ( Steedman , #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Similar approaches are being explored for parsing ( Steedman , #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The head words can be automatically extracted using a heuristic table lookup in the manner described by #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The head words can be automatically extracted using a heuristic table lookup in the manner described by #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Some well-known approaches include rule-based models ( #REF ) , backed-off models ( #REF ) , and a maximumentropy model ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some well-known approaches include rule-based models ( #REF ) , backed-off models ( #REF ) , and a maximumentropy model ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We follow the notation convention of #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We follow the notation convention of #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #TARGET_REF and #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #TARGET_REF and #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #REF ; #REF ; #REF ] , head-driven phrase structure grammar [ HPSG ] [ #REF ] , tree-adjoining grammar [ TAG ] [ #TARGET_REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #REF ; #REF ; #REF ] , head-driven phrase structure grammar [ HPSG ] [ #REF ] , tree-adjoining grammar [ TAG ] [ #TARGET_REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , #TARGET_REF questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , #TARGET_REF questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF attempts to improve on the approach of #REF by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF attempts to improve on the approach of #REF by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF , by comparison , employ 163 distinct predefined frames ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF , by comparison , employ 163 distinct predefined frames .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Unlike our approach , those of #TARGET_REF and Hockenmaier , Bierner , and #REF include a substantial initial correction and clean-up of the Penn-II trees ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Unlike our approach , those of #TARGET_REF and Hockenmaier , Bierner , and #REF include a substantial initial correction and clean-up of the Penn-II trees .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #TARGET_REF ; #REF ; Hockenmaier , Bierner , and #REF ; Nakanishi , Miyao , and #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #TARGET_REF ; #REF ; Hockenmaier , Bierner , and #REF ; Nakanishi , Miyao , and #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and #REF ) and ANLT ( #REF ) dictionaries and adding around 30 frames found by manual inspection ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and #REF ) and ANLT ( #REF ) dictionaries and adding around 30 frames found by manual inspection .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank ( #TARGET_REF ) and Penn Chinese Treebank (Xue, Chiou, and #REF), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (#REF) and Chinese (Burke, Lam, et al. 2004)."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank ( #TARGET_REF ) and Penn Chinese Treebank (Xue, Chiou, and #REF), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (#REF) and Chinese (Burke, Lam, et al. 2004).\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure ( #TARGET_REF ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure ( #TARGET_REF ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Lexical functional grammar ( #REF ; #REF ; #TARGET_REF ) is a member of the family of constraint-based grammars ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Lexical functional grammar ( #REF ; #REF ; #TARGET_REF ) is a member of the family of constraint-based grammars .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Lexical functional grammar ( #REF ; #TARGET_REF ; #REF ) is a member of the family of constraint-based grammars ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Lexical functional grammar ( #REF ; #TARGET_REF ; #REF ) is a member of the family of constraint-based grammars .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "According to #TARGET_REF , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: According to #TARGET_REF , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The extraction procedure utilizes a head percolation table as introduced by #TARGET_REF in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The extraction procedure utilizes a head percolation table as introduced by #TARGET_REF in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We applied lexical-redundancy rules ( #TARGET_REF ) to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We applied lexical-redundancy rules ( #TARGET_REF ) to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects.\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Both use the evaluation software and triple encoding presented in #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Both use the evaluation software and triple encoding presented in #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank ( #TARGET_REF ) , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank ( #TARGET_REF ) , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #TARGET_REF and #REF ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #TARGET_REF and #REF ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #REF ; #REF ; #REF ] , head-driven phrase structure grammar [ HPSG ] [ #REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #TARGET_REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #REF ; #REF ; #REF ] , head-driven phrase structure grammar [ HPSG ] [ #REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #TARGET_REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and #REF ; #TARGET_REF ; Sadler , van Genabith , and #REF ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and #REF ; #TARGET_REF ; Sadler , van Genabith , and #REF ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Recent work by #TARGET_REF on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recent work by #TARGET_REF on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In #REF and #TARGET_REF , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In #REF and #TARGET_REF , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #REF and #TARGET_REF ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #REF and #TARGET_REF ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF also presents a similar method for the extraction of a TAG from the Penn Treebank ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF also presents a similar method for the extraction of a TAG from the Penn Treebank .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #TARGET_REF ; #REF ; #REF ] , head-driven phrase structure grammar [ HPSG ] [ #REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #TARGET_REF ; #REF ; #REF ] , head-driven phrase structure grammar [ HPSG ] [ #REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It has been shown ( #TARGET_REF ) that the subcategorization tendencies of verbs vary across linguistic domains ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: It has been shown ( #TARGET_REF ) that the subcategorization tendencies of verbs vary across linguistic domains .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #REF ; #TARGET_REF ; Hockenmaier , Bierner , and #REF ; Nakanishi , Miyao , and #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #REF ; #TARGET_REF ; Hockenmaier , Bierner , and #REF ; Nakanishi , Miyao , and #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and #REF ) and ANLT ( #TARGET_REF ) dictionaries and adding around 30 frames found by manual inspection ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and #REF ) and ANLT ( #TARGET_REF ) dictionaries and adding around 30 frames found by manual inspection .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The annotation procedure is dependent on locating the head daughter , for which an amended version of #TARGET_REF is used ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The annotation procedure is dependent on locating the head daughter , for which an amended version of #TARGET_REF is used .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #REF ; #REF ; #TARGET_REF ] , head-driven phrase structure grammar [ HPSG ] [ #REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #REF ; #REF ; #TARGET_REF ] , head-driven phrase structure grammar [ HPSG ] [ #REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "However , more recent work ( #REF ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank ( #TARGET_REF ) , containing more than 1,000,000 words and 49,000 sentences ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However , more recent work ( #REF ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank ( #TARGET_REF ) , containing more than 1,000,000 words and 49,000 sentences .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #REF ; #REF ; #REF ] , head-driven phrase structure grammar [ HPSG ] [ #TARGET_REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #REF ; #REF ; #REF ] , head-driven phrase structure grammar [ HPSG ] [ #TARGET_REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Following Hockenmaier , Bierner , and #REF , #TARGET_REF , and Miyao , Ninomiya , and #REF , we extract a reference lexicon from Sections 02 -- 21 of the WSJ ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Following Hockenmaier , Bierner , and #REF , #TARGET_REF , and Miyao , Ninomiya , and #REF , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Lexical functional grammar ( #TARGET_REF ; #REF ; #REF ) is a member of the family of constraint-based grammars ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Lexical functional grammar ( #TARGET_REF ; #REF ; #REF ) is a member of the family of constraint-based grammars .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #REF and #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #REF and #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: #TARGET_REF argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "As noted above , it is well documented ( #TARGET_REF ) that subcategorization frames ( and their frequencies ) vary across domains ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: As noted above , it is well documented ( #TARGET_REF ) that subcategorization frames ( and their frequencies ) vary across domains .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In #TARGET_REF and #REF , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In #TARGET_REF and #REF , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: #TARGET_REF argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #REF ; #TARGET_REF ; #REF ] , head-driven phrase structure grammar [ HPSG ] [ #REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #REF ; #TARGET_REF ; #REF ] , head-driven phrase structure grammar [ HPSG ] [ #REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "As a generalization , #TARGET_REF notes that lexicons such as COMLEX tend to demonstrate high precision but low recall ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As a generalization , #TARGET_REF notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A previous work along this line is #TARGET_REF , which is based on weighted finite-state transducers ( FSTs ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A previous work along this line is #TARGET_REF , which is based on weighted finite-state transducers ( FSTs ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Viewed in this way , gradable adjectives are an extreme example of the \"efficiency of language\" ( #TARGET_REF ) : Far from meaning something concrete like \"larger than 8 cm\" -- a concept that would have very limited applicability -- or even something more general like \"larger than the average N , \"a word like large is applicable across a wide range of different situations ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Viewed in this way , gradable adjectives are an extreme example of the \"efficiency of language\" ( #TARGET_REF ) : Far from meaning something concrete like \"larger than 8 cm\" -- a concept that would have very limited applicability -- or even something more general like \"larger than the average N , \"a word like large is applicable across a wide range of different situations .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Cases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. , #TARGET_REF ) was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such that"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Cases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. , #TARGET_REF ) was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such that\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "3 The degree of precision of the measurement ( #TARGET_REF , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 3 The degree of precision of the measurement ( #TARGET_REF , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This is the strongest version of the sorites paradox ( e.g. , #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This is the strongest version of the sorites paradox ( e.g. , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #REF , #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #REF , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #TARGET_REF ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #TARGET_REF ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF ; also reported in #REF ) show that greater differences are most likely to be chosen , presumably because they are more striking ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF ; also reported in #REF ) show that greater differences are most likely to be chosen , presumably because they are more striking .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "While IA is generally thought to be consistent with findings on human language production ( #REF ; #REF ; #REF ; #TARGET_REF ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While IA is generally thought to be consistent with findings on human language production ( #REF ; #REF ; #REF ; #TARGET_REF ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Children use vague adjectives among their first dozens of words ( #REF ) and understand some of their intricacies as early as their 24th month ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Children use vague adjectives among their first dozens of words ( #REF ) and understand some of their intricacies as early as their 24th month ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #TARGET_REF ; #REF ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #TARGET_REF ; #REF ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #TARGET_REF , #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #TARGET_REF , #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Hermann and Deutsch ( 1976 ; also reported in #TARGET_REF ) show that greater differences are most likely to be chosen , presumably because they are more striking ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Hermann and Deutsch ( 1976 ; also reported in #TARGET_REF ) show that greater differences are most likely to be chosen , presumably because they are more striking .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Following #TARGET_REF , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "A similar problem is discussed in the psycholinguistics of interpretation ( #TARGET_REF ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A similar problem is discussed in the psycholinguistics of interpretation ( #TARGET_REF ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #TARGET_REF ] Chapter 8 ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #TARGET_REF ] Chapter 8 ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Common sense ( as well as the Gricean maxims ; #TARGET_REF ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Common sense ( as well as the Gricean maxims ; #TARGET_REF ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Similar things hold for multifaceted properties like intelligence ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Similar things hold for multifaceted properties like intelligence ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For some adjectives , including the ones that #TARGET_REF called evaluative ( as opposed to dimensional ) , this is clearly inadequate ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For some adjectives , including the ones that #TARGET_REF called evaluative ( as opposed to dimensional ) , this is clearly inadequate .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #TARGET_REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #TARGET_REF , Section 8.6.2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #TARGET_REF , Section 8.6.2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "While IA is generally thought to be consistent with findings on human language production ( #REF ; #REF ; #TARGET_REF ; #REF ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While IA is generally thought to be consistent with findings on human language production ( #REF ; #REF ; #TARGET_REF ; #REF ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "While IA is generally thought to be consistent with findings on human language production ( #REF ; #TARGET_REF ; #REF ; #REF ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While IA is generally thought to be consistent with findings on human language production ( #REF ; #TARGET_REF ; #REF ; #REF ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "(For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #TARGET_REF 1999, discussed in Section 7.2)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: (For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #TARGET_REF 1999, discussed in Section 7.2).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #TARGET_REF ) : The selected expression should also be felicitous ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #TARGET_REF ) : The selected expression should also be felicitous .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX , #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #REF ; cfXXX #TARGET_REF ; #REF , for other plans ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #REF ; cfXXX #TARGET_REF ; #REF , for other plans ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #REF ; #TARGET_REF ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #REF ; #TARGET_REF ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX , #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , consider a relational description ( cfXXX , #TARGET_REF ) involving a gradable adjective , as in the dog in the large shed ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , consider a relational description ( cfXXX , #TARGET_REF ) involving a gradable adjective , as in the dog in the large shed .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #TARGET_REF ; cfXXX #REF ; #REF , for other plans ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #TARGET_REF ; cfXXX #REF ; #REF , for other plans ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. , #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A more flexible approach is used by #TARGET_REF , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A more flexible approach is used by #TARGET_REF , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "While IA is generally thought to be consistent with findings on human language production ( #TARGET_REF ; #REF ; #REF ; #REF ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While IA is generally thought to be consistent with findings on human language production ( #TARGET_REF ; #REF ; #REF ; #REF ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #TARGET_REF , pages 10 -- 12 ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #TARGET_REF , pages 10 -- 12 ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF asked subjects to identify the target of a vague description in a visual scene ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF asked subjects to identify the target of a vague description in a visual scene .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #TARGET_REF ; also Section 8.1 of the present article ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #TARGET_REF ; also Section 8.1 of the present article ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #TARGET_REF ; see our Section 2 ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #TARGET_REF ; see our Section 2 ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The following four components have been identified as the key elements of a question related to patient care ( #TARGET_REF ) :"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The following four components have been identified as the key elements of a question related to patient care ( #TARGET_REF ) :\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #TARGET_REF )."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #TARGET_REF ).\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Although originally developed as a tool to assist in query formulation , #TARGET_REF pointed out that PICO frames can be employed to structure IR results for improving precision ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Although originally developed as a tool to assist in query formulation , #TARGET_REF pointed out that PICO frames can be employed to structure IR results for improving precision .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We first identified the most informative unigrams and bigrams using the information gain measure ( #TARGET_REF ) , and then selected only the positive outcome predictors using odds ratio ( #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We first identified the most informative unigrams and bigrams using the information gain measure ( #TARGET_REF ) , and then selected only the positive outcome predictors using odds ratio ( #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , #REF describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , #REF describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( #TARGET_REFb ) , but these features are also beyond the capabilities of current summarization systems ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( #TARGET_REFb ) , but these features are also beyond the capabilities of current summarization systems .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Previously , a user study ( #TARGET_REF ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Previously , a user study ( #TARGET_REF ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #TARGET_REF , #REF ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #TARGET_REF , #REF ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Second , software for utilizing this ontology already exists : MetaMap ( #REF ) identifies concepts in free text , and SemRep ( #TARGET_REF ) extracts relations between the concepts ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Second , software for utilizing this ontology already exists : MetaMap ( #REF ) identifies concepts in free text , and SemRep ( #TARGET_REF ) extracts relations between the concepts .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification ( #TARGET_REF ) , which can be described by the following equation:"
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification ( #TARGET_REF ) , which can be described by the following equation:\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Second , software for utilizing this ontology already exists : MetaMap ( #TARGET_REF ) identifies concepts in free text , and SemRep ( #REF ) extracts relations between the concepts ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Second , software for utilizing this ontology already exists : MetaMap ( #TARGET_REF ) identifies concepts in free text , and SemRep ( #REF ) extracts relations between the concepts .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The work of #TARGET_REF demonstrates that faceted queries can be converted into simple filtering constraints to boost precision ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The work of #TARGET_REF demonstrates that faceted queries can be converted into simple filtering constraints to boost precision .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Furthermore , the need to answer questions related to patient care at the point of service has been well studied and documented ( Covell , Uman , and #REF ; Gorman , Ash , and #REF ; #TARGET_REF , 2005 ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Furthermore , the need to answer questions related to patient care at the point of service has been well studied and documented ( Covell , Uman , and #REF ; Gorman , Ash , and #REF ; #TARGET_REF , 2005 ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #TARGET_REF and #REF."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #TARGET_REF and #REF.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "After much exploration , #TARGET_REF discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: After much exploration , #TARGET_REF discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We first identified the most informative unigrams and bigrams using the information gain measure ( #REF ) , and then selected only the positive outcome predictors using odds ratio ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We first identified the most informative unigrams and bigrams using the information gain measure ( #REF ) , and then selected only the positive outcome predictors using odds ratio ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #TARGET_REF ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #TARGET_REF .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #TARGET_REFa ) for a brief overview ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #TARGET_REFa ) for a brief overview .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity ( #TARGET_REF ; De #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity ( #TARGET_REF ; De #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Third , the paradigm of evidence-based medicine ( #TARGET_REF ) provides a task-based model of the clinical information-seeking process ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Third , the paradigm of evidence-based medicine ( #TARGET_REF ) provides a task-based model of the clinical information-seeking process .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #REF , #TARGET_REF ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #REF , #TARGET_REF ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Our knowledge extractors rely extensively on MetaMap ( #TARGET_REF ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Our knowledge extractors rely extensively on MetaMap ( #TARGET_REF ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This section , which elaborates on preliminary results reported in #TARGET_REF , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: This section , which elaborates on preliminary results reported in #TARGET_REF , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "The PICO framework ( #TARGET_REF ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The PICO framework ( #TARGET_REF ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Our re-ranking approach , like the approach to parse re-ranking of #TARGET_REF , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our re-ranking approach , like the approach to parse re-ranking of #TARGET_REF , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #TARGET_REF ) , which selects from likely assignments generated by a model which makes stronger independence assumptions ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #TARGET_REF ) , which selects from likely assignments generated by a model which makes stronger independence assumptions .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Following our previous work ( #TARGET_REF ; Althaus , Karamanis , and #REF ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Following our previous work ( #TARGET_REF ; Althaus , Karamanis , and #REF ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #TARGET_REF ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #TARGET_REF .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Such technologies require significant human input , and are difficult to create and maintain ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Such technologies require significant human input , and are difficult to create and maintain ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ Only an automatic evaluation was performed , which relied on having model responses ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: â¢ Only an automatic evaluation was performed , which relied on having model responses ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This method follows a traditional Information Retrieval paradigm ( #TARGET_REF ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: This method follows a traditional Information Retrieval paradigm ( #TARGET_REF ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "For the cases where retrieval took place , we used F-score ( #REF ; #TARGET_REF ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For the cases where retrieval took place , we used F-score ( #REF ; #TARGET_REF ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This situation suggests a response-automation approach that follows the document retrieval paradigm ( #TARGET_REF ) , where a new request is matched with existing response documents ( e-mails ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This situation suggests a response-automation approach that follows the document retrieval paradigm ( #TARGET_REF ) , where a new request is matched with existing response documents ( e-mails ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It is therefore no surprise that early attempts at response automation were knowledge-driven ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It is therefore no surprise that early attempts at response automation were knowledge-driven ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We then use the program Snob ( #REF ; #TARGET_REF ) to cluster these experiences ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We then use the program Snob ( #REF ; #TARGET_REF ) to cluster these experiences .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #TARGET_REF ) and case-based reasoning ( #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #TARGET_REF ) and case-based reasoning ( #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "7 We employed the LIBSVM package ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: 7 We employed the LIBSVM package ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #REF ) and case-based reasoning ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #REF ) and case-based reasoning ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "5 Significant bigrams are obtained using the n-gram statistics package NSP ( #TARGET_REF ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: 5 Significant bigrams are obtained using the n-gram statistics package NSP ( #TARGET_REF ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and #REF ; #TARGET_REF ; #REF ; Malik , Subramaniam , and #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and #REF ; #TARGET_REF ; #REF ; Malik , Subramaniam , and #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Two applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and #REF; #REF), and answer extraction in FAQs (Frequently Asked Questions) (#REF; #REF; Jijkoun and de #REF;  #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Two applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and #REF; #REF), and answer extraction in FAQs (Frequently Asked Questions) (#REF; #REF; Jijkoun and de #REF;  #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #TARGET_REF ; Barzilay , Elhadad , and #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #TARGET_REF ; Barzilay , Elhadad , and #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A common way to combine different models consists of selecting the model that is most confident regarding its decision ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A common way to combine different models consists of selecting the model that is most confident regarding its decision ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and #REF;#REF), and answer extraction in FAQs (Frequently Asked Questions) ( #TARGET_REF; #REF)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and #REF;#REF), and answer extraction in FAQs (Frequently Asked Questions) ( #TARGET_REF; #REF).\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and #REF ; Jijkoun and de #REF ) , or the corpus itself was significantly smaller than ours ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and #REF ; Jijkoun and de #REF ) , or the corpus itself was significantly smaller than ours ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ Only an automatic evaluation was performed , which relied on having model responses ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: â¢ Only an automatic evaluation was performed , which relied on having model responses ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In #TARGET_REFa ) we identified several systems that resemble ours in that they provide answers to queries ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In #TARGET_REFa ) we identified several systems that resemble ours in that they provide answers to queries .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In FAQs , #TARGET_REF employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In FAQs , #TARGET_REF employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and #REF ; Jijkoun and de #REF ) , or the corpus itself was significantly smaller than ours ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and #REF ; Jijkoun and de #REF ) , or the corpus itself was significantly smaller than ours ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and #REF ; #REF ; #TARGET_REF ; Malik , Subramaniam , and #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and #REF ; #REF ; #TARGET_REF ; Malik , Subramaniam , and #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Specifically , we used Decision Graphs ( #REF ) for Doc-Pred , and SVMs ( #TARGET_REF ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Specifically , we used Decision Graphs ( #REF ) for Doc-Pred , and SVMs ( #TARGET_REF ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #TARGET_REF to penalize redundant sentences in cohesive clusters ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #TARGET_REF to penalize redundant sentences in cohesive clusters .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The predictive model is a Decision Graph ( #TARGET_REF ) , which , like Snob , is based on the MML principle ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The predictive model is a Decision Graph ( #TARGET_REF ) , which , like Snob , is based on the MML principle .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "It is therefore no surprise that early attempts at response automation were knowledge-driven ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It is therefore no surprise that early attempts at response automation were knowledge-driven ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and #REF ; #TARGET_REF ; #REF ; Malik , Subramaniam , and #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and #REF ; #TARGET_REF ; #REF ; Malik , Subramaniam , and #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and #REF ; #REF ; #TARGET_REF ; Malik , Subramaniam , and #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and #REF ; #REF ; #TARGET_REF ; Malik , Subramaniam , and #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #TARGET_REF ) , but the simple binary bag-of-lemmas representation yielded similar results ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: 6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #TARGET_REF ) , but the simple binary bag-of-lemmas representation yielded similar results .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #TARGET_REF as follows ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #TARGET_REF as follows .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In Section 5 , we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system ( #TARGET_REFb ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: In Section 5 , we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system ( #TARGET_REFb ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "It is therefore no surprise that early attempts at response automation were knowledge-driven ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It is therefore no surprise that early attempts at response automation were knowledge-driven ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF compared two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF compared two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Following #TARGET_REF , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #REF ; Barzilay , Elhadad , and #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #REF ; Barzilay , Elhadad , and #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and #REF ; #TARGET_REF ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( #REF;#REF)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and #REF ; #TARGET_REF ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( #REF;#REF).\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Specifically , we used Decision Graphs ( #TARGET_REF ) for Doc-Pred , and SVMs ( #REF ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Specifically , we used Decision Graphs ( #TARGET_REF ) for Doc-Pred , and SVMs ( #REF ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "13 We also employed sequence-based measures using the ROUGE tool set ( #TARGET_REF ) , with similar results to those obtained with the word-by-word measures ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: 13 We also employed sequence-based measures using the ROUGE tool set ( #TARGET_REF ) , with similar results to those obtained with the word-by-word measures .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We then use the program Snob ( #TARGET_REF ; #REF ) to cluster these experiences ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We then use the program Snob ( #TARGET_REF ; #REF ) to cluster these experiences .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #TARGET_REFa ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #TARGET_REFa ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The question answering system developed by #TARGET_REF belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The question answering system developed by #TARGET_REF belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #TARGET_REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #TARGET_REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #TARGET_REF ) ; for equality constraints with slack , we use conjugate gradient ( #REF ) , noting that when A = 0 , the objective is not differentiable ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #TARGET_REF ) ; for equality constraints with slack , we use conjugate gradient ( #REF ) , noting that when A = 0 , the objective is not differentiable .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "See #TARGET_REF for further discussion ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: See #TARGET_REF for further discussion .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Many researchers use the GIZA + + software package ( #TARGET_REF ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Many researchers use the GIZA + + software package ( #TARGET_REF ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This is the approach taken by IBM Models 4 + ( #REFb ; #REF ) , and more recently by the LEAF model ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This is the approach taken by IBM Models 4 + ( #REFb ; #REF ) , and more recently by the LEAF model ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "5 The open source Moses ( #TARGET_REF ) toolkit from www.statmt.org/moses/ ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: 5 The open source Moses ( #TARGET_REF ) toolkit from www.statmt.org/moses/ .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "results are based on a corpus of movie subtitles ( #TARGET_REF ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: results are based on a corpus of movie subtitles ( #TARGET_REF ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and #REF ] and rules [ #REF ; #TARGET_REF ] ) as well as for MT system combination (Matusov, Ueffing, and #REF)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and #REF ] and rules [ #REF ; #TARGET_REF ] ) as well as for MT system combination (Matusov, Ueffing, and #REF).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding ( #TARGET_REF ; Liang , Taskar , and #REF ; Ganchev , and #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding ( #TARGET_REF ; Liang , Taskar , and #REF ; Ganchev , and #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We used a standard implementation of IBM Model 4 ( #TARGET_REF ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We used a standard implementation of IBM Model 4 ( #TARGET_REF ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #TARGET_REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "The standard approach is to train two models independently and then intersect their predictions ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The standard approach is to train two models independently and then intersect their predictions ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) ( #TARGET_REF ) :"
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) ( #TARGET_REF ) :\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This is the approach taken by IBM Models 4 + ( #REFb ; #TARGET_REF ) , and more recently by the LEAF model ( #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This is the approach taken by IBM Models 4 + ( #REFb ; #TARGET_REF ) , and more recently by the LEAF model ( #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In the context of word alignment , #TARGET_REF use a state-duration HMM in order to model word-to-phrase translations ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In the context of word alignment , #TARGET_REF use a state-duration HMM in order to model word-to-phrase translations .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "results are based on a corpus of movie subtitles ( #REF ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: results are based on a corpus of movie subtitles ( #REF ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This heuristic is called soft union ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: This heuristic is called soft union ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #TARGET_REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #TARGET_REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For MT the most commonly used heuristic is called grow diagonal final ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For MT the most commonly used heuristic is called grow diagonal final ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "PR is closely related to the work of #TARGET_REF , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: PR is closely related to the work of #TARGET_REF , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #TARGET_REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #TARGET_REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and #REF ] and rules [ #TARGET_REF ; #REF ] ) as well as for MT system combination (Matusov, Ueffing, and #REF)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and #REF ] and rules [ #TARGET_REF ; #REF ] ) as well as for MT system combination (Matusov, Ueffing, and #REF).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For the task of unsupervised dependency parsing , #TARGET_REF add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For the task of unsupervised dependency parsing , #TARGET_REF add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the agreement checker code developed by #TARGET_REF and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We use the agreement checker code developed by #TARGET_REF and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In comparison, the tag set of the Buckwalter Morphological Analyzer ( #TARGET_REF ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In comparison, the tag set of the Buckwalter Morphological Analyzer ( #TARGET_REF ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "For better comparison with work of others , we adopt the suggestion made by #TARGET_REF to evaluate the parsing quality on sentences up to 70 tokens long ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For better comparison with work of others , we adopt the suggestion made by #TARGET_REF to evaluate the parsing quality on sentences up to 70 tokens long .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #REF ; #REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #TARGET_REF ) and the CATiB ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #REF ; #REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #TARGET_REF ) and the CATiB ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the Columbia Arabic Treebank ( CATiB ) ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We use the Columbia Arabic Treebank ( CATiB ) ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , modeling CASE in Czech improves Czech parsing ( #TARGET_REF ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: For example , modeling CASE in Czech improves Czech parsing ( #TARGET_REF ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( #TARGET_REF ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( #TARGET_REF ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "18 In this article , we use a newer version of the corpus by #TARGET_REF than the one we used in Marton , Habash , and #REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: 18 In this article , we use a newer version of the corpus by #TARGET_REF than the one we used in Marton , Habash , and #REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In this article , we use an in-house system which provides functional gender , number , and rationality features ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: In this article , we use an in-house system which provides functional gender , number , and rationality features ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #REF ; #REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #REF ) and the CATiB ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #REF ; #REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #REF ) and the CATiB ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #TARGET_REF ) ( Section 6 ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #TARGET_REF ) ( Section 6 ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #TARGET_REF ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #TARGET_REF ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit ( #TARGET_REF ; Habash, Rambow, and #REF)15 (see Table 2, columns 5�7)."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit ( #TARGET_REF ; Habash, Rambow, and #REF)15 (see Table 2, columns 5�7).\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #REF , 2008 ; KÃ¼bler , McDonald , and #TARGET_REF ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term \"dev set\" to denote a non-blind test set, used for model development (feature selection and feature engineering)."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #REF , 2008 ; KÃ¼bler , McDonald , and #TARGET_REF ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term \"dev set\" to denote a non-blind test set, used for model development (feature selection and feature engineering).\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #TARGET_REF ) .19"
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #TARGET_REF ) .19\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) ( #TARGET_REF ) , the Buckwalter morphological analyzer (#REF), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (#REF;  Habash, Rambow, and #REF)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) ( #TARGET_REF ) , the Buckwalter morphological analyzer (#REF), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (#REF;  Habash, Rambow, and #REF).\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (#REF), the Buckwalter morphological analyzer (#REF), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit ( #TARGET_REF ;  Habash, Rambow, and #REF)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (#REF), the Buckwalter morphological analyzer (#REF), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit ( #TARGET_REF ;  Habash, Rambow, and #REF).\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #TARGET_REF ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #REF ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and #REF ] ) , both outperforming it : ( d ) Kulick , Gabbard , and #REF 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #TARGET_REF ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #REF ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and #REF ] ) , both outperforming it : ( d ) Kulick , Gabbard , and #REF 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (#REF), the Buckwalter morphological analyzer ( #TARGET_REF ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (#REF; Habash, Rambow, and #REF)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (#REF), the Buckwalter morphological analyzer ( #TARGET_REF ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (#REF; Habash, Rambow, and #REF).\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #REF ; #REF ) , the Prague Dependency Treebank ( PADT ) ( #TARGET_REF ; #REF ) and the CATiB ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #REF ; #REF ) , the Prague Dependency Treebank ( PADT ) ( #TARGET_REF ; #REF ) and the CATiB ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and #REF ; Nivre , Boguslavsky , and #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and #REF ; Nivre , Boguslavsky , and #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "The result holds for both the MaltParser ( #TARGET_REF ) and the Easy-First Parser ( #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The result holds for both the MaltParser ( #TARGET_REF ) and the Easy-First Parser ( #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #REF ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #TARGET_REF ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and #REF ] ) , both outperforming it : ( d ) Kulick , Gabbard , and #REF 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #REF ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #TARGET_REF ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and #REF ] ) , both outperforming it : ( d ) Kulick , Gabbard , and #REF 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "9 We do not relate to specific results in their study because it has been brought to our attention that #TARGET_REF are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: 9 We do not relate to specific results in their study because it has been brought to our attention that #TARGET_REF are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The result holds for both the MaltParser ( #REF ) and the Easy-First Parser ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The result holds for both the MaltParser ( #REF ) and the Easy-First Parser ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "A more detailed discussion of the various available Arabic tag sets can be found in #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A more detailed discussion of the various available Arabic tag sets can be found in #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For more information on CATiB , see #TARGET_REF and Habash , Faraj , and #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For more information on CATiB , see #TARGET_REF and Habash , Faraj , and #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some researchers , however , including #TARGET_REF , train on predicted feature values instead ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Some researchers , however , including #TARGET_REF , train on predicted feature values instead .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "19 The paper by #TARGET_REF presents additional , more sophisticated models that we do not use in this article ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: 19 The paper by #TARGET_REF presents additional , more sophisticated models that we do not use in this article .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #TARGET_REF ; #REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #REF ) and the CATiB ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #TARGET_REF ; #REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #REF ) and the CATiB ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "11 #TARGET_REF reports that non-projective and pseudo-projective algorithms outperform the `` eager  projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: 11 #TARGET_REF reports that non-projective and pseudo-projective algorithms outperform the `` eager  projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K�bler, McDonald, and #TARGET_REF describe a �typical� MaltParser model configuration of attributes and features.13"
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K�bler, McDonald, and #TARGET_REF describe a �typical� MaltParser model configuration of attributes and features.13\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #REF ; #TARGET_REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #REF ) and the CATiB ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #REF ; #TARGET_REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #REF ) and the CATiB ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #TARGET_REF , 2008 ; KÃ¼bler , McDonald , and #REF ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term \"dev set\" to denote a non-blind test set, used for model development (feature selection and feature engineering)."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #TARGET_REF , 2008 ; KÃ¼bler , McDonald , and #REF ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term \"dev set\" to denote a non-blind test set, used for model development (feature selection and feature engineering).\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF reports experiments on Arabic parsing using his MaltParser ( #TARGET_REF ) , trained on the PADT ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF reports experiments on Arabic parsing using his MaltParser ( #TARGET_REF ) , trained on the PADT .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Similarly , #TARGET_REF report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Similarly , #TARGET_REF report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "7 We ignore the rare \"false idafa\" construction ( #TARGET_REF , p. 102 ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 7 We ignore the rare \"false idafa\" construction ( #TARGET_REF , p. 102 ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and #REF ; Nivre , Boguslavsky , and #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and #REF ; Nivre , Boguslavsky , and #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This article represents an extension of our previous work on unsupervised event coreference resolution ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: This article represents an extension of our previous work on unsupervised event coreference resolution ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #REF , #REF , #TARGET_REF , #REF , #REF , #REF , and #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #REF , #REF , #TARGET_REF , #REF , #REF , #REF , and #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There is some literature on procedure acquisition such as the LISP synthesis work described in #REF and the PROLOG synthesis method of #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: There is some literature on procedure acquisition such as the LISP synthesis work described in #REF and the PROLOG synthesis method of #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #REF , â¢ a `` conditioning '' facility as described by #TARGET_REF , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #REF , â¢ a `` conditioning '' facility as described by #TARGET_REF , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "The problem of handling ill-formed input has been studied by #REF , #TARGET_REF , #REF , #REF , #REF , #REF , #REF , and #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The problem of handling ill-formed input has been studied by #REF , #TARGET_REF , #REF , #REF , #REF , #REF , #REF , and #REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The problem of handling ill-formed input has been studied by #REF , #REF , #REF , #REF , #REF , #TARGET_REF , #REF , and #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The problem of handling ill-formed input has been studied by #REF , #REF , #REF , #REF , #REF , #TARGET_REF , #REF , and #REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #REF , #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #REF , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF , assertional statements as in #REF , or semantic nets as in #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF , assertional statements as in #REF , or semantic nets as in #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Another dialogue acquisition system has been developed by #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Another dialogue acquisition system has been developed by #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , and #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , and #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The problem of handling ill-formed input has been studied by #REF , #REF , #REF , #TARGET_REF , #REF , #REF , #REF , and #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The problem of handling ill-formed input has been studied by #REF , #REF , #REF , #TARGET_REF , #REF , #REF , #REF , and #REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF , assertional statements as in #TARGET_REF , or semantic nets as in #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF , assertional statements as in #TARGET_REF , or semantic nets as in #REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #TARGET_REF where program flowcharts were constructed from traces of their behaviors ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #TARGET_REF where program flowcharts were constructed from traces of their behaviors .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph ( #TARGET_REF ) , a deep parse of Si , or some other representation ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph ( #TARGET_REF ) , a deep parse of Si , or some other representation .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #REF , #REF , #REF , #TARGET_REF , #REF , #REF , and #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #REF , #REF , #REF , #TARGET_REF , #REF , #REF , and #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #TARGET_REF , #REF , #REF , #REF , #REF , #REF , #REF , and #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #TARGET_REF , #REF , #REF , #REF , #REF , #REF , #REF , and #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The problem of handling ill-formed input has been studied by #REF , #REF , #TARGET_REF , #REF , #REF , #REF , #REF , and #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The problem of handling ill-formed input has been studied by #REF , #REF , #TARGET_REF , #REF , #REF , #REF , #REF , and #REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "[ The current system should be distinguished from an earlier voice system ( VNLC , #TARGET_REF ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: [ The current system should be distinguished from an earlier voice system ( VNLC , #TARGET_REF ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The expectation parser uses an ATN-like representation for its grammar ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The expectation parser uses an ATN-like representation for its grammar ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A number of speech understanding systems have been developed during the past fifteen years ( #TARGET_REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , and #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A number of speech understanding systems have been developed during the past fifteen years ( #TARGET_REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , and #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #TARGET_REF , â¢ a `` conditioning '' facility as described by #REF , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #TARGET_REF , â¢ a `` conditioning '' facility as described by #REF , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "The problem of handling ill-formed input has been studied by #TARGET_REF , #REF , #REF , #REF , #REF , #REF , #REF , and #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The problem of handling ill-formed input has been studied by #TARGET_REF , #REF , #REF , #REF , #REF , #REF , #REF , and #REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #TARGET_REF , assertional statements as in #REF , or semantic nets as in #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #TARGET_REF , assertional statements as in #REF , or semantic nets as in #REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #TARGET_REF , #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #TARGET_REF , #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #REF , #REF , #REF , #REF , #TARGET_REF , #REF , and #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #REF , #REF , #REF , #REF , #TARGET_REF , #REF , and #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "How it is done is beyond the scope of this paper but is explained in detail in #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: How it is done is beyond the scope of this paper but is explained in detail in #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There is some literature on procedure acquisition such as the LISP synthesis work described in #TARGET_REF and the PROLOG synthesis method of #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: There is some literature on procedure acquisition such as the LISP synthesis work described in #TARGET_REF and the PROLOG synthesis method of #REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #TARGET_REF , #REF , #REF , #REF , #REF , #REF , and #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #TARGET_REF , #REF , #REF , #REF , #REF , #REF , and #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper we focus on the exploitation of the LDOCE grammar coding system ; #REF and #TARGET_REF describe further research in Cambridge utilising different types of information available in LDOCE ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In this paper we focus on the exploitation of the LDOCE grammar coding system ; #REF and #TARGET_REF describe further research in Cambridge utilising different types of information available in LDOCE .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF and #REF provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF and #REF provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The research described below is taking place in the context of three collaborative projects ( #REF ; #REF ; #TARGET_REF ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The research described below is taking place in the context of three collaborative projects ( #REF ; #REF ; #TARGET_REF ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #TARGET_REF for further discussion ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #TARGET_REF for further discussion ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Clearly, there are other syntactic and semantic tests for this distinction, (see eg. #TARGET_REF:472 ), but these are the only ones which are explicit in the LDOCE coding system."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Clearly, there are other syntactic and semantic tests for this distinction, (see eg. #TARGET_REF:472 ), but these are the only ones which are explicit in the LDOCE coding system.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Two exceptions to this generalisation are the Linguistic String Project ( #REF ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( #REF ; #TARGET_REF ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Two exceptions to this generalisation are the Linguistic String Project ( #REF ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( #REF ; #TARGET_REF ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper we focus on the exploitation of the LDOCE grammar coding system ; #TARGET_REF and #REF describe further research in Cambridge utilising different types of information available in LDOCE ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In this paper we focus on the exploitation of the LDOCE grammar coding system ; #TARGET_REF and #REF describe further research in Cambridge utilising different types of information available in LDOCE .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "One approach to this problem is that taken by the ASCOT project ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One approach to this problem is that taken by the ASCOT project ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( #TARGET_REF and references therein ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( #TARGET_REF and references therein ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "One approach to this problem is that taken by the ASCOT project ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One approach to this problem is that taken by the ASCOT project ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Two exceptions to this generalisation are the Linguistic String Project ( #TARGET_REF ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( #REF ; #REF ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Two exceptions to this generalisation are the Linguistic String Project ( #TARGET_REF ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( #REF ; #REF ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #TARGET_REF ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #TARGET_REF ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition , #TARGET_REF note that our Object Raising rule would assign mean to this category incorrectly ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In addition , #TARGET_REF note that our Object Raising rule would assign mean to this category incorrectly .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #TARGET_REF for details ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #TARGET_REF for details ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( #REF ) , Lexical Functional Grammar ( LFG ) ( #TARGET_REF ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( #REFa ) , PATR-II ( #REF ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( #REF ) , Lexical Functional Grammar ( LFG ) ( #TARGET_REF ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( #REFa ) , PATR-II ( #REF ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This deficiency is rectified in the verb classification system employed by #TARGET_REF in the Brandeis verb catalogue ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This deficiency is rectified in the verb classification system employed by #TARGET_REF in the Brandeis verb catalogue .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The research described below is taking place in the context of three collaborative projects ( #REF ; #TARGET_REF ; #REF ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The research described below is taking place in the context of three collaborative projects ( #REF ; #TARGET_REF ; #REF ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #TARGET_REF , for further details ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #TARGET_REF , for further details ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF ; #REF ) consult relatively small lexicons , typically generated by hand ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF ; #REF ) consult relatively small lexicons , typically generated by hand .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #TARGET_REF , for further discussion ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #TARGET_REF , for further discussion ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "( #TARGET_REF contains further description and discussion of LDOCE . )"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: ( #TARGET_REF contains further description and discussion of LDOCE . )\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Expanding on a suggestion of #TARGET_REF , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Expanding on a suggestion of #TARGET_REF , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition to headwords , dictionary search through the pronunciation field is available ; #TARGET_REF has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: In addition to headwords , dictionary search through the pronunciation field is available ; #TARGET_REF has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #TARGET_REF , 1985 ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #TARGET_REF , 1985 ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF and #TARGET_REF provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF and #TARGET_REF provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( #REF ) , Lexical Functional Grammar ( LFG ) ( #REF ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( #REFa ) , PATR-II ( #TARGET_REF ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( #REF ) , Lexical Functional Grammar ( LFG ) ( #REF ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( #REFa ) , PATR-II ( #TARGET_REF ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition to headwords , dictionary search through the pronunciation field is available ; #REF has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In addition to headwords , dictionary search through the pronunciation field is available ; #REF has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( #TARGET_REF:460 ff . )"
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( #TARGET_REF:460 ff . )\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in #TARGET_REF and #REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in #TARGET_REF and #REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "As #TARGET_REF points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As #TARGET_REF points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The research described below is taking place in the context of three collaborative projects ( #TARGET_REF ; #REF ; #REF ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The research described below is taking place in the context of three collaborative projects ( #TARGET_REF ; #REF ; #REF ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Many investigators ( e.g. #TARGET_REF ; #REF ; #REF ; #REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Many investigators ( e.g. #TARGET_REF ; #REF ; #REF ; #REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "In previous work ( #REF ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In previous work ( #REF ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Many investigators ( e.g. #REF ; #REF ; #REF ; #TARGET_REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Many investigators ( e.g. #REF ; #REF ; #REF ; #TARGET_REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF and #REF also examine the relation between discourse and prosodic phrasing ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF and #REF also examine the relation between discourse and prosodic phrasing .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Sentences like 12 , from #TARGET_REF , are frequently cited ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Sentences like 12 , from #TARGET_REF , are frequently cited .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In previous work ( #TARGET_REF ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: In previous work ( #TARGET_REF ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The psycholinguistic studies of #REF , #REF , #REF , #REF , #REF , and #TARGET_REF , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The psycholinguistic studies of #REF , #REF , #REF , #REF , #REF , and #TARGET_REF , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #TARGET_REF , which \"are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option\" (p. 251) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: 3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #TARGET_REF , which \"are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option\" (p. 251) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In #TARGET_REF , this flattening process is not part of the grammar ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In #TARGET_REF , this flattening process is not part of the grammar .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "An alternative representation based on #TARGET_REF is presented in #REF , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: An alternative representation based on #TARGET_REF is presented in #REF , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The psycholinguistic studies of #REF , #TARGET_REF , #REF , #REF , #REF , and #REF , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The psycholinguistic studies of #REF , #TARGET_REF , #REF , #REF , #REF , and #REF , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Many investigators ( e.g. #REF ; #REF ; #TARGET_REF ; #REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Many investigators ( e.g. #REF ; #REF ; #TARGET_REF ; #REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #TARGET_REF use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #TARGET_REF use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous versions of our work , as described in #TARGET_REF also assume that phrasing is dependent on predicate-argument structure ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Previous versions of our work , as described in #TARGET_REF also assume that phrasing is dependent on predicate-argument structure .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "This observation has led some researchers , e.g. , #TARGET_REF , to claim a direct mapping between the syntactic phrase and the prosodic phrase ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This observation has led some researchers , e.g. , #TARGET_REF , to claim a direct mapping between the syntactic phrase and the prosodic phrase .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #TARGET_REF ( henceforth G&G ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #TARGET_REF ( henceforth G&G ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The relation between discourse and prosodic phrasing has been examined in some detail by #TARGET_REF , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The relation between discourse and prosodic phrasing has been examined in some detail by #TARGET_REF , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Our rules for phonological word formation are adopted , for the most part , from G & G , #TARGET_REF , and the account of monosyllabic destressing in #REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Our rules for phonological word formation are adopted , for the most part , from G & G , #TARGET_REF , and the account of monosyllabic destressing in #REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Secondly , the cooperative principle of #TARGET_REF , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Secondly , the cooperative principle of #TARGET_REF , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "We are going to make such a comparison with the theories proposed by J. #TARGET_REF , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. #REF , who are more interested in addressing psychological and cognitive aspects of discourse coherence ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: We are going to make such a comparison with the theories proposed by J. #TARGET_REF , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. #REF , who are more interested in addressing psychological and cognitive aspects of discourse coherence .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (#REF), \"domain closure as- sumption\" (ibid.), \"domain circumscription\" (cf. #TARGET_REF), and their kin."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (#REF), \"domain closure as- sumption\" (ibid.), \"domain circumscription\" (cf. #TARGET_REF), and their kin.\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "W. #TARGET_REF discussed sentences of the form * This is a chair but you can sit on it ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: W. #TARGET_REF discussed sentences of the form * This is a chair but you can sit on it .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Opposition (called \"adversative\" or \"contrary-to-expectation\" by #REF;cf. also #TARGET_REF , p. 672 ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Opposition (called \"adversative\" or \"contrary-to-expectation\" by #REF;cf. also #TARGET_REF , p. 672 ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Although there are other discussions of the paragraph as a central element of discourse ( e.g. #REF , #TARGET_REF , #REF , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Although there are other discussions of the paragraph as a central element of discourse ( e.g. #REF , #TARGET_REF , #REF , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ; #TARGET_REF , p. 546 ) gives 10 definitions of a sentence ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ; #TARGET_REF , p. 546 ) gives 10 definitions of a sentence .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This Principle of Finitism is also assumed by #TARGET_REF , #REF , #REF , and implicitly or explicitly by almost all researchers in computational linguistics ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This Principle of Finitism is also assumed by #TARGET_REF , #REF , #REF , and implicitly or explicitly by almost all researchers in computational linguistics .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of #TARGET_REF , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of #TARGET_REF , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Other factors , such as the role of focus ( #REF , 1978 ; #REF ) or quantifier scoping ( #TARGET_REF ) must play a role , too ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other factors , such as the role of focus ( #REF , 1978 ; #REF ) or quantifier scoping ( #TARGET_REF ) must play a role , too .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "According to #TARGET_REF , p. 67 ) , these two sentences are incoherent ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: According to #TARGET_REF , p. 67 ) , these two sentences are incoherent .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "For instance , relating \"they\" to \"apples\" in the sentence ( cfXXX #TARGET_REF p. 195 ; #REFa ) : We bought the boys apples because they were so cheap"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For instance , relating \"they\" to \"apples\" in the sentence ( cfXXX #TARGET_REF p. 195 ; #REFa ) : We bought the boys apples because they were so cheap\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #REF and #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #REF and #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally , it has been shown by #REF that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #TARGET_REF strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Finally , it has been shown by #REF that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #TARGET_REF strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #TARGET_REF , the `` diagnosis from first principles '' of #REF , `` explainability '' of #REF , and the subset principle of #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #TARGET_REF , the `` diagnosis from first principles '' of #REF , `` explainability '' of #REF , and the subset principle of #REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The reader may consult recent papers on this subject ( e.g. #REF ; #TARGET_REF ) to see what a formal interpretation of events in time might look like ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The reader may consult recent papers on this subject ( e.g. #REF ; #TARGET_REF ) to see what a formal interpretation of events in time might look like .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "According to #TARGET_REF , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: According to #TARGET_REF , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #REF , the `` diagnosis from first principles '' of #TARGET_REF , `` explainability '' of #REF , and the subset principle of #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #REF , the `` diagnosis from first principles '' of #TARGET_REF , `` explainability '' of #REF , and the subset principle of #REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing \"there exists\" by \"all\" in the above definitions (cf. #TARGET_REFb )."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing \"there exists\" by \"all\" in the above definitions (cf. #TARGET_REFb ).\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Adding selectional restrictions ( semantic feature information , #TARGET_REF ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Adding selectional restrictions ( semantic feature information , #TARGET_REF ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The reader may consult recent papers on this subject ( e.g. #TARGET_REF ; #REF ) to see what a formal interpretation of events in time might look like ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The reader may consult recent papers on this subject ( e.g. #TARGET_REF ; #REF ) to see what a formal interpretation of events in time might look like .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Although there are other discussions of the paragraph as a central element of discourse ( e.g. #REF , #REF , #TARGET_REF , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Although there are other discussions of the paragraph as a central element of discourse ( e.g. #REF , #REF , #TARGET_REF , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We have shown elsewhere ( #TARGET_REF ; #REFa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: We have shown elsewhere ( #TARGET_REF ; #REFa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "The necessity of this kind of merging of arguments has been recognized before : #TARGET_REF call it abductive unification/matching , Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The necessity of this kind of merging of arguments has been recognized before : #TARGET_REF call it abductive unification/matching , Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The logical notation of #TARGET_REF is more sophisticated , and may be considered another possibility ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The logical notation of #TARGET_REF is more sophisticated , and may be considered another possibility .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This Principle of Finitism is also assumed by #REF , #TARGET_REF , #REF , and implicitly or explicitly by almost all researchers in computational linguistics ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This Principle of Finitism is also assumed by #REF , #TARGET_REF , #REF , and implicitly or explicitly by almost all researchers in computational linguistics .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #TARGET_REF:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #TARGET_REF:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "An example of psycholinguistically oriented research work can be found in #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: An example of psycholinguistically oriented research work can be found in #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only \"petty conversational implicature\" ( #TARGET_REF ) , or the metarules of Section 5.2?"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only \"petty conversational implicature\" ( #TARGET_REF ) , or the metarules of Section 5.2?\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of #REF, the \"diagnosis from first principles\" of #REF, \"explainability\" of #REF, and the subset principle of #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of #REF, the \"diagnosis from first principles\" of #REF, \"explainability\" of #REF, and the subset principle of #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Although there are other discussions of the paragraph as a central element of discourse ( e.g. #REF , #REF , #REF , #TARGET_REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Although there are other discussions of the paragraph as a central element of discourse ( e.g. #REF , #REF , #REF , #TARGET_REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Other factors , such as the role of focus ( #REF , 1978 ; #TARGET_REF ) or quantifier scoping ( #REF ) must play a role , too ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other factors , such as the role of focus ( #REF , 1978 ; #TARGET_REF ) or quantifier scoping ( #REF ) must play a role , too .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some of the intuitions we associate with this notion have been very well expressed by #TARGET_REF , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some of the intuitions we associate with this notion have been very well expressed by #TARGET_REF , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #REF ; #TARGET_REF ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: `` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #REF ; #TARGET_REF ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Other factors , such as the role of focus ( #TARGET_REF , 1978 ; #REF ) or quantifier scoping ( #REF ) must play a role , too ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other factors , such as the role of focus ( #TARGET_REF , 1978 ; #REF ) or quantifier scoping ( #REF ) must play a role , too .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #TARGET_REF , p. 329 ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #TARGET_REF , p. 329 ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                " #TARGET_REF ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0:  #TARGET_REF ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #TARGET_REF ; #REF ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: `` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #TARGET_REF ; #REF ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #TARGET_REF ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #TARGET_REF .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Opposition ( called \"adversative\" or \"contrary-to-expectation\" by #TARGET_REF ; cfXXX also #REF , p. 672 ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Opposition ( called \"adversative\" or \"contrary-to-expectation\" by #TARGET_REF ; cfXXX also #REF , p. 672 ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF , #REF , #REF , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF , #REF , #REF , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This Principle of Finitism is also assumed by #REF , #REF , #TARGET_REF , and implicitly or explicitly by almost all researchers in computational linguistics ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This Principle of Finitism is also assumed by #REF , #REF , #TARGET_REF , and implicitly or explicitly by almost all researchers in computational linguistics .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #TARGET_REF and #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #TARGET_REF and #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #TARGET_REFb ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #TARGET_REFb ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Later , #TARGET_REF , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Later , #TARGET_REF , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The necessity of this kind of merging of arguments has been recognized before : #REF call it abductive unification/matching , #TARGET_REF , 1979 ) refers to such operations using the terms knitting or petty conversational implicature ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The necessity of this kind of merging of arguments has been recognized before : #REF call it abductive unification/matching , #TARGET_REF , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #TARGET_REF describes how first order logic can be augmented with such an operator ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #TARGET_REF describes how first order logic can be augmented with such an operator .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For instance , #TARGET_REF , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: For instance , #TARGET_REF , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Representative systems are described in #REF , De #REF , #REF , #TARGET_REF , and #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Representative systems are described in #REF , De #REF , #REF , #TARGET_REF , and #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A formula for the test set perplexity ( #TARGET_REF ) is :13"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A formula for the test set perplexity ( #TARGET_REF ) is :13\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Subsequent processing by the natural language and response generation components was done automatically by the computer ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Subsequent processing by the natural language and response generation components was done automatically by the computer ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This approach resembles the work by #REF and #TARGET_REF on selectional restrictions ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This approach resembles the work by #REF and #TARGET_REF on selectional restrictions .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The search algorithm is the standard Viterbi search ( #TARGET_REF ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The search algorithm is the standard Viterbi search ( #TARGET_REF ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Semantic filters can also be used to prevent multiple versions of the same case frame ( #TARGET_REF ) showing up as complements ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Semantic filters can also be used to prevent multiple versions of the same case frame ( #TARGET_REF ) showing up as complements .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Representative systems are described in #TARGET_REF , De #REF , #REF , #REF , and #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Representative systems are described in #TARGET_REF , De #REF , #REF , #REF , and #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "However , the method we are currently using in the ATIS domain ( #TARGET_REF ) represents our most promising approach to this problem ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: However , the method we are currently using in the ATIS domain ( #TARGET_REF ) represents our most promising approach to this problem .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "The second one, ATIS ( #TARGET_REF et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The second one, ATIS ( #TARGET_REF et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This approach resembles the work by #TARGET_REF and #REF on selectional restrictions ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This approach resembles the work by #TARGET_REF and #REF on selectional restrictions .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( #TARGET_REF ) by its generator ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( #TARGET_REF ) by its generator .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "One , the VOYAGER domain ( #TARGET_REF ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: One , the VOYAGER domain ( #TARGET_REF ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The example used to illustrate the power of ATNs ( #TARGET_REF ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The example used to illustrate the power of ATNs ( #TARGET_REF ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Representative systems are described in #REF , De #REF , #TARGET_REF , #REF , and #REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Representative systems are described in #REF , De #REF , #TARGET_REF , #REF , and #REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The second version ( RM ) concerns the Resource Management task ( #TARGET_REF ) that has been popular within the DARPA community in recent years ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The second version ( RM ) concerns the Resource Management task ( #TARGET_REF ) that has been popular within the DARPA community in recent years .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "For the A * algorithm ( #TARGET_REF ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For the A * algorithm ( #TARGET_REF ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The recognizer for these systems is the SUMMIT system ( #TARGET_REF ) , which uses a segmental-based framework and includes an auditory model in the front-end processing ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The recognizer for these systems is the SUMMIT system ( #TARGET_REF ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The gap mechanism resembles the Hold register idea of ATNs ( #TARGET_REF ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( #REF , p. 235 ff . ), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The gap mechanism resembles the Hold register idea of ATNs ( #TARGET_REF ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( #REF , p. 235 ff . ), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes.\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Representative systems are described in #REF , De #REF , #REF , #REF , and #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Representative systems are described in #REF , De #REF , #REF , #REF , and #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #TARGET_REF ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( #REF , #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #TARGET_REF ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( #REF , #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #TARGET_REF ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #TARGET_REF ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The formalization of DLRs provided by #TARGET_REF defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The formalization of DLRs provided by #TARGET_REF defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The reader is referred to #TARGET_REF for a more detailed discussion of our use of constraint propagation."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The reader is referred to #TARGET_REF for a more detailed discussion of our use of constraint propagation.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This approach is taken , for example , in LKB ( #REF ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #TARGET_REF , 31 ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This approach is taken , for example , in LKB ( #REF ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #TARGET_REF , 31 ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "27 #TARGET_REF argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 27 #TARGET_REF argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (#REF) that also use lexical rules such as the Complement Extraction Lexical Rule (#REF) or the Complement Extraction Lexical Rule ( #TARGET_REF ) to operate on those raised elements."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: /b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (#REF) that also use lexical rules such as the Complement Extraction Lexical Rule (#REF) or the Complement Extraction Lexical Rule ( #TARGET_REF ) to operate on those raised elements.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #REF ; #REF ; #TARGET_REF ) can be used to circumvent constraint propagation ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #REF ; #REF ; #TARGET_REF ) can be used to circumvent constraint propagation .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "12 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 12 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "6 The Partial-VP Topicalization Lexical Rule proposed by #TARGET_REF , 10 ) is a linguistic example ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 6 The Partial-VP Topicalization Lexical Rule proposed by #TARGET_REF , 10 ) is a linguistic example .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "4 This interpretation of the signature is sometimes referred to as closed world ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 4 This interpretation of the signature is sometimes referred to as closed world ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "As shown in #TARGET_REF this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: As shown in #TARGET_REF this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #REF; #REF; #REF; #TARGET_REF ; #REF)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #REF; #REF; #REF; #TARGET_REF ; #REF).\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "A similar method is included in PATR-II ( #TARGET_REF ) and can be used to encode lexical rules as binary relations in the CUF system ( #REF ; #REFb ) or the TFS system ( #REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A similar method is included in PATR-II ( #TARGET_REF ) and can be used to encode lexical rules as binary relations in the CUF system ( #REF ; #REFb ) or the TFS system ( #REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "A similar method is included in PATR-II ( #REF ) and can be used to encode lexical rules as binary relations in the CUF system ( #REF ; #REFb ) or the TFS system ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A similar method is included in PATR-II ( #REF ) and can be used to encode lexical rules as binary relations in the CUF system ( #REF ; #REFb ) or the TFS system ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #REF ; #TARGET_REF ) and the ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #REF ; #TARGET_REF ) and the .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #TARGET_REF ; #REFa ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #TARGET_REF ; #REFa ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #TARGET_REF ; #REF ) and the ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #TARGET_REF ; #REF ) and the .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "/b4home.html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #TARGET_REF ) that also use lexical rules such as the Complement Extraction Lexical Rule (#REF) or the Complement Cliticization Lexical Rule (#REF) to operate on those raised elements."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: /b4home.html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #TARGET_REF ) that also use lexical rules such as the Complement Extraction Lexical Rule (#REF) or the Complement Cliticization Lexical Rule (#REF) to operate on those raised elements.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A common computational treatment of lexical rules adopted , for example , in the ALE system ( #TARGET_REF ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A common computational treatment of lexical rules adopted , for example , in the ALE system ( #TARGET_REF ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "A logic that provides the formal architecture required by #REF was defined by #TARGET_REF , 1994 ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A logic that provides the formal architecture required by #REF was defined by #TARGET_REF , 1994 ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The unfolding transformation is also referred to as partial execution , for example , by #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The unfolding transformation is also referred to as partial execution , for example , by #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A similar method is included in PATR-II ( #REF ) and can be used to encode lexical rules as binary relations in the CUF system ( #REF ; #REFb ) or the TFS system ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A similar method is included in PATR-II ( #REF ) and can be used to encode lexical rules as binary relations in the CUF system ( #REF ; #REFb ) or the TFS system ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This description can then be given the standard set-theoretical interpretation of #TARGET_REF , 1994 ) . '"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This description can then be given the standard set-theoretical interpretation of #TARGET_REF , 1994 ) . '\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "11 #TARGET_REF proposes to unify these two steps by including an update operator in the description language."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 11 #TARGET_REF proposes to unify these two steps by including an update operator in the description language.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #TARGET_REF ; #REF ; #REF ) can be used to circumvent constraint propagation ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #TARGET_REF ; #REF ; #REF ) can be used to circumvent constraint propagation .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The powerful mechanism of lexical rules ( #TARGET_REF ) has been used in many natural language processing systems ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The powerful mechanism of lexical rules ( #TARGET_REF ) has been used in many natural language processing systems .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( #TARGET_REF ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( #TARGET_REF ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Using an accumulator passing technique ( #TARGET_REF ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Using an accumulator passing technique ( #TARGET_REF ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "15 #TARGET_REF show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 15 #TARGET_REF show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #REF ) that also use lexical rules such as the Complement Extraction Lexical Rule ( #REF ) or the Complement Cliticization Lexical Rule ( #TARGET_REF ) to operate on those raised elements ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #REF ) that also use lexical rules such as the Complement Extraction Lexical Rule ( #REF ) or the Complement Cliticization Lexical Rule ( #TARGET_REF ) to operate on those raised elements .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "16 A linguistic example based on the signature given by #TARGET_REF would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given by Pollard and Sag (1994, p. 360, fn. 20)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 16 A linguistic example based on the signature given by #TARGET_REF would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given by Pollard and Sag (1994, p. 360, fn. 20).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #TARGET_REF , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #TARGET_REF , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "In a number of proposals , lexical generalizations are captured using lexical underspecification ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF; #REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In a number of proposals , lexical generalizations are captured using lexical underspecification ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF ; #REF ; #REF ; #REF; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF ; #REF ; #REF ; #REF; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (#REF) that also use lexical rules such as the Complement Extraction Lexical Rule ( #TARGET_REF ) or the Complement Cliticization Lexical Rule (#REF) to operate on those raised elements."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: /b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (#REF) that also use lexical rules such as the Complement Extraction Lexical Rule ( #TARGET_REF ) or the Complement Cliticization Lexical Rule (#REF) to operate on those raised elements.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Based on the research results reported in #TARGET_REF , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Based on the research results reported in #TARGET_REF , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "This approach is taken , for example , in LKB ( #TARGET_REF ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #REF , 31 ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This approach is taken , for example , in LKB ( #TARGET_REF ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #REF , 31 ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "However , as discussed by #TARGET_REF , creating several instances of lexical rules can be avoided ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: However , as discussed by #TARGET_REF , creating several instances of lexical rules can be avoided .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #TARGET_REF , ch ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #TARGET_REF , ch .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In a number of proposals , lexical generalizations are captured using lexical underspecification ( #REF ; #TARGET_REF ; #REF; #REF; #REF; #REF; #REF)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In a number of proposals , lexical generalizations are captured using lexical underspecification ( #REF ; #TARGET_REF ; #REF; #REF; #REF; #REF; #REF).\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #TARGET_REF , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #TARGET_REF , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; #REF; #REF) and the description-level lexical rules ( DLRs ; #TARGET_REF )"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; #REF; #REF) and the description-level lexical rules ( DLRs ; #TARGET_REF )\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For this mention-pair coreference model Ï ( u , v ) , we use the same set of features used in #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For this mention-pair coreference model Ï ( u , v ) , we use the same set of features used in #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For Berkeley system , we use the reported results from #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For Berkeley system , we use the reported results from #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( #REF ) , Berkeley system ( #TARGET_REF ) and HOTCoref system ( Bj Â¨ orkelund and #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( #REF ) , Berkeley system ( #TARGET_REF ) and HOTCoref system ( Bj Â¨ orkelund and #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We then use Illinois Chunker ( #REF ) 6 to extract more noun phrases from the text and employ Collins head rules ( #TARGET_REF ) to identify their heads ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We then use Illinois Chunker ( #REF ) 6 to extract more noun phrases from the text and employ Collins head rules ( #TARGET_REF ) to identify their heads .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #TARGET_REF ; #REF ; #REF ; Bj Â¨ orkelund and #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #TARGET_REF ; #REF ; #REF ; Bj Â¨ orkelund and #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #TARGET_REF ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #TARGET_REF .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #TARGET_REF ) , contains 3,145 annotated documents ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #TARGET_REF ) , contains 3,145 annotated documents .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Developed Systems Our developed system is built on the work by #TARGET_REF , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Developed Systems Our developed system is built on the work by #TARGET_REF , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #TARGET_REF in our experiments ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #TARGET_REF in our experiments .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "More details can be found in #TARGET_REF et al. (2013)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: More details can be found in #TARGET_REF et al. (2013).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF model entity coreference and event coreference jointly ; #TARGET_REF consider joint coreference and entity-linking ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF model entity coreference and event coreference jointly ; #TARGET_REF consider joint coreference and entity-linking .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #TARGET_REF ) with gold constituency parsing information and gold named entity information ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #TARGET_REF ) with gold constituency parsing information and gold named entity information .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our work is inspired by the latent left-linking model in #TARGET_REF and the ILP formulation from #REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Our work is inspired by the latent left-linking model in #TARGET_REF and the ILP formulation from #REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We can define PCAT using a probabilistic grammar ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: We can define PCAT using a probabilistic grammar ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF 's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF 's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to estimate the parameters of our model , we develop a blocked sampler based on that of #TARGET_REF to sample parse trees for sentences in the raw training corpus according to their posterior probabilities ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: In order to estimate the parameters of our model , we develop a blocked sampler based on that of #TARGET_REF to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluated on the English CCGBank ( #REF ) , which is a transformation of the Penn Treebank ( #TARGET_REF ) ; the CTBCCG ( #REF ) transformation of the Penn Chinese Treebank ( #REF ) ; and the CCG-TUT corpus ( #REF ) , built from the TUT corpus of Italian text ( #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We evaluated on the English CCGBank ( #REF ) , which is a transformation of the Penn Treebank ( #TARGET_REF ) ; the CTBCCG ( #REF ) transformation of the Penn Chinese Treebank ( #REF ) ; and the CCG-TUT corpus ( #REF ) , built from the TUT corpus of Italian text ( #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the same splits as #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We use the same splits as #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This is similar to the \"deletion\" strategy employed by #TARGET_REF , but we do it directly in the grammar ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This is similar to the \"deletion\" strategy employed by #TARGET_REF , but we do it directly in the grammar .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #TARGET_REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #TARGET_REF ; #REF ) .4"
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #TARGET_REF ; #REF ) .4\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Since we are not generating from the model , this does not introduce difficulties ( #TARGET_REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Since we are not generating from the model , this does not introduce difficulties ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "We further add rules for combining with punctuation to the left and right and allow for the merge rule X â X X of #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We further add rules for combining with punctuation to the left and right and allow for the merge rule X â X X of #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by #REF and used by #TARGET_REF that samples entire parse trees ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by #REF and used by #TARGET_REF that samples entire parse trees .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our strategy is based on the approach presented by #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Our strategy is based on the approach presented by #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "One important example is the constituentcontext model ( CCM ) of #TARGET_REF , which was specifically designed to capture the linguistic observation made by #REF that there are regularities to the contexts in which constituents appear ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One important example is the constituentcontext model ( CCM ) of #TARGET_REF , which was specifically designed to capture the linguistic observation made by #REF that there are regularities to the contexts in which constituents appear .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We follow #TARGET_REF in allowing a small set of generic , linguistically-plausible unary and binary grammar rules ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We follow #TARGET_REF in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #TARGET_REF ) for a recent example with further references ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Previous work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #TARGET_REF ) for a recent example with further references ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The SPR uses rules automatically learned from training data , using techniques similar to ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The SPR uses rules automatically learned from training data , using techniques similar to ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "These operations are not domain-specific and are similar to those of previous aggregation components ( #REF ; #TARGET_REF ; #REF ) , although the various MERGE operations are , to our knowledge , novel in this form ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: These operations are not domain-specific and are similar to those of previous aggregation components ( #REF ; #TARGET_REF ; #REF ) , although the various MERGE operations are , to our knowledge , novel in this form .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The representations used by #REF , #REF , or #TARGET_REF are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The representations used by #REF , #REF , or #TARGET_REF are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "2The algorithm was implemented by the the authors , following the description in #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: 2The algorithm was implemented by the the authors , following the description in #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ The regular TBL , as described in section 2 ; â¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â¢ The FastTBL algorithm ; â¢ The ICA algorithm ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: â¢ The regular TBL , as described in section 2 ; â¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â¢ The FastTBL algorithm ; â¢ The ICA algorithm ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The ICA system ( #TARGET_REF ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The ICA system ( #TARGET_REF ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Using the tree-cut technique described above , our previous work ( #TARGET_REF ) extracted systematic polysemy from WordNet ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Using the tree-cut technique described above , our previous work ( #TARGET_REF ) extracted systematic polysemy from WordNet .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "In our previous work ( #TARGET_REF ) , we applied this method to a small subset of WordNet nouns and showed potential applicability ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: In our previous work ( #TARGET_REF ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to obtain semantic representations of each word , we apply our previous strategy ( #TARGET_REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: In order to obtain semantic representations of each word , we apply our previous strategy ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( #REFa ; #TARGET_REF ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( #REF ; #REFb ) and made freely available in the openSMILE Toolkit ( #REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( #REFa ; #TARGET_REF ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( #REF ; #REFb ) and made freely available in the openSMILE Toolkit ( #REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Finite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation ( #REF ) , as have been bilingual stochastic grammars ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Finite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation ( #REF ) , as have been bilingual stochastic grammars ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It also shows the structural identity to bilingual grammars as used in ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: It also shows the structural identity to bilingual grammars as used in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "It can be shown ( #TARGET_REF ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: It can be shown ( #TARGET_REF ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "As ( #TARGET_REF ) show , lexical information improves on NP and VP chunking as well ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: As ( #TARGET_REF ) show , lexical information improves on NP and VP chunking as well .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "The system was trained on the Penn Treebank ( #REF ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #TARGET_REF , #REF , and #REF , and became a common testbed ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The system was trained on the Penn Treebank ( #REF ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #TARGET_REF , #REF , and #REF , and became a common testbed .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , #REF , #REF , #TARGET_REF , #REF , and #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , #REF , #REF , #TARGET_REF , #REF , and #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It is not aimed at handling dependencies , which require heavy use of lexical information ( #TARGET_REF , for PP attachment ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: It is not aimed at handling dependencies , which require heavy use of lexical information ( #TARGET_REF , for PP attachment ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Another approach for partial parsing was presented by #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Another approach for partial parsing was presented by #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , #REF , #REF , #REF , #TARGET_REF , and #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , #REF , #REF , #REF , #TARGET_REF , and #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Our results are lower than those of full parsers , e.g. , #TARGET_REF as might be expected since much less structural data , and no lexical data are being used ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our results are lower than those of full parsers , e.g. , #TARGET_REF as might be expected since much less structural data , and no lexical data are being used .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In a similar vain to #REF and #TARGET_REF , the method extends an existing flat shallow-parsing method to handle composite structures ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: In a similar vain to #REF and #TARGET_REF , the method extends an existing flat shallow-parsing method to handle composite structures .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , #REF , #TARGET_REF , #REF , #REF , and #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , #REF , #TARGET_REF , #REF , #REF , and #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The system was trained on the Penn Treebank ( #REF ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #REF , #REF , and #TARGET_REF , and became a common testbed ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The system was trained on the Penn Treebank ( #REF ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #REF , #REF , and #TARGET_REF , and became a common testbed .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The system was trained on the Penn Treebank ( #REF ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #REF , #TARGET_REF , and #REF , and became a common testbed ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The system was trained on the Penn Treebank ( #REF ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #REF , #TARGET_REF , and #REF , and became a common testbed .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "One approach to partial parsing was presented by #TARGET_REF , who extended a shallow-parsing technique to partial parsing ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One approach to partial parsing was presented by #TARGET_REF , who extended a shallow-parsing technique to partial parsing .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In a similar vain to #TARGET_REF and #REF , the method extends an existing flat shallow-parsing method to handle composite structures ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: In a similar vain to #TARGET_REF and #REF , the method extends an existing flat shallow-parsing method to handle composite structures .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "To quantify the relative strengths of these transitive inferences , #TARGET_REF propose to assign a weight to each link ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: To quantify the relative strengths of these transitive inferences , #TARGET_REF propose to assign a weight to each link .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #TARGET_REF ) could be applied to extract semantic classes from the corpus itself ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #TARGET_REF ) could be applied to extract semantic classes from the corpus itself .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "The simplest strategy for ordering adjectives is what #TARGET_REF call the direct evidence method ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The simplest strategy for ordering adjectives is what #TARGET_REF call the direct evidence method .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular , boosting ( #REF ; #TARGET_REF ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: In particular , boosting ( #REF ; #TARGET_REF ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "One approach to this more general problem , taken by the ` Nitrogen ' generator ( #TARGET_REFa ; #REFb ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: One approach to this more general problem , taken by the ` Nitrogen ' generator ( #TARGET_REFa ; #REFb ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #REF ) and machine translation ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #REF ) and machine translation ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "fÎ¸ on demand ( #TARGET_REF ) can pay off here , since only part of fÎ¸ may be needed subsequently . )"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: fÎ¸ on demand ( #TARGET_REF ) can pay off here , since only part of fÎ¸ may be needed subsequently . )\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: #TARGET_REF give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The availability of toolkits for this weighted case ( #TARGET_REF ; #REF ) promises to unify much of statistical NLP ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The availability of toolkits for this weighted case ( #TARGET_REF ; #REF ) promises to unify much of statistical NLP .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , the forward-backward algorithm ( #REF ) trains only Hidden Markov Models , while ( #TARGET_REF ) trains only stochastic edit distance ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , the forward-backward algorithm ( #REF ) trains only Hidden Markov Models , while ( #TARGET_REF ) trains only stochastic edit distance .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The forward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The forward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "4To prove ( 1 ) â ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( #TARGET_REF ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: 4To prove ( 1 ) â ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( #TARGET_REF ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Efficient hardware implementation is also possible via chip-level parallelism ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Efficient hardware implementation is also possible via chip-level parallelism ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Per-state joint normalization ( #TARGET_REFb , Â§ 8.2 ) is similar but drops the dependence on a ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Per-state joint normalization ( #TARGET_REFb , Â§ 8.2 ) is similar but drops the dependence on a .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The EM algorithm ( #TARGET_REF ) can maximize these functions ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The EM algorithm ( #TARGET_REF ) can maximize these functions .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #REF ) , ( 3 ) by compilation of decision trees ( #TARGET_REF ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and #REF ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #REF ) , ( 3 ) by compilation of decision trees ( #TARGET_REF ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and #REF ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Now for some important remarks on efficiency : â¢ Computing ti is an instance of the well-known algebraic path problem ( #TARGET_REF ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Now for some important remarks on efficiency : â¢ Computing ti is an instance of the well-known algebraic path problem ( #TARGET_REF ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Such approaches have been tried recently in restricted cases ( #REF ; #REFb ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Such approaches have been tried recently in restricted cases ( #REF ; #REFb ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #TARGET_REF , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #TARGET_REF , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #TARGET_REF ) and machine translation ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #TARGET_REF ) and machine translation ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #TARGET_REF ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #TARGET_REF ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Undesirable consequences of this fact have been termed \"label bias\" ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Undesirable consequences of this fact have been termed \"label bias\" ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A brief version of this work, with some additional material, first appeared as ( #TARGET_REFa ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: A brief version of this work, with some additional material, first appeared as ( #TARGET_REFa ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "A more subtle example is weighted FSAs that approximate PCFGs ( #REF ; #TARGET_REF ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A more subtle example is weighted FSAs that approximate PCFGs ( #REF ; #TARGET_REF ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #TARGET_REF ) , ( 3 ) by compilation of decision trees ( #REF ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and #REF ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #TARGET_REF ) , ( 3 ) by compilation of decision trees ( #REF ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and #REF ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Such approaches have been tried recently in restricted cases ( #TARGET_REF ; #REFb ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Such approaches have been tried recently in restricted cases ( #TARGET_REF ; #REFb ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A more subtle example is weighted FSAs that approximate PCFGs ( #TARGET_REF ; #REF ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A more subtle example is weighted FSAs that approximate PCFGs ( #TARGET_REF ; #REF ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , the forward-backward algorithm ( #TARGET_REF ) trains only Hidden Markov Models , while ( #REF ) trains only stochastic edit distance ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , the forward-backward algorithm ( #TARGET_REF ) trains only Hidden Markov Models , while ( #REF ) trains only stochastic edit distance .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Such approaches have been tried recently in restricted cases ( #REF ; #TARGET_REFb ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Such approaches have been tried recently in restricted cases ( #REF ; #TARGET_REFb ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #TARGET_REF ; Chen and"
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #TARGET_REF ; Chen and\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The system utilizes several large size biological databases including three NCBI databases ( #REF , RefSeq #TARGET_REF , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and"
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The system utilizes several large size biological databases including three NCBI databases ( #REF , RefSeq #TARGET_REF , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) #TARGET_REF , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) #TARGET_REF , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #TARGET_REF , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #TARGET_REF , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #TARGET_REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #TARGET_REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) #TARGET_REF , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) #TARGET_REF , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Three UniRef tables UniRef100 , #REF and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and #REF and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #TARGET_REF such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Three UniRef tables UniRef100 , #REF and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and #REF and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #TARGET_REF such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) #TARGET_REF , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) #TARGET_REF , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The system utilizes several large size biological databases including three NCBI databases ( #REF , #REF , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #TARGET_REF , and"
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The system utilizes several large size biological databases including three NCBI databases ( #REF , #REF , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #TARGET_REF , and\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase #TARGET_REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase #TARGET_REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #TARGET_REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "While these approaches have been reasonably successful ( see #REF ) , #TARGET_REF speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While these approaches have been reasonably successful ( see #REF ) , #TARGET_REF speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #TARGET_REF ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #TARGET_REF ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , #REF , #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , #REF , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #TARGET_REF ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #TARGET_REF ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "While these approaches have been reasonably successful ( see #TARGET_REF ) , #REF speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While these approaches have been reasonably successful ( see #TARGET_REF ) , #REF speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #TARGET_REF )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #TARGET_REF )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features.\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "( 4 ) NE : We use BBN 's IdentiFinder ( #TARGET_REF ) , a MUC-style NE recognizer to determine the NE type of NPZ ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: ( 4 ) NE : We use BBN 's IdentiFinder ( #TARGET_REF ) , a MUC-style NE recognizer to determine the NE type of NPZ .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #REF , #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #REF , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #TARGET_REF , motivated by its success in the related tasks of word sense disambiguation ( #REF ) and NE classification ( #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #TARGET_REF , motivated by its success in the related tasks of word sense disambiguation ( #REF ) and NE classification ( #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #TARGET_REF ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #TARGET_REF ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Following #TARGET_REF , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Following previous work (e.g., #REF and #REF), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\\x0e, NPi+2, � � �, NPj positional features that have been employed by high- performing resolvers such as #REF and #TARGET_REF , as described below ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Following previous work (e.g., #REF and #REF), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\\x0e, NPi+2, � � �, NPj positional features that have been employed by high- performing resolvers such as #REF and #TARGET_REF , as described below .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our baseline coreference system uses the C4 .5 decision tree learner ( #TARGET_REF ) to acquire a classifier on the training texts for determining whether two NPs are coreferent ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Our baseline coreference system uses the C4 .5 decision tree learner ( #TARGET_REF ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #TARGET_REF and #REF, as described below."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #TARGET_REF and #REF, as described below.\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Following #TARGET_REF , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #REF ) , their semantic similarity as computed using WordNet ( e.g. , #REF ) or Wikipedia ( #REF ) , and the contextual role played by an NP ( see #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #REF ) , their semantic similarity as computed using WordNet ( e.g. , #REF ) or Wikipedia ( #REF ) , and the contextual role played by an NP ( see #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #TARGET_REF , #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #TARGET_REF , #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Following previous work ( e.g. , #TARGET_REF and #REF ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Following previous work ( e.g. , #TARGET_REF and #REF ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #REF , motivated by its success in the related tasks of word sense disambiguation ( #TARGET_REF ) and NE classification ( #REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #REF , motivated by its success in the related tasks of word sense disambiguation ( #TARGET_REF ) and NE classification ( #REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #REF ) , their semantic similarity as computed using WordNet ( e.g. , #REF ) or Wikipedia ( #TARGET_REF ) , and the contextual role played by an NP ( see #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #REF ) , their semantic similarity as computed using WordNet ( e.g. , #REF ) or Wikipedia ( #TARGET_REF ) , and the contextual role played by an NP ( see #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #TARGET_REF ) , their semantic similarity as computed using WordNet ( e.g. , #REF ) or Wikipedia ( #REF ) , and the contextual role played by an NP ( see #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #TARGET_REF ) , their semantic similarity as computed using WordNet ( e.g. , #REF ) or Wikipedia ( #REF ) , and the contextual role played by an NP ( see #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #TARGET_REFa ) ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: ( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #TARGET_REFa ) ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #TARGET_REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #TARGET_REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #REF ; #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #REF ; #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #TARGET_REF examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #TARGET_REF examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #TARGET_REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #TARGET_REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #TARGET_REF for an overview ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #TARGET_REF for an overview ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #REF ) 1 and corrected kappa ( #TARGET_REF ) 2 ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #REF ) 1 and corrected kappa ( #TARGET_REF ) 2 .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF and #REF study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #TARGET_REF ) and find correlations between the various modalities both within and across speakers ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #REF and #REF study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #TARGET_REF ) and find correlations between the various modalities both within and across speakers .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #REF examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #REF examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #TARGET_REF ) 1 and corrected kappa ( #REF ) 2 ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #TARGET_REF ) 1 and corrected kappa ( #REF ) 2 .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #REF ; #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #REF ; #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #TARGET_REF , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent (#REF)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #TARGET_REF , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent (#REF).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Related are also the studies by Rieks op den #REF and #TARGET_REF : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Related are also the studies by Rieks op den #REF and #TARGET_REF : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , #TARGET_REF and #REF find that machine learning algorithms can be trained to recognise some of the functions of head movements , while #REF show that there is a dependence between focus of attention and assignment of dialogue act labels ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , #TARGET_REF and #REF find that machine learning algorithms can be trained to recognise some of the functions of head movements , while #REF show that there is a dependence between focus of attention and assignment of dialogue act labels .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , #REF and #TARGET_REF find that machine learning algorithms can be trained to recognise some of the functions of head movements , while #REF show that there is a dependence between focus of attention and assignment of dialogue act labels ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , #REF and #TARGET_REF find that machine learning algorithms can be trained to recognise some of the functions of head movements , while #REF show that there is a dependence between focus of attention and assignment of dialogue act labels .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #REF, it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #REF, it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #TARGET_REF ) , but are still sat -isfactory given the high number of categories provided by the scheme."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #TARGET_REF ) , but are still sat -isfactory given the high number of categories provided by the scheme.\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The results , which partly confirm those obtained on a smaller dataset in #TARGET_REF , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The results , which partly confirm those obtained on a smaller dataset in #TARGET_REF , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Both kinds of annotation were carried out using ANVIL ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Both kinds of annotation were carried out using ANVIL ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The Praat tool was used ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The Praat tool was used ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The diagnoser , based on #TARGET_REFb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: The diagnoser , based on #TARGET_REFb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( #TARGET_REF ) generation system to produce the appropriate text ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( #TARGET_REF ) generation system to produce the appropriate text .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The contextual interpreter then uses a reference resolution approach similar to #TARGET_REF , and an ontology mapping mechanism ( #REFa ) to produce a domain-specific semantic representation of the student 's output ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The contextual interpreter then uses a reference resolution approach similar to #TARGET_REF , and an ontology mapping mechanism ( #REFa ) to produce a domain-specific semantic representation of the student 's output .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The BEETLE II system architecture is designed to overcome these limitations ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The BEETLE II system architecture is designed to overcome these limitations ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #TARGET_REF ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #TARGET_REF .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The contextual interpreter then uses a reference resolution approach similar to #REF , and an ontology mapping mechanism ( #TARGET_REFa ) to produce a domain-specific semantic representation of the student 's output ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The contextual interpreter then uses a reference resolution approach similar to #REF , and an ontology mapping mechanism ( #TARGET_REFa ) to produce a domain-specific semantic representation of the student 's output .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Other factors such as student confidence could be considered as well ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Other factors such as student confidence could be considered as well ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Our recovery policy is modeled on the TargetedHelp ( #TARGET_REF ) policy used in task-oriented dialogue ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Our recovery policy is modeled on the TargetedHelp ( #TARGET_REF ) policy used in task-oriented dialogue .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the TRIPS dialogue parser ( #TARGET_REF ) to parse the utterances ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We use the TRIPS dialogue parser ( #TARGET_REF ) to parse the utterances .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The system uses a knowledge base implemented in the KM representation language ( #REF ; #TARGET_REF ) to represent the state of the world ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The system uses a knowledge base implemented in the KM representation language ( #REF ; #TARGET_REF ) to represent the state of the world .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "All current approaches to monolingual TE , either syntactically oriented ( #REF ) , or applying logical inference ( #REF ) , or adopting transformation-based techniques ( #TARGET_REF ; #REF ) , incorporate different types of lexical knowledge to support textual inference ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: All current approaches to monolingual TE , either syntactically oriented ( #REF ) , or applying logical inference ( #REF ) , or adopting transformation-based techniques ( #TARGET_REF ; #REF ) , incorporate different types of lexical knowledge to support textual inference .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "ones , DIRT ( #TARGET_REF ) , VerbOcean ( #REF ) , FrameNet ( #REF ) , and Wikipedia ( #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: ones , DIRT ( #TARGET_REF ) , VerbOcean ( #REF ) , FrameNet ( #REF ) , and Wikipedia ( #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "All current approaches to monolingual TE , either syntactically oriented ( #REF ) , or applying logical inference ( Tatu and #TARGET_REF ) , or adopting transformation-based techniques ( #REF ; #REF ) , incorporate different types of lexical knowledge to support textual inference ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: All current approaches to monolingual TE , either syntactically oriented ( #REF ) , or applying logical inference ( Tatu and #TARGET_REF ) , or adopting transformation-based techniques ( #REF ; #REF ) , incorporate different types of lexical knowledge to support textual inference .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( #TARGET_REF ) as an extension of Textual Entailment ( #REF ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( #TARGET_REF ) as an extension of Textual Entailment ( #REF ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Using the basic solution proposed by ( #TARGET_REF ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Using the basic solution proposed by ( #TARGET_REF ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "We run TreeTagger ( #REF ) for tokenization , and used the Giza + + ( #TARGET_REF ) to align the tokenized corpora at the word level ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We run TreeTagger ( #REF ) for tokenization , and used the Giza + + ( #TARGET_REF ) to align the tokenized corpora at the word level .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #TARGET_REF ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #TARGET_REF ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Some previous works ( #TARGET_REF ; #REF ; #REF ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some previous works ( #TARGET_REF ; #REF ; #REF ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "To combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight ( #TARGET_REF ) , using each score as a feature ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: To combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight ( #TARGET_REF ) , using each score as a feature .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "They proved to be useful in a number of NLP applications such as natural language generation ( #TARGET_REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: They proved to be useful in a number of NLP applications such as natural language generation ( #TARGET_REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "These include, just to mention the most popular ones , DIRT ( #REF ) , VerbOcean ( #TARGET_REF ) , FrameNet ( #REF ) , and Wikipedia ( #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: These include, just to mention the most popular ones , DIRT ( #REF ) , VerbOcean ( #TARGET_REF ) , FrameNet ( #REF ) , and Wikipedia ( #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #TARGET_REF ) ) have been created for several languages , with different degrees of coverage ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #TARGET_REF ) ) have been created for several languages , with different degrees of coverage .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #TARGET_REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "These include, just to mention the most popular ones , DIRT ( #REF ) , VerbOcean ( #REF ) , FrameNet ( #REF ) , and Wikipedia ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: These include, just to mention the most popular ones , DIRT ( #REF ) , VerbOcean ( #REF ) , FrameNet ( #REF ) , and Wikipedia ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( #REF ) as an extension of Textual Entailment ( #TARGET_REF ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( #REF ) as an extension of Textual Entailment ( #TARGET_REF ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "These include, just to mention the most popular ones , DIRT ( #REF ) , VerbOcean ( #REF ) , FrameNet ( #TARGET_REF ) , and Wikipedia ( #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: These include, just to mention the most popular ones , DIRT ( #REF ) , VerbOcean ( #REF ) , FrameNet ( #TARGET_REF ) , and Wikipedia ( #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "After the extraction , pruning techniques ( #TARGET_REF ) can be applied to increase the precision of the extracted paraphrases ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: After the extraction , pruning techniques ( #TARGET_REF ) can be applied to increase the precision of the extracted paraphrases .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #TARGET_REF ) , and TE ( #REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #TARGET_REF ) , and TE ( #REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Some previous works ( #REF ; #TARGET_REF ; #REF ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some previous works ( #REF ; #TARGET_REF ; #REF ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #TARGET_REF ) to measure the relatedness between words in the dataset ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #TARGET_REF ) to measure the relatedness between words in the dataset .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Second , in line with the findings of ( #TARGET_REF ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Second , in line with the findings of ( #TARGET_REF ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Such questions are typically answered by designing appropriate priming experiments ( #TARGET_REF ) or other lexical decision tasks ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Such questions are typically answered by designing appropriate priming experiments ( #TARGET_REF ) or other lexical decision tasks .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We measure the inter annotator agreement using the Fleiss Kappa ( #TARGET_REF ) measure ( x ) where the agreement lies around 0.79 ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We measure the inter annotator agreement using the Fleiss Kappa ( #TARGET_REF ) measure ( x ) where the agreement lies around 0.79 .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Similar findings have been proposed by #TARGET_REF that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Similar findings have been proposed by #TARGET_REF that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We followed the same experimental procedure as discussed in ( #TARGET_REF ) for English polymorphemic words ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We followed the same experimental procedure as discussed in ( #TARGET_REF ) for English polymorphemic words .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\"."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\".\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Similar observation for surface word frequency was also observed by ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) where it has been claimed that words having low surface frequency tends to decompose ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Similar observation for surface word frequency was also observed by ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) where it has been claimed that words having low surface frequency tends to decompose .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( #REF ; #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( #REF ; #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #TARGET_REF ; Bentin , S. and #REF ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #TARGET_REF ; Bentin , S. and #REF ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #TARGET_REF ; #REF ; #REF ; #REF ) for Bangla morphologically complex words ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #TARGET_REF ; #REF ; #REF ; #REF ) for Bangla morphologically complex words .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( #REF ; #TARGET_REF ; Grainger , et al. , 1991 ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( #REF ; #TARGET_REF ; Grainger , et al. , 1991 ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #REF ; #REF ; #TARGET_REF ; #REF ) for Bangla morphologically complex words ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #REF ; #REF ; #TARGET_REF ; #REF ) for Bangla morphologically complex words .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Their computational significance arises from the issue of their storage in lexical resources like WordNet ( #TARGET_REF ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Their computational significance arises from the issue of their storage in lexical resources like WordNet ( #TARGET_REF ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF considers the second verb V2 as an aspectual complex comparable to the auxiliaries ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF considers the second verb V2 as an aspectual complex comparable to the auxiliaries .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Similar observation for surface word frequency was also observed by ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ) where it has been claimed that words having low surface frequency tends to decompose ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Similar observation for surface word frequency was also observed by ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ) where it has been claimed that words having low surface frequency tends to decompose .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( #REF ; #REF ; Grainger , et al. , 1991 ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( #REF ; #REF ; Grainger , et al. , 1991 ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #TARGET_REFc ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #TARGET_REFc ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #REF ; #TARGET_REF ; #REFa ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #REF ; #TARGET_REF ; #REFa ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates '' ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates '' ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-#REF), certain machine-assisted translation tools (e.g. ( #TARGET_REF ; #REFb ) ), concordancing for bilingual lexicography (#REF; Gale & #REF), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &#REF)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-#REF), certain machine-assisted translation tools (e.g. ( #TARGET_REF ; #REFb ) ), concordancing for bilingual lexicography (#REF; Gale & #REF), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &#REF).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This imbalance foils thresholding strategies , clever as they might be ( #TARGET_REF ; Wu & #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This imbalance foils thresholding strategies , clever as they might be ( #TARGET_REF ; Wu & #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-#REF), certain machine-assisted translation tools (e.g. (#REF; #REFb)), concordancing for bilingual lexicography (#REF; #TARGET_REF ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &#REF)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-#REF), certain machine-assisted translation tools (e.g. (#REF; #REFb)), concordancing for bilingual lexicography (#REF; #TARGET_REF ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &#REF).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-#REF), certain machine-assisted translation tools (e.g. (#REF; #TARGET_REFb )), concordancing for bilingual lexicography (#REF; Gale & #REF ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &#REF)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-#REF), certain machine-assisted translation tools (e.g. (#REF; #TARGET_REFb )), concordancing for bilingual lexicography (#REF; Gale & #REF ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &#REF).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "2We could just as easily use other symmetric \"association\" measures, such as 02 ( Gale & #REF ) or the Dice coefficient ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: 2We could just as easily use other symmetric \"association\" measures, such as 02 ( Gale & #REF ) or the Dice coefficient ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #TARGET_REF ; #REF ; #REFa ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #TARGET_REF ; #REF ; #REFa ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & #REF ; #TARGET_REFa ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & #REF ; #TARGET_REFa ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Co-occurrence With the exception of (#REFb), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #TARGET_REF ; Kumano & #REF;#REFa;#REF)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Co-occurrence With the exception of (#REFb), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #TARGET_REF ; Kumano & #REF;#REFa;#REF).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including \"crummy\" MT on the World Wide Web ( Church & #REF ) , certain machine-assisted translation tools ( e.g. ( #REF ; #REFb ) ) , concordancing for bilingual lexicography ( #TARGET_REF ; Gale & #REF ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &#REF)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including \"crummy\" MT on the World Wide Web ( Church & #REF ) , certain machine-assisted translation tools ( e.g. ( #REF ; #REFb ) ) , concordancing for bilingual lexicography ( #TARGET_REF ; Gale & #REF ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &#REF).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #TARGET_REF ) 2 ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #TARGET_REF ) 2 .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "With the exception of ( #REFb ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & #REF ; Kumano & #REF ; #REFa ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: With the exception of ( #REFb ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & #REF ; Kumano & #REF ; #REFa ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "By using the EM algorithm ( #TARGET_REF ) , they can guarantee convergence towards the globally optimum parameter set ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: By using the EM algorithm ( #TARGET_REF ) , they can guarantee convergence towards the globally optimum parameter set .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , frequent words are translated less consistently than rare words ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , frequent words are translated less consistently than rare words ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "2We could just as easily use other symmetric `` association '' measures , such as 02 ( #TARGET_REF ) or the Dice coefficient ( #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: 2We could just as easily use other symmetric `` association '' measures , such as 02 ( #TARGET_REF ) or the Dice coefficient ( #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #REF ; #REF ; #TARGET_REFa ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #REF ; #REF ; #TARGET_REFa ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This imbalance foils thresholding strategies , clever as they might be ( Gale & #REF ; Wu & #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This imbalance foils thresholding strategies , clever as they might be ( Gale & #REF ; Wu & #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #TARGET_REFb ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #TARGET_REFb ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The most detailed evaluation of link tokens to date was performed by ( #TARGET_REF ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The most detailed evaluation of link tokens to date was performed by ( #TARGET_REF ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "11 From ( #TARGET_REF ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: 11 From ( #TARGET_REF ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Differently , #TARGET_REF designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Differently , #TARGET_REF designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The system is implemented based on ( #REF ) and ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The system is implemented based on ( #REF ) and ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our previous work ( #TARGET_REF ) designed an EMbased method to construct unsupervised trees for tree-based translation models ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our previous work ( #TARGET_REF ) designed an EMbased method to construct unsupervised trees for tree-based translation models .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Then , we binarize the English parse trees using the head binarization approach ( #TARGET_REF ) and use the resulting binary parse trees to build another s2t system ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Then , we binarize the English parse trees using the head binarization approach ( #TARGET_REF ) and use the resulting binary parse trees to build another s2t system .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #REF , 2009 ; #REF ; #REF , 2006 ; #TARGET_REF ; #REF ; #REFb ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #REF , 2009 ; #REF ; #REF , 2006 ; #TARGET_REF ; #REF ; #REFb ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #REF , 2009 ; #REF ; #REF , 2006 ; #REF ; #TARGET_REF ; #REFb ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #REF , 2009 ; #REF ; #REF , 2006 ; #REF ; #TARGET_REF ; #REFb ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #TARGET_REF and decompose the prior probability P0 ( r | N ) into two factors as follows :"
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #TARGET_REF and decompose the prior probability P0 ( r | N ) into two factors as follows :\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF further labeled the SCFG rules with POS tags and unsupervised word classes ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF further labeled the SCFG rules with POS tags and unsupervised word classes .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF re-trained the linguistic parsers bilingually based on word alignment ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF re-trained the linguistic parsers bilingually based on word alignment .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The statistical significance test is performed by the re-sampling approach ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The statistical significance test is performed by the re-sampling approach ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The system is implemented based on ( #TARGET_REF ) and ( #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The system is implemented based on ( #TARGET_REF ) and ( #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF employed a Bayesian method to learn discontinuous SCFG rules ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF employed a Bayesian method to learn discontinuous SCFG rules .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF and #REF focused on joint parsing and alignment ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF and #REF focused on joint parsing and alignment .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "9 We only use the minimal GHKM rules ( #TARGET_REF ) here to reduce the complexity of the sampler ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: 9 We only use the minimal GHKM rules ( #TARGET_REF ) here to reduce the complexity of the sampler .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In the system , we extract both the minimal GHKM rules ( #TARGET_REF ) , and the rules of SPMT Model 1 ( #REF ) with phrases up to length L = 5 on the source side ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: In the system , we extract both the minimal GHKM rules ( #TARGET_REF ) , and the rules of SPMT Model 1 ( #REF ) with phrases up to length L = 5 on the source side .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Using the GHKM algorithm ( #TARGET_REF ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Using the GHKM algorithm ( #TARGET_REF ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #REF , 2009 ; #REF ; #TARGET_REF , 2006 ; #REF ; #REF ; #REFb ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #REF , 2009 ; #REF ; #TARGET_REF , 2006 ; #REF ; #REF ; #REFb ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #REF , 2009 ; #TARGET_REF ; #REF , 2006 ; #REF ; #REF ; #REFb ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #REF , 2009 ; #TARGET_REF ; #REF , 2006 ; #REF ; #REF ; #REFb ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "To create the baseline system , we use the opensource Joshua 4.0 system ( #REF ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #TARGET_REF ) respectively ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: To create the baseline system , we use the opensource Joshua 4.0 system ( #REF ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #TARGET_REF ) respectively .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This is because the binary structure has been verified to be very effective for tree-based translation ( #TARGET_REF ; #REFa ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: This is because the binary structure has been verified to be very effective for tree-based translation ( #TARGET_REF ; #REFa ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #TARGET_REF , 2009 ; #REF ; #REF , 2006 ; #REF ; #REF ; #REFb ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #TARGET_REF , 2009 ; #REF ; #REF , 2006 ; #REF ; #REF ; #REFb ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF and #TARGET_REF focused on joint parsing and alignment ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #REF and #TARGET_REF focused on joint parsing and alignment .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Inspired by ( #REF ) and ( #TARGET_REF ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Inspired by ( #REF ) and ( #TARGET_REF ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Inspired by ( #TARGET_REF ) and ( #REF ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Inspired by ( #TARGET_REF ) and ( #REF ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #TARGET_REF ) 5 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #TARGET_REF ) 5 .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #TARGET_REF ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #TARGET_REF ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #TARGET_REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #TARGET_REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #REF ; #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #REF ; #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #REF ; #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #REF ; #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #TARGET_REFb ) , a video-recording programming system , a schedule management system ( #REFa ) , and a weather infomiation system ( #REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #TARGET_REFb ) , a video-recording programming system , a schedule management system ( #REFa ) , and a weather infomiation system ( #REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #REF ; #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #REF ; #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #TARGET_REFa ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #TARGET_REFa ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #REFb ) , a video-recording programming system , a schedule management system ( #REFa ) , and a weather infomiation system ( #TARGET_REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #REFb ) , a video-recording programming system , a schedule management system ( #REFa ) , and a weather infomiation system ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "WIT features an incremental understanding method ( #TARGET_REFb ) that makes it possible to build a robust and real-time system ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: WIT features an incremental understanding method ( #TARGET_REFb ) that makes it possible to build a robust and real-time system .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The priorities are used for disambiguating interpretation in the incremental understanding method ( #TARGET_REFb ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The priorities are used for disambiguating interpretation in the incremental understanding method ( #TARGET_REFb ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "To this end , several toolkits for building spoken dialogue systems have been developed ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: To this end , several toolkits for building spoken dialogue systems have been developed ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #TARGET_REFb ) , which is an integrated parsing and discourse processing method ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #TARGET_REFb ) , which is an integrated parsing and discourse processing method .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Another common approach is term translation, e.g., via a bilingual lexicon. (#REF; #TARGET_REF ; #REF)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Another common approach is term translation, e.g., via a bilingual lexicon. (#REF; #TARGET_REF ; #REF).\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "There are several variations of such a method ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: There are several variations of such a method ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The one-sided t-test ( #TARGET_REF ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The one-sided t-test ( #TARGET_REF ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ The transition probability a is 0.7 using the EM algorithm ( #TARGET_REF ) on the TREC4 ad-hoc query set ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: â¢ The transition probability a is 0.7 using the EM algorithm ( #TARGET_REF ) on the TREC4 ad-hoc query set .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF studied the issue of disambiguation for mono-lingual M."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF studied the issue of disambiguation for mono-lingual M.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de #REF ; #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de #REF ; #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "There are several variations of such a method ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: There are several variations of such a method ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                " However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0:  However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A cooccurrence based stemmer ( #TARGET_REF ) was used to stem Spanish words ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: A cooccurrence based stemmer ( #TARGET_REF ) was used to stem Spanish words .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Other studies which view lR as a query generation process include #REF ; #REF ; #TARGET_REF ; Miller et al , 1999 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Other studies which view lR as a query generation process include #REF ; #REF ; #TARGET_REF ; Miller et al , 1999 .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Other studies which view lR as a query generation process include #TARGET_REF ; #REF ; #REF ; Miller et al , 1999 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Other studies which view lR as a query generation process include #TARGET_REF ; #REF ; #REF ; Miller et al , 1999 .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Following #TARGET_REF , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #TARGET_REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #TARGET_REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #TARGET_REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #TARGET_REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #REFb ; #REFa ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #REFb ; #REFa ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For the full parser , we use the one developed by Michael Collins ( #REF ; #TARGET_REF ) -- one of the most accurate full parsers around ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For the full parser , we use the one developed by Michael Collins ( #REF ; #TARGET_REF ) -- one of the most accurate full parsers around .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #TARGET_REFb ; #REFa ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #TARGET_REFb ; #REFa ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "SNoW ( #TARGET_REF ; #REF ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: SNoW ( #TARGET_REF ; #REF ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; Tjong Kim #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; Tjong Kim #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #REF ; #REF ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #TARGET_REF ) to compare it to other shallow parsers ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #REF ; #REF ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #TARGET_REF ) to compare it to other shallow parsers .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; Tjong Kim #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; Tjong Kim #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #REFb ; #REFa ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #REFb ; #REFa ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The shallow parser used is the SNoW-based CSCL parser ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The shallow parser used is the SNoW-based CSCL parser ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #TARGET_REF ) terminology ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #TARGET_REF ) terminology .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #REF ; #TARGET_REF ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #REF ) to compare it to other shallow parsers ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #REF ; #TARGET_REF ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #REF ) to compare it to other shallow parsers .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Research on shallow parsing was inspired by psycholinguistics arguments ( #TARGET_REF ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Research on shallow parsing was inspired by psycholinguistics arguments ( #TARGET_REF ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For the full parser , we use the one developed by Michael Collins ( #TARGET_REF ; #REF ) -- one of the most accurate full parsers around ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: For the full parser , we use the one developed by Michael Collins ( #TARGET_REF ; #REF ) -- one of the most accurate full parsers around .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The shallow parser used is the SNoW-based CSCL parser ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The shallow parser used is the SNoW-based CSCL parser ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "SNoW ( #REF ; #TARGET_REF ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: SNoW ( #REF ; #TARGET_REF ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "would be chunked as follows ( Tjong Kim #TARGET_REF ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: would be chunked as follows ( Tjong Kim #TARGET_REF ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; Tjong Kim #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; Tjong Kim #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #TARGET_REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #TARGET_REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #TARGET_REF ; #REF ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #REF ) to compare it to other shallow parsers ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #TARGET_REF ; #REF ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #REF ) to compare it to other shallow parsers .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Training was done on the Penn Treebank ( #TARGET_REF ) Wall Street Journal data , sections 02-21 ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Training was done on the Penn Treebank ( #TARGET_REF ) Wall Street Journal data , sections 02-21 .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Tateisi et al. also translated LTAG into HPSG ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Tateisi et al. also translated LTAG into HPSG ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "There have been many studies on parsing techniques ( #REF ; #REF ) , ones on disambiguation models ( #REF ; #TARGET_REF ) , and ones on programming/grammar-development environ -  (#REF;#REF;#REF)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There have been many studies on parsing techniques ( #REF ; #REF ) , ones on disambiguation models ( #REF ; #TARGET_REF ) , and ones on programming/grammar-development environ -  (#REF;#REF;#REF).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "TNT refers to the HPSG parser ( #TARGET_REF ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: TNT refers to the HPSG parser ( #TARGET_REF ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #REF ; #REF ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #TARGET_REF ) by a method of grammar conversion ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #REF ; #REF ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #TARGET_REF ) by a method of grammar conversion .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "FBLTAG ( #REF ; #TARGET_REF ) is an extension of the LTAG formalism ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: FBLTAG ( #REF ; #TARGET_REF ) is an extension of the LTAG formalism .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "ment ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: ment ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #REF ; #TARGET_REF ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #REF ) by a method of grammar conversion ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #REF ; #TARGET_REF ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #REF ) by a method of grammar conversion .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "LTAG ( #TARGET_REF ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: LTAG ( #TARGET_REF ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Our group has developed a wide-coverage HPSG grammar for Japanese ( #TARGET_REF ) , which is used in a high-accuracy Japanese dependency analyzer ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Our group has developed a wide-coverage HPSG grammar for Japanese ( #TARGET_REF ) , which is used in a high-accuracy Japanese dependency analyzer ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Other works ( #REF ; #TARGET_REF ) convert HPSG grammars into LTAG grammars ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Other works ( #REF ; #TARGET_REF ) convert HPSG grammars into LTAG grammars .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our group has developed a wide-coverage HPSG grammar for Japanese ( #REF ) , which is used in a high-accuracy Japanese dependency analyzer ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Our group has developed a wide-coverage HPSG grammar for Japanese ( #REF ) , which is used in a high-accuracy Japanese dependency analyzer ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #TARGET_REF ) , which is a large-scale FB-LTAG grammar ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #TARGET_REF ) , which is a large-scale FB-LTAG grammar .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The XTAG group ( #TARGET_REF ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The XTAG group ( #TARGET_REF ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In Table 2 , lem refers to the LTAG parser ( #TARGET_REF ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( #REF ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In Table 2 , lem refers to the LTAG parser ( #TARGET_REF ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( #REF ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "FBLTAG ( #TARGET_REF ; #REF ) is an extension of the LTAG formalism ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: FBLTAG ( #TARGET_REF ; #REF ) is an extension of the LTAG formalism .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There have been many studies on parsing techniques (#REF; #REF), ones on disambiguation models (#REF; #REF), and ones on programming/grammar-development environment ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There have been many studies on parsing techniques (#REF; #REF), ones on disambiguation models (#REF; #REF), and ones on programming/grammar-development environment ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Another paper ( #TARGET_REF ) describes the detailed analysis on the factor of the difference of parsing performance ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Another paper ( #TARGET_REF ) describes the detailed analysis on the factor of the difference of parsing performance .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #TARGET_REF ) 6 ( the average length is 6.32 words ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #TARGET_REF ) 6 ( the average length is 6.32 words ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The RenTAL system is implemented in LiLFeS ( #TARGET_REF ) 2 ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The RenTAL system is implemented in LiLFeS ( #TARGET_REF ) 2 .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #TARGET_REF ; #REF ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #REF ) by a method of grammar conversion ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #TARGET_REF ; #REF ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #REF ) by a method of grammar conversion .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There have been many studies on parsing techniques (#REF; #REF), ones on disambiguation models (#REF; #REF), and ones on programming/grammar-development ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There have been many studies on parsing techniques (#REF; #REF), ones on disambiguation models (#REF; #REF), and ones on programming/grammar-development ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There have been many studies on parsing techniques ( #TARGET_REF ; #REF ) , ones on disambiguation models ( #REF ; #REF ) , and ones on programming/grammar-development environment (#REF; #REF; #REF)."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There have been many studies on parsing techniques ( #TARGET_REF ; #REF ) , ones on disambiguation models ( #REF ; #REF ) , and ones on programming/grammar-development environment (#REF; #REF; #REF).\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The grammar conversion from LTAG to HPSG ( #TARGET_REF ) is the core portion of the RenTAL system ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The grammar conversion from LTAG to HPSG ( #TARGET_REF ) is the core portion of the RenTAL system .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We applied our system to the XTAG English grammar ( The XTAG Research #TARGET_REF ) 3 , which is a large-scale FB-LTAG grammar for English ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We applied our system to the XTAG English grammar ( The XTAG Research #TARGET_REF ) 3 , which is a large-scale FB-LTAG grammar for English .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "There has been some controversy , at least for simple stemmers ( #REF ; #REF ) , about the effectiveness of morphological analysis for document retrieval ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There has been some controversy , at least for simple stemmers ( #REF ; #REF ) , about the effectiveness of morphological analysis for document retrieval ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "mers ( #TARGET_REF ; #REF ) demonstrably improve retrieval performance ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: mers ( #TARGET_REF ; #REF ) demonstrably improve retrieval performance .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There has been some controversy , at least for simple stemmers ( #REF ; #REF ) , about the effectiveness of morphological analysis for document retrieval ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There has been some controversy , at least for simple stemmers ( #REF ; #REF ) , about the effectiveness of morphological analysis for document retrieval ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "mers ( #REF ; #TARGET_REF ) demonstrably improve retrieval performance ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: mers ( #REF ; #TARGET_REF ) demonstrably improve retrieval performance .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #REF ; J Â¨ appinen and Niemist Â¨ o , 1988 ; #TARGET_REF ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #REF ; J Â¨ appinen and Niemist Â¨ o , 1988 ; #TARGET_REF ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There has been some controversy , at least for simple stemmers ( #REF ; #TARGET_REF ) , about the effectiveness of morphological analysis for document retrieval ( #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There has been some controversy , at least for simple stemmers ( #REF ; #TARGET_REF ) , about the effectiveness of morphological analysis for document retrieval ( #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #TARGET_REF ) ) one can not really recommend this method ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #TARGET_REF ) ) one can not really recommend this method .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The retrieval process relies on the vector space model ( #TARGET_REF ) , with the cosine measure expressing the similarity between a query and a document ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The retrieval process relies on the vector space model ( #TARGET_REF ) , with the cosine measure expressing the similarity between a query and a document .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "There has been some controversy , at least for simple stemmers ( #TARGET_REF ; #REF ) , about the effectiveness of morphological analysis for document retrieval ( #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There has been some controversy , at least for simple stemmers ( #TARGET_REF ; #REF ) , about the effectiveness of morphological analysis for document retrieval ( #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #TARGET_REF ) ) are incorporated into our system ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #TARGET_REF ) ) are incorporated into our system .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "There has been some controversy , at least for simple stemmers ( #REF ; #REF ) , about the effectiveness of morphological analysis for document retrieval ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There has been some controversy , at least for simple stemmers ( #REF ; #REF ) , about the effectiveness of morphological analysis for document retrieval ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; #REF ; #TARGET_REF ; Ekmekc Â¸ ioglu et al. , 1995 ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; #REF ; #TARGET_REF ; Ekmekc Â¸ ioglu et al. , 1995 ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Alternatively , we may think of user-centered comparative studies ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Alternatively , we may think of user-centered comparative studies ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #TARGET_REF ) , turns out to be infeasible , at least for German and related languages ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #TARGET_REF ) , turns out to be infeasible , at least for German and related languages .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; #REF ; #REF ; Ekmekc Â¸ ioglu et al. , 1995 ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; #REF ; #REF ; Ekmekc Â¸ ioglu et al. , 1995 ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #TARGET_REF ; J Â¨ appinen and Niemist Â¨ o , 1988 ; #REF ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #TARGET_REF ; J Â¨ appinen and Niemist Â¨ o , 1988 ; #REF ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #TARGET_REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #TARGET_REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #TARGET_REF ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #TARGET_REF ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #TARGET_REF to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #TARGET_REF to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF presented an approach for constructing a BKB based on the S-SSTC ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF presented an approach for constructing a BKB based on the S-SSTC .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #TARGET_REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #TARGET_REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It allows the construction of a non-TAL ( #REF ) , ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It allows the construction of a non-TAL ( #REF ) , ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #TARGET_REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #TARGET_REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #TARGET_REF cases ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #TARGET_REF cases ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #TARGET_REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #TARGET_REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #TARGET_REF ) , such as the relation between syntax and semantic ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #TARGET_REF ) , such as the relation between syntax and semantic .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For instance, when building translation units in EBMT approaches ( #REF ) , ( #REF ) , ( AlAdhaileh & #REF ) , ( Sato & #REF ) , ( #REF ) , ( #TARGET_REF ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For instance, when building translation units in EBMT approaches ( #REF ) , ( #REF ) , ( AlAdhaileh & #REF ) , ( Sato & #REF ) , ( #REF ) , ( #TARGET_REF ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #TARGET_REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #TARGET_REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For more details on the proprieties of SSTC , see #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For more details on the proprieties of SSTC , see #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #TARGET_REF ) and Boitet & #REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #TARGET_REF ) and Boitet & #REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #TARGET_REF ) , ( Al-Adhaileh & #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #TARGET_REF ) , ( Al-Adhaileh & #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It allows the construction of a non-TAL ( #TARGET_REF ) , ( Harbusch & #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It allows the construction of a non-TAL ( #TARGET_REF ) , ( Harbusch & #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #TARGET_REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #TARGET_REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #REF ) and #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #REF ) and #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There have already been several attempts to develop distributed NLP systems for dialogue systems ( #TARGET_REF ) and speech recognition ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There have already been several attempts to develop distributed NLP systems for dialogue systems ( #TARGET_REF ) and speech recognition ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Also , advanced methods often require many training iterations , for example active learning ( #REF ) and co-training ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Also , advanced methods often require many training iterations , for example active learning ( #REF ) and co-training ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #TARGET_REF ) and the Alembic Workbench ( #REF ) ) as well as NLP tools and resources that can be manipulated from the GUI ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #TARGET_REF ) and the Alembic Workbench ( #REF ) ) as well as NLP tools and resources that can be manipulated from the GUI .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "There have already been several attempts to develop distributed NLP systems for dialogue systems ( #REF ) and speech recognition ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: There have already been several attempts to develop distributed NLP systems for dialogue systems ( #REF ) and speech recognition ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For instance , implementing an efficient version of the MXPOST POS tagger ( #TARGET_REF ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: For instance , implementing an efficient version of the MXPOST POS tagger ( #TARGET_REF ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "The implementation has been inspired by experience in extracting information from very large corpora ( #REF ) and performing experiments on maximum entropy sequence tagging ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: The implementation has been inspired by experience in extracting information from very large corpora ( #REF ) and performing experiments on maximum entropy sequence tagging ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "The implementation has been inspired by experience in extracting information from very large corpora ( #TARGET_REF ) and performing experiments on maximum entropy sequence tagging ( #REF ; #REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: The implementation has been inspired by experience in extracting information from very large corpora ( #TARGET_REF ) and performing experiments on maximum entropy sequence tagging ( #REF ; #REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #TARGET_REF that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #TARGET_REF that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Other tools have been designed around particular techniques , such as finite state machines ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other tools have been designed around particular techniques , such as finite state machines ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Recent work ( #REF ; #TARGET_REF ) has suggested that some tasks will benefit from using significantly more data ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recent work ( #REF ; #TARGET_REF ) has suggested that some tasks will benefit from using significantly more data .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , 10 million words of the American National Corpus ( #REF ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #TARGET_REF ) , currently used for training POS taggers ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , 10 million words of the American National Corpus ( #REF ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #TARGET_REF ) , currently used for training POS taggers .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It has already been used to implement a framework for teaching NLP ( #TARGET_REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: It has already been used to implement a framework for teaching NLP ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #REF ) and the Alembic Workbench ( #TARGET_REF ) ) as well as NLP tools and resources that can be manipulated from the GUI ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #REF ) and the Alembic Workbench ( #TARGET_REF ) ) as well as NLP tools and resources that can be manipulated from the GUI .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Software engineering research on Generative Programming ( #TARGET_REF ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Software engineering research on Generative Programming ( #TARGET_REF ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , 10 million words of the American National Corpus ( #TARGET_REF ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #REF ) , currently used for training POS taggers ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , 10 million words of the American National Corpus ( #TARGET_REF ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #REF ) , currently used for training POS taggers .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , the suite of LT tools ( #REF ; #TARGET_REF ) perform tokenization , tagging and chunking on XML marked-up text directly ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , the suite of LT tools ( #REF ; #TARGET_REF ) perform tokenization , tagging and chunking on XML marked-up text directly .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "To provide the required configurability in the static version of the code we will use policy templates ( #TARGET_REF ) , and for the dynamic version we will use configuration classes ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: To provide the required configurability in the static version of the code we will use policy templates ( #TARGET_REF ) , and for the dynamic version we will use configuration classes .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Recent work ( #TARGET_REF ; #REF ) has suggested that some tasks will benefit from using significantly more data ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Recent work ( #TARGET_REF ; #REF ) has suggested that some tasks will benefit from using significantly more data .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , the suite of LT tools ( #TARGET_REF ; #REF ) perform tokenization , tagging and chunking on XML marked-up text directly ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , the suite of LT tools ( #TARGET_REF ; #REF ) perform tokenization , tagging and chunking on XML marked-up text directly .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #TARGET_REF ) and Memory-based learning ( MBL ) ( #REF ) have been applied to many different problems , so a single interchangeable component should be used to represent each method ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #TARGET_REF ) and Memory-based learning ( MBL ) ( #REF ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #TARGET_REF ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #TARGET_REF ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #REF ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #REF ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The implementation has been inspired by experience in extracting information from very large corpora ( #REF ) and performing experiments on maximum entropy sequence tagging ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: The implementation has been inspired by experience in extracting information from very large corpora ( #REF ) and performing experiments on maximum entropy sequence tagging ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #REF that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #REF that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The TNT POS tagger ( #TARGET_REF ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The TNT POS tagger ( #TARGET_REF ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "GATE goes beyond earlier systems by using a component-based infrastructure ( #TARGET_REF ) which the GUI is built on top of ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: GATE goes beyond earlier systems by using a component-based infrastructure ( #TARGET_REF ) which the GUI is built on top of .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus for instance , ( #TARGET_REF ; #REF ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #REF ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus for instance , ( #TARGET_REF ; #REF ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #REF ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Similarly , ( #REF ) and ( #TARGET_REF ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Similarly , ( #REF ) and ( #TARGET_REF ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "And ( #TARGET_REF ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: And ( #TARGET_REF ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "To address this problem , we are currently working on developing a metagrammar in the sense of ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: To address this problem , we are currently working on developing a metagrammar in the sense of ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Similarly , ( #TARGET_REF ) and ( #REF ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Similarly , ( #TARGET_REF ) and ( #REF ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "For instance , ( #TARGET_REF ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For instance , ( #TARGET_REF ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #TARGET_REF ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #TARGET_REF ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The language chosen for semantic representation is a flat semantics along the line of ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The language chosen for semantic representation is a flat semantics along the line of ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "While corpus driven efforts along the PARSEVAL lines ( #TARGET_REF ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While corpus driven efforts along the PARSEVAL lines ( #TARGET_REF ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The language chosen for semantic representation is a flat semantics along the line of ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The language chosen for semantic representation is a flat semantics along the line of ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular , ( #TARGET_REF ) lists the converses of some 3 500 predicative nouns ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: In particular , ( #TARGET_REF ) lists the converses of some 3 500 predicative nouns .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #TARGET_REF ) thus ensuring an additional level of abstraction ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #TARGET_REF ) thus ensuring an additional level of abstraction .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "For complementing this database and for converse constructions , the LADL tables ( #TARGET_REF ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: For complementing this database and for converse constructions , the LADL tables ( #TARGET_REF ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #REF ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #REF ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus for instance , ( #REF ; #REF ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #TARGET_REF ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus for instance , ( #REF ; #REF ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #TARGET_REF ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Semantic construction proceeds from the derived tree ( #TARGET_REF ) rather than -- as is more common in TAG -- from the derivation tree ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Semantic construction proceeds from the derived tree ( #TARGET_REF ) rather than -- as is more common in TAG -- from the derivation tree .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus for instance , ( #REF ; #TARGET_REF ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #REF ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Thus for instance , ( #REF ; #TARGET_REF ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #REF ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The language chosen for semantic representation is a flat semantics along the line of ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The language chosen for semantic representation is a flat semantics along the line of ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "For shuffling paraphrases , french alternations are partially described in ( #TARGET_REF ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For shuffling paraphrases , french alternations are partially described in ( #TARGET_REF ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It compares favorably to other stemming or root extraction algorithms ( #TARGET_REF ; #REF ; and #REF ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: It compares favorably to other stemming or root extraction algorithms ( #TARGET_REF ; #REF ; and #REF ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "It compares favorably to other stemming or root extraction algorithms ( #REF ; #REF ; and #TARGET_REF ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: It compares favorably to other stemming or root extraction algorithms ( #REF ; #REF ; and #TARGET_REF ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #TARGET_REF ) , minimum description length principal ( #REF ) , and the X2 statistic ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #TARGET_REF ) , minimum description length principal ( #REF ) , and the X2 statistic .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This work is a continuation of that initiated in ( #TARGET_REF ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: This work is a continuation of that initiated in ( #TARGET_REF ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "A good study comparing document categorization algorithms can be found in ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A good study comparing document categorization algorithms can be found in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This is mainly due to the fact that Arabic is a non-concatenative language ( #TARGET_REF ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This is mainly due to the fact that Arabic is a non-concatenative language ( #TARGET_REF ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "( #TARGET_REF ) has found strong correlations between DF , IG and the X2 statistic for a term ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: ( #TARGET_REF ) has found strong correlations between DF , IG and the X2 statistic for a term .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "More recently , ( #TARGET_REF ) has performed a good survey of document categorization ; recent works can also be found in ( #REF ) , ( #REF ) , and ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: More recently , ( #TARGET_REF ) has performed a good survey of document categorization ; recent works can also be found in ( #REF ) , ( #REF ) , and ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #TARGET_REF ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #TARGET_REF ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #REF ) , minimum description length principal ( #TARGET_REF ) , and the X2 statistic ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #REF ) , minimum description length principal ( #TARGET_REF ) , and the X2 statistic .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #TARGET_REF ) , ( #REF ) and ( #REF ) , respectively ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #TARGET_REF ) , ( #REF ) and ( #REF ) , respectively .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "More recently , ( #REF ) has performed a good survey of document categorization ; recent works can also be found in ( #TARGET_REF ) , ( #REF ) , and ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: More recently , ( #REF ) has performed a good survey of document categorization ; recent works can also be found in ( #TARGET_REF ) , ( #REF ) , and ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , ( #TARGET_REF ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , ( #TARGET_REF ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Most of these endeavours have focused on purely statistical acquisition techniques (#REF ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( #REF ) or , more frequently , on a combination of the two ( #REF ; #TARGET_REF , for example ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Most of these endeavours have focused on purely statistical acquisition techniques (#REF ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( #REF ) or , more frequently , on a combination of the two ( #REF ; #TARGET_REF , for example ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #TARGET_REF , for example ) , does not specify the relationship itself ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #TARGET_REF , for example ) , does not specify the relationship itself .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #TARGET_REF uses derivational morphology ; #REF use , as a starting point , a number of identical characters ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #TARGET_REF uses derivational morphology ; #REF use , as a starting point , a number of identical characters .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The terms have been identified as the most specific to our corpus by a program developed by #TARGET_REF and called TER1vloSTAT ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The terms have been identified as the most specific to our corpus by a program developed by #TARGET_REF and called TER1vloSTAT .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #REF uses derivational morphology ; #TARGET_REF use , as a starting point , a number of identical characters ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #REF uses derivational morphology ; #TARGET_REF use , as a starting point , a number of identical characters .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Most of these endeavours have focused on purely statistical acquisition techniques (#REF), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #TARGET_REF ) or, more frequently, on a combination of the two (#REF; Kilgarri\\x1b and #REF, for example)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Most of these endeavours have focused on purely statistical acquisition techniques (#REF), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #TARGET_REF ) or, more frequently, on a combination of the two (#REF; Kilgarri\\x1b and #REF, for example).\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) ( #TARGET_REF ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E â\\x88\\x92 ) of the elements one wants to acquire and their context."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) ( #TARGET_REF ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E â\\x88\\x92 ) of the elements one wants to acquire and their context.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "However , most strategies are based on `` internal  or `` external methods  ( #TARGET_REF ) , i.e. methods that rely on the form of terms or on the information gathered from contexts ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: However , most strategies are based on `` internal  or `` external methods  ( #TARGET_REF ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #TARGET_REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #TARGET_REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #TARGET_REF ) and called qualia relations ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #TARGET_REF ) and called qualia relations ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #TARGET_REF with WoRDNET relations ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #TARGET_REF with WoRDNET relations ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #REF ) and called qualia relations ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #REF ) and called qualia relations ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #TARGET_REF ) for a review ) , these patterns allow :"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #TARGET_REF ) for a review ) , these patterns allow :\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "ASARES is presented in detail in ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: ASARES is presented in detail in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "A number of applications have relied on distributional analysis ( #TARGET_REF ) in order to build classes of semantically related terms ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: A number of applications have relied on distributional analysis ( #TARGET_REF ) in order to build classes of semantically related terms .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #TARGET_REF ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #TARGET_REF ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #TARGET_REF Arabic data ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #TARGET_REF Arabic data .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Character classes , such as punctuation , are defined according to the Unicode Standard ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Character classes , such as punctuation , are defined according to the Unicode Standard ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The coreference system system is similar to the Bell tree algorithm as described by ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The coreference system system is similar to the Bell tree algorithm as described by ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Arabic has two kinds of plurals : broken plurals and sound plurals ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Arabic has two kinds of plurals : broken plurals and sound plurals ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "the mention sub-type , which is a sub-category of the mention type ( #TARGET_REF ) ( e.g. OrgGovernmental , FacilityPath , etc. ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: the mention sub-type , which is a sub-category of the mention type ( #TARGET_REF ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: #TARGET_REF demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Both systems are built around from the maximum-entropy technique ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Both systems are built around from the maximum-entropy technique ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #TARGET_REF ) and the coreference resolution system is similar to the one described in ( #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #TARGET_REF ) and the coreference resolution system is similar to the one described in ( #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "As in ( #TARGET_REF ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: As in ( #TARGET_REF ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We introduce here a clearly defined and replicable split of the #TARGET_REF data , so that future investigations can accurately and correctly compare against the results presented here ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We introduce here a clearly defined and replicable split of the #TARGET_REF data , so that future investigations can accurately and correctly compare against the results presented here .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The final machine is a trigram language model , specifically a Kneser-Ney ( #TARGET_REF ) based backoff language model ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The final machine is a trigram language model , specifically a Kneser-Ney ( #TARGET_REF ) based backoff language model .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #TARGET_REF ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #TARGET_REF ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Arabic has two kinds of plurals : broken plurals and sound plurals ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Arabic has two kinds of plurals : broken plurals and sound plurals ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #REF ) and the coreference resolution system is similar to the one described in ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #REF ) and the coreference resolution system is similar to the one described in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "As stated before , the experiments are run in the ACE '04 framework ( #TARGET_REF ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: As stated before , the experiments are run in the ACE '04 framework ( #TARGET_REF ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF observed that some annotators were not familiar with the exact definition of semantic relatedness ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: #TARGET_REF observed that some annotators were not familiar with the exact definition of semantic relatedness .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "words and their surrounding context), words or concepts ( #TARGET_REF ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: words and their surrounding context), words or concepts ( #TARGET_REF ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #TARGET_REF ) . Table 1: Comparison of previous experiments."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #TARGET_REF ) . Table 1: Comparison of previous experiments.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "According to #TARGET_REF , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: According to #TARGET_REF , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This experiment was again replicated by #TARGET_REF with 10 subjects ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This experiment was again replicated by #TARGET_REF with 10 subjects .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular , the `` Semantic Information Retrieval '' project ( SIR #TARGET_REF ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: In particular , the `` Semantic Information Retrieval '' project ( SIR #TARGET_REF ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "dictionary-based ( #REF ) , ontology-based ( #REF ; #REF ) , information-based ( #REF ; #REF ) or distributional ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: dictionary-based ( #REF ) , ontology-based ( #REF ; #REF ) , information-based ( #REF ; #REF ) or distributional ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #TARGET_REF ) , the German equivalent to WordNet , as a sense inventory for each word ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #TARGET_REF ) , the German equivalent to WordNet , as a sense inventory for each word .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #TARGET_REF ) .4"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #TARGET_REF ) .4\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "We used the revised experimental setup ( #TARGET_REF ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We used the revised experimental setup ( #TARGET_REF ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "In our experiment , we annotated a high number of pairs similar in size to the test sets by #REF and #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: In our experiment , we annotated a high number of pairs similar in size to the test sets by #REF and #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( #REF ) , ontology-based ( #REF ; #TARGET_REF ) , information-based ( #REF ; #REF ) or distributional ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( #REF ) , ontology-based ( #REF ; #TARGET_REF ) , information-based ( #REF ; #REF ) or distributional ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In the seminal work by #TARGET_REF , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In the seminal work by #TARGET_REF , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #TARGET_REF ) or malapropism detection ( #REF ) . #REF argue for application-specific evaluation of similarity measures, because measures are always used for some task."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #TARGET_REF ) or malapropism detection ( #REF ) . #REF argue for application-specific evaluation of similarity measures, because measures are always used for some task.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF argue for application-specific evaluation of similarity measures , because measures are always used for some task ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF argue for application-specific evaluation of similarity measures , because measures are always used for some task .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF reported a correlation of r = .69 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF reported a correlation of r = .69 .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF annotated a larger set of word pairs ( 353 ) , too ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF annotated a larger set of word pairs ( 353 ) , too .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "dictionary-based ( #REF ) , ontology-based ( #TARGET_REF ; #REF ) , information-based ( #REF ; #REF ) or distributional ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: dictionary-based ( #REF ) , ontology-based ( #TARGET_REF ; #REF ) , information-based ( #REF ; #REF ) or distributional ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "dictionary-based ( #REF ) , ontology-based ( #REF ; #REF ) , information-based ( #TARGET_REF ; #REF ) or distributional ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: dictionary-based ( #REF ) , ontology-based ( #REF ; #REF ) , information-based ( #TARGET_REF ; #REF ) or distributional ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "dictionary-based ( #TARGET_REF ) , ontology-based ( #REF ; #REF ) , information-based ( #TARGET_REF ; #REF ) or distributional ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: dictionary-based ( #TARGET_REF ) , ontology-based ( #REF ; #REF ) , information-based ( #TARGET_REF ; #REF ) or distributional ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #TARGET_REF ) . 6"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #TARGET_REF ) . 6\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Furthermore , manually selected word pairs are often biased towards highly related pairs ( #TARGET_REF ) , because human annotators tend to select only highly related pairs connected by relations they are aware of ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Furthermore , manually selected word pairs are often biased towards highly related pairs ( #TARGET_REF ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF did not report inter-subject correlation for their larger dataset ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF did not report inter-subject correlation for their larger dataset .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: #TARGET_REF reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore , inter-subject correlation is lower than the results obtained by #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Therefore , inter-subject correlation is lower than the results obtained by #TARGET_REF .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( #REF ) , ontology-based ( #REF ; #REF ) , information-based ( #REF ; #TARGET_REF ) or distributional ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( #REF ) , ontology-based ( #REF ; #REF ) , information-based ( #REF ; #TARGET_REF ) or distributional ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #TARGET_REFb ) and shingling techniques described by #REF ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #TARGET_REFb ) and shingling techniques described by #REF .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( #REF ) , KWiCFinder ( #REFa ) and the Linguist 's Search Engine ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( #REF ) , KWiCFinder ( #REFa ) and the Linguist 's Search Engine ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #REF : 56 ) unless the web is used as a corpus ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #REF : 56 ) unless the web is used as a corpus ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #TARGET_REF ; Hughes et al , 2004 ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #TARGET_REF ; Hughes et al , 2004 ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( #REF ) , KWiCFinder ( #REFa ) and the Linguist 's Search Engine ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( #REF ) , KWiCFinder ( #REFa ) and the Linguist 's Search Engine ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( #REF ) , KWiCFinder ( #TARGET_REFa ) and the Linguist 's Search Engine ( #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( #REF ) , KWiCFinder ( #TARGET_REFa ) and the Linguist 's Search Engine ( #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #REF ; #REF ; #REF ; #TARGET_REF , 2004b ) and received a special issue of the journal Computational Linguistics ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #REF ; #REF ; #REF ; #TARGET_REF , 2004b ) and received a special issue of the journal Computational Linguistics ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Word frequency counts in internet search engines are inconsistent and unreliable ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Word frequency counts in internet search engines are inconsistent and unreliable ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #REF ; #TARGET_REF ; #REF ; #REF , 2004b ) and received a special issue of the journal Computational Linguistics ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #REF ; #TARGET_REF ; #REF ; #REF , 2004b ) and received a special issue of the journal Computational Linguistics ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition , the advantages of using linguistically annotated data over raw data are well documented ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In addition , the advantages of using linguistically annotated data over raw data are well documented ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #TARGET_REF : 56 ) unless the web is used as a corpus ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #TARGET_REF : 56 ) unless the web is used as a corpus ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #TARGET_REF ) , distributed virtual worlds ( #REF ) and digital library management ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #TARGET_REF ) , distributed virtual worlds ( #REF ) and digital library management ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #REF ) , distributed virtual worlds ( #REF ) and digital library management ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #REF ) , distributed virtual worlds ( #REF ) and digital library management ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #REF ; #REF ; #TARGET_REF ; #REF , 2004b ) and received a special issue of the journal Computational Linguistics ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #REF ; #REF ; #TARGET_REF ; #REF , 2004b ) and received a special issue of the journal Computational Linguistics ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #REF ; #REF ; #REF ; #REF , 2004b ) and received a special issue of the journal Computational Linguistics ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #REF ; #REF ; #REF ; #REF , 2004b ) and received a special issue of the journal Computational Linguistics ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF built a corpus by iteratively searching Google for a small set of seed terms ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF built a corpus by iteratively searching Google for a small set of seed terms .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The Gsearch system ( #TARGET_REF ) also selects sentences by syntactic criteria from large on-line text collections ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The Gsearch system ( #TARGET_REF ) also selects sentences by syntactic criteria from large on-line text collections .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #TARGET_REF ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #TARGET_REF ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #TARGET_REF ; #REF ; #REF ; #REF , 2004b ) and received a special issue of the journal Computational Linguistics ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #TARGET_REF ; #REF ; #REF ; #REF , 2004b ) and received a special issue of the journal Computational Linguistics ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF extracts word co-occurrence probabilities from unlabelled text collected from a web crawler ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #TARGET_REFa ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #TARGET_REFa ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition , the advantages of using linguistically annotated data over raw data are well documented ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In addition , the advantages of using linguistically annotated data over raw data are well documented ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF"
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Intermedia is no more developed and nobody of us had the opportunity to try it ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Intermedia is no more developed and nobody of us had the opportunity to try it ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Following the example of #TARGET_REF , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by #REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Following the example of #TARGET_REF , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by #REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , a ` web page ' is more similar to an infinite canvas than a written page ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , a ` web page ' is more similar to an infinite canvas than a written page ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "While wikis have spread from a detailed design ( #TARGET_REF ) , unfortunately blogs have not been designed under a model ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: While wikis have spread from a detailed design ( #TARGET_REF ) , unfortunately blogs have not been designed under a model .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Generally speaking , we find that the personal public diary metaphor behind blogs ( #TARGET_REF ) may bring to an unsatisfactory representation of the context ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Generally speaking , we find that the personal public diary metaphor behind blogs ( #TARGET_REF ) may bring to an unsatisfactory representation of the context .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "1.1 Hypertext as a New Writing Space #TARGET_REF was the first scholar who stressed the impact of the digital revolution to the medium of writing."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: 1.1 Hypertext as a New Writing Space #TARGET_REF was the first scholar who stressed the impact of the digital revolution to the medium of writing.\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In other words AJAX is a web development technique for creating interactive web applications using a combination of XHTML and CSS , Document Object Model ( or DOM ) , the XMLHTTPRequest object ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In other words AJAX is a web development technique for creating interactive web applications using a combination of XHTML and CSS , Document Object Model ( or DOM ) , the XMLHTTPRequest object ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Every arc always has a definite direction , i.e. arcs are arrows ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Every arc always has a definite direction , i.e. arcs are arrows ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The Ruby on #TARGET_REF framework permits us to quickly develop web applications without rewriting common functions and classes ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The Ruby on #TARGET_REF framework permits us to quickly develop web applications without rewriting common functions and classes .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Henceforth the collaborative traits of blogs and wikis ( #TARGET_REF ) emphasize annotation , comment , and strong editing ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Henceforth the collaborative traits of blogs and wikis ( #TARGET_REF ) emphasize annotation , comment , and strong editing .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The paradigm is \"write many , read many\" ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The paradigm is \"write many , read many\" ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This is noticeable for German ( #REF ) and Portuguese ( #TARGET_REF ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der #REF ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This is noticeable for German ( #REF ) and Portuguese ( #TARGET_REF ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der #REF ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ Graph transformations for recovering nonprojective structures ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: â¢ Graph transformations for recovering nonprojective structures ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "This is noticeable for German ( #TARGET_REF ) and Portuguese ( #REF ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der #REF ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: This is noticeable for German ( #TARGET_REF ) and Portuguese ( #REF ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der #REF ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by #REF and extended to labeled dependency parsing by #TARGET_REF ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by #REF and extended to labeled dependency parsing by #TARGET_REF .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Typical examples are Bulgarian ( #REF ; #REF ) , Chinese ( #REF ) , Danish ( #REF ) , and Swedish ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Typical examples are Bulgarian ( #REF ; #REF ) , Chinese ( #REF ) , Danish ( #REF ) , and Swedish ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Typical examples are Bulgarian ( #TARGET_REF ; #REF ) , Chinese ( #REF ) , Danish ( #REF ) , and Swedish ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Typical examples are Bulgarian ( #TARGET_REF ; #REF ) , Chinese ( #REF ) , Danish ( #REF ) , and Swedish ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Typical examples are Bulgarian ( #REF ; #REF ) , Chinese ( #TARGET_REF ) , Danish ( #REF ) , and Swedish ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Typical examples are Bulgarian ( #REF ; #REF ) , Chinese ( #TARGET_REF ) , Danish ( #REF ) , and Swedish ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "â¢ History-based feature models for predicting the next parser action ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: â¢ History-based feature models for predicting the next parser action ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "All experiments have been performed using MaltParser ( #TARGET_REF ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .1"
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: All experiments have been performed using MaltParser ( #TARGET_REF ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .1\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "More specifically , we use LIBSVM ( #TARGET_REF ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: More specifically , we use LIBSVM ( #TARGET_REF ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "6The analysis is reminiscent of the treatment of coordination in the Collins parser ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: 6The analysis is reminiscent of the treatment of coordination in the Collins parser ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Although the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of #TARGET_REF ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Although the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of #TARGET_REF .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Typical examples are Bulgarian ( #REF ; #TARGET_REF ) , Chinese ( #REF ) , Danish ( #REF ) , and Swedish ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Typical examples are Bulgarian ( #REF ; #TARGET_REF ) , Chinese ( #REF ) , Danish ( #REF ) , and Swedish ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Japanese ( #TARGET_REF ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Japanese ( #TARGET_REF ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Support vector machines for mapping histories to parser actions ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Support vector machines for mapping histories to parser actions ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "By contrast , Turkish ( #REF ; #TARGET_REF ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: By contrast , Turkish ( #REF ; #TARGET_REF ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: #TARGET_REF have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "For example , #TARGET_REF experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: For example , #TARGET_REF experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: #TARGET_REF describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Such a component would serve as the first stage of a clinical question answering system ( #TARGET_REF ) or summarization system ( #REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Such a component would serve as the first stage of a clinical question answering system ( #TARGET_REF ) or summarization system ( #REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #REF and #TARGET_REF , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Following #REF and #TARGET_REF , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #TARGET_REF ) , information retrieval ( #REF ) , information extraction ( #REF ) , and question answering ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #TARGET_REF ) , information retrieval ( #REF ) , information extraction ( #REF ) , and question answering .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Using the section labels , the HMM was trained using the HTK toolkit ( #TARGET_REF ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Using the section labels , the HMM was trained using the HTK toolkit ( #TARGET_REF ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction\"  , \"methods\"  , \"results\"  , and \"conclusions\"  ( #REF ; #TARGET_REF ; OrË\\x98asan , 2001 ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: As an example, scientific abstracts across many different fields generally follow the pattern of \"introduction\"  , \"methods\"  , \"results\"  , and \"conclusions\"  ( #REF ; #TARGET_REF ; OrË\\x98asan , 2001 ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( #REF ) , and the availability of software that leverages this knowledge -- MetaMap ( #TARGET_REF ) for concept identification and SemRep ( #REF ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( #REF ) , and the availability of software that leverages this knowledge -- MetaMap ( #TARGET_REF ) for concept identification and SemRep ( #REF ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The need for information systems to support physicians at the point of care has been well studied ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The need for information systems to support physicians at the point of care has been well studied ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Our task is closer to the work of #TARGET_REF , who looked at the problem of intellectual attribution in scientific texts ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our task is closer to the work of #TARGET_REF , who looked at the problem of intellectual attribution in scientific texts .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Such a component would serve as the first stage of a clinical question answering system ( #REF ) or summarization system ( #TARGET_REF ) ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Such a component would serve as the first stage of a clinical question answering system ( #REF ) or summarization system ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Table (b) again reproduces the results from #TARGET_REF (2003) for a comparable task on a different subset of 206 unstructured abstracts."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Table (b) again reproduces the results from #TARGET_REF (2003) for a comparable task on a different subset of 206 unstructured abstracts.\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #REF ) , information retrieval ( #REF ) , information extraction ( #TARGET_REF ) , and question answering ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #REF ) , information retrieval ( #REF ) , information extraction ( #TARGET_REF ) , and question answering .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The need for information systems to support physicians at the point of care has been well studied ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The need for information systems to support physicians at the point of care has been well studied ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #REF ) , information retrieval ( #TARGET_REF ) , information extraction ( #REF ) , and question answering ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #REF ) , information retrieval ( #TARGET_REF ) , information extraction ( #REF ) , and question answering .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The table also presents the closest comparable experimental results reported by #TARGET_REF .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: The table also presents the closest comparable experimental results reported by #TARGET_REF .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Building on the work of #REF in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Building on the work of #REF in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The need for information systems to support physicians at the point of care has been well studied ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The need for information systems to support physicians at the point of care has been well studied ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( #REF ) , and the availability of software that leverages this knowledge -- MetaMap ( #REF ) for concept identification and SemRep ( #TARGET_REF ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( #REF ) , and the availability of software that leverages this knowledge -- MetaMap ( #REF ) for concept identification and SemRep ( #TARGET_REF ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Although not the first to employ a generative approach to directly model content , the seminal work of #TARGET_REF is a noteworthy point of reference and comparison ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Although not the first to employ a generative approach to directly model content , the seminal work of #TARGET_REF is a noteworthy point of reference and comparison .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "We work with a semi-technical text on meteorological phenomena ( #TARGET_REF ) , meant for primary school students ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: We work with a semi-technical text on meteorological phenomena ( #TARGET_REF ) , meant for primary school students .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Such systems extract information from some types of syntactic units ( clauses in ( #REF ; #REF ; #TARGET_REF ) ; noun phrases in ( #REF ; #REF ) ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Such systems extract information from some types of syntactic units ( clauses in ( #REF ; #REF ; #TARGET_REF ) ; noun phrases in ( #REF ; #REF ) ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This idea was inspired by #TARGET_REF , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: This idea was inspired by #TARGET_REF , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Most approaches rely on VerbNet ( #TARGET_REF ) and FrameNet ( #REF ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( #REF ; #REF ) and also ( #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Most approaches rely on VerbNet ( #TARGET_REF ) and FrameNet ( #REF ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( #REF ; #REF ) and also ( #REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent ( #TARGET_REFa ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent ( #TARGET_REFa ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #TARGET_REF ) or the system ( #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #TARGET_REF ) or the system ( #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Such systems extract information from some types of syntactic units ( clauses in ( #REF ; #TARGET_REF ; #REF ) ; noun phrases in ( #REF ; #REF ) ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Such systems extract information from some types of syntactic units ( clauses in ( #REF ; #TARGET_REF ; #REF ) ; noun phrases in ( #REF ; #REF ) ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "It helps them build complex knowledge bases by combining components : events , entities and modifiers ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: It helps them build complex knowledge bases by combining components : events , entities and modifiers ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #REF ) or the system ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #REF ) or the system ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "He was a grammarian who analysed Sanskrit ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: He was a grammarian who analysed Sanskrit ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Most approaches rely on VerbNet ( #REF ) and FrameNet ( #REF ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( #REF ; #REF ) and also ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Most approaches rely on VerbNet ( #REF ) and FrameNet ( #REF ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( #REF ; #REF ) and also ( #REF ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This idea was expanded to include nouns and their modifiers through verb nominalizations ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: This idea was expanded to include nouns and their modifiers through verb nominalizations ( #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Most approaches rely on VerbNet ( #REF ) and FrameNet ( #REF ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( #REF ; #REF ) and also ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Most approaches rely on VerbNet ( #REF ) and FrameNet ( #REF ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( #REF ; #REF ) and also ( #REF ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( #TARGET_REF ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Such systems extract information from some types of syntactic units ( clauses in ( #REF ; #REF ; #REF ) ; noun phrases in ( #REF ; #TARGET_REF ) ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Such systems extract information from some types of syntactic units ( clauses in ( #REF ; #REF ; #REF ) ; noun phrases in ( #REF ; #TARGET_REF ) ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #TARGET_REF ; #REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #REF ; #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "This design idea was adopted from TANKA ( #TARGET_REFb ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: This design idea was adopted from TANKA ( #TARGET_REFb ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Such systems extract information from some types of syntactic units ( clauses in ( #TARGET_REF ; #REF ; #REF ) ; noun phrases in ( #REF ; #REF ) ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Such systems extract information from some types of syntactic units ( clauses in ( #TARGET_REF ; #REF ; #REF ) ; noun phrases in ( #REF ; #REF ) ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #TARGET_REF ) ."
            ],
            "label": [
                "BACKGROUND"
            ]
        },
        "input": "sent0: Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"BACKGROUND\"]}"
    },
    {
        "gold": {
            "text": [
                "The list of semantic relations with which we work is based on extensive literature study ( #TARGET_REFa ) ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: The list of semantic relations with which we work is based on extensive literature study ( #TARGET_REFa ) .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #TARGET_REF ) , we tried to adapt the same approach to the German-English language pair ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #TARGET_REF ) , we tried to adapt the same approach to the German-English language pair .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Future research should apply the work of #TARGET_REF and #REF , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Future research should apply the work of #TARGET_REF and #REF , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "Future research should apply the work of #REF and #TARGET_REF , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars ."
            ],
            "label": [
                "MOTIVATION"
            ]
        },
        "input": "sent0: Future research should apply the work of #REF and #TARGET_REF , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .\n",
        "output": "{\"label\": [\"MOTIVATION\"]}"
    },
    {
        "gold": {
            "text": [
                "In our prior work ( #TARGET_REF ) , we examined whether techniques used for predicting the helpfulness of product reviews ( #REF ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: In our prior work ( #TARGET_REF ) , we examined whether techniques used for predicting the helpfulness of product reviews ( #REF ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "( Details of how the average-expert model performs can be found in our prior work ( #TARGET_REF ) . )"
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: ( Details of how the average-expert model performs can be found in our prior work ( #TARGET_REF ) . )\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "We follow our previous work ( #TARGET_REF ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also #REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: We follow our previous work ( #TARGET_REF ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also #REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #TARGET_REF ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #TARGET_REF ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #TARGET_REF ; #REF ; #REF ) .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document ."
            ],
            "label": [
                "FUTURE"
            ]
        },
        "input": "sent0: Following #TARGET_REF , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document .\n",
        "output": "{\"label\": [\"FUTURE\"]}"
    },
    {
        "gold": {
            "text": [
                "It is inspired by the system described in #TARGET_REF ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: It is inspired by the system described in #TARGET_REF .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "This choice is inspired by recent work on learning syntactic categories ( #TARGET_REF ) , which successfully utilized such language models to represent word window contexts of target words ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: This choice is inspired by recent work on learning syntactic categories ( #TARGET_REF ) , which successfully utilized such language models to represent word window contexts of target words .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #REF ) ( #REF ) ( #REF ) or song lyrics ( #TARGET_REF ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #REF ) ( #REF ) ( #REF ) or song lyrics ( #TARGET_REF ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our motivation for generation of material for language education exists in work such as #REF and #TARGET_REF , which deal with automatic generation of classic fill in the blank questions ."
            ],
            "label": [
                "EXTENSION"
            ]
        },
        "input": "sent0: Our motivation for generation of material for language education exists in work such as #REF and #TARGET_REF , which deal with automatic generation of classic fill in the blank questions .\n",
        "output": "{\"label\": [\"EXTENSION\"]}"
    },
    {
        "gold": {
            "text": [
                "Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #TARGET_REF ) ( #REF ) ( #REF ) or song lyrics ( #REF ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #TARGET_REF ) ( #REF ) ( #REF ) or song lyrics ( #REF ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #REF ) ( #REF ) ( #REF ) or song lyrics ( #REF ) ( Ramakrishnan #TARGET_REF ) , where specified meter or rhyme schemes are enforced ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "sent0: Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #REF ) ( #REF ) ( #REF ) or song lyrics ( #REF ) ( Ramakrishnan #TARGET_REF ) , where specified meter or rhyme schemes are enforced .\n",
        "output": "{\"label\": [\"USE\"]}"
    },
    {
        "gold": {
            "text": [
                "3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #TARGET_REF ; #REF ) and create multiple features for length using a decision tree ( J48 ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: 3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #TARGET_REF ; #REF ) and create multiple features for length using a decision tree ( J48 ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "raw length value as a feature , we follow our previous work ( #REF ; #TARGET_REF ) and create multiple features for length using a decision tree ( J48 ) ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: raw length value as a feature , we follow our previous work ( #REF ; #TARGET_REF ) and create multiple features for length using a decision tree ( J48 ) .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    },
    {
        "gold": {
            "text": [
                "The only disambiguation metric that we used in our previous work ( #TARGET_REFb ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right ."
            ],
            "label": [
                "COMPARE_CONTRAST"
            ]
        },
        "input": "sent0: The only disambiguation metric that we used in our previous work ( #TARGET_REFb ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right .\n",
        "output": "{\"label\": [\"COMPARE_CONTRAST\"]}"
    }
]