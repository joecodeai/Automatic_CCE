[
    {
        "gold": {
            "text": [
                "#REF did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.",
                "They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.",
                "This paper made the first attempt on Chinese SRL and produced promising results.",
                "After the PropBank (#REF) was built,  and #REF have produced more complete and systematic research on Chinese SRL.",
                "#TARGET_REF has made some preliminary attempt on the idea of hierarchical semantic role labeling.",
                "However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.",
                "So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.",
                "#REF did very encouraging work on the feature calibration of semantic role labeling.",
                "They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.",
                "For semantic analysis, developing features that capture the right kind of information is crucial.",
                "Experiments on Chinese SRL (Xue and#REF, #REF) reassured these findings."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:#REF did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (#REF) was built,  and #REF have produced more complete and systematic research on Chinese SRL. #TARGET_REF has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #REF did very encouraging work on the feature calibration of semantic role labeling. They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL (Xue and#REF, #REF) reassured these findings.",
        "output": "introduction:#REF did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (#REF) was built,  and #REF have produced more complete and systematic research on Chinese SRL. #TARGET_REF<BACK/> has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #REF did very encouraging work on the feature calibration of semantic role labeling. They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL (Xue and#REF, #REF) reassured these findings."
    },
    {
        "gold": {
            "text": [
                "9 We do not relate to specific results in their study because it has been brought to our attention that #TARGET_REF are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:9 We do not relate to specific results in their study because it has been brought to our attention that #TARGET_REF are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) .",
        "output": "related work:9 We do not relate to specific results in their study because it has been brought to our attention that #TARGET_REF<USE/> are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) ."
    },
    {
        "gold": {
            "text": [
                "The process study most similar to ours is that of Koehn (2009a), who compared scratch, post-edit, and simple interactive modes.",
                "However, he used undergraduate, non-professional subjects, and did not consider re-tuning.",
                "Our experimental design with professional bilingual translators follows our previous work #TARGET_REFa ) comparing scratch translation to post-edit .",
                "Many research translation UIs have been proposed including TransType (#REF), Caitra (#REFb), Thot (Ortiz-Martínez and #REF), TransCenter (#REFb), and CasmaCat (#REF).",
                "However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "related work:The process study most similar to ours is that of Koehn (2009a), who compared scratch, post-edit, and simple interactive modes. However, he used undergraduate, non-professional subjects, and did not consider re-tuning. Our experimental design with professional bilingual translators follows our previous work #TARGET_REFa ) comparing scratch translation to post-edit . Many research translation UIs have been proposed including TransType (#REF), Caitra (#REFb), Thot (Ortiz-Martínez and #REF), TransCenter (#REFb), and CasmaCat (#REF). However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature.",
        "output": "related work:The process study most similar to ours is that of Koehn (2009a), who compared scratch, post-edit, and simple interactive modes. However, he used undergraduate, non-professional subjects, and did not consider re-tuning. Our experimental design with professional bilingual translators follows our previous work #TARGET_REF<COMP/>a ) comparing scratch translation to post-edit . Many research translation UIs have been proposed including TransType (#REF), Caitra (#REFb), Thot (Ortiz-Martínez and #REF), TransCenter (#REFb), and CasmaCat (#REF). However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature."
    },
    {
        "gold": {
            "text": [
                "We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.",
                "Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (#REF;#REF) for self training.",
                "Self-training should also benefit other discriminatively trained parsers with latent annotations ( #TARGET_REF ) , although training would be much slower compared to using generative models , as in our case ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (#REF;#REF) for self training. Self-training should also benefit other discriminatively trained parsers with latent annotations ( #TARGET_REF ) , although training would be much slower compared to using generative models , as in our case .",
        "output": "conclusion:We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (#REF;#REF) for self training. Self-training should also benefit other discriminatively trained parsers with latent annotations ( #TARGET_REF<MOT/> ) , although training would be much slower compared to using generative models , as in our case ."
    },
    {
        "gold": {
            "text": [
                "Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #TARGET_REF ) and Memory-based learning ( MBL ) ( #REF ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .",
                "We will base these components on the design of Weka (#REF)."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #TARGET_REF ) and Memory-based learning ( MBL ) ( #REF ) have been applied to many different problems , so a single interchangeable component should be used to represent each method . We will base these components on the design of Weka (#REF).",
        "output": "nan:Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #TARGET_REF<EXT/> ) and Memory-based learning ( MBL ) ( #REF ) have been applied to many different problems , so a single interchangeable component should be used to represent each method . We will base these components on the design of Weka (#REF)."
    },
    {
        "gold": {
            "text": [
                "The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).",
                "The system is implemented based on ( #REF ) and ( #TARGET_REF ) .",
                "In the system, we extract both the minimal GHKM rules (#REF), and the rules of SPMT Model 1 (#REF) with phrases up to length L=5 on the source side."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on ( #REF ) and ( #TARGET_REF ) . In the system, we extract both the minimal GHKM rules (#REF), and the rules of SPMT Model 1 (#REF) with phrases up to length L=5 on the source side.",
        "output": "experiments:The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on ( #REF ) and ( #TARGET_REF<FUT/> ) . In the system, we extract both the minimal GHKM rules (#REF), and the rules of SPMT Model 1 (#REF) with phrases up to length L=5 on the source side."
    },
    {
        "gold": {
            "text": [
                "In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).",
                "Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #REF , 2009 ; #REF ; #REF , 2006 ; #TARGET_REF ; #REF ; #REFb ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #REF , 2009 ; #REF ; #REF , 2006 ; #TARGET_REF ; #REF ; #REFb ) .",
        "output": "introduction:In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #REF , 2009 ; #REF ; #REF , 2006 ; #TARGET_REF<BACK/> ; #REF ; #REFb ) ."
    },
    {
        "gold": {
            "text": [
                "For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #TARGET_REF ) .",
                "Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #TARGET_REF ) . Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.",
        "output": "experiments:For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #TARGET_REF<USE/> ) . Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario."
    },
    {
        "gold": {
            "text": [
                "In our prior work ( #TARGET_REF ) , we examined whether techniques used for predicting the helpfulness of product reviews ( #REF ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review .",
                "While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating (e.g.",
                "author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model.",
                "In fact, we observe that peer-review helpfulness seems to differ not only between students and experts (example 1), but also between types of experts (example 2)."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:In our prior work ( #TARGET_REF ) , we examined whether techniques used for predicting the helpfulness of product reviews ( #REF ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review . While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating (e.g. author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model. In fact, we observe that peer-review helpfulness seems to differ not only between students and experts (example 1), but also between types of experts (example 2).",
        "output": "introduction:In our prior work ( #TARGET_REF<COMP/> ) , we examined whether techniques used for predicting the helpfulness of product reviews ( #REF ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review . While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating (e.g. author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model. In fact, we observe that peer-review helpfulness seems to differ not only between students and experts (example 1), but also between types of experts (example 2)."
    },
    {
        "gold": {
            "text": [
                "The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.",
                "The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student's answer.",
                "At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #TARGET_REF ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "experiments:The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness. The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student's answer. At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #TARGET_REF .",
        "output": "experiments:The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness. The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student's answer. At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #TARGET_REF<MOT/> ."
    },
    {
        "gold": {
            "text": [
                "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.",
                "They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #TARGET_REF ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) .",
                "One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF).",
                "With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.",
                "After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #TARGET_REF ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases.",
        "output": "nan:Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #TARGET_REF<EXT/> ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases."
    },
    {
        "gold": {
            "text": [
                "For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #TARGET_REF , 2008 ; KÃ¼bler , McDonald , and #REF ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term \"dev set\" to denote a non-blind test set, used for model development (feature selection and feature engineering).",
                "We do not perform further weight optimization (which, if done, is done on a separate \"tuning set\").",
                "to predict the next state in the parse derivation.",
                "All experiments were done using the Nivre \"eager\" algorithm.",
                "11 There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent).",
                "There are default MaltParser features (in the machine learning sense), 12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #TARGET_REF , 2008 ; KÃ¼bler , McDonald , and #REF ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term \"dev set\" to denote a non-blind test set, used for model development (feature selection and feature engineering). We do not perform further weight optimization (which, if done, is done on a separate \"tuning set\"). to predict the next state in the parse derivation. All experiments were done using the Nivre \"eager\" algorithm. 11 There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent). There are default MaltParser features (in the machine learning sense), 12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers.",
        "output": "related work:For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #TARGET_REF<FUT/> , 2008 ; KÃ¼bler , McDonald , and #REF ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term \"dev set\" to denote a non-blind test set, used for model development (feature selection and feature engineering). We do not perform further weight optimization (which, if done, is done on a separate \"tuning set\"). to predict the next state in the parse derivation. All experiments were done using the Nivre \"eager\" algorithm. 11 There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent). There are default MaltParser features (in the machine learning sense), 12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers."
    },
    {
        "gold": {
            "text": [
                "More details can be found in #TARGET_REF et al. (2013).",
                "The difference here is that we also consider the validity of mention heads using �(u),�(m)"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:More details can be found in #TARGET_REF et al. (2013). The difference here is that we also consider the validity of mention heads using �(u),�(m)",
        "output": "nan:More details can be found in #TARGET_REF<BACK/> et al. (2013). The difference here is that we also consider the validity of mention heads using �(u),�(m)"
    },
    {
        "gold": {
            "text": [
                "This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #TARGET_REF .",
                "It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #TARGET_REF . It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations.",
        "output": "nan:This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #TARGET_REF<USE/> . It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations."
    },
    {
        "gold": {
            "text": [
                "The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #TARGET_REF ) .",
                "(#REF).",
                "From all 4,832 user turns, 68.5% were non-angry, 14.3% slightly angry, 5.0% very angry and 12.2% contained garbage, i.e. nonspeech events.",
                "In total, the number of interaction parameters servings as input variables for the model amounts to 52."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #TARGET_REF ) . (#REF). From all 4,832 user turns, 68.5% were non-angry, 14.3% slightly angry, 5.0% very angry and 12.2% contained garbage, i.e. nonspeech events. In total, the number of interaction parameters servings as input variables for the model amounts to 52.",
        "output": "nan:The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #TARGET_REF<COMP/> ) . (#REF). From all 4,832 user turns, 68.5% were non-angry, 14.3% slightly angry, 5.0% very angry and 12.2% contained garbage, i.e. nonspeech events. In total, the number of interaction parameters servings as input variables for the model amounts to 52."
    },
    {
        "gold": {
            "text": [
                "Even better accuracy can be achieved with a more fine-grained link class structure.",
                "Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .",
                "Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #TARGET_REF ) .",
                "If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.",
                "In this manner, the model can account for a wider range of translation phenomena."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy . Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #TARGET_REF ) . If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena.",
        "output": "conclusion:Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy . Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #TARGET_REF<MOT/> ) . If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena."
    },
    {
        "gold": {
            "text": [
                "The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form.",
                "This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.",
                "In this work, we use the Arabic root extraction technique in (El #REF).",
                "It compares favorably to other stemming or root extraction algorithms ( #TARGET_REF ; #REF ; and #REF ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .",
                "In the remainder of this paper, we will use the term \"root\" and \"term\" interchangeably to refer to canonical forms obtained through this root extraction process."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form. This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root. In this work, we use the Arabic root extraction technique in (El #REF). It compares favorably to other stemming or root extraction algorithms ( #TARGET_REF ; #REF ; and #REF ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs . In the remainder of this paper, we will use the term \"root\" and \"term\" interchangeably to refer to canonical forms obtained through this root extraction process.",
        "output": "nan:The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form. This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root. In this work, we use the Arabic root extraction technique in (El #REF). It compares favorably to other stemming or root extraction algorithms ( #TARGET_REF<EXT/> ; #REF ; and #REF ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs . In the remainder of this paper, we will use the term \"root\" and \"term\" interchangeably to refer to canonical forms obtained through this root extraction process."
    },
    {
        "gold": {
            "text": [
                "In this section, we present the coreference results on the devtest defined earlier.",
                "First, to see the effect of stem matching features, we compare two coreference systems: one with the stem features, the other with- out.",
                "We test the two systems on both \"true\" and system mentions of the devtest set.",
                "True mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.",
                "We report results with two metrics: ECM-F and ACE- Value.",
                "ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #TARGET_REF ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .",
                "The result is shown in Table 4: the baseline numbers without stem features are listed under \\Base,\" and the results of the coreference system with stem features are listed under \\Base+Stem.\""
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:In this section, we present the coreference results on the devtest defined earlier. First, to see the effect of stem matching features, we compare two coreference systems: one with the stem features, the other with- out. We test the two systems on both \"true\" and system mentions of the devtest set. True mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system. We report results with two metrics: ECM-F and ACE- Value. ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #TARGET_REF ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric . The result is shown in Table 4: the baseline numbers without stem features are listed under \\Base,\" and the results of the coreference system with stem features are listed under \\Base+Stem.\"",
        "output": "experiments:In this section, we present the coreference results on the devtest defined earlier. First, to see the effect of stem matching features, we compare two coreference systems: one with the stem features, the other with- out. We test the two systems on both \"true\" and system mentions of the devtest set. True mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system. We report results with two metrics: ECM-F and ACE- Value. ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #TARGET_REF<FUT/> ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric . The result is shown in Table 4: the baseline numbers without stem features are listed under \\Base,\" and the results of the coreference system with stem features are listed under \\Base+Stem.\""
    },
    {
        "gold": {
            "text": [
                "Combining control strategies depends on a way to differentiate between types of constraints.",
                "Proceedings of EACL '99 example , the ALE parser ( #TARGET_REF ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown .",
                "In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted.",
                "A literal (goal) is considered a parse lype literal (goal) if it has as its single argument a typed feature structure of a type specified as a parse type.",
                "1° All types in the type hierarchy can be used as parse types.",
                "This way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering.",
                "However, in the remainder we will concentrate on a specific class of parse types: We assume the specification of type sign and its subtypes as parse types.",
                "11 This choice is based on the observation that the constraints on type sign and its sub-types play an important guiding role in the parsing process and are best interpreted bottom-up given the lexical orientation of I-IPSG."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Combining control strategies depends on a way to differentiate between types of constraints. Proceedings of EACL '99 example , the ALE parser ( #TARGET_REF ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown . In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted. A literal (goal) is considered a parse lype literal (goal) if it has as its single argument a typed feature structure of a type specified as a parse type. 1° All types in the type hierarchy can be used as parse types. This way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering. However, in the remainder we will concentrate on a specific class of parse types: We assume the specification of type sign and its subtypes as parse types. 11 This choice is based on the observation that the constraints on type sign and its sub-types play an important guiding role in the parsing process and are best interpreted bottom-up given the lexical orientation of I-IPSG.",
        "output": "nan:Combining control strategies depends on a way to differentiate between types of constraints. Proceedings of EACL '99 example , the ALE parser ( #TARGET_REF<BACK/> ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown . In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted. A literal (goal) is considered a parse lype literal (goal) if it has as its single argument a typed feature structure of a type specified as a parse type. 1° All types in the type hierarchy can be used as parse types. This way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering. However, in the remainder we will concentrate on a specific class of parse types: We assume the specification of type sign and its subtypes as parse types. 11 This choice is based on the observation that the constraints on type sign and its sub-types play an important guiding role in the parsing process and are best interpreted bottom-up given the lexical orientation of I-IPSG."
    },
    {
        "gold": {
            "text": [
                "Several works have proposed discriminative techniques to train log-linear model for SMT.",
                "( #TARGET_REF ; #REF ) used maximum likelihood estimation to learn weights for MT.",
                "(#REF;#REF;#REF; #REF) employed an evaluation metric as a loss function and directly optimized it.",
                "(#REF;#REF;#REF) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Several works have proposed discriminative techniques to train log-linear model for SMT. ( #TARGET_REF ; #REF ) used maximum likelihood estimation to learn weights for MT. (#REF;#REF;#REF; #REF) employed an evaluation metric as a loss function and directly optimized it. (#REF;#REF;#REF) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.",
        "output": "related work:Several works have proposed discriminative techniques to train log-linear model for SMT. ( #TARGET_REF<USE/> ; #REF ) used maximum likelihood estimation to learn weights for MT. (#REF;#REF;#REF; #REF) employed an evaluation metric as a loss function and directly optimized it. (#REF;#REF;#REF) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions."
    },
    {
        "gold": {
            "text": [
                "To address this limitation , our previous work ( #TARGET_REF ) has initiated an investigation on the problem of conversation entailment .",
                "The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H.",
                "For instance, as in Example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot.",
                "While our previous work has provided some interesting preliminary observations, it mostly focused on data collection and initial experiments and analysis using a small set of development data.",
                "It is not clear whether the previous results are generally applicable, how different components in the entailment framework interact with each other, and how different representations may influence the entailment outcome."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:To address this limitation , our previous work ( #TARGET_REF ) has initiated an investigation on the problem of conversation entailment . The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H. For instance, as in Example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot. While our previous work has provided some interesting preliminary observations, it mostly focused on data collection and initial experiments and analysis using a small set of development data. It is not clear whether the previous results are generally applicable, how different components in the entailment framework interact with each other, and how different representations may influence the entailment outcome.",
        "output": "introduction:To address this limitation , our previous work ( #TARGET_REF<COMP/> ) has initiated an investigation on the problem of conversation entailment . The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H. For instance, as in Example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot. While our previous work has provided some interesting preliminary observations, it mostly focused on data collection and initial experiments and analysis using a small set of development data. It is not clear whether the previous results are generally applicable, how different components in the entailment framework interact with each other, and how different representations may influence the entailment outcome."
    },
    {
        "gold": {
            "text": [
                "Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.",
                "This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.",
                "Future research should apply the work of #REF and #TARGET_REF , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations. This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension. Future research should apply the work of #REF and #TARGET_REF , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .",
        "output": "conclusion:Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations. This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension. Future research should apply the work of #REF and #TARGET_REF<MOT/> , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars ."
    },
    {
        "gold": {
            "text": [
                "Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors.",
                "In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.",
                "For example , modeling CASE in Czech improves Czech parsing ( #TARGET_REF ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .",
                "It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and #REF;Nivre, Boguslavsky, and #REF;."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors. In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. For example , modeling CASE in Czech improves Czech parsing ( #TARGET_REF ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy . It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and #REF;Nivre, Boguslavsky, and #REF;.",
        "output": "introduction:Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors. In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. For example , modeling CASE in Czech improves Czech parsing ( #TARGET_REF<EXT/> ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy . It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and #REF;Nivre, Boguslavsky, and #REF;."
    },
    {
        "gold": {
            "text": [
                "In our case, the clustering is performed by the program Snob, which implements mixture model- ing combined with model selection based on the Minimum Message Length (MML) criterion (#REF; #REF).",
                "We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2).",
                "The input to Snob is a set of binary vectors, one vector per response document.",
                "The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency).4",
                "The predictive model is a Decision Graph ( #TARGET_REF ) , which , like Snob , is based on the MML principle .",
                "The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature.",
                "The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct.",
                "This probability is our indicator of whether the Doc-Pred method can address a new request."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:In our case, the clustering is performed by the program Snob, which implements mixture model- ing combined with model selection based on the Minimum Message Length (MML) criterion (#REF; #REF). We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency).4 The predictive model is a Decision Graph ( #TARGET_REF ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct. This probability is our indicator of whether the Doc-Pred method can address a new request.",
        "output": "method:In our case, the clustering is performed by the program Snob, which implements mixture model- ing combined with model selection based on the Minimum Message Length (MML) criterion (#REF; #REF). We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency).4 The predictive model is a Decision Graph ( #TARGET_REF<FUT/> ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct. This probability is our indicator of whether the Doc-Pred method can address a new request."
    },
    {
        "gold": {
            "text": [
                "To copy otherwise, or to republish, requires a fee and/or specific permission.",
                "0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (#REFb).",
                "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.",
                "#REF;#REF) consult relatively small lexicons, typically generated by hand.",
                "Two exceptions to this generalisation are the Linguistic String Project (#REF) and the IBM CRITIQUE (formerly EPISTLE) Project (#REF;#REF); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources.",
                "In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #TARGET_REF for details ) .",
                "However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.",
                "In this paper we provide an evaluation of the LDOCE grammar code system from this perspective."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (#REFb). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #REF;#REF) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project (#REF) and the IBM CRITIQUE (formerly EPISTLE) Project (#REF;#REF); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #TARGET_REF for details ) . However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.",
        "output": "introduction:To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (#REFb). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #REF;#REF) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project (#REF) and the IBM CRITIQUE (formerly EPISTLE) Project (#REF;#REF); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #TARGET_REF<BACK/> for details ) . However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective."
    },
    {
        "gold": {
            "text": [
                "The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, I5, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear.",
                "On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic.",
                "#REF and #REF provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description.",
                "#TARGET_REF comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, I5, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear. On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic. #REF and #REF provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description. #TARGET_REF comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .",
        "output": "nan:The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, I5, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear. On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic. #REF and #REF provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description. #TARGET_REF<USE/> comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated ."
    },
    {
        "gold": {
            "text": [
                "We do not claim that Gla is the best or unique way of expressing the rule \"assume that the writer did not say too much.\"",
                "Rather, we stress the possibility that one can axiomatize and productively use such a rule.",
                "We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #TARGET_REF ."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:We do not claim that Gla is the best or unique way of expressing the rule \"assume that the writer did not say too much.\" Rather, we stress the possibility that one can axiomatize and productively use such a rule. We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #TARGET_REF .",
        "output": "introduction:We do not claim that Gla is the best or unique way of expressing the rule \"assume that the writer did not say too much.\" Rather, we stress the possibility that one can axiomatize and productively use such a rule. We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #TARGET_REF<COMP/> ."
    },
    {
        "gold": {
            "text": [
                "Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.",
                "For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #REF ; #REF ) and history-based parsing ( #TARGET_REF ) .",
                "We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF)."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #REF ; #REF ) and history-based parsing ( #TARGET_REF ) . We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF).",
        "output": "conclusion:Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #REF ; #REF ) and history-based parsing ( #TARGET_REF<MOT/> ) . We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF)."
    },
    {
        "gold": {
            "text": [
                "To process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P .",
                "This idea was inspired by #TARGET_REF , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) .",
                "In recent approaches, syntactic information is translated into features which, together with information from FrameNet, WordNet or VerbNet, will be used with ML tools to make predictions for each example in the test set (#REF;#REF)."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:To process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P . This idea was inspired by #TARGET_REF , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) . In recent approaches, syntactic information is translated into features which, together with information from FrameNet, WordNet or VerbNet, will be used with ML tools to make predictions for each example in the test set (#REF;#REF).",
        "output": "nan:To process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P . This idea was inspired by #TARGET_REF<EXT/> , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) . In recent approaches, syntactic information is translated into features which, together with information from FrameNet, WordNet or VerbNet, will be used with ML tools to make predictions for each example in the test set (#REF;#REF)."
    },
    {
        "gold": {
            "text": [
                "To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#TARGET_REF ) .",
                "#REF).",
                "FrameNet is an online lexical resource for English based on the principles of Frame Semantics.",
                "In this approach, a word evokes a frame i.e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.",
                "Finally each frame is associated with a set of target words, the words that evoke that frame."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#TARGET_REF ) . #REF). FrameNet is an online lexical resource for English based on the principles of Frame Semantics. In this approach, a word evokes a frame i.e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame. Finally each frame is associated with a set of target words, the words that evoke that frame.",
        "output": "nan:To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#TARGET_REF<FUT/> ) . #REF). FrameNet is an online lexical resource for English based on the principles of Frame Semantics. In this approach, a word evokes a frame i.e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame. Finally each frame is associated with a set of target words, the words that evoke that frame."
    },
    {
        "gold": {
            "text": [
                "Many different features have been used to represent documents where an ambiguous name is mentioned.",
                "The most basic is a Bag of Words (BoW) representation of the document text.",
                "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #REF ; #TARGET_REF ) .",
                "Nevertheless, the full document text is present in most systems, sometimes as the only feature (#REF) and sometimes in combination with otherssee for instance (#REF;#REF)-.",
                "Other representations use the link structure (#REF) or generate graph representations of the extracted features (#REF).",
                "Some researchers (#REF;#REF) have explored the use of Wikipedia information to improve the disambiguation process.",
                "Wikipedia provides candidate entities that are linked to specific mentions in a text.",
                "The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.",
                "These approaches are yet to be applied to the specific task of grouping search results."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #REF ; #TARGET_REF ) . Nevertheless, the full document text is present in most systems, sometimes as the only feature (#REF) and sometimes in combination with otherssee for instance (#REF;#REF)-. Other representations use the link structure (#REF) or generate graph representations of the extracted features (#REF). Some researchers (#REF;#REF) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results.",
        "output": "related work:Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #REF ; #TARGET_REF<BACK/> ) . Nevertheless, the full document text is present in most systems, sometimes as the only feature (#REF) and sometimes in combination with otherssee for instance (#REF;#REF)-. Other representations use the link structure (#REF) or generate graph representations of the extracted features (#REF). Some researchers (#REF;#REF) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results."
    },
    {
        "gold": {
            "text": [
                "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF.",
                "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF, assertional statements as in #REF, or semantic nets as in #REF.",
                "That is, the current system learns procedures rather than data structures.",
                "There is some literature on procedure acquisition such as the LISP synthesis work described in #REF and the PROLOG synthesis method of #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF. The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF, assertional statements as in #REF, or semantic nets as in #REF. That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in #REF and the PROLOG synthesis method of #TARGET_REF .",
        "output": "nan:The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF. The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF, assertional statements as in #REF, or semantic nets as in #REF. That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in #REF and the PROLOG synthesis method of #TARGET_REF<USE/> ."
    },
    {
        "gold": {
            "text": [
                "For both languages, we resolved coreference by using SWIZZLE,our implementation of a bilingual coreference resolver.",
                "SWIZZLE is a multilingual enhancement of COCKTAIL ( #TARGET_REF ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  .",
                "When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively.",
                "When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuris- tics for coreference.",
                "Our experiments show that SWIZZLEoutperformed COCKTAILon both English and Romanian test documents."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:For both languages, we resolved coreference by using SWIZZLE,our implementation of a bilingual coreference resolver. SWIZZLE is a multilingual enhancement of COCKTAIL ( #TARGET_REF ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  . When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively. When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuris- tics for coreference. Our experiments show that SWIZZLEoutperformed COCKTAILon both English and Romanian test documents.",
        "output": "introduction:For both languages, we resolved coreference by using SWIZZLE,our implementation of a bilingual coreference resolver. SWIZZLE is a multilingual enhancement of COCKTAIL ( #TARGET_REF<COMP/> ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  . When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively. When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuris- tics for coreference. Our experiments show that SWIZZLEoutperformed COCKTAILon both English and Romanian test documents."
    },
    {
        "gold": {
            "text": [
                "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #TARGET_REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #TARGET_REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) .",
        "output": "conclusion:Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #TARGET_REF<MOT/>a ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus.",
                "The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine.",
                "For the hyperparameters, we set Į to 0.1 and p expand = 1/3 to give a preference to the rules with small fragments.",
                "We built an s2t translation system with the achieved U-trees after the 1000th iteration.",
                "We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11",
                "11 From ( #TARGET_REF ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .",
                "Thus, to be convenient, we only conduct experiments with the SAMT system."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus. The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine. For the hyperparameters, we set Į to 0.1 and p expand = 1/3 to give a preference to the rules with small fragments. We built an s2t translation system with the achieved U-trees after the 1000th iteration. We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11 11 From ( #TARGET_REF ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags . Thus, to be convenient, we only conduct experiments with the SAMT system.",
        "output": "experiments:For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus. The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine. For the hyperparameters, we set Į to 0.1 and p expand = 1/3 to give a preference to the rules with small fragments. We built an s2t translation system with the achieved U-trees after the 1000th iteration. We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11 11 From ( #TARGET_REF<EXT/> ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags . Thus, to be convenient, we only conduct experiments with the SAMT system."
    },
    {
        "gold": {
            "text": [
                "All results are reported mainly in terms of labeled attachment accuracy score (the parent word and the type of dependency relation to it, abbreviated as LAS), which is also used for greedy (hill-climbing) decisions for feature combination.",
                "Unlabeled attachment accuracy score (UAS) and label accuracy (dependency relation regardless of parent, LS) are also given.",
                "For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #TARGET_REF .",
                "We denote p < 0.05 and p < 0.01 with + and ++ , respectively."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:All results are reported mainly in terms of labeled attachment accuracy score (the parent word and the type of dependency relation to it, abbreviated as LAS), which is also used for greedy (hill-climbing) decisions for feature combination. Unlabeled attachment accuracy score (UAS) and label accuracy (dependency relation regardless of parent, LS) are also given. For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #TARGET_REF . We denote p < 0.05 and p < 0.01 with + and ++ , respectively.",
        "output": "related work:All results are reported mainly in terms of labeled attachment accuracy score (the parent word and the type of dependency relation to it, abbreviated as LAS), which is also used for greedy (hill-climbing) decisions for feature combination. Unlabeled attachment accuracy score (UAS) and label accuracy (dependency relation regardless of parent, LS) are also given. For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #TARGET_REF<FUT/> . We denote p < 0.05 and p < 0.01 with + and ++ , respectively."
    },
    {
        "gold": {
            "text": [
                "Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.",
                "However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #REFb ; #REFa ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability. However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #REFb ; #REFa ; #TARGET_REF ; #REF ) .",
        "output": "introduction:Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability. However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #REFb ; #REFa ; #TARGET_REF<BACK/> ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "Another common approach to lexical rules is to encode them as unary phrase structure rules.",
                "This approach is taken , for example , in LKB ( #TARGET_REF ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #REF , 31 ) .",
                "A similar method is included in PATR-II (#REF) and can be used to encode lexical rules as binary relations in the CUF system (#REF; D6rre and #REFb) or the TFS system (#REF;#REF).",
                "The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken , for example , in LKB ( #TARGET_REF ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #REF , 31 ) . A similar method is included in PATR-II (#REF) and can be used to encode lexical rules as binary relations in the CUF system (#REF; D6rre and #REFb) or the TFS system (#REF;#REF). The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.",
        "output": "related work:Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken , for example , in LKB ( #TARGET_REF<USE/> ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #REF , 31 ) . A similar method is included in PATR-II (#REF) and can be used to encode lexical rules as binary relations in the CUF system (#REF; D6rre and #REFb) or the TFS system (#REF;#REF). The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."
    },
    {
        "gold": {
            "text": [
                "Bridging or associative anaphora has been widely discussed in the linguistic literature (#REF;#REF;#REF;Löbner, 1998).",
                "#REF and #REF include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference).",
                "We follow our previous work ( #TARGET_REFb ) and restrict bridging to non-coreferential cases .",
                "We also exclude comparative anaphora (#REF)."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:Bridging or associative anaphora has been widely discussed in the linguistic literature (#REF;#REF;#REF;Löbner, 1998). #REF and #REF include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work ( #TARGET_REFb ) and restrict bridging to non-coreferential cases . We also exclude comparative anaphora (#REF).",
        "output": "introduction:Bridging or associative anaphora has been widely discussed in the linguistic literature (#REF;#REF;#REF;Löbner, 1998). #REF and #REF include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work ( #TARGET_REF<COMP/>b ) and restrict bridging to non-coreferential cases . We also exclude comparative anaphora (#REF)."
    },
    {
        "gold": {
            "text": [
                "• Site based corpus annotation -in which the user can specify a web site to annotate • Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate • Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger).",
                "Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #TARGET_REFb ) and shingling techniques described by #REF ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "method:• Site based corpus annotation -in which the user can specify a web site to annotate • Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate • Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger). Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #TARGET_REFb ) and shingling techniques described by #REF .",
        "output": "method:• Site based corpus annotation -in which the user can specify a web site to annotate • Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate • Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger). Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #TARGET_REF<MOT/>b ) and shingling techniques described by #REF ."
    },
    {
        "gold": {
            "text": [
                "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.",
                "They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #TARGET_REF ) .",
                "One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF).",
                "With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.",
                "After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #TARGET_REF ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases.",
        "output": "nan:Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #TARGET_REF<EXT/> ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases."
    },
    {
        "gold": {
            "text": [
                "Training was done on the Penn Treebank ( #TARGET_REF ) Wall Street Journal data , sections 02-21 .",
                "To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.",
                "This is done using the \"Chunklink\" program provided for CoNLL-2000 (Tjong Kim Sang and#REF)."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:Training was done on the Penn Treebank ( #TARGET_REF ) Wall Street Journal data , sections 02-21 . To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations. This is done using the \"Chunklink\" program provided for CoNLL-2000 (Tjong Kim Sang and#REF).",
        "output": "experiments:Training was done on the Penn Treebank ( #TARGET_REF<FUT/> ) Wall Street Journal data , sections 02-21 . To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations. This is done using the \"Chunklink\" program provided for CoNLL-2000 (Tjong Kim Sang and#REF)."
    },
    {
        "gold": {
            "text": [
                "One approach to partial parsing was presented by #TARGET_REF , who extended a shallow-parsing technique to partial parsing ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:One approach to partial parsing was presented by #TARGET_REF , who extended a shallow-parsing technique to partial parsing .",
        "output": "introduction:One approach to partial parsing was presented by #TARGET_REF<BACK/> , who extended a shallow-parsing technique to partial parsing ."
    },
    {
        "gold": {
            "text": [
                "19 The paper by #TARGET_REF presents additional , more sophisticated models that we do not use in this article ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:19 The paper by #TARGET_REF presents additional , more sophisticated models that we do not use in this article .",
        "output": "related work:19 The paper by #TARGET_REF<USE/> presents additional , more sophisticated models that we do not use in this article ."
    },
    {
        "gold": {
            "text": [
                "The tutorial policy makes a high-level decision as to which strategy to use (for example, \"acknowledge the correct part and give a high specificity hint\") based on the answer analysis and dialogue context.",
                "At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy .",
                "Since the system accepts unrestricted input, interpretation errors are unavoidable.",
                "Our recovery policy is modeled on the TargetedHelp ( #TARGET_REF ) policy used in task-oriented dialogue .",
                "If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I'm sorry, I'm having a problem understanding.",
                "I don't know the word power.\"",
                "The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:The tutorial policy makes a high-level decision as to which strategy to use (for example, \"acknowledge the correct part and give a high specificity hint\") based on the answer analysis and dialogue context. At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp ( #TARGET_REF ) policy used in task-oriented dialogue . If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I'm sorry, I'm having a problem understanding. I don't know the word power.\" The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.",
        "output": "experiments:The tutorial policy makes a high-level decision as to which strategy to use (for example, \"acknowledge the correct part and give a high specificity hint\") based on the answer analysis and dialogue context. At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp ( #TARGET_REF<COMP/> ) policy used in task-oriented dialogue . If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I'm sorry, I'm having a problem understanding. I don't know the word power.\" The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers."
    },
    {
        "gold": {
            "text": [
                "In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification.",
                "Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.",
                "#REF have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #TARGET_REF ; #REF ) .",
                "The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification. Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes. #REF have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #TARGET_REF ; #REF ) . The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation.",
        "output": "conclusion:In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification. Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes. #REF have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #TARGET_REF<MOT/> ; #REF ) . The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation."
    },
    {
        "gold": {
            "text": [
                "Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.",
                "Many investigators (e.g.",
                "Many investigators ( e.g. #REF ; #REF ; #REF ; #TARGET_REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .",
                "And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in #REF and O'#REF---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem.",
                "As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.",
                "We believe that one reason for the improvement has to do with the increased pitch range that our system uses.",
                "Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. #REF ; #REF ; #REF ; #TARGET_REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in #REF and O'#REF---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses. Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.",
        "output": "experiments:Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. #REF ; #REF ; #REF ; #TARGET_REF<EXT/> ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in #REF and O'#REF---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses. Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations."
    },
    {
        "gold": {
            "text": [
                "With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #TARGET_REF ; Bentin , S. and #REF ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .",
                "Our cross-modal and masked priming experiment on Bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically/orthographically unrelated.",
                "These observations are similar to those reported for English and indicate that derivationally suffixed words in Bangla are in general accessed through decomposition of the word into its constituent morphemes.",
                "Further, based on the experimental data we have developed a series of computational models that can be used to predict the decomposition of Bangla polymorphemic words.",
                "Our evaluation result shows that decomposition of a polymorphemic word depends on several factors like, frequency, productivity of the suffix and the compositionality between the stem and the suffix."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #TARGET_REF ; Bentin , S. and #REF ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla . Our cross-modal and masked priming experiment on Bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically/orthographically unrelated. These observations are similar to those reported for English and indicate that derivationally suffixed words in Bangla are in general accessed through decomposition of the word into its constituent morphemes. Further, based on the experimental data we have developed a series of computational models that can be used to predict the decomposition of Bangla polymorphemic words. Our evaluation result shows that decomposition of a polymorphemic word depends on several factors like, frequency, productivity of the suffix and the compositionality between the stem and the suffix.",
        "output": "introduction:With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #TARGET_REF<FUT/> ; Bentin , S. and #REF ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla . Our cross-modal and masked priming experiment on Bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically/orthographically unrelated. These observations are similar to those reported for English and indicate that derivationally suffixed words in Bangla are in general accessed through decomposition of the word into its constituent morphemes. Further, based on the experimental data we have developed a series of computational models that can be used to predict the decomposition of Bangla polymorphemic words. Our evaluation result shows that decomposition of a polymorphemic word depends on several factors like, frequency, productivity of the suffix and the compositionality between the stem and the suffix."
    },
    {
        "gold": {
            "text": [
                "The final interface we intend to implement is a collection of web services for NLP.",
                "A web service provides a remote procedure that can be called using XML based encodings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP.",
                "Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.",
                "This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.",
                "There have already been several attempts to develop distributed NLP systems for dialogue systems ( #TARGET_REF ) and speech recognition ( #REF ) .",
                "Web services will allow components developed by different researchers in different locations to be composed to build larger systems."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:The final interface we intend to implement is a collection of web services for NLP. A web service provides a remote procedure that can be called using XML based encodings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP. Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI. This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed. There have already been several attempts to develop distributed NLP systems for dialogue systems ( #TARGET_REF ) and speech recognition ( #REF ) . Web services will allow components developed by different researchers in different locations to be composed to build larger systems.",
        "output": "nan:The final interface we intend to implement is a collection of web services for NLP. A web service provides a remote procedure that can be called using XML based encodings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP. Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI. This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed. There have already been several attempts to develop distributed NLP systems for dialogue systems ( #TARGET_REF<BACK/> ) and speech recognition ( #REF ) . Web services will allow components developed by different researchers in different locations to be composed to build larger systems."
    },
    {
        "gold": {
            "text": [
                "For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.",
                "It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases.",
                "This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.",
                "Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.",
                "As a generalization, #REF notes that lexicons such as COMLEX tend to demonstrate high precision but low recall.",
                "#TARGET_REF report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .",
                "Precision was quite high (95%), but recall was low (84%)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon. It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases. This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization, #REF notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. #TARGET_REF report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%).",
        "output": "nan:For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon. It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases. This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization, #REF notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. #TARGET_REF<USE/> report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%)."
    },
    {
        "gold": {
            "text": [
                "In P2, on the other hand, we recast SC as a se- quence labeling task.",
                "In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance la- bel for each post in the input post sequence.",
                "This choice is motivated by an observation we made previously ( #TARGET_REFa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3"
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:In P2, on the other hand, we recast SC as a se- quence labeling task. In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance la- bel for each post in the input post sequence. This choice is motivated by an observation we made previously ( #TARGET_REFa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3",
        "output": "experiments:In P2, on the other hand, we recast SC as a se- quence labeling task. In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance la- bel for each post in the input post sequence. This choice is motivated by an observation we made previously ( #TARGET_REF<COMP/>a ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3"
    },
    {
        "gold": {
            "text": [
                "Although exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.",
                "In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled.",
                "The true utility of content models is to structure abstracts that have no structure to begin with.",
                "Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance.",
                "Such a component would serve as the first stage of a clinical question answering system ( #TARGET_REF ) or summarization system ( #REF ) .",
                "We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Although exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process. In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled. The true utility of content models is to structure abstracts that have no structure to begin with. Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. Such a component would serve as the first stage of a clinical question answering system ( #TARGET_REF ) or summarization system ( #REF ) . We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured.",
        "output": "conclusion:Although exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process. In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled. The true utility of content models is to structure abstracts that have no structure to begin with. Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. Such a component would serve as the first stage of a clinical question answering system ( #TARGET_REF<MOT/> ) or summarization system ( #REF ) . We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured."
    },
    {
        "gold": {
            "text": [
                "It can be shown ( #TARGET_REF ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "method:It can be shown ( #TARGET_REF ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .",
        "output": "method:It can be shown ( #TARGET_REF<EXT/> ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events ."
    },
    {
        "gold": {
            "text": [
                "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #TARGET_REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #TARGET_REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .",
        "output": "nan:Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #TARGET_REF<FUT/> , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
    },
    {
        "gold": {
            "text": [
                "The psycholinguistic studies of #REF , #TARGET_REF , #REF , #REF , #REF , and #REF , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .",
                "For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.",
                "#REF claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.",
                "Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The psycholinguistic studies of #REF , #TARGET_REF , #REF , #REF , #REF , and #REF , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency . For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings. #REF claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it. Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.",
        "output": "introduction:The psycholinguistic studies of #REF , #TARGET_REF<BACK/> , #REF , #REF , #REF , and #REF , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency . For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings. #REF claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it. Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly."
    },
    {
        "gold": {
            "text": [
                "Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles.",
                "In comparison, the tag set of the Buckwalter Morphological Analyzer ( #TARGET_REF ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8",
                "Cross-linguistically, a core set containing around 12 tags is often assumed as a �universal tag set� (#REF; Petrov, Das, and #REF).",
                "We have adapted the list from #REF for Arabic, and call it here CORE12.",
                "It contains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle (PRT), abbreviation (AB), and punctuation (PNX).",
                "The CATIB6 tag set can be viewed as a further reduction, with the exception that CATIB6 contains a passive voice tag (a mor- phological feature); this tag constitutes only 0.5% of the tags in the training, however."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles. In comparison, the tag set of the Buckwalter Morphological Analyzer ( #TARGET_REF ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8 Cross-linguistically, a core set containing around 12 tags is often assumed as a �universal tag set� (#REF; Petrov, Das, and #REF). We have adapted the list from #REF for Arabic, and call it here CORE12. It contains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle (PRT), abbreviation (AB), and punctuation (PNX). The CATIB6 tag set can be viewed as a further reduction, with the exception that CATIB6 contains a passive voice tag (a mor- phological feature); this tag constitutes only 0.5% of the tags in the training, however.",
        "output": "experiments:Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles. In comparison, the tag set of the Buckwalter Morphological Analyzer ( #TARGET_REF<USE/> ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8 Cross-linguistically, a core set containing around 12 tags is often assumed as a �universal tag set� (#REF; Petrov, Das, and #REF). We have adapted the list from #REF for Arabic, and call it here CORE12. It contains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle (PRT), abbreviation (AB), and punctuation (PNX). The CATIB6 tag set can be viewed as a further reduction, with the exception that CATIB6 contains a passive voice tag (a mor- phological feature); this tag constitutes only 0.5% of the tags in the training, however."
    },
    {
        "gold": {
            "text": [
                "The features can be easily obtained by modifying the TAT extraction algorithm described in ( #TARGET_REF ) .",
                "When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees.",
                "Then we use the toolkit implemented by #REF to train MERS models for the ambiguous source syntactic trees separately.",
                "We set the iteration number to 100 and Gaussian prior to 1."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:The features can be easily obtained by modifying the TAT extraction algorithm described in ( #TARGET_REF ) . When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees. Then we use the toolkit implemented by #REF to train MERS models for the ambiguous source syntactic trees separately. We set the iteration number to 100 and Gaussian prior to 1.",
        "output": "experiments:The features can be easily obtained by modifying the TAT extraction algorithm described in ( #TARGET_REF<COMP/> ) . When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees. Then we use the toolkit implemented by #REF to train MERS models for the ambiguous source syntactic trees separately. We set the iteration number to 100 and Gaussian prior to 1."
    },
    {
        "gold": {
            "text": [
                "Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (#REF).",
                "Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.",
                "For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #REF ; #TARGET_REF ) .",
                "techniques."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (#REF). Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries. For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #REF ; #TARGET_REF ) . techniques.",
        "output": "nan:Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (#REF). Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries. For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #REF ; #TARGET_REF<MOT/> ) . techniques."
    },
    {
        "gold": {
            "text": [
                "A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.",
                "For instance , #TARGET_REF , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''",
                "The pairing of these two sentences may be said to create a small paragraph.",
                "Our point is that an acceptable structure can be assigned to the utterance \"Reagan thinks bananas\" only within the paragraph in which this utterance occurs.",
                "We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task.",
                "That is, we can ask in the first sentence of a paragraph about Kissinger's favorite fruit, elaborate the question and the circumstances in the next few sentences, and give the above answer at the end.",
                "We do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism (although it may be); rather, it has the grammatical role of providing functional structures that can be assigned to strings."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings. For instance , #TARGET_REF , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? '' The pairing of these two sentences may be said to create a small paragraph. Our point is that an acceptable structure can be assigned to the utterance \"Reagan thinks bananas\" only within the paragraph in which this utterance occurs. We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task. That is, we can ask in the first sentence of a paragraph about Kissinger's favorite fruit, elaborate the question and the circumstances in the next few sentences, and give the above answer at the end. We do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism (although it may be); rather, it has the grammatical role of providing functional structures that can be assigned to strings.",
        "output": "introduction:A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings. For instance , #TARGET_REF<EXT/> , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? '' The pairing of these two sentences may be said to create a small paragraph. Our point is that an acceptable structure can be assigned to the utterance \"Reagan thinks bananas\" only within the paragraph in which this utterance occurs. We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task. That is, we can ask in the first sentence of a paragraph about Kissinger's favorite fruit, elaborate the question and the circumstances in the next few sentences, and give the above answer at the end. We do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism (although it may be); rather, it has the grammatical role of providing functional structures that can be assigned to strings."
    },
    {
        "gold": {
            "text": [
                "In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #TARGET_REF ) .",
                "We also make use of the Brown corpus, and the Question Treebank (QTB) (#REF"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #TARGET_REF ) . We also make use of the Brown corpus, and the Question Treebank (QTB) (#REF",
        "output": "experiments:In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #TARGET_REF<FUT/> ) . We also make use of the Brown corpus, and the Question Treebank (QTB) (#REF"
    },
    {
        "gold": {
            "text": [
                "The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( #REF ; #TARGET_REF ) .",
                "But IA may be replaced by any other reasonable GRE algorithm, for example, one that always exactly minimizes the number of properties expressed, or one that always \"greedily\" selects the property that removes the maximum number of distractors.",
                "Let G be any such GRE algorithm, then we can proceed as follows:"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( #REF ; #TARGET_REF ) . But IA may be replaced by any other reasonable GRE algorithm, for example, one that always exactly minimizes the number of properties expressed, or one that always \"greedily\" selects the property that removes the maximum number of distractors. Let G be any such GRE algorithm, then we can proceed as follows:",
        "output": "experiments:The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( #REF ; #TARGET_REF<BACK/> ) . But IA may be replaced by any other reasonable GRE algorithm, for example, one that always exactly minimizes the number of properties expressed, or one that always \"greedily\" selects the property that removes the maximum number of distractors. Let G be any such GRE algorithm, then we can proceed as follows:"
    },
    {
        "gold": {
            "text": [
                "These results make it clear that doing under-sampling greatly improves the performance of the classifier, despite the fact that we are using less training data (fewer negative examples).",
                "This is as expected because we are evaluating on F1-measure and the classifier is optimizing for accuracy.",
                "absolute.",
                "As in the baseline system, a combination of structures performs best.",
                "As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall.",
                "Here , the PET and GR kernel perform similar : this is different from the results of ( #TARGET_REF ) where GR performed much worse than PET for ACE data .",
                "This exemplifies the difference in the nature of our event annotations from that of ACE relations.",
                "Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger.",
                "This means that implicit feature space is much sparser and thus not the best representation.",
                "4 presents results for using the oversampling method with transformation that produces synthetic positive examples by using a transformation on dependency trees such that the new synthetic examples are \"close\" to the original examples.",
                "This method achieves a gain 16.78% over the baseline system."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:These results make it clear that doing under-sampling greatly improves the performance of the classifier, despite the fact that we are using less training data (fewer negative examples). This is as expected because we are evaluating on F1-measure and the classifier is optimizing for accuracy. absolute. As in the baseline system, a combination of structures performs best. As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. Here , the PET and GR kernel perform similar : this is different from the results of ( #TARGET_REF ) where GR performed much worse than PET for ACE data . This exemplifies the difference in the nature of our event annotations from that of ACE relations. Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger. This means that implicit feature space is much sparser and thus not the best representation. 4 presents results for using the oversampling method with transformation that produces synthetic positive examples by using a transformation on dependency trees such that the new synthetic examples are \"close\" to the original examples. This method achieves a gain 16.78% over the baseline system.",
        "output": "experiments:These results make it clear that doing under-sampling greatly improves the performance of the classifier, despite the fact that we are using less training data (fewer negative examples). This is as expected because we are evaluating on F1-measure and the classifier is optimizing for accuracy. absolute. As in the baseline system, a combination of structures performs best. As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. Here , the PET and GR kernel perform similar : this is different from the results of ( #TARGET_REF<USE/> ) where GR performed much worse than PET for ACE data . This exemplifies the difference in the nature of our event annotations from that of ACE relations. Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger. This means that implicit feature space is much sparser and thus not the best representation. 4 presents results for using the oversampling method with transformation that produces synthetic positive examples by using a transformation on dependency trees such that the new synthetic examples are \"close\" to the original examples. This method achieves a gain 16.78% over the baseline system."
    },
    {
        "gold": {
            "text": [
                "Frame-semantic features.",
                "While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence.",
                "Following our previous work on stance classification ( #TARGET_REFc ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( #REF ) .",
                "Frame-word interaction features encode whether two words appear in different elements of the same frame.",
                "Hence, each frame-word interaction feature consists of (1) the name of the frame f from which it is created, and"
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:Frame-semantic features. While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence. Following our previous work on stance classification ( #TARGET_REFc ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( #REF ) . Frame-word interaction features encode whether two words appear in different elements of the same frame. Hence, each frame-word interaction feature consists of (1) the name of the frame f from which it is created, and",
        "output": "experiments:Frame-semantic features. While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence. Following our previous work on stance classification ( #TARGET_REF<COMP/>c ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( #REF ) . Frame-word interaction features encode whether two words appear in different elements of the same frame. Hence, each frame-word interaction feature consists of (1) the name of the frame f from which it is created, and"
    },
    {
        "gold": {
            "text": [
                "Even better accuracy can be achieved with a more fine-grained link class structure.",
                "Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #TARGET_REF ) .",
                "Another interesting extension is to broaden the definition of a \"word\" to include multi-word lexical units (#REF).",
                "If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.",
                "In this manner, the model can account for a wider range of translation phenomena."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #TARGET_REF ) . Another interesting extension is to broaden the definition of a \"word\" to include multi-word lexical units (#REF). If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena.",
        "output": "conclusion:Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #TARGET_REF<MOT/> ) . Another interesting extension is to broaden the definition of a \"word\" to include multi-word lexical units (#REF). If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena."
    },
    {
        "gold": {
            "text": [
                "#REF did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.",
                "They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.",
                "This paper made the first attempt on Chinese SRL and produced promising results.",
                "After the PropBank (#REF) was built,  and #REF have produced more complete and systematic research on Chinese SRL.",
                "#REF has made some preliminary attempt on the idea of hierarchical semantic role labeling.",
                "However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.",
                "So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.",
                "#REF did very encouraging work on the feature calibration of semantic role labeling.",
                "They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.",
                "For semantic analysis, developing features that capture the right kind of information is crucial.",
                "Experiments on Chinese SRL ( #TARGET_REF , #REF ) reassured these findings ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:#REF did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (#REF) was built,  and #REF have produced more complete and systematic research on Chinese SRL. #REF has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #REF did very encouraging work on the feature calibration of semantic role labeling. They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL ( #TARGET_REF , #REF ) reassured these findings .",
        "output": "introduction:#REF did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (#REF) was built,  and #REF have produced more complete and systematic research on Chinese SRL. #REF has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #REF did very encouraging work on the feature calibration of semantic role labeling. They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL ( #TARGET_REF<EXT/> , #REF ) reassured these findings ."
    },
    {
        "gold": {
            "text": [
                "For compound splitting , we follow #TARGET_REF , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .",
                "Other approaches use less deep linguistic resources (e.g., POS-tags #REF) or are (almost) knowledge-free (e.g., #REF).",
                "Compound merging is less well studied.",
                "#REF used a simple, list-based merging approach, merging all consecutive words included in a merging list.",
                "This approach resulted in too many compounds.",
                "We follow #REF, for compound merging.",
                "We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:For compound splitting , we follow #TARGET_REF , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies . Other approaches use less deep linguistic resources (e.g., POS-tags #REF) or are (almost) knowledge-free (e.g., #REF). Compound merging is less well studied. #REF used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow #REF, for compound merging. We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets.",
        "output": "related work:For compound splitting , we follow #TARGET_REF<FUT/> , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies . Other approaches use less deep linguistic resources (e.g., POS-tags #REF) or are (almost) knowledge-free (e.g., #REF). Compound merging is less well studied. #REF used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow #REF, for compound merging. We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets."
    },
    {
        "gold": {
            "text": [
                "There are a number of generalised NLP systems in the literature.",
                "Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #TARGET_REF ) and the Alembic Workbench ( #REF ) ) as well as NLP tools and resources that can be manipulated from the GUI .",
                "For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (#REF).",
                "GATE goes beyond earlier systems by using a component-based infrastructure (#REF) which the GUI is built on top of.",
                "This allows components to be highly configurable and simplifies the addition of new components to the system."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #TARGET_REF ) and the Alembic Workbench ( #REF ) ) as well as NLP tools and resources that can be manipulated from the GUI . For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (#REF). GATE goes beyond earlier systems by using a component-based infrastructure (#REF) which the GUI is built on top of. This allows components to be highly configurable and simplifies the addition of new components to the system.",
        "output": "experiments:There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #TARGET_REF<BACK/> ) and the Alembic Workbench ( #REF ) ) as well as NLP tools and resources that can be manipulated from the GUI . For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (#REF). GATE goes beyond earlier systems by using a component-based infrastructure (#REF) which the GUI is built on top of. This allows components to be highly configurable and simplifies the addition of new components to the system."
    },
    {
        "gold": {
            "text": [
                "11 #TARGET_REF reports that non-projective and pseudo-projective algorithms outperform the `` eager  projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .",
                "The Nivre �standard� algorithm is also reported there to do better on Arabic, but in a preliminary experimentation, it did slightly worse than the �eager� one, perhaps due to the high percentage of right branching (left headed structures) in our Arabic training set�an observation already noted in #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:11 #TARGET_REF reports that non-projective and pseudo-projective algorithms outperform the `` eager  projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies . The Nivre �standard� algorithm is also reported there to do better on Arabic, but in a preliminary experimentation, it did slightly worse than the �eager� one, perhaps due to the high percentage of right branching (left headed structures) in our Arabic training set�an observation already noted in #REF.",
        "output": "related work:11 #TARGET_REF<USE/> reports that non-projective and pseudo-projective algorithms outperform the `` eager  projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies . The Nivre �standard� algorithm is also reported there to do better on Arabic, but in a preliminary experimentation, it did slightly worse than the �eager� one, perhaps due to the high percentage of right branching (left headed structures) in our Arabic training set�an observation already noted in #REF."
    },
    {
        "gold": {
            "text": [
                "10 Both algorithms are provided by Weka (http://www.cs.waikato.ac.nz/ml/weka/).",
                "provide complementary perspectives.",
                "While the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features.",
                "To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.",
                "Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.",
                "( Details of how the average-expert model performs can be found in our prior work ( #TARGET_REF ) . )"
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:10 Both algorithms are provided by Weka (http://www.cs.waikato.ac.nz/ml/weka/). provide complementary perspectives. While the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features. To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below. Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings. ( Details of how the average-expert model performs can be found in our prior work ( #TARGET_REF ) . )",
        "output": "experiments:10 Both algorithms are provided by Weka (http://www.cs.waikato.ac.nz/ml/weka/). provide complementary perspectives. While the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features. To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below. Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings. ( Details of how the average-expert model performs can be found in our prior work ( #TARGET_REF<COMP/> ) . )"
    },
    {
        "gold": {
            "text": [
                "In a similar vain to #REF and #TARGET_REF , the method extends an existing flat shallow-parsing method to handle composite structures ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:In a similar vain to #REF and #TARGET_REF , the method extends an existing flat shallow-parsing method to handle composite structures .",
        "output": "conclusion:In a similar vain to #REF and #TARGET_REF<MOT/> , the method extends an existing flat shallow-parsing method to handle composite structures ."
    },
    {
        "gold": {
            "text": [
                "#REF; #REF).",
                "The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.",
                "This approach has now gained wide usage , as exemplified by the work of #TARGET_REF , 1999 ) , Charniak ( 1996 , 1997 ) , #REF , #REF , and many others ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:#REF; #REF). The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar. This approach has now gained wide usage , as exemplified by the work of #TARGET_REF , 1999 ) , Charniak ( 1996 , 1997 ) , #REF , #REF , and many others .",
        "output": "introduction:#REF; #REF). The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar. This approach has now gained wide usage , as exemplified by the work of #TARGET_REF<EXT/> , 1999 ) , Charniak ( 1996 , 1997 ) , #REF , #REF , and many others ."
    },
    {
        "gold": {
            "text": [
                "With our typology of links, we aim to solve the framing problem as defined in Section 1.2.",
                "We want to model views as dynamic objects -the creation of context will be still arbitrary, but changes are very easily.",
                "We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:With our typology of links, we aim to solve the framing problem as defined in Section 1.2. We want to model views as dynamic objects -the creation of context will be still arbitrary, but changes are very easily. We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #TARGET_REF ) .",
        "output": "nan:With our typology of links, we aim to solve the framing problem as defined in Section 1.2. We want to model views as dynamic objects -the creation of context will be still arbitrary, but changes are very easily. We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "Although the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of #TARGET_REF ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "method:Although the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of #TARGET_REF .",
        "output": "method:Although the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of #TARGET_REF<BACK/> ."
    },
    {
        "gold": {
            "text": [
                "There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and #REF ; #TARGET_REF ; #REF ; Malik , Subramaniam , and #REF ) .",
                "eResponder, the system developed by Carmel, Shtalhaim, and #REF, retrieves a list of request-response pairs and presents a ranked list of responses to the user.",
                "If the user is unsatisfied with this list, an operator is asked to generate a new response.",
                "The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses.",
                "However, there is no attempt to automatically generate a single response.",
                "#REF compared the performance of document retrieval and document prediction for generating help-desk responses.",
                "Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs.",
                "Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes.",
                "The generated response is the answer that is closest to the centroid of the cluster."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and #REF ; #TARGET_REF ; #REF ; Malik , Subramaniam , and #REF ) . eResponder, the system developed by Carmel, Shtalhaim, and #REF, retrieves a list of request-response pairs and presents a ranked list of responses to the user. If the user is unsatisfied with this list, an operator is asked to generate a new response. The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses. However, there is no attempt to automatically generate a single response. #REF compared the performance of document retrieval and document prediction for generating help-desk responses. Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs. Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes. The generated response is the answer that is closest to the centroid of the cluster.",
        "output": "nan:There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and #REF ; #TARGET_REF<USE/> ; #REF ; Malik , Subramaniam , and #REF ) . eResponder, the system developed by Carmel, Shtalhaim, and #REF, retrieves a list of request-response pairs and presents a ranked list of responses to the user. If the user is unsatisfied with this list, an operator is asked to generate a new response. The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses. However, there is no attempt to automatically generate a single response. #REF compared the performance of document retrieval and document prediction for generating help-desk responses. Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs. Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes. The generated response is the answer that is closest to the centroid of the cluster."
    },
    {
        "gold": {
            "text": [
                "We also included two user-based features, gender and pretest score.",
                "Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( #REFa ; #TARGET_REF ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( #REF ; #REFb ) and made freely available in the openSMILE Toolkit ( #REF ) .",
                "To date, however, these features have only decreased the crossvalidation performance of our models.",
                "8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise).",
                "Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented and deployed.",
                "To that end, we now describe the results of our intrinsic and extrinsic evaluations of our DISE model, aimed at determining whether it is ready to be evaluated with real users."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:We also included two user-based features, gender and pretest score. Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( #REFa ; #TARGET_REF ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( #REF ; #REFb ) and made freely available in the openSMILE Toolkit ( #REF ) . To date, however, these features have only decreased the crossvalidation performance of our models. 8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise). Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented and deployed. To that end, we now describe the results of our intrinsic and extrinsic evaluations of our DISE model, aimed at determining whether it is ready to be evaluated with real users.",
        "output": "nan:We also included two user-based features, gender and pretest score. Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( #REFa ; #TARGET_REF<COMP/> ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( #REF ; #REFb ) and made freely available in the openSMILE Toolkit ( #REF ) . To date, however, these features have only decreased the crossvalidation performance of our models. 8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise). Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented and deployed. To that end, we now describe the results of our intrinsic and extrinsic evaluations of our DISE model, aimed at determining whether it is ready to be evaluated with real users."
    },
    {
        "gold": {
            "text": [
                "Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.",
                "For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #TARGET_REF ; #REF ) and history-based parsing ( #REF ) .",
                "We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF)."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #TARGET_REF ; #REF ) and history-based parsing ( #REF ) . We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF).",
        "output": "conclusion:Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #TARGET_REF<MOT/> ; #REF ) and history-based parsing ( #REF ) . We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF)."
    },
    {
        "gold": {
            "text": [
                "To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.",
                "In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.",
                "In the latter case, we can also take care of transferring the value of z.",
                "However , as discussed by #TARGET_REF , creating several instances of lexical rules can be avoided .",
                "Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.",
                "This is accomplished by having each lexical rule predicate call a so-called framepredicate,which can have multiple defining clauses.",
                "So for the lexical rule 1, the frame specification is taken care of by extending the predicate in Figure 6 with a call to a frame predicate, as shown in Figure 8.17"
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case. In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value. In the latter case, we can also take care of transferring the value of z. However , as discussed by #TARGET_REF , creating several instances of lexical rules can be avoided . Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule. This is accomplished by having each lexical rule predicate call a so-called framepredicate,which can have multiple defining clauses. So for the lexical rule 1, the frame specification is taken care of by extending the predicate in Figure 6 with a call to a frame predicate, as shown in Figure 8.17",
        "output": "introduction:To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case. In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value. In the latter case, we can also take care of transferring the value of z. However , as discussed by #TARGET_REF<EXT/> , creating several instances of lexical rules can be avoided . Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule. This is accomplished by having each lexical rule predicate call a so-called framepredicate,which can have multiple defining clauses. So for the lexical rule 1, the frame specification is taken care of by extending the predicate in Figure 6 with a call to a frame predicate, as shown in Figure 8.17"
    },
    {
        "gold": {
            "text": [
                "Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #TARGET_REF ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .",
                "Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.",
                "We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (#REF).",
                "Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #TARGET_REF ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing . Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer. We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (#REF). Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.",
        "output": "nan:Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #TARGET_REF<FUT/> ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing . Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer. We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (#REF). Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model."
    },
    {
        "gold": {
            "text": [
                "Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (#REF).",
                "In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #REF ) or the system ( #TARGET_REF ) .",
                "Such systems extract information from some types of syntactic units (clauses in (#REF;#REF;#REF); noun phrases in (#REF;#REF)).",
                "Lists of semantic relations are designed to capture salient domain information."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (#REF). In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #REF ) or the system ( #TARGET_REF ) . Such systems extract information from some types of syntactic units (clauses in (#REF;#REF;#REF); noun phrases in (#REF;#REF)). Lists of semantic relations are designed to capture salient domain information.",
        "output": "related work:Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (#REF). In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #REF ) or the system ( #TARGET_REF<BACK/> ) . Such systems extract information from some types of syntactic units (clauses in (#REF;#REF;#REF); noun phrases in (#REF;#REF)). Lists of semantic relations are designed to capture salient domain information."
    },
    {
        "gold": {
            "text": [
                "This paper investigates the idea, still unexplored, of a tighter integration of MT and TE algorithms and techniques.",
                "Our aim is to embed cross-lingual processing techniques inside the TE recognition process in order to avoid any dependency on external MT components, and eventually gain full control of the system�s behaviour.",
                "Along this direction, we start from the acquisition and use of lexical knowl- edge, which represents the basic building block of any TE system.",
                "Using the basic solution proposed by ( #TARGET_REF ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:This paper investigates the idea, still unexplored, of a tighter integration of MT and TE algorithms and techniques. Our aim is to embed cross-lingual processing techniques inside the TE recognition process in order to avoid any dependency on external MT components, and eventually gain full control of the system�s behaviour. Along this direction, we start from the acquisition and use of lexical knowl- edge, which represents the basic building block of any TE system. Using the basic solution proposed by ( #TARGET_REF ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :",
        "output": "introduction:This paper investigates the idea, still unexplored, of a tighter integration of MT and TE algorithms and techniques. Our aim is to embed cross-lingual processing techniques inside the TE recognition process in order to avoid any dependency on external MT components, and eventually gain full control of the system�s behaviour. Along this direction, we start from the acquisition and use of lexical knowl- edge, which represents the basic building block of any TE system. Using the basic solution proposed by ( #TARGET_REF<USE/> ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :"
    },
    {
        "gold": {
            "text": [
                "A brief version of this work, with some additional material, first appeared as ( #TARGET_REFa ) .",
                "A leisurely journal-length version with more details has been prepared and is available."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:A brief version of this work, with some additional material, first appeared as ( #TARGET_REFa ) . A leisurely journal-length version with more details has been prepared and is available.",
        "output": "introduction:A brief version of this work, with some additional material, first appeared as ( #TARGET_REF<COMP/>a ) . A leisurely journal-length version with more details has been prepared and is available."
    },
    {
        "gold": {
            "text": [
                "Table 2 depicts that testing each sentence with local training method takes 2.9 seconds, which is comparable to the testing time 2.0 seconds with global training method 4 .",
                "This shows that the local method is efficient.",
                "Further, compared to the retrieval, the local training is not the bottleneck.",
                "Actually , if we use LSH technique ( #TARGET_REF ) in retrieval process , the local method can be easily scaled to a larger training data ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "experiments:Table 2 depicts that testing each sentence with local training method takes 2.9 seconds, which is comparable to the testing time 2.0 seconds with global training method 4 . This shows that the local method is efficient. Further, compared to the retrieval, the local training is not the bottleneck. Actually , if we use LSH technique ( #TARGET_REF ) in retrieval process , the local method can be easily scaled to a larger training data .",
        "output": "experiments:Table 2 depicts that testing each sentence with local training method takes 2.9 seconds, which is comparable to the testing time 2.0 seconds with global training method 4 . This shows that the local method is efficient. Further, compared to the retrieval, the local training is not the bottleneck. Actually , if we use LSH technique ( #TARGET_REF<MOT/> ) in retrieval process , the local method can be easily scaled to a larger training data ."
    },
    {
        "gold": {
            "text": [
                "Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) .",
                "Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) . Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.",
        "output": "introduction:Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF<EXT/> ; #REF ) . Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation."
    },
    {
        "gold": {
            "text": [
                "results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition.",
                "In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions.",
                "This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations.",
                "The result holds for both the MaltParser ( #REF ) and the Easy-First Parser ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition. In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions. This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations. The result holds for both the MaltParser ( #REF ) and the Easy-First Parser ( #TARGET_REF ) .",
        "output": "introduction:results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition. In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions. This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations. The result holds for both the MaltParser ( #REF ) and the Easy-First Parser ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #TARGET_REF , for further details ) .",
                "However, exploiting this information to the full would be a non-trivial task, because it would require accessing the relevant knowledge about the words contained in the qualifier fields from their LDOCE entries."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #TARGET_REF , for further details ) . However, exploiting this information to the full would be a non-trivial task, because it would require accessing the relevant knowledge about the words contained in the qualifier fields from their LDOCE entries.",
        "output": "nan:There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #TARGET_REF<BACK/> , for further details ) . However, exploiting this information to the full would be a non-trivial task, because it would require accessing the relevant knowledge about the words contained in the qualifier fields from their LDOCE entries."
    },
    {
        "gold": {
            "text": [
                "Although there are other discussions of the paragraph as a central element of discourse (e.g.",
                "#TARGET_REF , #REF , #REF , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .",
                "Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.",
                "Our interest, however, lies precisely in that area."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Although there are other discussions of the paragraph as a central element of discourse (e.g. #TARGET_REF , #REF , #REF , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area.",
        "output": "introduction:Although there are other discussions of the paragraph as a central element of discourse (e.g. #TARGET_REF<USE/> , #REF , #REF , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area."
    },
    {
        "gold": {
            "text": [
                "The research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations.",
                "Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #TARGET_REF ) .",
                "However, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans.",
                "In addition, because that work was based on features synthesized over the entire dialogues, the hypotheses that were learned could not be used for prediction during runtime."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "conclusion:The research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations. Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #TARGET_REF ) . However, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans. In addition, because that work was based on features synthesized over the entire dialogues, the hypotheses that were learned could not be used for prediction during runtime.",
        "output": "conclusion:The research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations. Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #TARGET_REF<COMP/> ) . However, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans. In addition, because that work was based on features synthesized over the entire dialogues, the hypotheses that were learned could not be used for prediction during runtime."
    },
    {
        "gold": {
            "text": [
                "Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.",
                "For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (#REF;#REF) and history-based parsing (#REF).",
                "We could also introduce new variables , e.g. , nonterminal refinements ( #TARGET_REF ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( #REF ; #REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (#REF;#REF) and history-based parsing (#REF). We could also introduce new variables , e.g. , nonterminal refinements ( #TARGET_REF ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( #REF ; #REF ) .",
        "output": "conclusion:Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (#REF;#REF) and history-based parsing (#REF). We could also introduce new variables , e.g. , nonterminal refinements ( #TARGET_REF<MOT/> ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( #REF ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "However, there are at least three arguments against iterating PT.",
                "First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time.",
                "Secondly, the cooperative principle of Grice (1975Grice ( , 1978, under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader.",
                "Finally , it has been shown by #REF that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #TARGET_REF strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .",
                "Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly, the cooperative principle of Grice (1975Grice ( , 1978, under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally , it has been shown by #REF that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #TARGET_REF strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph . Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.",
        "output": "introduction:However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly, the cooperative principle of Grice (1975Grice ( , 1978, under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally , it has been shown by #REF that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #TARGET_REF<EXT/> strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph . Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions."
    },
    {
        "gold": {
            "text": [
                "As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.",
                "We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonly-used MUC scorer (#REF), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.",
                "Following #TARGET_REF , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .",
                "In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts. We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonly-used MUC scorer (#REF), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved. Following #TARGET_REF , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition . In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder.",
        "output": "nan:As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts. We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonly-used MUC scorer (#REF), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved. Following #TARGET_REF<FUT/> , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition . In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder."
    },
    {
        "gold": {
            "text": [
                "An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.",
                "before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.",
                "If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.",
                "Typical examples are Bulgarian ( #REF ; #REF ) , Chinese ( #TARGET_REF ) , Danish ( #REF ) , and Swedish ( #REF ) .",
                "Japanese (#REF), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6. Typical examples are Bulgarian ( #REF ; #REF ) , Chinese ( #TARGET_REF ) , Danish ( #REF ) , and Swedish ( #REF ) . Japanese (#REF), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.",
        "output": "experiments:An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6. Typical examples are Bulgarian ( #REF ; #REF ) , Chinese ( #TARGET_REF<BACK/> ) , Danish ( #REF ) , and Swedish ( #REF ) . Japanese (#REF), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances."
    },
    {
        "gold": {
            "text": [
                "It is not aimed at handling dependencies , which require heavy use of lexical information ( #TARGET_REF , for PP attachment ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:It is not aimed at handling dependencies , which require heavy use of lexical information ( #TARGET_REF , for PP attachment ) .",
        "output": "conclusion:It is not aimed at handling dependencies , which require heavy use of lexical information ( #TARGET_REF<USE/> , for PP attachment ) ."
    },
    {
        "gold": {
            "text": [
                "The reordering models we describe follow our previous work using function word models for translation ( #TARGET_REF ; #REF ) .",
                "The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.",
                "To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.",
                "This section provides a high level overview of our reordering model, which attempts to leverage this information."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:The reordering models we describe follow our previous work using function word models for translation ( #TARGET_REF ; #REF ) . The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them. To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases. This section provides a high level overview of our reordering model, which attempts to leverage this information.",
        "output": "nan:The reordering models we describe follow our previous work using function word models for translation ( #TARGET_REF<COMP/> ; #REF ) . The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them. To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases. This section provides a high level overview of our reordering model, which attempts to leverage this information."
    },
    {
        "gold": {
            "text": [
                "As ( #TARGET_REF ) show , lexical information improves on NP and VP chunking as well ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:As ( #TARGET_REF ) show , lexical information improves on NP and VP chunking as well .",
        "output": "conclusion:As ( #TARGET_REF<MOT/> ) show , lexical information improves on NP and VP chunking as well ."
    },
    {
        "gold": {
            "text": [
                "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.",
                "They proved to be useful in a number of NLP applications such as natural language generation ( #TARGET_REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) .",
                "One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF).",
                "With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.",
                "After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #TARGET_REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases.",
        "output": "nan:Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #TARGET_REF<EXT/> ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases."
    },
    {
        "gold": {
            "text": [
                "In this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags.",
                "In order to estimate the parameters of our model , we develop a blocked sampler based on that of #TARGET_REF to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .",
                "However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior.",
                "Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:In this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags. In order to estimate the parameters of our model , we develop a blocked sampler based on that of #TARGET_REF to sample parse trees for sentences in the raw training corpus according to their posterior probabilities . However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior. Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios.",
        "output": "introduction:In this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags. In order to estimate the parameters of our model , we develop a blocked sampler based on that of #TARGET_REF<FUT/> to sample parse trees for sentences in the raw training corpus according to their posterior probabilities . However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior. Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios."
    },
    {
        "gold": {
            "text": [
                "The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #REF , #TARGET_REF ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .",
                "Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #REF , #TARGET_REF ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 . Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine.",
        "output": "related work:The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #REF , #TARGET_REF<BACK/> ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 . Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine."
    },
    {
        "gold": {
            "text": [
                "An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #TARGET_REF ) .",
                "This technique provides two important advantages.",
                "First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics.",
                "This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training.",
                "Second, using continuous distributions allows us to leverage a variety of tools (e.g., LDA) that have been shown to be successful in other fields, such as speech recognition (#REF)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #TARGET_REF ) . This technique provides two important advantages. First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics. This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training. Second, using continuous distributions allows us to leverage a variety of tools (e.g., LDA) that have been shown to be successful in other fields, such as speech recognition (#REF).",
        "output": "conclusion:An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #TARGET_REF<USE/> ) . This technique provides two important advantages. First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics. This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training. Second, using continuous distributions allows us to leverage a variety of tools (e.g., LDA) that have been shown to be successful in other fields, such as speech recognition (#REF)."
    },
    {
        "gold": {
            "text": [
                "The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (#REF).",
                "The first direct application of parse forest in translation is our previous work ( #TARGET_REF ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .",
                "This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "related work:The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (#REF). The first direct application of parse forest in translation is our previous work ( #TARGET_REF ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) . This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding.",
        "output": "related work:The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (#REF). The first direct application of parse forest in translation is our previous work ( #TARGET_REF<COMP/> ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) . This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding."
    },
    {
        "gold": {
            "text": [
                "â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #REF , â¢ a `` conditioning '' facility as described by #TARGET_REF , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #REF , â¢ a `` conditioning '' facility as described by #TARGET_REF , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns .",
        "output": "conclusion:â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #REF , â¢ a `` conditioning '' facility as described by #TARGET_REF<MOT/> , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns ."
    },
    {
        "gold": {
            "text": [
                "We hypothesize that an underlying model that could capture syntagmatic patterns in large word contexts, yet is flexible enough to deal with data sparseness, is desired.",
                "It is generally accepted that the semantics of verbs in particular are correlated with their syntagmatic properties (#REF;#REF).",
                "This provides grounds to expect that such model has the potential to excel for verbs.",
                "To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme.",
                "This choice is inspired by recent work on learning syntactic categories ( #TARGET_REF ) , which successfully utilized such language models to represent word window contexts of target words .",
                "However, we note that other richer types of language models, such as class-based (#REF) or hybrid (#REF), can be seamlessly integrated into our scheme."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:We hypothesize that an underlying model that could capture syntagmatic patterns in large word contexts, yet is flexible enough to deal with data sparseness, is desired. It is generally accepted that the semantics of verbs in particular are correlated with their syntagmatic properties (#REF;#REF). This provides grounds to expect that such model has the potential to excel for verbs. To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme. This choice is inspired by recent work on learning syntactic categories ( #TARGET_REF ) , which successfully utilized such language models to represent word window contexts of target words . However, we note that other richer types of language models, such as class-based (#REF) or hybrid (#REF), can be seamlessly integrated into our scheme.",
        "output": "introduction:We hypothesize that an underlying model that could capture syntagmatic patterns in large word contexts, yet is flexible enough to deal with data sparseness, is desired. It is generally accepted that the semantics of verbs in particular are correlated with their syntagmatic properties (#REF;#REF). This provides grounds to expect that such model has the potential to excel for verbs. To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme. This choice is inspired by recent work on learning syntactic categories ( #TARGET_REF<EXT/> ) , which successfully utilized such language models to represent word window contexts of target words . However, we note that other richer types of language models, such as class-based (#REF) or hybrid (#REF), can be seamlessly integrated into our scheme."
    },
    {
        "gold": {
            "text": [
                "Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #TARGET_REF ) .",
                "Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.",
                "For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.",
                "In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.",
                "Differently,  designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment.",
                "We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.",
                "This subject will be one of our future work topics."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #TARGET_REF ) . Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment. For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler. Differently,  designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment. We believe that it is possible to investigate the space of both tree structure and alignment simultaneously. This subject will be one of our future work topics.",
        "output": "method:Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #TARGET_REF<FUT/> ) . Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment. For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler. Differently,  designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment. We believe that it is possible to investigate the space of both tree structure and alignment simultaneously. This subject will be one of our future work topics."
    },
    {
        "gold": {
            "text": [
                "All EBMT systems, from the initial proposal by #REF to the recent collection of #REF, are premised on the availability of subsentential alignments derived from the input bitext.",
                "There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3",
                " Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.",
                "#TARGET_REF attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora .",
                "#REF replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.",
                "#REF use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.",
                "The respective lengths of the putative alignments in terms of characters is also an important factor.",
                "Ahrenberg, Andersson, and #REF observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:All EBMT systems, from the initial proposal by #REF to the recent collection of #REF, are premised on the availability of subsentential alignments derived from the input bitext. There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3  Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. #TARGET_REF attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . #REF replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. #REF use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and #REF observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.",
        "output": "introduction:All EBMT systems, from the initial proposal by #REF to the recent collection of #REF, are premised on the availability of subsentential alignments derived from the input bitext. There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3  Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. #TARGET_REF<BACK/> attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora . #REF replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. #REF use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and #REF observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking."
    },
    {
        "gold": {
            "text": [
                "1Our rules are similar to those from #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:1Our rules are similar to those from #TARGET_REF .",
        "output": "experiments:1Our rules are similar to those from #TARGET_REF<USE/> ."
    },
    {
        "gold": {
            "text": [
                "Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (#REF).",
                "Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) .",
                "For the referring expression generation task here, we also need a lexicon with grounded semantics."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (#REF). Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) . For the referring expression generation task here, we also need a lexicon with grounded semantics.",
        "output": "nan:Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (#REF). Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF<COMP/> ) . For the referring expression generation task here, we also need a lexicon with grounded semantics."
    },
    {
        "gold": {
            "text": [
                "We have presented a comparative evaluation of our approach on datasets from four different domains.",
                "In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets.",
                "Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.",
                "Our CRF-based approach also yields promising results in the crossdomain setting.",
                "The features we employ scale well across domains, given that the opinion target vocabularies are substantially different.",
                "For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #TARGET_REF ; #REF ) , perform in comparison to our approach .",
                "Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.",
                "We observe similar challenges as #REF regarding the analysis of complex sentences.",
                "Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.",
                "It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:We have presented a comparative evaluation of our approach on datasets from four different domains. In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets. Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction. Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #TARGET_REF ; #REF ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as #REF regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality. It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse.",
        "output": "conclusion:We have presented a comparative evaluation of our approach on datasets from four different domains. In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets. Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction. Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #TARGET_REF<MOT/> ; #REF ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as #REF regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality. It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse."
    },
    {
        "gold": {
            "text": [
                "is the probability of producing the target tree fragment frag.",
                "To generate frag,  used a geometric prior to decide how many child nodes to assign each node.",
                "Differently, we require that each multi-word non-terminal node must have two child nodes.",
                "This is because the binary structure has been verified to be very effective for tree-based translation ( #TARGET_REF ; #REFa ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "method:is the probability of producing the target tree fragment frag. To generate frag,  used a geometric prior to decide how many child nodes to assign each node. Differently, we require that each multi-word non-terminal node must have two child nodes. This is because the binary structure has been verified to be very effective for tree-based translation ( #TARGET_REF ; #REFa ) .",
        "output": "method:is the probability of producing the target tree fragment frag. To generate frag,  used a geometric prior to decide how many child nodes to assign each node. Differently, we require that each multi-word non-terminal node must have two child nodes. This is because the binary structure has been verified to be very effective for tree-based translation ( #TARGET_REF<EXT/> ; #REFa ) ."
    },
    {
        "gold": {
            "text": [
                "The second key task of our approach is the disambiguation of capitalized words that follow a potential sentence boundary punctuation sign.",
                "Apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the SBD task, as we showed in Section 3. We tackle capitalized words in a similar fashion as we tackled the abbreviations: through a document-centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.",
                "This is implemented as a cascade of simple strategies , which were briefly described in #TARGET_REF ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The second key task of our approach is the disambiguation of capitalized words that follow a potential sentence boundary punctuation sign. Apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the SBD task, as we showed in Section 3. We tackle capitalized words in a similar fashion as we tackled the abbreviations: through a document-centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document. This is implemented as a cascade of simple strategies , which were briefly described in #TARGET_REF .",
        "output": "nan:The second key task of our approach is the disambiguation of capitalized words that follow a potential sentence boundary punctuation sign. Apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the SBD task, as we showed in Section 3. We tackle capitalized words in a similar fashion as we tackled the abbreviations: through a document-centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document. This is implemented as a cascade of simple strategies , which were briefly described in #TARGET_REF<FUT/> ."
    },
    {
        "gold": {
            "text": [
                "If computing the intersection of two sets takes constant time then this makes the complexity of interpreting non-vague descriptions linear: O(nd), where nd is the number of properties used.",
                "In a vague description, the property last added to the description is context dependent.",
                "Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #TARGET_REF ] Chapter 8 ) .",
                "Once again, the most time-consuming part of the calculation can be performed off-line, since it is the same for all referring expressions."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:If computing the intersection of two sets takes constant time then this makes the complexity of interpreting non-vague descriptions linear: O(nd), where nd is the number of properties used. In a vague description, the property last added to the description is context dependent. Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #TARGET_REF ] Chapter 8 ) . Once again, the most time-consuming part of the calculation can be performed off-line, since it is the same for all referring expressions.",
        "output": "introduction:If computing the intersection of two sets takes constant time then this makes the complexity of interpreting non-vague descriptions linear: O(nd), where nd is the number of properties used. In a vague description, the property last added to the description is context dependent. Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #TARGET_REF<BACK/> ] Chapter 8 ) . Once again, the most time-consuming part of the calculation can be performed off-line, since it is the same for all referring expressions."
    },
    {
        "gold": {
            "text": [
                "Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.",
                "They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state. They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #TARGET_REF .",
        "output": "nan:Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state. They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #TARGET_REF<USE/> ."
    },
    {
        "gold": {
            "text": [
                "The reordering models we describe follow our previous work using function word models for translation ( #REF ; #TARGET_REF ) .",
                "The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.",
                "To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.",
                "This section provides a high level overview of our reordering model, which attempts to leverage this information."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:The reordering models we describe follow our previous work using function word models for translation ( #REF ; #TARGET_REF ) . The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them. To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases. This section provides a high level overview of our reordering model, which attempts to leverage this information.",
        "output": "nan:The reordering models we describe follow our previous work using function word models for translation ( #REF ; #TARGET_REF<COMP/> ) . The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them. To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases. This section provides a high level overview of our reordering model, which attempts to leverage this information."
    },
    {
        "gold": {
            "text": [
                "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) .",
        "output": "conclusion:Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF<MOT/> ) ."
    },
    {
        "gold": {
            "text": [
                "The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.",
                "Labeled attachment score is used as our base model loss function.",
                "In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i.",
                "The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).",
                "The score is normalized by the summed arc lengths for the sentence.",
                "The labeled version of this score requires that the labels of the arc are also correct.",
                "Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #TARGET_REF ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (#REF) and textual entailment (#REF)."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well. Labeled attachment score is used as our base model loss function. In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i. The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise). The score is normalized by the summed arc lengths for the sentence. The labeled version of this score requires that the labels of the arc are also correct. Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #TARGET_REF ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (#REF) and textual entailment (#REF).",
        "output": "experiments:The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well. Labeled attachment score is used as our base model loss function. In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i. The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise). The score is normalized by the summed arc lengths for the sentence. The labeled version of this score requires that the labels of the arc are also correct. Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #TARGET_REF<EXT/> ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (#REF) and textual entailment (#REF)."
    },
    {
        "gold": {
            "text": [
                "where f (•) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and Z θ (x) def = y u(x, y) is a normalizer.",
                "We use the same set of binary features as in previous work on this dataset ( #TARGET_REF ; #REF ; #REF ) .",
                "Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count ≥ 4 in the full 2000-document corpus.",
                "Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise.",
                "Thus θ ∈ R 17744 , and positive weights in θ favor class label y = +1 and equally discourage y = −1, while negative weights do the opposite.",
                "This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.",
                "Future work should consider more complex features and how they are signaled by rationales, as discussed in section 3.2."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:where f (•) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and Z θ (x) def = y u(x, y) is a normalizer. We use the same set of binary features as in previous work on this dataset ( #TARGET_REF ; #REF ; #REF ) . Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count ≥ 4 in the full 2000-document corpus. Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise. Thus θ ∈ R 17744 , and positive weights in θ favor class label y = +1 and equally discourage y = −1, while negative weights do the opposite. This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales. Future work should consider more complex features and how they are signaled by rationales, as discussed in section 3.2.",
        "output": "method:where f (•) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and Z θ (x) def = y u(x, y) is a normalizer. We use the same set of binary features as in previous work on this dataset ( #TARGET_REF<FUT/> ; #REF ; #REF ) . Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count ≥ 4 in the full 2000-document corpus. Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise. Thus θ ∈ R 17744 , and positive weights in θ favor class label y = +1 and equally discourage y = −1, while negative weights do the opposite. This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales. Future work should consider more complex features and how they are signaled by rationales, as discussed in section 3.2."
    },
    {
        "gold": {
            "text": [
                "Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e.",
                "Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.",
                "For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #TARGET_REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #TARGET_REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) .",
        "output": "nan:Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #TARGET_REF<BACK/> ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) ."
    },
    {
        "gold": {
            "text": [
                "To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #TARGET_REF ) .",
                "In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments.",
                "The corresponding performances in (#REF) are 75.6% and 50%, respectively.",
                "Thus, the overall performance (including cross validation and evaluation set experiments) in this work is comparable to that in (#REF).",
                "This offers some indication that the performance of NB algorithm in classifying Arabic documents is not sensitive to the Arabic root extraction algorithm.",
                "Future work will be directed at experimenting with other root extraction algorithms.",
                "Further improvement of NB's performance may be effected by using unlabeled documents; e.g., EM has been used successfully for this purpose in (Nigam et al.,200), where EM has increased the classification accuracy by 30% for classifying English documents."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #TARGET_REF ) . In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments. The corresponding performances in (#REF) are 75.6% and 50%, respectively. Thus, the overall performance (including cross validation and evaluation set experiments) in this work is comparable to that in (#REF). This offers some indication that the performance of NB algorithm in classifying Arabic documents is not sensitive to the Arabic root extraction algorithm. Future work will be directed at experimenting with other root extraction algorithms. Further improvement of NB's performance may be effected by using unlabeled documents; e.g., EM has been used successfully for this purpose in (Nigam et al.,200), where EM has increased the classification accuracy by 30% for classifying English documents.",
        "output": "conclusion:To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #TARGET_REF<USE/> ) . In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments. The corresponding performances in (#REF) are 75.6% and 50%, respectively. Thus, the overall performance (including cross validation and evaluation set experiments) in this work is comparable to that in (#REF). This offers some indication that the performance of NB algorithm in classifying Arabic documents is not sensitive to the Arabic root extraction algorithm. Future work will be directed at experimenting with other root extraction algorithms. Further improvement of NB's performance may be effected by using unlabeled documents; e.g., EM has been used successfully for this purpose in (Nigam et al.,200), where EM has increased the classification accuracy by 30% for classifying English documents."
    },
    {
        "gold": {
            "text": [
                "The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.",
                "This section , which elaborates on preliminary results reported in #TARGET_REF , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .",
                "For an example of a completely annotated abstract, see Figure 2.",
                "Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering. This section , which elaborates on preliminary results reported in #TARGET_REF , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence . For an example of a completely annotated abstract, see Figure 2. Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases.",
        "output": "nan:The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering. This section , which elaborates on preliminary results reported in #TARGET_REF<COMP/> , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence . For an example of a completely annotated abstract, see Figure 2. Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases."
    },
    {
        "gold": {
            "text": [
                "In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting.",
                "We have presented a comparative evaluation of our approach on datasets from four different domains.",
                "In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets.",
                "Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.",
                "Our CRF-based approach also yields promising results in the crossdomain setting.",
                "The features we employ scale well across domains, given that the opinion target vocabularies are substantially different.",
                "For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #REF ; #TARGET_REF ) , perform in comparison to our approach .",
                "Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.",
                "We observe similar challenges as #REF regarding the analysis of complex sentences.",
                "Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting. We have presented a comparative evaluation of our approach on datasets from four different domains. In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets. Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction. Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #REF ; #TARGET_REF ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as #REF regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.",
        "output": "conclusion:In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting. We have presented a comparative evaluation of our approach on datasets from four different domains. In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets. Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction. Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #REF ; #TARGET_REF<MOT/> ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as #REF regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality."
    },
    {
        "gold": {
            "text": [
                "The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.",
                "This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (#REF;#REF) that do not include nonlexicalized fragments.",
                "However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.",
                "While the models of #REF and #REF restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (#REF;#REFa).",
                "The importance of including nonheadwords has become uncontroversial (e.g.",
                "#REF;#REF;#REF).",
                "And #REF argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #TARGET_REF who use exactly the same set of ( all ) tree fragments as proposed in #REF ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "['M Collins', 'N Duffy']:The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset. This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (#REF;#REF) that do not include nonlexicalized fragments. However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions. While the models of #REF and #REF restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (#REF;#REFa). The importance of including nonheadwords has become uncontroversial (e.g. #REF;#REF;#REF). And #REF argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #TARGET_REF who use exactly the same set of ( all ) tree fragments as proposed in #REF .",
        "output": "['M Collins', 'N Duffy']:The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset. This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (#REF;#REF) that do not include nonlexicalized fragments. However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions. While the models of #REF and #REF restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (#REF;#REFa). The importance of including nonheadwords has become uncontroversial (e.g. #REF;#REF;#REF). And #REF argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #TARGET_REF<EXT/> who use exactly the same set of ( all ) tree fragments as proposed in #REF ."
    },
    {
        "gold": {
            "text": [
                "They are widely used in MT as a way to figure out how to translate input in one language into output in another language (#REF).",
                "There are several methods to build phrase tables.",
                "The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.",
                "In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .",
                "We run TreeTagger ( #REF ) for tokenization , and used the Giza + + ( #TARGET_REF ) to align the tokenized corpora at the word level .",
                "Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (#REF).",
                "Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.",
                "In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:They are widely used in MT as a way to figure out how to translate input in one language into output in another language (#REF). There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 . We run TreeTagger ( #REF ) for tokenization , and used the Giza + + ( #TARGET_REF ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (#REF). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.",
        "output": "nan:They are widely used in MT as a way to figure out how to translate input in one language into output in another language (#REF). There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 . We run TreeTagger ( #REF ) for tokenization , and used the Giza + + ( #TARGET_REF<FUT/> ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (#REF). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5."
    },
    {
        "gold": {
            "text": [
                "The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.",
                "Labeled attachment score is used as our base model loss function.",
                "In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i.",
                "The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).",
                "The score is normalized by the summed arc lengths for the sentence.",
                "The labeled version of this score requires that the labels of the arc are also correct.",
                "Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (#REF) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction ( #REF ) and textual entailment ( #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well. Labeled attachment score is used as our base model loss function. In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i. The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise). The score is normalized by the summed arc lengths for the sentence. The labeled version of this score requires that the labels of the arc are also correct. Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (#REF) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction ( #REF ) and textual entailment ( #TARGET_REF ) .",
        "output": "experiments:The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well. Labeled attachment score is used as our base model loss function. In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i. The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise). The score is normalized by the summed arc lengths for the sentence. The labeled version of this score requires that the labels of the arc are also correct. Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (#REF) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction ( #REF ) and textual entailment ( #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors).",
                "However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results.",
                "As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors). However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results. As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #TARGET_REF ; #REF ) .",
        "output": "experiments:Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors). However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results. As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #TARGET_REF<USE/> ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "Our approach to extract and classify social events builds on our previous work ( #TARGET_REF ) , which in turn builds on work from the relation extraction community ( #REF ) .",
                "Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.",
                "Researchers have used other notions of semantics in the literature such as latent semantic analysis (#REF) and relation-specific semantics (#REF;#REF).",
                "To the best of our knowledge, there is only one work that uses frame semantics for relation extraction (#REF).",
                "#REF propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity between two dependency trees.",
                "They, however, do not propose data representations that are based on frame parses and the resulting arborescent structures, instead adding features to syntactic trees.",
                "We believe the implicit feature space of kernels based on our data representation encode a richer and larger feature space than the one proposed by #REF."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "related work:Our approach to extract and classify social events builds on our previous work ( #TARGET_REF ) , which in turn builds on work from the relation extraction community ( #REF ) . Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper. Researchers have used other notions of semantics in the literature such as latent semantic analysis (#REF) and relation-specific semantics (#REF;#REF). To the best of our knowledge, there is only one work that uses frame semantics for relation extraction (#REF). #REF propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity between two dependency trees. They, however, do not propose data representations that are based on frame parses and the resulting arborescent structures, instead adding features to syntactic trees. We believe the implicit feature space of kernels based on our data representation encode a richer and larger feature space than the one proposed by #REF.",
        "output": "related work:Our approach to extract and classify social events builds on our previous work ( #TARGET_REF<COMP/> ) , which in turn builds on work from the relation extraction community ( #REF ) . Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper. Researchers have used other notions of semantics in the literature such as latent semantic analysis (#REF) and relation-specific semantics (#REF;#REF). To the best of our knowledge, there is only one work that uses frame semantics for relation extraction (#REF). #REF propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity between two dependency trees. They, however, do not propose data representations that are based on frame parses and the resulting arborescent structures, instead adding features to syntactic trees. We believe the implicit feature space of kernels based on our data representation encode a richer and larger feature space than the one proposed by #REF."
    },
    {
        "gold": {
            "text": [
                "While the combined MBL method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement.",
                "Future work will pursue at least two directions for improving the results.",
                "First, while semantic information is not available for all adjectives, it is clearly available for some.",
                "Furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available.",
                "More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #TARGET_REF ) could be applied to extract semantic classes from the corpus itself .",
                "Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:While the combined MBL method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement. Future work will pursue at least two directions for improving the results. First, while semantic information is not available for all adjectives, it is clearly available for some. Furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available. More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #TARGET_REF ) could be applied to extract semantic classes from the corpus itself . Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results.",
        "output": "conclusion:While the combined MBL method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement. Future work will pursue at least two directions for improving the results. First, while semantic information is not available for all adjectives, it is clearly available for some. Furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available. More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #TARGET_REF<MOT/> ) could be applied to extract semantic classes from the corpus itself . Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results."
    },
    {
        "gold": {
            "text": [
                "Our motivation for generation of material for language education exists in work such as #REF and #TARGET_REF , which deal with automatic generation of classic fill in the blank questions .",
                "Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "related work:Our motivation for generation of material for language education exists in work such as #REF and #TARGET_REF , which deal with automatic generation of classic fill in the blank questions . Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences.",
        "output": "related work:Our motivation for generation of material for language education exists in work such as #REF and #TARGET_REF<EXT/> , which deal with automatic generation of classic fill in the blank questions . Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences."
    },
    {
        "gold": {
            "text": [
                "Most of the head mentions proposed by the algorithms described in Sec. 3 are positive examples.",
                "We ensure a balanced training of the mention head detection model by adding sub-sampled invalid mention head candidates as negative examples.",
                "Specifically, after mention head candidate generation (described in Sec.",
                "3), we train on a set of candidates with precision larger than 50%.",
                "We then use Illinois Chunker ( #REF ) 6 to extract more noun phrases from the text and employ Collins head rules ( #TARGET_REF ) to identify their heads .",
                "When these extracted heads do not overlap with gold mention heads, we treat them as negative examples."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Most of the head mentions proposed by the algorithms described in Sec. 3 are positive examples. We ensure a balanced training of the mention head detection model by adding sub-sampled invalid mention head candidates as negative examples. Specifically, after mention head candidate generation (described in Sec. 3), we train on a set of candidates with precision larger than 50%. We then use Illinois Chunker ( #REF ) 6 to extract more noun phrases from the text and employ Collins head rules ( #TARGET_REF ) to identify their heads . When these extracted heads do not overlap with gold mention heads, we treat them as negative examples.",
        "output": "nan:Most of the head mentions proposed by the algorithms described in Sec. 3 are positive examples. We ensure a balanced training of the mention head detection model by adding sub-sampled invalid mention head candidates as negative examples. Specifically, after mention head candidate generation (described in Sec. 3), we train on a set of candidates with precision larger than 50%. We then use Illinois Chunker ( #REF ) 6 to extract more noun phrases from the text and employ Collins head rules ( #TARGET_REF<FUT/> ) to identify their heads . When these extracted heads do not overlap with gold mention heads, we treat them as negative examples."
    },
    {
        "gold": {
            "text": [
                "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.",
                "This includes work on question answering (#REF), sentiment analysis (#REF), MT reordering (#REF), and many other tasks.",
                "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #TARGET_REF ; #REF ; #REF ; #REF ) .",
                "But these accuracies are measured with respect to gold-standard out-of-domain parse trees.",
                "There are few tasks that actually depend on the complete parse tree.",
                "Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.",
                "While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.",
                "The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (#REF), sentiment analysis (#REF), MT reordering (#REF), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #TARGET_REF ; #REF ; #REF ; #REF ) . But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training. The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.",
        "output": "introduction:The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (#REF), sentiment analysis (#REF), MT reordering (#REF), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #TARGET_REF<BACK/> ; #REF ; #REF ; #REF ) . But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training. The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure."
    },
    {
        "gold": {
            "text": [
                "A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #TARGET_REF , #REF , #REF , #REF , #REF , #REF , #REF , and #REF ) .",
                "Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.",
                "While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.",
                "A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #TARGET_REF , #REF , #REF , #REF , #REF , #REF , #REF , and #REF ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF.",
        "output": "nan:A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #TARGET_REF<USE/> , #REF , #REF , #REF , #REF , #REF , #REF , and #REF ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF."
    },
    {
        "gold": {
            "text": [
                "The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.",
                "The diagnoser , based on #TARGET_REFb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer .",
                "At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to #REF."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness. The diagnoser , based on #TARGET_REFb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer . At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to #REF.",
        "output": "experiments:The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness. The diagnoser , based on #TARGET_REF<COMP/>b ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer . At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to #REF."
    },
    {
        "gold": {
            "text": [
                "Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.",
                "For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #REF ; #TARGET_REF ) and history-based parsing ( #REF ) .",
                "We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF)."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #REF ; #TARGET_REF ) and history-based parsing ( #REF ) . We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF).",
        "output": "conclusion:Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #REF ; #TARGET_REF<MOT/> ) and history-based parsing ( #REF ) . We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF)."
    },
    {
        "gold": {
            "text": [
                "Self-supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling.",
                "Rather, Welling at al. demonstrated its effectiveness in modeling hand-written digits and on synthetic data.",
                "Đn both cases essentially linear classifiers were used as features.",
                "As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples.",
                "Unfortunately , as shown in ( #TARGET_REF ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .",
                "While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data.",
                "This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications.",
                "For this reason we use a different sampling scheme which we refer to as rejection sampling."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:Self-supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling. Rather, Welling at al. demonstrated its effectiveness in modeling hand-written digits and on synthetic data. Đn both cases essentially linear classifiers were used as features. As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately , as shown in ( #TARGET_REF ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data. This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications. For this reason we use a different sampling scheme which we refer to as rejection sampling.",
        "output": "nan:Self-supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling. Rather, Welling at al. demonstrated its effectiveness in modeling hand-written digits and on synthetic data. Đn both cases essentially linear classifiers were used as features. As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately , as shown in ( #TARGET_REF<EXT/> ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data. This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications. For this reason we use a different sampling scheme which we refer to as rejection sampling."
    },
    {
        "gold": {
            "text": [
                "From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.",
                "In addition to headwords , dictionary search through the pronunciation field is available ; #TARGET_REF has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #REF ) .",
                "In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (#REF).",
                "Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes. In addition to headwords , dictionary search through the pronunciation field is available ; #TARGET_REF has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #REF ) . In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (#REF). Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample.",
        "output": "nan:From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes. In addition to headwords , dictionary search through the pronunciation field is available ; #TARGET_REF<FUT/> has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #REF ) . In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (#REF). Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample."
    },
    {
        "gold": {
            "text": [
                "Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (#REF).",
                "In other methods, lexical resources are specifically tailored to meet the requirements of the domain (#REF) or the system (#REF).",
                "Such systems extract information from some types of syntactic units ( clauses in ( #REF ; #TARGET_REF ; #REF ) ; noun phrases in ( #REF ; #REF ) ) .",
                "Lists of semantic relations are designed to capture salient domain information."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (#REF). In other methods, lexical resources are specifically tailored to meet the requirements of the domain (#REF) or the system (#REF). Such systems extract information from some types of syntactic units ( clauses in ( #REF ; #TARGET_REF ; #REF ) ; noun phrases in ( #REF ; #REF ) ) . Lists of semantic relations are designed to capture salient domain information.",
        "output": "related work:Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (#REF). In other methods, lexical resources are specifically tailored to meet the requirements of the domain (#REF) or the system (#REF). Such systems extract information from some types of syntactic units ( clauses in ( #REF ; #TARGET_REF<BACK/> ; #REF ) ; noun phrases in ( #REF ; #REF ) ) . Lists of semantic relations are designed to capture salient domain information."
    },
    {
        "gold": {
            "text": [
                "For compound splitting, we follow #REF, using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.",
                "Other approaches use less deep linguistic resources ( e.g. , POS-tags #REF ) or are ( almost ) knowledge-free ( e.g. , #TARGET_REF ) .",
                "Compound merging is less well studied.",
                "#REF used a simple, list-based merging approach, merging all consecutive words included in a merging list.",
                "This approach resulted in too many compounds.",
                "We follow #REF, for compound merging.",
                "We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:For compound splitting, we follow #REF, using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources ( e.g. , POS-tags #REF ) or are ( almost ) knowledge-free ( e.g. , #TARGET_REF ) . Compound merging is less well studied. #REF used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow #REF, for compound merging. We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets.",
        "output": "related work:For compound splitting, we follow #REF, using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources ( e.g. , POS-tags #REF ) or are ( almost ) knowledge-free ( e.g. , #TARGET_REF<USE/> ) . Compound merging is less well studied. #REF used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow #REF, for compound merging. We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets."
    },
    {
        "gold": {
            "text": [
                "WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #REFb ) , a video-recording programming system , a schedule management system ( #REFa ) , and a weather infomiation system ( #TARGET_REF ) .",
                "The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.",
                "A sample dialogue between this system and a naive user is shown in Figure 2.",
                "This system employs HTK as the speech recognition engine.",
                "The weather information system can answer the user's questions about weather forecasts in Japan.",
                "The vocabulary size is around 500, and the number of phrase structure rules is 31.",
                "The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #REFb ) , a video-recording programming system , a schedule management system ( #REFa ) , and a weather infomiation system ( #TARGET_REF ) . The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue between this system and a naive user is shown in Figure 2. This system employs HTK as the speech recognition engine. The weather information system can answer the user's questions about weather forecasts in Japan. The vocabulary size is around 500, and the number of phrase structure rules is 31. The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000.",
        "output": "experiments:WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #REFb ) , a video-recording programming system , a schedule management system ( #REFa ) , and a weather infomiation system ( #TARGET_REF<COMP/> ) . The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue between this system and a naive user is shown in Figure 2. This system employs HTK as the speech recognition engine. The weather information system can answer the user's questions about weather forecasts in Japan. The vocabulary size is around 500, and the number of phrase structure rules is 31. The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000."
    },
    {
        "gold": {
            "text": [
                "The sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach.",
                "In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( #REF ) , class n-grams ( #TARGET_REF ) , grammatical features ( #REF ) , etc ' ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:The sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach. In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( #REF ) , class n-grams ( #TARGET_REF ) , grammatical features ( #REF ) , etc ' .",
        "output": "conclusion:The sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach. In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( #REF ) , class n-grams ( #TARGET_REF<MOT/> ) , grammatical features ( #REF ) , etc ' ."
    },
    {
        "gold": {
            "text": [
                "We also compute GIST vectors (#REF) for every image using LearGIST (#REF).",
                "Unlike SURF descriptors, GIST produces a single vector representation for an image.",
                "The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall \"gist\" of the whole image.",
                "It is frequently used in tasks like scene identification , and #TARGET_REF shows that distance in GIST space correlates well with semantic distance in WordNet .",
                "After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:We also compute GIST vectors (#REF) for every image using LearGIST (#REF). Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall \"gist\" of the whole image. It is frequently used in tasks like scene identification , and #TARGET_REF shows that distance in GIST space correlates well with semantic distance in WordNet . After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.",
        "output": "experiments:We also compute GIST vectors (#REF) for every image using LearGIST (#REF). Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall \"gist\" of the whole image. It is frequently used in tasks like scene identification , and #TARGET_REF<EXT/> shows that distance in GIST space correlates well with semantic distance in WordNet . After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality."
    },
    {
        "gold": {
            "text": [
                "The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish.",
                "It consists of 1600 pairs derived from the RTE3 development and test sets (800+800).",
                "Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #TARGET_REF ) .",
                "The method relies on translation-validation cycles, defined as separate jobs routed to MTurk's workforce.",
                "Translation jobs return one Spanish version for each hypothesis.",
                "Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference.",
                "At each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job.",
                "Although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired CLTE corpus."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish. It consists of 1600 pairs derived from the RTE3 development and test sets (800+800). Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #TARGET_REF ) . The method relies on translation-validation cycles, defined as separate jobs routed to MTurk's workforce. Translation jobs return one Spanish version for each hypothesis. Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference. At each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job. Although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired CLTE corpus.",
        "output": "experiments:The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish. It consists of 1600 pairs derived from the RTE3 development and test sets (800+800). Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #TARGET_REF<FUT/> ) . The method relies on translation-validation cycles, defined as separate jobs routed to MTurk's workforce. Translation jobs return one Spanish version for each hypothesis. Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference. At each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job. Although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired CLTE corpus."
    },
    {
        "gold": {
            "text": [
                "In addition to the different response-generation methods, we have proposed a metalevel strategy to combine them.",
                "This kind of meta-learning is referred to as stacking by the Data Mining community (#REF).",
                "#REF implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version.",
                "They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #TARGET_REF as follows .",
                "The merging category corresponds to techniques where the individual methods affect each other in different ways (this category encompasses Burke's feature combination, cascade, feature augmentation, and meta-level sub-categories).",
                "The ensemble category corresponds to techniques where the predictions of the individual methods are combined to produce a final prediction (this category encompasses Burke's weighted, switching, and mixed sub-categories)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:In addition to the different response-generation methods, we have proposed a metalevel strategy to combine them. This kind of meta-learning is referred to as stacking by the Data Mining community (#REF). #REF implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version. They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #TARGET_REF as follows . The merging category corresponds to techniques where the individual methods affect each other in different ways (this category encompasses Burke's feature combination, cascade, feature augmentation, and meta-level sub-categories). The ensemble category corresponds to techniques where the predictions of the individual methods are combined to produce a final prediction (this category encompasses Burke's weighted, switching, and mixed sub-categories).",
        "output": "nan:In addition to the different response-generation methods, we have proposed a metalevel strategy to combine them. This kind of meta-learning is referred to as stacking by the Data Mining community (#REF). #REF implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version. They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #TARGET_REF<BACK/> as follows . The merging category corresponds to techniques where the individual methods affect each other in different ways (this category encompasses Burke's feature combination, cascade, feature augmentation, and meta-level sub-categories). The ensemble category corresponds to techniques where the predictions of the individual methods are combined to produce a final prediction (this category encompasses Burke's weighted, switching, and mixed sub-categories)."
    },
    {
        "gold": {
            "text": [
                "The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #TARGET_REF , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) .",
                "In the CLE-QLF approach, as ra-tionally reconstructed by #REF and #REF, the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.",
                "Just as here, these QLFs represent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #TARGET_REF , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) . In the CLE-QLF approach, as ra-tionally reconstructed by #REF and #REF, the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules. Just as here, these QLFs represent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.",
        "output": "nan:The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #TARGET_REF<USE/> , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) . In the CLE-QLF approach, as ra-tionally reconstructed by #REF and #REF, the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules. Just as here, these QLFs represent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context."
    },
    {
        "gold": {
            "text": [
                "In our previous work ( #TARGET_REF ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â§ ... â§ dm , and a hypothesis H represented by another set of clauses H = h1 â§ ... â§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .",
                "This is based on a simple as- sumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:In our previous work ( #TARGET_REF ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â§ ... â§ dm , and a hypothesis H represented by another set of clauses H = h1 â§ ... â§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows . This is based on a simple as- sumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses.",
        "output": "nan:In our previous work ( #TARGET_REF<COMP/> ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â§ ... â§ dm , and a hypothesis H represented by another set of clauses H = h1 â§ ... â§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows . This is based on a simple as- sumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses."
    },
    {
        "gold": {
            "text": [
                "We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.",
                "Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #REF ; #TARGET_REF ) for self training .",
                "Self-training should also benefit other discriminatively trained parsers with latent annotations (#REF), although training would be much slower compared to using generative models, as in our case."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #REF ; #TARGET_REF ) for self training . Self-training should also benefit other discriminatively trained parsers with latent annotations (#REF), although training would be much slower compared to using generative models, as in our case.",
        "output": "conclusion:We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #REF ; #TARGET_REF<MOT/> ) for self training . Self-training should also benefit other discriminatively trained parsers with latent annotations (#REF), although training would be much slower compared to using generative models, as in our case."
    },
    {
        "gold": {
            "text": [
                "P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.",
                "Inspired by ( #TARGET_REF ) and ( #REF ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "method:P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables. Inspired by ( #TARGET_REF ) and ( #REF ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .",
        "output": "method:P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables. Inspired by ( #TARGET_REF<EXT/> ) and ( #REF ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string ."
    },
    {
        "gold": {
            "text": [
                "In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #TARGET_REF .",
                "We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in their ability to predict association norms over traditional text-only LDA.",
                "SURF features also provided significant gains over text-only LDA in predicting the compositionality of noun compounds."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "conclusion:In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #TARGET_REF . We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in their ability to predict association norms over traditional text-only LDA. SURF features also provided significant gains over text-only LDA in predicting the compositionality of noun compounds.",
        "output": "conclusion:In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #TARGET_REF<FUT/> . We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in their ability to predict association norms over traditional text-only LDA. SURF features also provided significant gains over text-only LDA in predicting the compositionality of noun compounds."
    },
    {
        "gold": {
            "text": [
                "where f and e (e ) are source and target sentences, respectively.",
                "h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .",
                "Some methods are based on likelihood ( #REF ; #REF ) , error rate ( #TARGET_REF ; #REF ; #REF ; #REF ) , margin ( #REF ; #REF ) and ranking ( #REF ) , and among which minimum error rate training ( MERT ) ( #REF ) is the most popular one .",
                "All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.",
                "We call them a global training method.",
                "One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.",
                "However, due to the diversity and uneven distribution of source sentences (#REF), there are some shortcomings in this pipeline."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( #REF ; #REF ) , error rate ( #TARGET_REF ; #REF ; #REF ; #REF ) , margin ( #REF ; #REF ) and ranking ( #REF ) , and among which minimum error rate training ( MERT ) ( #REF ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences (#REF), there are some shortcomings in this pipeline.",
        "output": "introduction:where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( #REF ; #REF ) , error rate ( #TARGET_REF<BACK/> ; #REF ; #REF ; #REF ) , margin ( #REF ; #REF ) and ranking ( #REF ) , and among which minimum error rate training ( MERT ) ( #REF ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences (#REF), there are some shortcomings in this pipeline."
    },
    {
        "gold": {
            "text": [
                "In FAQs,  employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document.",
                "They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.",
                "compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).",
                "Jijkoun and de #REF compared different variants of retrieval techniques.",
                "#TARGET_REF compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .",
                "Two significant differences between help-desk and FAQs are the following."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:In FAQs,  employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document. They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models. compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models). Jijkoun and de #REF compared different variants of retrieval techniques. #TARGET_REF compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval . Two significant differences between help-desk and FAQs are the following.",
        "output": "nan:In FAQs,  employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document. They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models. compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models). Jijkoun and de #REF compared different variants of retrieval techniques. #TARGET_REF<USE/> compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval . Two significant differences between help-desk and FAQs are the following."
    },
    {
        "gold": {
            "text": [
                "That is, we simply take the original mLDA model of #TARGET_REF (2009) and generalize it in the same way they generalize LDA.",
                "At first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a tuple are conditionally independent given the topic assignment zi."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "related work:That is, we simply take the original mLDA model of #TARGET_REF (2009) and generalize it in the same way they generalize LDA. At first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a tuple are conditionally independent given the topic assignment zi.",
        "output": "related work:That is, we simply take the original mLDA model of #TARGET_REF<COMP/> (2009) and generalize it in the same way they generalize LDA. At first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a tuple are conditionally independent given the topic assignment zi."
    },
    {
        "gold": {
            "text": [
                "In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.",
                "The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn't explicitly explain the reason why different terminology is needed (#REF).",
                "Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #TARGET_REF ) .",
                "Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment. The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn't explicitly explain the reason why different terminology is needed (#REF). Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #TARGET_REF ) . Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.",
        "output": "conclusion:In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment. The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn't explicitly explain the reason why different terminology is needed (#REF). Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #TARGET_REF<MOT/> ) . Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail."
    },
    {
        "gold": {
            "text": [
                "In this section we describe in detail the baseline NER system we use.",
                "It is inspired by the system described in #TARGET_REF .",
                "Because NER annotations are commonly not nested (for example, in the text \"the US Army\", \"US Army\" is treated as a single entity, instead of the location \"US\" and the organization \"US Army\") it is possible to treat NER as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity.",
                "Following #REF we use the BILOU encoding, where each token can either BEGIN an entity, be INSIDE an entity, be the LAST token in an entity, be OUTSIDE an entity, or be the single UNIQUE token in an entity."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:In this section we describe in detail the baseline NER system we use. It is inspired by the system described in #TARGET_REF . Because NER annotations are commonly not nested (for example, in the text \"the US Army\", \"US Army\" is treated as a single entity, instead of the location \"US\" and the organization \"US Army\") it is possible to treat NER as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity. Following #REF we use the BILOU encoding, where each token can either BEGIN an entity, be INSIDE an entity, be the LAST token in an entity, be OUTSIDE an entity, or be the single UNIQUE token in an entity.",
        "output": "introduction:In this section we describe in detail the baseline NER system we use. It is inspired by the system described in #TARGET_REF<EXT/> . Because NER annotations are commonly not nested (for example, in the text \"the US Army\", \"US Army\" is treated as a single entity, instead of the location \"US\" and the organization \"US Army\") it is possible to treat NER as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity. Following #REF we use the BILOU encoding, where each token can either BEGIN an entity, be INSIDE an entity, be the LAST token in an entity, be OUTSIDE an entity, or be the single UNIQUE token in an entity."
    },
    {
        "gold": {
            "text": [
                "The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4.",
                "These two sets of data were used for automatic dialogue act classification, which was run in the Weka system (#REF).",
                "We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .",
                "The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #TARGET_REF ) .",
                "Therefore, here we show the results of this classifier.",
                "Ten-folds crossvalidation was applied throughout."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4. These two sets of data were used for automatic dialogue act classification, which was run in the Weka system (#REF). We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table . The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #TARGET_REF ) . Therefore, here we show the results of this classifier. Ten-folds crossvalidation was applied throughout.",
        "output": "nan:The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4. These two sets of data were used for automatic dialogue act classification, which was run in the Weka system (#REF). We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table . The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #TARGET_REF<FUT/> ) . Therefore, here we show the results of this classifier. Ten-folds crossvalidation was applied throughout."
    },
    {
        "gold": {
            "text": [
                "In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #TARGET_REF ; #REF ; #REF ] , head-driven phrase structure grammar [ HPSG ] [ #REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #TARGET_REF ; #REF ; #REF ] , head-driven phrase structure grammar [ HPSG ] [ #REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .",
        "output": "introduction:In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #TARGET_REF<BACK/> ; #REF ; #REF ] , head-driven phrase structure grammar [ HPSG ] [ #REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."
    },
    {
        "gold": {
            "text": [
                "All logical notions that we are going to consider, such as theory or model, will be finitary.",
                "For example, a model would typically contain fewer than a hundred elements of different logical sorts.",
                "Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.",
                "The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.",
                "This Principle of Finitism is also assumed by #TARGET_REF , #REF , #REF , and implicitly or explicitly by almost all researchers in computational linguistics .",
                "As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.",
                "#REF)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred elements of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by #TARGET_REF , #REF , #REF , and implicitly or explicitly by almost all researchers in computational linguistics . As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. #REF).",
        "output": "introduction:All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred elements of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by #TARGET_REF<USE/> , #REF , #REF , and implicitly or explicitly by almost all researchers in computational linguistics . As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. #REF)."
    },
    {
        "gold": {
            "text": [
                "Baldridge observed is that, cross-linguistically, grammars prefer simpler syntactic structures when possible, and that due to the natural correspondence of categories and syntactic structure, biasing toward simpler categories encourages simpler structures.",
                "In previous work, we were able to incorporate this preference into a Bayesian parsing model, biasing PCFG productions toward sim-pler categories by encoding a notion of category simplicity into a prior (#REF).",
                "Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable.",
                "We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #TARGET_REF ) ."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:Baldridge observed is that, cross-linguistically, grammars prefer simpler syntactic structures when possible, and that due to the natural correspondence of categories and syntactic structure, biasing toward simpler categories encourages simpler structures. In previous work, we were able to incorporate this preference into a Bayesian parsing model, biasing PCFG productions toward sim-pler categories by encoding a notion of category simplicity into a prior (#REF). Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable. We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #TARGET_REF ) .",
        "output": "introduction:Baldridge observed is that, cross-linguistically, grammars prefer simpler syntactic structures when possible, and that due to the natural correspondence of categories and syntactic structure, biasing toward simpler categories encourages simpler structures. In previous work, we were able to incorporate this preference into a Bayesian parsing model, biasing PCFG productions toward sim-pler categories by encoding a notion of category simplicity into a prior (#REF). Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable. We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #TARGET_REF<COMP/> ) ."
    },
    {
        "gold": {
            "text": [
                "When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.",
                "In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.",
                "Note that this ensures that greater importance is attributed to longer chunks, as is usual in most EBMT systems (cf.",
                "#REF;#REF;#REF). 7",
                "s an example, consider the translation into French of the house collapsed.",
                "Assume the conditional probabilities in ( 33  These mistranslations are all caused by boundary friction.",
                "Each of the translations in ( 37) and ( 38) would be output with an associated weight and ranked by the system.",
                "We would like to incorporate into our model a procedure whereby translation chunks extracted from the phrasal and marker lexicons are more highly regarded than those constructed by inserting words from the word-level lexicon into generalized marker chunks.",
                "That is, we want to allocate a larger portion of the probability space to the phrasal and marker lexicons than to the generalized or wordlevel lexicons.",
                "We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( #TARGET_REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "introduction:When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string. In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. Note that this ensures that greater importance is attributed to longer chunks, as is usual in most EBMT systems (cf. #REF;#REF;#REF). 7 s an example, consider the translation into French of the house collapsed. Assume the conditional probabilities in ( 33  These mistranslations are all caused by boundary friction. Each of the translations in ( 37) and ( 38) would be output with an associated weight and ranked by the system. We would like to incorporate into our model a procedure whereby translation chunks extracted from the phrasal and marker lexicons are more highly regarded than those constructed by inserting words from the word-level lexicon into generalized marker chunks. That is, we want to allocate a larger portion of the probability space to the phrasal and marker lexicons than to the generalized or wordlevel lexicons. We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( #TARGET_REF ) .",
        "output": "introduction:When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string. In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. Note that this ensures that greater importance is attributed to longer chunks, as is usual in most EBMT systems (cf. #REF;#REF;#REF). 7 s an example, consider the translation into French of the house collapsed. Assume the conditional probabilities in ( 33  These mistranslations are all caused by boundary friction. Each of the translations in ( 37) and ( 38) would be output with an associated weight and ranked by the system. We would like to incorporate into our model a procedure whereby translation chunks extracted from the phrasal and marker lexicons are more highly regarded than those constructed by inserting words from the word-level lexicon into generalized marker chunks. That is, we want to allocate a larger portion of the probability space to the phrasal and marker lexicons than to the generalized or wordlevel lexicons. We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( #TARGET_REF<MOT/> ) ."
    },
    {
        "gold": {
            "text": [
                "The implementation has been inspired by experience in extracting information from very large corpora ( #REF ) and performing experiments on maximum entropy sequence tagging ( #TARGET_REF ; #REF ) .",
                "We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.",
                "These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.",
                "These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (#REF).",
                "We expect even faster training times when we move to conjugate gradient methods."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:The implementation has been inspired by experience in extracting information from very large corpora ( #REF ) and performing experiments on maximum entropy sequence tagging ( #TARGET_REF ; #REF ) . We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (#REF). We expect even faster training times when we move to conjugate gradient methods.",
        "output": "experiments:The implementation has been inspired by experience in extracting information from very large corpora ( #REF ) and performing experiments on maximum entropy sequence tagging ( #TARGET_REF<EXT/> ; #REF ) . We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (#REF). We expect even faster training times when we move to conjugate gradient methods."
    },
    {
        "gold": {
            "text": [
                "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) #TARGET_REF , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) #TARGET_REF , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .",
        "output": "nan:Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) #TARGET_REF<FUT/> , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
    },
    {
        "gold": {
            "text": [
                "Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.",
                "Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.",
                "Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.",
                "The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.",
                "This imbalance foils thresholding strategies , clever as they might be ( #TARGET_REF ; Wu & #REF ; #REF ) .",
                "The likelihoods in the word-to-word model remain unnormalized, so they do not compete."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models. Part of the difficulty stems from the implicit assumption in other models that each word has only one sense. Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations. The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation. This imbalance foils thresholding strategies , clever as they might be ( #TARGET_REF ; Wu & #REF ; #REF ) . The likelihoods in the word-to-word model remain unnormalized, so they do not compete.",
        "output": "nan:Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models. Part of the difficulty stems from the implicit assumption in other models that each word has only one sense. Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations. The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation. This imbalance foils thresholding strategies , clever as they might be ( #TARGET_REF<BACK/> ; Wu & #REF ; #REF ) . The likelihoods in the word-to-word model remain unnormalized, so they do not compete."
    },
    {
        "gold": {
            "text": [
                "Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12.",
                "But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.",
                "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.",
                "This is roughly an 11 % relative reduction in error rate over #TARGET_REF and Bods PCFG-reduction reported in Table 1 .",
                "Compared to the reranking technique in #REF, who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction.",
                "While SL-DOP and LS-DOP have been compared before in"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12. But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11 % relative reduction in error rate over #TARGET_REF and Bods PCFG-reduction reported in Table 1 . Compared to the reranking technique in #REF, who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. While SL-DOP and LS-DOP have been compared before in",
        "output": "conclusion:Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12. But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11 % relative reduction in error rate over #TARGET_REF<USE/> and Bods PCFG-reduction reported in Table 1 . Compared to the reranking technique in #REF, who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. While SL-DOP and LS-DOP have been compared before in"
    },
    {
        "gold": {
            "text": [
                "magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.",
                "In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv£:;G#REF).",
                "Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #REF ) as discussed in ( #TARGET_REFa ) and ( #REF ) .",
                "Typed feature grammar constraints that are inexpensive to resolve are dealt with using the top-down interpreter of the ConTroll grammar development system (G#REFb) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements. In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv£:;G#REF). Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #REF ) as discussed in ( #TARGET_REFa ) and ( #REF ) . Typed feature grammar constraints that are inexpensive to resolve are dealt with using the top-down interpreter of the ConTroll grammar development system (G#REFb) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation.",
        "output": "introduction:magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements. In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv£:;G#REF). Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #REF ) as discussed in ( #TARGET_REF<COMP/>a ) and ( #REF ) . Typed feature grammar constraints that are inexpensive to resolve are dealt with using the top-down interpreter of the ConTroll grammar development system (G#REFb) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation."
    },
    {
        "gold": {
            "text": [
                "We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.",
                "Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #TARGET_REF ; #REF ) for self training .",
                "Self-training should also benefit other discriminatively trained parsers with latent annotations (#REF), although training would be much slower compared to using generative models, as in our case."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #TARGET_REF ; #REF ) for self training . Self-training should also benefit other discriminatively trained parsers with latent annotations (#REF), although training would be much slower compared to using generative models, as in our case.",
        "output": "conclusion:We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #TARGET_REF<MOT/> ; #REF ) for self training . Self-training should also benefit other discriminatively trained parsers with latent annotations (#REF), although training would be much slower compared to using generative models, as in our case."
    },
    {
        "gold": {
            "text": [
                "This makes it possible to classify words and their semantic forms based on their syntactic category and reduces the risk of inaccurate assignment of subcategorization frame frequencies due to POS ambiguity, distinguishing, for example, between the nominal and verbal occurrences of the lemma fight.",
                "With this, the output for the verb impose in Figure 4 is impose (v,[subj, obj, obl:on]).",
                "For some of our experiments, we conflate the different verbal (and other) tags used in the Penn Treebanks to a single verbal marker (Table 4).",
                "As a further extension, the extraction procedure reads off the syntactic category of the head of each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]). 3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details.",
                "#TARGET_REF argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .",
                "In contrast to much of the work reviewed in Section 3, which limits itself to the extraction of surface syntactic subcategorization details, our system can provide this information as well as details of grammatical function."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "method:This makes it possible to classify words and their semantic forms based on their syntactic category and reduces the risk of inaccurate assignment of subcategorization frame frequencies due to POS ambiguity, distinguishing, for example, between the nominal and verbal occurrences of the lemma fight. With this, the output for the verb impose in Figure 4 is impose (v,[subj, obj, obl:on]). For some of our experiments, we conflate the different verbal (and other) tags used in the Penn Treebanks to a single verbal marker (Table 4). As a further extension, the extraction procedure reads off the syntactic category of the head of each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]). 3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details. #TARGET_REF argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization . In contrast to much of the work reviewed in Section 3, which limits itself to the extraction of surface syntactic subcategorization details, our system can provide this information as well as details of grammatical function.",
        "output": "method:This makes it possible to classify words and their semantic forms based on their syntactic category and reduces the risk of inaccurate assignment of subcategorization frame frequencies due to POS ambiguity, distinguishing, for example, between the nominal and verbal occurrences of the lemma fight. With this, the output for the verb impose in Figure 4 is impose (v,[subj, obj, obl:on]). For some of our experiments, we conflate the different verbal (and other) tags used in the Penn Treebanks to a single verbal marker (Table 4). As a further extension, the extraction procedure reads off the syntactic category of the head of each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]). 3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details. #TARGET_REF<EXT/> argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization . In contrast to much of the work reviewed in Section 3, which limits itself to the extraction of surface syntactic subcategorization details, our system can provide this information as well as details of grammatical function."
    },
    {
        "gold": {
            "text": [
                "This section describes our joint coreference resolution and mention head detection framework.",
                "Our work is inspired by the latent left-linking model in #TARGET_REF and the ILP formulation from #REF .",
                "The joint learning and inference model takes as input mention head candidates (Sec.",
                "3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.",
                "This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier.",
                "The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones.",
                "By learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from #REF.",
                "This joint framework aims to improve performance on both mention head detection and on coreference."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:This section describes our joint coreference resolution and mention head detection framework. Our work is inspired by the latent left-linking model in #TARGET_REF and the ILP formulation from #REF . The joint learning and inference model takes as input mention head candidates (Sec. 3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions. This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier. The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones. By learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from #REF. This joint framework aims to improve performance on both mention head detection and on coreference.",
        "output": "nan:This section describes our joint coreference resolution and mention head detection framework. Our work is inspired by the latent left-linking model in #TARGET_REF<FUT/> and the ILP formulation from #REF . The joint learning and inference model takes as input mention head candidates (Sec. 3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions. This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier. The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones. By learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from #REF. This joint framework aims to improve performance on both mention head detection and on coreference."
    },
    {
        "gold": {
            "text": [
                "All logical notions that we are going to consider, such as theory or model, will be finitary.",
                "For example, a model would typically contain fewer than a hundred elements of different logical sorts.",
                "Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.",
                "The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.",
                "This Principle of Finitism is also assumed by #REF, Jackendoff (1983, #REF, and implicitly or explicitly by almost all researchers in computational linguistics.",
                "As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #TARGET_REF ) .",
                "#REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred elements of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by #REF, Jackendoff (1983, #REF, and implicitly or explicitly by almost all researchers in computational linguistics. As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #TARGET_REF ) . #REF).",
        "output": "introduction:All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred elements of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by #REF, Jackendoff (1983, #REF, and implicitly or explicitly by almost all researchers in computational linguistics. As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #TARGET_REF<BACK/> ) . #REF)."
    },
    {
        "gold": {
            "text": [
                "6The analysis is reminiscent of the treatment of coordination in the Collins parser ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:6The analysis is reminiscent of the treatment of coordination in the Collins parser ( #TARGET_REF ) .",
        "output": "experiments:6The analysis is reminiscent of the treatment of coordination in the Collins parser ( #TARGET_REF<USE/> ) ."
    },
    {
        "gold": {
            "text": [
                "1. Char-n-grams (G): We start with a character n-gram-based approach (#REF), which is most common and followed by many language identification researchers.",
                "Following the work of #REF, we select character n-grams (n=1 to 5) and the word as the features in our experiments.",
                "2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments.",
                "3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #TARGET_REF ; #REF ) and create multiple features for length using a decision tree ( J48 ) .",
                "We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.",
                "4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:1. Char-n-grams (G): We start with a character n-gram-based approach (#REF), which is most common and followed by many language identification researchers. Following the work of #REF, we select character n-grams (n=1 to 5) and the word as the features in our experiments. 2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #TARGET_REF ; #REF ) and create multiple features for length using a decision tree ( J48 ) . We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.",
        "output": "experiments:1. Char-n-grams (G): We start with a character n-gram-based approach (#REF), which is most common and followed by many language identification researchers. Following the work of #REF, we select character n-grams (n=1 to 5) and the word as the features in our experiments. 2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #TARGET_REF<COMP/> ; #REF ) and create multiple features for length using a decision tree ( J48 ) . We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized."
    },
    {
        "gold": {
            "text": [
                "However, synonym mapping is still incomplete in the current state of our subword dictionary.",
                "A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.",
                "We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5).",
                "It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.",
                "This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system.",
                "Alternatively , we may think of user-centered comparative studies ( #TARGET_REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "experiments:However, synonym mapping is still incomplete in the current state of our subword dictionary. A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing. We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5). It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents. This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system. Alternatively , we may think of user-centered comparative studies ( #TARGET_REF ) .",
        "output": "experiments:However, synonym mapping is still incomplete in the current state of our subword dictionary. A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing. We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5). It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents. This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system. Alternatively , we may think of user-centered comparative studies ( #TARGET_REF<MOT/> ) ."
    },
    {
        "gold": {
            "text": [
                "A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon.",
                "Most current HPSG analyses of Dutch, German, Italian, and French fall into that category.",
                "1 Furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run-time.",
                "Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry.",
                "This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned.",
                "As shown in #TARGET_REF this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon. Most current HPSG analyses of Dutch, German, Italian, and French fall into that category. 1 Furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run-time. Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry. This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned. As shown in #TARGET_REF this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .",
        "output": "introduction:A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon. Most current HPSG analyses of Dutch, German, Italian, and French fall into that category. 1 Furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run-time. Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry. This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned. As shown in #TARGET_REF<EXT/> this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries ."
    },
    {
        "gold": {
            "text": [
                "The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0",
                "dataset only has mention annotations.",
                "Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #TARGET_REF ) with gold constituency parsing information and gold named entity information .",
                "The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.",
                "We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec.",
                "3.1.1.",
                "The nonoverlapping mention head assumption in Sec.",
                "3.1.1",
                "can be verified empirically on both ACE-2004 and OntoNotes-5.0",
                "datasets.",
                "Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (#REF), Berkeley system (#REF) and HOTCoref system (Björkelund and #REF).",
                "Developed Systems Our developed system is built on the work by #REF, using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0 dataset only has mention annotations. Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #TARGET_REF ) with gold constituency parsing information and gold named entity information . The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec. 3.1.1. The nonoverlapping mention head assumption in Sec. 3.1.1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (#REF), Berkeley system (#REF) and HOTCoref system (Björkelund and #REF). Developed Systems Our developed system is built on the work by #REF, using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 .",
        "output": "experiments:The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0 dataset only has mention annotations. Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #TARGET_REF<FUT/> ) with gold constituency parsing information and gold named entity information . The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec. 3.1.1. The nonoverlapping mention head assumption in Sec. 3.1.1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (#REF), Berkeley system (#REF) and HOTCoref system (Björkelund and #REF). Developed Systems Our developed system is built on the work by #REF, using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 ."
    },
    {
        "gold": {
            "text": [
                "Clinical question answering is an emerging area of research that has only recently begun to receive serious attention.",
                "As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated.",
                "In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems.",
                "For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (#REF).",
                "In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #TARGET_REFa ) for a brief overview ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Clinical question answering is an emerging area of research that has only recently begun to receive serious attention. As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated. In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (#REF). In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #TARGET_REFa ) for a brief overview .",
        "output": "related work:Clinical question answering is an emerging area of research that has only recently begun to receive serious attention. As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated. In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (#REF). In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #TARGET_REF<BACK/>a ) for a brief overview ."
    },
    {
        "gold": {
            "text": [
                "It is worth emphasizing that we do not necessarily expect the dependency representations produced by the training method to be traditional dependency structures for the two languages.",
                "Instead, the aim is to produce bilingual (i.e., synchronized, see below) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus.",
                "For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching.",
                "This contrasts with one of the traditional approaches ( e.g. , #TARGET_REF ; #REF ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:It is worth emphasizing that we do not necessarily expect the dependency representations produced by the training method to be traditional dependency structures for the two languages. Instead, the aim is to produce bilingual (i.e., synchronized, see below) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus. For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching. This contrasts with one of the traditional approaches ( e.g. , #TARGET_REF ; #REF ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .",
        "output": "method:It is worth emphasizing that we do not necessarily expect the dependency representations produced by the training method to be traditional dependency structures for the two languages. Instead, the aim is to produce bilingual (i.e., synchronized, see below) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus. For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching. This contrasts with one of the traditional approaches ( e.g. , #TARGET_REF<USE/> ; #REF ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language ."
    },
    {
        "gold": {
            "text": [
                "We have shown elsewhere ( #TARGET_REF ; #REFa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .",
                "This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:We have shown elsewhere ( #TARGET_REF ; #REFa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment . This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.",
        "output": "introduction:We have shown elsewhere ( #TARGET_REF<COMP/> ; #REFa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment . This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory."
    },
    {
        "gold": {
            "text": [
                "In a similar vain to #TARGET_REF and #REF , the method extends an existing flat shallow-parsing method to handle composite structures ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:In a similar vain to #TARGET_REF and #REF , the method extends an existing flat shallow-parsing method to handle composite structures .",
        "output": "conclusion:In a similar vain to #TARGET_REF<MOT/> and #REF , the method extends an existing flat shallow-parsing method to handle composite structures ."
    },
    {
        "gold": {
            "text": [
                "Due to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here.",
                "Motivated by (#TARGET_REF, 2003; #REF), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW−WbII2+ A � j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[αW · h(fj, e)] Pα(e|fj; W) = (7) Ee'Ec; exp[αW · h(fj, e')], where α > 0 is a real number valued smoother.",
                "One can see that, in the extreme case, for α —* oc, (6) converges to (5)."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:Due to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here. Motivated by (#TARGET_REF, 2003; #REF), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW−WbII2+ A � j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[αW · h(fj, e)] Pα(e|fj; W) = (7) Ee'Ec; exp[αW · h(fj, e')], where α > 0 is a real number valued smoother. One can see that, in the extreme case, for α —* oc, (6) converges to (5).",
        "output": "introduction:Due to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here. Motivated by (#TARGET_REF<EXT/>, 2003; #REF), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW−WbII2+ A � j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[αW · h(fj, e)] Pα(e|fj; W) = (7) Ee'Ec; exp[αW · h(fj, e')], where α > 0 is a real number valued smoother. One can see that, in the extreme case, for α —* oc, (6) converges to (5)."
    },
    {
        "gold": {
            "text": [
                "For Spanish, we downloaded a bilingual English-Spanish lexicon from the Internet (http://www.activa.arrakis.es)",
                "containing around 22,000 English words (16,000 English stems) and processed it similarly.",
                "Each English word has around 1.5 translations on average.",
                "A cooccurrence based stemmer ( #TARGET_REF ) was used to stem Spanish words .",
                "One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.",
                "This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:For Spanish, we downloaded a bilingual English-Spanish lexicon from the Internet (http://www.activa.arrakis.es) containing around 22,000 English words (16,000 English stems) and processed it similarly. Each English word has around 1.5 translations on average. A cooccurrence based stemmer ( #TARGET_REF ) was used to stem Spanish words . One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon. This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon.",
        "output": "experiments:For Spanish, we downloaded a bilingual English-Spanish lexicon from the Internet (http://www.activa.arrakis.es) containing around 22,000 English words (16,000 English stems) and processed it similarly. Each English word has around 1.5 translations on average. A cooccurrence based stemmer ( #TARGET_REF<FUT/> ) was used to stem Spanish words . One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon. This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon."
    },
    {
        "gold": {
            "text": [
                "Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.",
                "#REF run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.",
                "The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.",
                "#REF employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames.",
                "#TARGET_REF attempts to improve on the approach of #REF by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .",
                "He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.",
                "The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following #REF.",
                "Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames. #REF run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes. The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. #REF employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #TARGET_REF attempts to improve on the approach of #REF by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur . He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following #REF. Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.",
        "output": "related work:Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames. #REF run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes. The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. #REF employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #TARGET_REF<BACK/> attempts to improve on the approach of #REF by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur . He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following #REF. Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb."
    },
    {
        "gold": {
            "text": [
                "The example used to illustrate the power of ATNs ( #TARGET_REF ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator .",
                "Cases of crossed traces are automatically blocked because the second CURRENT-FOCUS gets moved into the FLOAT-OBJECT position at the time of the second activator, overriding the preexisting FLOAT-OBJECT set up by the earlier activator.",
                "The wrong FLOAT-OBJECT is available at the position of the first trace, and the parse dies: *(Which books)/did you ask John (where)j Bill bought (ti) (tj)?",
                "The CURRENT-FOCUS slot is not restricted to nodes that represent nouns.",
                "Some of the generators are adverbial or adjectival parts of speech (pos).",
                "An absorber checks for agreement in POS before it can accept the FLOAT-OBJECT as its subparse.",
                "As an example, the question, \"(How oily)/do you like your salad dressing (ti)?\" contains a [q-subject] \"how oily\" that is an adjective."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The example used to illustrate the power of ATNs ( #TARGET_REF ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator . Cases of crossed traces are automatically blocked because the second CURRENT-FOCUS gets moved into the FLOAT-OBJECT position at the time of the second activator, overriding the preexisting FLOAT-OBJECT set up by the earlier activator. The wrong FLOAT-OBJECT is available at the position of the first trace, and the parse dies: *(Which books)/did you ask John (where)j Bill bought (ti) (tj)? The CURRENT-FOCUS slot is not restricted to nodes that represent nouns. Some of the generators are adverbial or adjectival parts of speech (pos). An absorber checks for agreement in POS before it can accept the FLOAT-OBJECT as its subparse. As an example, the question, \"(How oily)/do you like your salad dressing (ti)?\" contains a [q-subject] \"how oily\" that is an adjective.",
        "output": "nan:The example used to illustrate the power of ATNs ( #TARGET_REF<USE/> ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator . Cases of crossed traces are automatically blocked because the second CURRENT-FOCUS gets moved into the FLOAT-OBJECT position at the time of the second activator, overriding the preexisting FLOAT-OBJECT set up by the earlier activator. The wrong FLOAT-OBJECT is available at the position of the first trace, and the parse dies: *(Which books)/did you ask John (where)j Bill bought (ti) (tj)? The CURRENT-FOCUS slot is not restricted to nodes that represent nouns. Some of the generators are adverbial or adjectival parts of speech (pos). An absorber checks for agreement in POS before it can accept the FLOAT-OBJECT as its subparse. As an example, the question, \"(How oily)/do you like your salad dressing (ti)?\" contains a [q-subject] \"how oily\" that is an adjective."
    },
    {
        "gold": {
            "text": [
                "WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #TARGET_REFb ) , a video-recording programming system , a schedule management system ( #REFa ) , and a weather infomiation system ( #REF ) .",
                "The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.",
                "A sample dialogue between this system and a naive user is shown in Figure 2.",
                "This system employs HTK as the speech recognition engine.",
                "The weather information system can answer the user's questions about weather forecasts in Japan.",
                "The vocabulary size is around 500, and the number of phrase structure rules is 31.",
                "The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #TARGET_REFb ) , a video-recording programming system , a schedule management system ( #REFa ) , and a weather infomiation system ( #REF ) . The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue between this system and a naive user is shown in Figure 2. This system employs HTK as the speech recognition engine. The weather information system can answer the user's questions about weather forecasts in Japan. The vocabulary size is around 500, and the number of phrase structure rules is 31. The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000.",
        "output": "experiments:WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #TARGET_REF<COMP/>b ) , a video-recording programming system , a schedule management system ( #REFa ) , and a weather infomiation system ( #REF ) . The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue between this system and a naive user is shown in Figure 2. This system employs HTK as the speech recognition engine. The weather information system can answer the user's questions about weather forecasts in Japan. The vocabulary size is around 500, and the number of phrase structure rules is 31. The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000."
    },
    {
        "gold": {
            "text": [
                "There are several stategies that might be pursued.",
                "One is to adopt Pinkal's \"radical underspecification\" approach (#REF) and use underspecified representations for all types of ambiguity, even syntactic ambiguity.",
                "The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( #TARGET_REF ) , with the resolution process as described here .",
                "Alternatively, I believe it is worth exploring the approach to disambiguation described in #REF, which would mesh nicely with the theory presented here."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:There are several stategies that might be pursued. One is to adopt Pinkal's \"radical underspecification\" approach (#REF) and use underspecified representations for all types of ambiguity, even syntactic ambiguity. The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( #TARGET_REF ) , with the resolution process as described here . Alternatively, I believe it is worth exploring the approach to disambiguation described in #REF, which would mesh nicely with the theory presented here.",
        "output": "nan:There are several stategies that might be pursued. One is to adopt Pinkal's \"radical underspecification\" approach (#REF) and use underspecified representations for all types of ambiguity, even syntactic ambiguity. The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( #TARGET_REF<MOT/> ) , with the resolution process as described here . Alternatively, I believe it is worth exploring the approach to disambiguation described in #REF, which would mesh nicely with the theory presented here."
    },
    {
        "gold": {
            "text": [
                "Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.",
                "Many investigators (e.g.",
                "Many investigators ( e.g. #TARGET_REF ; #REF ; #REF ; #REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .",
                "And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in #REF and O'#REF---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem.",
                "As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.",
                "We believe that one reason for the improvement has to do with the increased pitch range that our system uses.",
                "Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. #TARGET_REF ; #REF ; #REF ; #REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in #REF and O'#REF---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses. Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.",
        "output": "experiments:Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. #TARGET_REF<EXT/> ; #REF ; #REF ; #REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in #REF and O'#REF---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses. Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations."
    },
    {
        "gold": {
            "text": [
                "We use support vector machines to predict the next parser action from a feature vector representing the history.",
                "More specifically , we use LIBSVM ( #TARGET_REF ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification .",
                "Symbolic features are converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components. 4",
                "or some languages, we divide the training data into smaller sets, based on some feature s (normally the CPOS or POS of the next input token), which may reduce training times without a significant loss in accuracy (#REF).",
                "To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:We use support vector machines to predict the next parser action from a feature vector representing the history. More specifically , we use LIBSVM ( #TARGET_REF ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification . Symbolic features are converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components. 4 or some languages, we divide the training data into smaller sets, based on some feature s (normally the CPOS or POS of the next input token), which may reduce training times without a significant loss in accuracy (#REF). To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t.",
        "output": "method:We use support vector machines to predict the next parser action from a feature vector representing the history. More specifically , we use LIBSVM ( #TARGET_REF<FUT/> ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification . Symbolic features are converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components. 4 or some languages, we divide the training data into smaller sets, based on some feature s (normally the CPOS or POS of the next input token), which may reduce training times without a significant loss in accuracy (#REF). To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t."
    },
    {
        "gold": {
            "text": [
                "The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von #REF ) , computing power , improved computer vision models ( #REF ; #REF ; #REF ; #REF ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von #REF ) , computing power , improved computer vision models ( #REF ; #REF ; #REF ; #REF ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #REF ; #TARGET_REF ) .",
        "output": "related work:The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von #REF ) , computing power , improved computer vision models ( #REF ; #REF ; #REF ; #REF ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #REF ; #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "Hockenmaier, Bierner, and #REF outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.",
                "For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.",
                "The first step is to label each node as either a head, complement, or adjunct based on the approaches of #REF and #REF.",
                "Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.",
                "The algorithm handles \"like\" coordination and exploits the traces used in the treebank in order to interpret LDDs.",
                "Unlike our approach , those of #TARGET_REF and Hockenmaier , Bierner , and #REF include a substantial initial correction and clean-up of the Penn-II trees .",
                "Miyao, Ninomiya, and #REF and Nakanishi, Miyao, and #REF describe a methodology for acquiring an English HPSG from the Penn-II Treebank."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Hockenmaier, Bierner, and #REF outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank. For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner. The first step is to label each node as either a head, complement, or adjunct based on the approaches of #REF and #REF. Each node is subsequently assigned the relevant category based on its constituent type and surface configuration. The algorithm handles \"like\" coordination and exploits the traces used in the treebank in order to interpret LDDs. Unlike our approach , those of #TARGET_REF and Hockenmaier , Bierner , and #REF include a substantial initial correction and clean-up of the Penn-II trees . Miyao, Ninomiya, and #REF and Nakanishi, Miyao, and #REF describe a methodology for acquiring an English HPSG from the Penn-II Treebank.",
        "output": "related work:Hockenmaier, Bierner, and #REF outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank. For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner. The first step is to label each node as either a head, complement, or adjunct based on the approaches of #REF and #REF. Each node is subsequently assigned the relevant category based on its constituent type and surface configuration. The algorithm handles \"like\" coordination and exploits the traces used in the treebank in order to interpret LDDs. Unlike our approach , those of #TARGET_REF<USE/> and Hockenmaier , Bierner , and #REF include a substantial initial correction and clean-up of the Penn-II trees . Miyao, Ninomiya, and #REF and Nakanishi, Miyao, and #REF describe a methodology for acquiring an English HPSG from the Penn-II Treebank."
    },
    {
        "gold": {
            "text": [
                "Using the tree-cut technique described above , our previous work ( #TARGET_REF ) extracted systematic polysemy from WordNet .",
                "In this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:Using the tree-cut technique described above , our previous work ( #TARGET_REF ) extracted systematic polysemy from WordNet . In this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method.",
        "output": "experiments:Using the tree-cut technique described above , our previous work ( #TARGET_REF<COMP/> ) extracted systematic polysemy from WordNet . In this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method."
    },
    {
        "gold": {
            "text": [
                "In the longer term, it is clear that neither the contents nor form of any existing published dictionary meet all the requirements of a natural language processing system.",
                "A substantial component of the research reported above has been devoted to restructuring LDOCE to make it more suitable for automatic analysis.",
                "However, even after this process much of the information in LDOCE remains difficult to access, essentially because it is aimed at a human reader, as opposed to a computer system.",
                "This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis.",
                "In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #TARGET_REF ) .",
                "However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:In the longer term, it is clear that neither the contents nor form of any existing published dictionary meet all the requirements of a natural language processing system. A substantial component of the research reported above has been devoted to restructuring LDOCE to make it more suitable for automatic analysis. However, even after this process much of the information in LDOCE remains difficult to access, essentially because it is aimed at a human reader, as opposed to a computer system. This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis. In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #TARGET_REF ) . However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly.",
        "output": "conclusion:In the longer term, it is clear that neither the contents nor form of any existing published dictionary meet all the requirements of a natural language processing system. A substantial component of the research reported above has been devoted to restructuring LDOCE to make it more suitable for automatic analysis. However, even after this process much of the information in LDOCE remains difficult to access, essentially because it is aimed at a human reader, as opposed to a computer system. This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis. In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #TARGET_REF<MOT/> ) . However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly."
    },
    {
        "gold": {
            "text": [
                "The implementation has been inspired by experience in extracting information from very large corpora ( #REF ) and performing experiments on maximum entropy sequence tagging ( #REF ; #TARGET_REF ) .",
                "We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.",
                "These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.",
                "These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (#REF).",
                "We expect even faster training times when we move to conjugate gradient methods."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:The implementation has been inspired by experience in extracting information from very large corpora ( #REF ) and performing experiments on maximum entropy sequence tagging ( #REF ; #TARGET_REF ) . We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (#REF). We expect even faster training times when we move to conjugate gradient methods.",
        "output": "experiments:The implementation has been inspired by experience in extracting information from very large corpora ( #REF ) and performing experiments on maximum entropy sequence tagging ( #REF ; #TARGET_REF<EXT/> ) . We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (#REF). We expect even faster training times when we move to conjugate gradient methods."
    },
    {
        "gold": {
            "text": [
                "We start by reporting the results in which we compare the full parser and the shallow parser on the \"clean\" WSJ data.",
                "Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #TARGET_REF ) terminology .",
                "The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.",
                "Next, we compared the performance of the parsers on the task of identifying atomic phrases 2 .",
                "Here, again, the shallow parser exhibits significantly better performance.",
                "Table 3 shows the results of extracting atomic phrases."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We start by reporting the results in which we compare the full parser and the shallow parser on the \"clean\" WSJ data. Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #TARGET_REF ) terminology . The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser. Next, we compared the performance of the parsers on the task of identifying atomic phrases 2 . Here, again, the shallow parser exhibits significantly better performance. Table 3 shows the results of extracting atomic phrases.",
        "output": "experiments:We start by reporting the results in which we compare the full parser and the shallow parser on the \"clean\" WSJ data. Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #TARGET_REF<FUT/> ) terminology . The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser. Next, we compared the performance of the parsers on the task of identifying atomic phrases 2 . Here, again, the shallow parser exhibits significantly better performance. Table 3 shows the results of extracting atomic phrases."
    },
    {
        "gold": {
            "text": [
                "More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.",
                "Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.",
                "For instance, (#REF) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.",
                "Similarly , ( #REF ) and ( #TARGET_REF ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .",
                "And (#REF) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content. Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. For instance, (#REF) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Similarly , ( #REF ) and ( #TARGET_REF ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source . And (#REF) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.",
        "output": "introduction:More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content. Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. For instance, (#REF) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning. Similarly , ( #REF ) and ( #TARGET_REF<BACK/> ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source . And (#REF) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts."
    },
    {
        "gold": {
            "text": [
                "#REF achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.",
                "Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions.",
                "Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3.",
                "#TARGET_REF evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .",
                "However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument�adjunct distinctions posited by their sys- tem.",
                "The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German.",
                "She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (#REF).",
                "We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:#REF achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each. Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions. Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. #TARGET_REF evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument�adjunct distinctions posited by their sys- tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German. She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (#REF). We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.",
        "output": "nan:#REF achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each. Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions. Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. #TARGET_REF<USE/> evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument�adjunct distinctions posited by their sys- tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German. She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (#REF). We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3."
    },
    {
        "gold": {
            "text": [
                "Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #REF ; #TARGET_REF ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #REF ) to compare it to other shallow parsers .",
                "Table 1 shows that it ranks among the top shallow parsers evaluated there 1 ."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #REF ; #TARGET_REF ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #REF ) to compare it to other shallow parsers . Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .",
        "output": "experiments:Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #REF ; #TARGET_REF<COMP/> ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #REF ) to compare it to other shallow parsers . Table 1 shows that it ranks among the top shallow parsers evaluated there 1 ."
    },
    {
        "gold": {
            "text": [
                "Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.",
                "This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.",
                "Future research should apply the work of #TARGET_REF and #REF , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations. This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension. Future research should apply the work of #TARGET_REF and #REF , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .",
        "output": "conclusion:Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations. This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension. Future research should apply the work of #TARGET_REF<MOT/> and #REF , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars ."
    },
    {
        "gold": {
            "text": [
                "Given a one-toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus.",
                "The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest post-merge log-likelihood, doing so until all classes have been merged.",
                "This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #TARGET_REF ; #REF ; #REF ) .",
                "There are other similar models of distributional clustering of English words which can be similarly effective (#REF)."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:Given a one-toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus. The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest post-merge log-likelihood, doing so until all classes have been merged. This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #TARGET_REF ; #REF ; #REF ) . There are other similar models of distributional clustering of English words which can be similarly effective (#REF).",
        "output": "introduction:Given a one-toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus. The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest post-merge log-likelihood, doing so until all classes have been merged. This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #TARGET_REF<EXT/> ; #REF ; #REF ) . There are other similar models of distributional clustering of English words which can be similarly effective (#REF)."
    },
    {
        "gold": {
            "text": [
                "We train the system by clustering the \"experiences\" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).",
                "We then use the program Snob ( #TARGET_REF ; #REF ) to cluster these experiences .",
                "Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.",
                "shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15",
                "These clusters were chosen because they illustrate clearly three situations of interest."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:We train the system by clustering the \"experiences\" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively). We then use the program Snob ( #TARGET_REF ; #REF ) to cluster these experiences . Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data. shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15 These clusters were chosen because they illustrate clearly three situations of interest.",
        "output": "nan:We train the system by clustering the \"experiences\" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively). We then use the program Snob ( #TARGET_REF<FUT/> ; #REF ) to cluster these experiences . Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data. shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15 These clusters were chosen because they illustrate clearly three situations of interest."
    },
    {
        "gold": {
            "text": [
                "In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).",
                "Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #REF , 2009 ; #REF ; #REF , 2006 ; #REF ; #TARGET_REF ; #REFb ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #REF , 2009 ; #REF ; #REF , 2006 ; #REF ; #TARGET_REF ; #REFb ) .",
        "output": "introduction:In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #REF , 2009 ; #REF ; #REF , 2006 ; #REF ; #TARGET_REF<BACK/> ; #REFb ) ."
    },
    {
        "gold": {
            "text": [
                "Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (#REF;#REF;#REF;#REF).",
                "Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.",
                "In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.",
                "Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.",
                "We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.",
                "The results , which partly confirm those obtained on a smaller dataset in #TARGET_REF , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions .",
                "The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (#REF;#REF;#REF;#REF). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features. Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information. We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions. The results , which partly confirm those obtained on a smaller dataset in #TARGET_REF , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions . The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.",
        "output": "introduction:Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (#REF;#REF;#REF;#REF). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features. Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information. We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions. The results , which partly confirm those obtained on a smaller dataset in #TARGET_REF<USE/> , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions . The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category."
    },
    {
        "gold": {
            "text": [
                "How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #TARGET_REF ) .",
                "In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.",
                "Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue.",
                "Robots have much lower perceptual capabilities of the environment than humans.",
                "How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to?"
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #TARGET_REF ) . In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment. Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue. Robots have much lower perceptual capabilities of the environment than humans. How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to?",
        "output": "introduction:How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #TARGET_REF<COMP/> ) . In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment. Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue. Robots have much lower perceptual capabilities of the environment than humans. How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to?"
    },
    {
        "gold": {
            "text": [
                "Whereas previous finite-state-model-based toolkits place many severe restrictions on domain descriptions, WIT has enough descriptive power to build a variety of dialogue systems.",
                "Although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information.",
                "For example, it is possible to represent a discourse stack whose depth is limited.",
                "Recording some dialogue history is also possible.",
                "Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.",
                "For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #TARGET_REF ; #REF ) .",
                "The language generation module features Common Lisp functions, so there is no limitation on the description.",
                "Some of the systems we have developed feature a generation method based on hierarchical planning (#REF).",
                "It is also possible to build a simple finite-state-model-based dialogue system using WIT.",
                "States can be represented by dialogue phases in WIT."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Whereas previous finite-state-model-based toolkits place many severe restrictions on domain descriptions, WIT has enough descriptive power to build a variety of dialogue systems. Although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information. For example, it is possible to represent a discourse stack whose depth is limited. Recording some dialogue history is also possible. Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #TARGET_REF ; #REF ) . The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (#REF). It is also possible to build a simple finite-state-model-based dialogue system using WIT. States can be represented by dialogue phases in WIT.",
        "output": "conclusion:Whereas previous finite-state-model-based toolkits place many severe restrictions on domain descriptions, WIT has enough descriptive power to build a variety of dialogue systems. Although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information. For example, it is possible to represent a discourse stack whose depth is limited. Recording some dialogue history is also possible. Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #TARGET_REF<MOT/> ; #REF ) . The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (#REF). It is also possible to build a simple finite-state-model-based dialogue system using WIT. States can be represented by dialogue phases in WIT."
    },
    {
        "gold": {
            "text": [
                "In this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization.",
                "The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #TARGET_REF ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:In this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization. The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #TARGET_REF ) .",
        "output": "introduction:In this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization. The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #TARGET_REF<EXT/> ) ."
    },
    {
        "gold": {
            "text": [
                "We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #TARGET_REF ) .",
                "(#REF).",
                "Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #TARGET_REF ) . (#REF). Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets.",
        "output": "experiments:We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #TARGET_REF<FUT/> ) . (#REF). Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets."
    },
    {
        "gold": {
            "text": [
                "When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.",
                "For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e.",
                "literary criticism -unlike in the previous times (#REF).",
                "Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (#REF).",
                "For example , a ` web page ' is more similar to an infinite canvas than a written page ( #TARGET_REF ) .",
                "Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.",
                "From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962).",
                "From a more pessimistic one, an author may feel to have lost power in this openness.",
                "Henceforth the collaborative traits of blogs and wikis (#REF) emphasize annotation, comment, and strong editing.",
                "They give more power to readers, eventually filling the gap -the so-called active readers become authors as well."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose. For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e. literary criticism -unlike in the previous times (#REF). Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (#REF). For example , a ` web page ' is more similar to an infinite canvas than a written page ( #TARGET_REF ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis (#REF) emphasize annotation, comment, and strong editing. They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.",
        "output": "introduction:When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose. For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e. literary criticism -unlike in the previous times (#REF). Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (#REF). For example , a ` web page ' is more similar to an infinite canvas than a written page ( #TARGET_REF<BACK/> ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis (#REF) emphasize annotation, comment, and strong editing. They give more power to readers, eventually filling the gap -the so-called active readers become authors as well."
    },
    {
        "gold": {
            "text": [
                "1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.",
                "This step significantly reduces the computational burden of the algorithm.",
                "It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #TARGET_REF ; #REF ; #REF ) .",
                "To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #TARGET_REF ; #REF ; #REF ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.",
        "output": "method:1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #TARGET_REF<USE/> ; #REF ; #REF ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly."
    }
]