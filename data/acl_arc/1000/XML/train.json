[
    {
        "gold": {
            "text": [
                "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) .",
                "However, most existing systems use pre-authored tutor responses for addressing student errors.",
                "The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.",
                "The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.",
                "It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state. It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.",
        "output": "introduction:Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #TARGET_REF<BACK/> ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state. It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations."
    },
    {
        "gold": {
            "text": [
                "â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #REF ; #TARGET_REF ) .",
        "output": "method:â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #REF ; #TARGET_REF<USE/> ) ."
    },
    {
        "gold": {
            "text": [
                "Frame-semantic features.",
                "While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence.",
                "Following our previous work on stance classification ( #TARGET_REFc ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( #REF ) .",
                "Frame-word interaction features encode whether two words appear in different elements of the same frame.",
                "Hence, each frame-word interaction feature consists of (1) the name of the frame f from which it is created, and"
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:Frame-semantic features. While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence. Following our previous work on stance classification ( #TARGET_REFc ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( #REF ) . Frame-word interaction features encode whether two words appear in different elements of the same frame. Hence, each frame-word interaction feature consists of (1) the name of the frame f from which it is created, and",
        "output": "experiments:Frame-semantic features. While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence. Following our previous work on stance classification ( #TARGET_REF<COMP/>c ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( #REF ) . Frame-word interaction features encode whether two words appear in different elements of the same frame. Hence, each frame-word interaction feature consists of (1) the name of the frame f from which it is created, and"
    },
    {
        "gold": {
            "text": [
                "As ( #TARGET_REF ) show , lexical information improves on NP and VP chunking as well ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:As ( #TARGET_REF ) show , lexical information improves on NP and VP chunking as well .",
        "output": "conclusion:As ( #TARGET_REF<MOT/> ) show , lexical information improves on NP and VP chunking as well ."
    },
    {
        "gold": {
            "text": [
                "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.",
                "They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #TARGET_REF ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) .",
                "One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF).",
                "With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.",
                "After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #TARGET_REF ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases.",
        "output": "nan:Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #TARGET_REF<EXT/> ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases."
    },
    {
        "gold": {
            "text": [
                "Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task (e.g., #REF).",
                "Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).",
                "Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #TARGET_REF ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.",
                "This provides us with a training set that is approximately five times bigger than that of ACE.",
                "More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #REF); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task (e.g., #REF). Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #TARGET_REF ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs. This provides us with a training set that is approximately five times bigger than that of ACE. More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #REF); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.",
        "output": "introduction:Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task (e.g., #REF). Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #TARGET_REF<FUT/> ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs. This provides us with a training set that is approximately five times bigger than that of ACE. More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #REF); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms."
    },
    {
        "gold": {
            "text": [
                "The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields --head word, pronunciation, grammar codes, definitions, examples, and so forth.",
                "However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #TARGET_REF for further discussion ) .",
                "For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information.",
                "For example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words (themselves defined elsewhere in terms of this vocabulary) are used.",
                "These words always appear in small capitals and can therefore be recognised because they will be preceded by a font change control character.",
                "In Figure 1 above the definition of rivet as verb includes the noun definition of \"RIVET 1'', as signalled by the font change and the numerical superscript which indicates that it is the first (i.e.",
                "noun entry) homograph; additional notation exists for word senses within homographs.",
                "On the typesetting tape, font control characters are indicated by hexadecimal numbers within curly brackets."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields --head word, pronunciation, grammar codes, definitions, examples, and so forth. However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #TARGET_REF for further discussion ) . For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information. For example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words (themselves defined elsewhere in terms of this vocabulary) are used. These words always appear in small capitals and can therefore be recognised because they will be preceded by a font change control character. In Figure 1 above the definition of rivet as verb includes the noun definition of \"RIVET 1'', as signalled by the font change and the numerical superscript which indicates that it is the first (i.e. noun entry) homograph; additional notation exists for word senses within homographs. On the typesetting tape, font control characters are indicated by hexadecimal numbers within curly brackets.",
        "output": "nan:The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields --head word, pronunciation, grammar codes, definitions, examples, and so forth. However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #TARGET_REF<BACK/> for further discussion ) . For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information. For example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words (themselves defined elsewhere in terms of this vocabulary) are used. These words always appear in small capitals and can therefore be recognised because they will be preceded by a font change control character. In Figure 1 above the definition of rivet as verb includes the noun definition of \"RIVET 1'', as signalled by the font change and the numerical superscript which indicates that it is the first (i.e. noun entry) homograph; additional notation exists for word senses within homographs. On the typesetting tape, font control characters are indicated by hexadecimal numbers within curly brackets."
    },
    {
        "gold": {
            "text": [
                "In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries.",
                "These systems addressed the evaluation issue as follows.",
                "â¢ Only an automatic evaluation was performed , which relied on having model responses ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows. â¢ Only an automatic evaluation was performed , which relied on having model responses ( #TARGET_REF ; #REF ) .",
        "output": "method:In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries. These systems addressed the evaluation issue as follows. â¢ Only an automatic evaluation was performed , which relied on having model responses ( #TARGET_REF<USE/> ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "We have shown elsewhere ( #TARGET_REF ; #REFa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .",
                "This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:We have shown elsewhere ( #TARGET_REF ; #REFa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment . This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.",
        "output": "introduction:We have shown elsewhere ( #TARGET_REF<COMP/> ; #REFa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment . This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory."
    },
    {
        "gold": {
            "text": [
                "Whereas previous finite-state-model-based toolkits place many severe restrictions on domain descriptions, WIT has enough descriptive power to build a variety of dialogue systems.",
                "Although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information.",
                "For example, it is possible to represent a discourse stack whose depth is limited.",
                "Recording some dialogue history is also possible.",
                "Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.",
                "For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #TARGET_REF ; #REF ) .",
                "The language generation module features Common Lisp functions, so there is no limitation on the description.",
                "Some of the systems we have developed feature a generation method based on hierarchical planning (#REF).",
                "It is also possible to build a simple finite-state-model-based dialogue system using WIT.",
                "States can be represented by dialogue phases in WIT."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Whereas previous finite-state-model-based toolkits place many severe restrictions on domain descriptions, WIT has enough descriptive power to build a variety of dialogue systems. Although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information. For example, it is possible to represent a discourse stack whose depth is limited. Recording some dialogue history is also possible. Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #TARGET_REF ; #REF ) . The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (#REF). It is also possible to build a simple finite-state-model-based dialogue system using WIT. States can be represented by dialogue phases in WIT.",
        "output": "conclusion:Whereas previous finite-state-model-based toolkits place many severe restrictions on domain descriptions, WIT has enough descriptive power to build a variety of dialogue systems. Although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information. For example, it is possible to represent a discourse stack whose depth is limited. Recording some dialogue history is also possible. Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #TARGET_REF<MOT/> ; #REF ) . The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (#REF). It is also possible to build a simple finite-state-model-based dialogue system using WIT. States can be represented by dialogue phases in WIT."
    },
    {
        "gold": {
            "text": [
                "P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.",
                "Inspired by ( #TARGET_REF ) and ( #REF ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "method:P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables. Inspired by ( #TARGET_REF ) and ( #REF ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .",
        "output": "method:P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables. Inspired by ( #TARGET_REF<EXT/> ) and ( #REF ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string ."
    },
    {
        "gold": {
            "text": [
                "The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms.",
                "To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( #TARGET_REF and references therein ) .",
                "PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we were constructing entries for a system based on GPSG, FUG or LFG.",
                "We The latter employs a grammatical formalism based on GPSG; the comparatively theory neutral lexical entries that we construct from LDOCE should translate straightforwardly into this framework as well."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms. To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( #TARGET_REF and references therein ) . PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we were constructing entries for a system based on GPSG, FUG or LFG. We The latter employs a grammatical formalism based on GPSG; the comparatively theory neutral lexical entries that we construct from LDOCE should translate straightforwardly into this framework as well.",
        "output": "nan:The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms. To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( #TARGET_REF<FUT/> and references therein ) . PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we were constructing entries for a system based on GPSG, FUG or LFG. We The latter employs a grammatical formalism based on GPSG; the comparatively theory neutral lexical entries that we construct from LDOCE should translate straightforwardly into this framework as well."
    },
    {
        "gold": {
            "text": [
                "Now for some important remarks on efficiency : â¢ Computing ti is an instance of the well-known algebraic path problem ( #TARGET_REF ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted).",
                "It is wasteful to compute ti as suggested earlier, by minimizing (__xi)_f_(yi__), since then the real work is done by an _-closure step (#REF) that implements the all-pairs version of algebraic path, whereas all we need is the single-source version.",
                "If n and m are the number of states and edges,19 then both problems are O(n3) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (#REFb).",
                "For a general graph Ti, Tarjan (1981b) shows how to partition into �hard� subgraphs that localize the cyclicity or irreducibility, then run the O(n3) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.",
                "The overhead of partitioning and recombining is essentially only O(m)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Now for some important remarks on efficiency : â¢ Computing ti is an instance of the well-known algebraic path problem ( #TARGET_REF ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted). It is wasteful to compute ti as suggested earlier, by minimizing (__xi)_f_(yi__), since then the real work is done by an _-closure step (#REF) that implements the all-pairs version of algebraic path, whereas all we need is the single-source version. If n and m are the number of states and edges,19 then both problems are O(n3) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (#REFb). For a general graph Ti, Tarjan (1981b) shows how to partition into �hard� subgraphs that localize the cyclicity or irreducibility, then run the O(n3) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results. The overhead of partitioning and recombining is essentially only O(m).",
        "output": "nan:Now for some important remarks on efficiency : â¢ Computing ti is an instance of the well-known algebraic path problem ( #TARGET_REF<BACK/> ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted). It is wasteful to compute ti as suggested earlier, by minimizing (__xi)_f_(yi__), since then the real work is done by an _-closure step (#REF) that implements the all-pairs version of algebraic path, whereas all we need is the single-source version. If n and m are the number of states and edges,19 then both problems are O(n3) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (#REFb). For a general graph Ti, Tarjan (1981b) shows how to partition into �hard� subgraphs that localize the cyclicity or irreducibility, then run the O(n3) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results. The overhead of partitioning and recombining is essentially only O(m)."
    },
    {
        "gold": {
            "text": [
                "A second method is to structure the translated query, separating the translations for one term from translations for other terms.",
                "This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.",
                "There are several variations of such a method ( #REF ; #REF ; #TARGET_REF ) .",
                "One such method is to treat different translations of the same term as synonyms.",
                "Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.",
                "However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.",
                "By contrast, our HMM approach supports translation probabilities.",
                "The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function.",
                "Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:A second method is to structure the translated query, separating the translations for one term from translations for other terms. This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. There are several variations of such a method ( #REF ; #REF ; #TARGET_REF ) . One such method is to treat different translations of the same term as synonyms. Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms. However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other. By contrast, our HMM approach supports translation probabilities. The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function. Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations.",
        "output": "method:A second method is to structure the translated query, separating the translations for one term from translations for other terms. This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. There are several variations of such a method ( #REF ; #REF ; #TARGET_REF<USE/> ) . One such method is to treat different translations of the same term as synonyms. Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms. However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other. By contrast, our HMM approach supports translation probabilities. The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function. Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations."
    },
    {
        "gold": {
            "text": [
                "The features can be easily obtained by modifying the TAT extraction algorithm described in ( #TARGET_REF ) .",
                "When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees.",
                "Then we use the toolkit implemented by #REF to train MERS models for the ambiguous source syntactic trees separately.",
                "We set the iteration number to 100 and Gaussian prior to 1."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:The features can be easily obtained by modifying the TAT extraction algorithm described in ( #TARGET_REF ) . When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees. Then we use the toolkit implemented by #REF to train MERS models for the ambiguous source syntactic trees separately. We set the iteration number to 100 and Gaussian prior to 1.",
        "output": "experiments:The features can be easily obtained by modifying the TAT extraction algorithm described in ( #TARGET_REF<COMP/> ) . When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees. Then we use the toolkit implemented by #REF to train MERS models for the ambiguous source syntactic trees separately. We set the iteration number to 100 and Gaussian prior to 1."
    },
    {
        "gold": {
            "text": [
                "However, synonym mapping is still incomplete in the current state of our subword dictionary.",
                "A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.",
                "We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5).",
                "It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.",
                "This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #TARGET_REF ) ) are incorporated into our system .",
                "Alternatively, we may think of user-centered comparative studies (#REF)."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "experiments:However, synonym mapping is still incomplete in the current state of our subword dictionary. A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing. We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5). It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents. This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #TARGET_REF ) ) are incorporated into our system . Alternatively, we may think of user-centered comparative studies (#REF).",
        "output": "experiments:However, synonym mapping is still incomplete in the current state of our subword dictionary. A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing. We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5). It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents. This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #TARGET_REF<MOT/> ) ) are incorporated into our system . Alternatively, we may think of user-centered comparative studies (#REF)."
    },
    {
        "gold": {
            "text": [
                "We hypothesize that an underlying model that could capture syntagmatic patterns in large word contexts, yet is flexible enough to deal with data sparseness, is desired.",
                "It is generally accepted that the semantics of verbs in particular are correlated with their syntagmatic properties (#REF;#REF).",
                "This provides grounds to expect that such model has the potential to excel for verbs.",
                "To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme.",
                "This choice is inspired by recent work on learning syntactic categories ( #TARGET_REF ) , which successfully utilized such language models to represent word window contexts of target words .",
                "However, we note that other richer types of language models, such as class-based (#REF) or hybrid (#REF), can be seamlessly integrated into our scheme."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:We hypothesize that an underlying model that could capture syntagmatic patterns in large word contexts, yet is flexible enough to deal with data sparseness, is desired. It is generally accepted that the semantics of verbs in particular are correlated with their syntagmatic properties (#REF;#REF). This provides grounds to expect that such model has the potential to excel for verbs. To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme. This choice is inspired by recent work on learning syntactic categories ( #TARGET_REF ) , which successfully utilized such language models to represent word window contexts of target words . However, we note that other richer types of language models, such as class-based (#REF) or hybrid (#REF), can be seamlessly integrated into our scheme.",
        "output": "introduction:We hypothesize that an underlying model that could capture syntagmatic patterns in large word contexts, yet is flexible enough to deal with data sparseness, is desired. It is generally accepted that the semantics of verbs in particular are correlated with their syntagmatic properties (#REF;#REF). This provides grounds to expect that such model has the potential to excel for verbs. To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme. This choice is inspired by recent work on learning syntactic categories ( #TARGET_REF<EXT/> ) , which successfully utilized such language models to represent word window contexts of target words . However, we note that other richer types of language models, such as class-based (#REF) or hybrid (#REF), can be seamlessly integrated into our scheme."
    },
    {
        "gold": {
            "text": [
                "In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #TARGET_REF ) .",
                "As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values.",
                "The Easy-First Parser is a shift-reduce parser (as is MaltParser).",
                "Unlike MaltParser, however, it does not attempt to attach arcs \"eagerly\" as early as possible (as in previous sections), or at the latest possible stage (an option we abandoned early on in preliminary experiments).",
                "Instead, the Easy-First Parser keeps a stack of partially built treelets, and attaches them to one another in order of confidence (from high confidence, \"easy\" attachment, to low, as estimated by the classifier).",
                "Labeling the relation arcs is done in a second pass, with a separate training step, after all attachments have been decided (the code for which was added after the publication of #REF, which only included an unlabeled attachment version)."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #TARGET_REF ) . As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values. The Easy-First Parser is a shift-reduce parser (as is MaltParser). Unlike MaltParser, however, it does not attempt to attach arcs \"eagerly\" as early as possible (as in previous sections), or at the latest possible stage (an option we abandoned early on in preliminary experiments). Instead, the Easy-First Parser keeps a stack of partially built treelets, and attaches them to one another in order of confidence (from high confidence, \"easy\" attachment, to low, as estimated by the classifier). Labeling the relation arcs is done in a second pass, with a separate training step, after all attachments have been decided (the code for which was added after the publication of #REF, which only included an unlabeled attachment version).",
        "output": "experiments:In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #TARGET_REF<FUT/> ) . As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values. The Easy-First Parser is a shift-reduce parser (as is MaltParser). Unlike MaltParser, however, it does not attempt to attach arcs \"eagerly\" as early as possible (as in previous sections), or at the latest possible stage (an option we abandoned early on in preliminary experiments). Instead, the Easy-First Parser keeps a stack of partially built treelets, and attaches them to one another in order of confidence (from high confidence, \"easy\" attachment, to low, as estimated by the classifier). Labeling the relation arcs is done in a second pass, with a separate training step, after all attachments have been decided (the code for which was added after the publication of #REF, which only included an unlabeled attachment version)."
    },
    {
        "gold": {
            "text": [
                "The Computer Vision community has also benefited greatly from efforts to unify the two modalities.",
                "To name a few examples , #TARGET_REF and #REF show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #REF show that verb clusters can be used to improve activity recognition in videos ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples , #TARGET_REF and #REF show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #REF show that verb clusters can be used to improve activity recognition in videos .",
        "output": "related work:The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples , #TARGET_REF<BACK/> and #REF show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #REF show that verb clusters can be used to improve activity recognition in videos ."
    },
    {
        "gold": {
            "text": [
                "Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #REF ; #TARGET_REF ) .",
                "Our task is closer to the work of #REF, who looked at the problem of intellectual attribution in scientific texts."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #REF ; #TARGET_REF ) . Our task is closer to the work of #REF, who looked at the problem of intellectual attribution in scientific texts.",
        "output": "related work:Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #REF ; #TARGET_REF<USE/> ) . Our task is closer to the work of #REF, who looked at the problem of intellectual attribution in scientific texts."
    },
    {
        "gold": {
            "text": [
                "Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #TARGET_REF ; #REF ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #REF ) to compare it to other shallow parsers .",
                "Table 1 shows that it ranks among the top shallow parsers evaluated there 1 ."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #TARGET_REF ; #REF ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #REF ) to compare it to other shallow parsers . Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .",
        "output": "experiments:Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #TARGET_REF<COMP/> ; #REF ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #REF ) to compare it to other shallow parsers . Table 1 shows that it ranks among the top shallow parsers evaluated there 1 ."
    },
    {
        "gold": {
            "text": [
                "While the combined MBL method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement.",
                "Future work will pursue at least two directions for improving the results.",
                "First, while semantic information is not available for all adjectives, it is clearly available for some.",
                "Furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available.",
                "More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #TARGET_REF ) could be applied to extract semantic classes from the corpus itself .",
                "Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:While the combined MBL method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement. Future work will pursue at least two directions for improving the results. First, while semantic information is not available for all adjectives, it is clearly available for some. Furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available. More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #TARGET_REF ) could be applied to extract semantic classes from the corpus itself . Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results.",
        "output": "conclusion:While the combined MBL method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement. Future work will pursue at least two directions for improving the results. First, while semantic information is not available for all adjectives, it is clearly available for some. Furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available. More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #TARGET_REF<MOT/> ) could be applied to extract semantic classes from the corpus itself . Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results."
    },
    {
        "gold": {
            "text": [
                "Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.",
                "Many investigators (e.g.",
                "Many investigators ( e.g. #REF ; #REF ; #REF ; #TARGET_REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .",
                "And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in #REF and O'#REF---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem.",
                "As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.",
                "We believe that one reason for the improvement has to do with the increased pitch range that our system uses.",
                "Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. #REF ; #REF ; #REF ; #TARGET_REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in #REF and O'#REF---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses. Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.",
        "output": "experiments:Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. #REF ; #REF ; #REF ; #TARGET_REF<EXT/> ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in #REF and O'#REF---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses. Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations."
    },
    {
        "gold": {
            "text": [
                "GermaNet contains only a few conceptual glosses.",
                "As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #TARGET_REF ) .",
                "We removed words which had more than three senses."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:GermaNet contains only a few conceptual glosses. As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #TARGET_REF ) . We removed words which had more than three senses.",
        "output": "experiments:GermaNet contains only a few conceptual glosses. As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #TARGET_REF<FUT/> ) . We removed words which had more than three senses."
    },
    {
        "gold": {
            "text": [
                "From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #TARGET_REF ) .",
                "The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #TARGET_REF ) . The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community.",
        "output": "nan:From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #TARGET_REF<BACK/> ) . The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community."
    },
    {
        "gold": {
            "text": [
                "#REF pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names.",
                "They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.",
                "The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.",
                "It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and #REF).",
                "Gale, Church, and #REF showed that words strongly tend to exhibit only one sense in a document or discourse (\"one sense per discourse\").",
                "Since then this idea has been applied to several tasks, including word sense disambiguation (#REF) and named-entity recognition (#REF).",
                "Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations.",
                "In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).",
                "This is similar to \"one sense per collocation\" idea of #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:#REF pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names. They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists. It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and #REF). Gale, Church, and #REF showed that words strongly tend to exhibit only one sense in a document or discourse (\"one sense per discourse\"). Since then this idea has been applied to several tasks, including word sense disambiguation (#REF) and named-entity recognition (#REF). Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations. In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams). This is similar to \"one sense per collocation\" idea of #TARGET_REF .",
        "output": "nan:#REF pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names. They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists. It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and #REF). Gale, Church, and #REF showed that words strongly tend to exhibit only one sense in a document or discourse (\"one sense per discourse\"). Since then this idea has been applied to several tasks, including word sense disambiguation (#REF) and named-entity recognition (#REF). Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations. In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams). This is similar to \"one sense per collocation\" idea of #TARGET_REF<USE/> ."
    },
    {
        "gold": {
            "text": [
                "The first of these characteristics makes a dependency tree structure a very useful representation for MT and multilingual NLG, since it gives linguists a representation that allows them to abstract over numerous crosslinguistic divergences due to language specific ordering (Polgu~re, 1991 In the implemented applications, the DSyntSs are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic-based MT (#REF).",
                "Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( #REF ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( #TARGET_REF ) ."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:The first of these characteristics makes a dependency tree structure a very useful representation for MT and multilingual NLG, since it gives linguists a representation that allows them to abstract over numerous crosslinguistic divergences due to language specific ordering (Polgu~re, 1991 In the implemented applications, the DSyntSs are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic-based MT (#REF). Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( #REF ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( #TARGET_REF ) .",
        "output": "nan:The first of these characteristics makes a dependency tree structure a very useful representation for MT and multilingual NLG, since it gives linguists a representation that allows them to abstract over numerous crosslinguistic divergences due to language specific ordering (Polgu~re, 1991 In the implemented applications, the DSyntSs are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic-based MT (#REF). Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( #REF ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( #TARGET_REF<COMP/> ) ."
    },
    {
        "gold": {
            "text": [
                "Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.",
                "For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #TARGET_REF ; #REF ) and history-based parsing ( #REF ) .",
                "We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF)."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #TARGET_REF ; #REF ) and history-based parsing ( #REF ) . We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF).",
        "output": "conclusion:Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #TARGET_REF<MOT/> ; #REF ) and history-based parsing ( #REF ) . We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF)."
    },
    {
        "gold": {
            "text": [
                "Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) .",
                "Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) . Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.",
        "output": "introduction:Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #REF ; #TARGET_REF<EXT/> ; #REF ; #REF ) . Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation."
    },
    {
        "gold": {
            "text": [
                "The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4.",
                "These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #TARGET_REF ) .",
                "We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .",
                "The best results on most of our data were obtained using Hidden Naive Bayes (HNB) (#REF).",
                "Therefore, here we show the results of this classifier.",
                "Ten-folds crossvalidation was applied throughout."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4. These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #TARGET_REF ) . We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table . The best results on most of our data were obtained using Hidden Naive Bayes (HNB) (#REF). Therefore, here we show the results of this classifier. Ten-folds crossvalidation was applied throughout.",
        "output": "nan:The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4. These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #TARGET_REF<FUT/> ) . We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table . The best results on most of our data were obtained using Hidden Naive Bayes (HNB) (#REF). Therefore, here we show the results of this classifier. Ten-folds crossvalidation was applied throughout."
    },
    {
        "gold": {
            "text": [
                "Semantic Role labeling ( SRL ) was first defined in #TARGET_REF .",
                "The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.",
                "The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.",
                "Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.",
                "Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (#REF), Information Extraction (#REF), and Machine Translation (#REF).",
                "With the efforts of many researchers (Carreras and Màrquez 2004, #REF, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Semantic Role labeling ( SRL ) was first defined in #TARGET_REF . The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (#REF), Information Extraction (#REF), and Machine Translation (#REF). With the efforts of many researchers (Carreras and Màrquez 2004, #REF, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.",
        "output": "introduction:Semantic Role labeling ( SRL ) was first defined in #TARGET_REF<BACK/> . The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (#REF), Information Extraction (#REF), and Machine Translation (#REF). With the efforts of many researchers (Carreras and Màrquez 2004, #REF, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast."
    },
    {
        "gold": {
            "text": [
                "Table (b) again reproduces the results from #TARGET_REF (2003) for a comparable task on a different subset of 206 unstructured abstracts."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Table (b) again reproduces the results from #TARGET_REF (2003) for a comparable task on a different subset of 206 unstructured abstracts.",
        "output": "introduction:Table (b) again reproduces the results from #TARGET_REF<USE/> (2003) for a comparable task on a different subset of 206 unstructured abstracts."
    },
    {
        "gold": {
            "text": [
                "1. Char-n-grams (G): We start with a character n-gram-based approach (#REF), which is most common and followed by many language identification researchers.",
                "Following the work of #REF, we select character n-grams (n=1 to 5) and the word as the features in our experiments.",
                "2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments.",
                "3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #TARGET_REF ; #REF ) and create multiple features for length using a decision tree ( J48 ) .",
                "We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.",
                "4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:1. Char-n-grams (G): We start with a character n-gram-based approach (#REF), which is most common and followed by many language identification researchers. Following the work of #REF, we select character n-grams (n=1 to 5) and the word as the features in our experiments. 2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #TARGET_REF ; #REF ) and create multiple features for length using a decision tree ( J48 ) . We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.",
        "output": "experiments:1. Char-n-grams (G): We start with a character n-gram-based approach (#REF), which is most common and followed by many language identification researchers. Following the work of #REF, we select character n-grams (n=1 to 5) and the word as the features in our experiments. 2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #TARGET_REF<COMP/> ; #REF ) and create multiple features for length using a decision tree ( J48 ) . We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized."
    },
    {
        "gold": {
            "text": [
                "Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.",
                "For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (#REF;#REF) and history-based parsing (#REF).",
                "We could also introduce new variables , e.g. , nonterminal refinements ( #TARGET_REF ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( #REF ; #REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (#REF;#REF) and history-based parsing (#REF). We could also introduce new variables , e.g. , nonterminal refinements ( #TARGET_REF ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( #REF ; #REF ) .",
        "output": "conclusion:Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (#REF;#REF) and history-based parsing (#REF). We could also introduce new variables , e.g. , nonterminal refinements ( #TARGET_REF<MOT/> ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( #REF ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "Again, this represents our attempt to coarsely model subcategorization.",
                "(4) NE: We use BBN's IdentiFinder (#REF)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value.",
                "These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #TARGET_REF ) .",
                "Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:Again, this represents our attempt to coarsely model subcategorization. (4) NE: We use BBN's IdentiFinder (#REF)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #TARGET_REF ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.",
        "output": "introduction:Again, this represents our attempt to coarsely model subcategorization. (4) NE: We use BBN's IdentiFinder (#REF)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #TARGET_REF<EXT/> ) . Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations."
    },
    {
        "gold": {
            "text": [
                "The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( #TARGET_REF ) .",
                "As will be described herein, the population, problem, and the intervention extractors are based largely on recognition of semantic types and a few manually constructed rules; the outcome extrac-tor, in contrast, is implemented as an ensemble of classifiers trained using supervised machine learning techniques ).",
                "These two very different approaches can be attributed to differences in the nature of the frame elements: Whereas problems and interventions can be directly mapped to UMLS concepts, and populations easily mapped to patterns that include UMLS concepts, outcome statements follow no predictable pattern.",
                "The initial goal of the annotation effort was to identify outcome statements in abstract text.",
                "A physician, two registered nurses, and an engineering researcher manually identified sentences that describe outcomes in 633 MEDLINE abstracts; a post hoc analysis demonstrates good agreement (κ = 0.77).",
                "The annotated abstracts were retrieved using PubMed and attempted to model different user behaviors ranging from naive to expert (where advanced search features were employed)."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( #TARGET_REF ) . As will be described herein, the population, problem, and the intervention extractors are based largely on recognition of semantic types and a few manually constructed rules; the outcome extrac-tor, in contrast, is implemented as an ensemble of classifiers trained using supervised machine learning techniques ). These two very different approaches can be attributed to differences in the nature of the frame elements: Whereas problems and interventions can be directly mapped to UMLS concepts, and populations easily mapped to patterns that include UMLS concepts, outcome statements follow no predictable pattern. The initial goal of the annotation effort was to identify outcome statements in abstract text. A physician, two registered nurses, and an engineering researcher manually identified sentences that describe outcomes in 633 MEDLINE abstracts; a post hoc analysis demonstrates good agreement (κ = 0.77). The annotated abstracts were retrieved using PubMed and attempted to model different user behaviors ranging from naive to expert (where advanced search features were employed).",
        "output": "nan:The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( #TARGET_REF<FUT/> ) . As will be described herein, the population, problem, and the intervention extractors are based largely on recognition of semantic types and a few manually constructed rules; the outcome extrac-tor, in contrast, is implemented as an ensemble of classifiers trained using supervised machine learning techniques ). These two very different approaches can be attributed to differences in the nature of the frame elements: Whereas problems and interventions can be directly mapped to UMLS concepts, and populations easily mapped to patterns that include UMLS concepts, outcome statements follow no predictable pattern. The initial goal of the annotation effort was to identify outcome statements in abstract text. A physician, two registered nurses, and an engineering researcher manually identified sentences that describe outcomes in 633 MEDLINE abstracts; a post hoc analysis demonstrates good agreement (κ = 0.77). The annotated abstracts were retrieved using PubMed and attempted to model different user behaviors ranging from naive to expert (where advanced search features were employed)."
    },
    {
        "gold": {
            "text": [
                "The syntactic functions COMP and XCOMP refer to clausal complements with different predicate control patterns as described in Section 2. However, as it stands, neither of these functions betrays anything about the syntactic nature of the constructs in question.",
                "Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #TARGET_REF ) .",
                "With only a slight modification, our system, along with the details provided by the automatically generated f-structures, allows us to extract frames with an equivalent level of detail.",
                "For example, to identify a that-clause, we use"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "method:The syntactic functions COMP and XCOMP refer to clausal complements with different predicate control patterns as described in Section 2. However, as it stands, neither of these functions betrays anything about the syntactic nature of the constructs in question. Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #TARGET_REF ) . With only a slight modification, our system, along with the details provided by the automatically generated f-structures, allows us to extract frames with an equivalent level of detail. For example, to identify a that-clause, we use",
        "output": "method:The syntactic functions COMP and XCOMP refer to clausal complements with different predicate control patterns as described in Section 2. However, as it stands, neither of these functions betrays anything about the syntactic nature of the constructs in question. Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #TARGET_REF<BACK/> ) . With only a slight modification, our system, along with the details provided by the automatically generated f-structures, allows us to extract frames with an equivalent level of detail. For example, to identify a that-clause, we use"
    },
    {
        "gold": {
            "text": [
                "Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #REF , #TARGET_REF and others .",
                "Toutanova et.",
                "al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information.",
                "Using additional source side information beyond the markup did not produce a gain in performance."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #REF , #TARGET_REF and others . Toutanova et. al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information. Using additional source side information beyond the markup did not produce a gain in performance.",
        "output": "related work:Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #REF , #TARGET_REF<USE/> and others . Toutanova et. al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information. Using additional source side information beyond the markup did not produce a gain in performance."
    },
    {
        "gold": {
            "text": [
                "Baldridge observed is that, cross-linguistically, grammars prefer simpler syntactic structures when possible, and that due to the natural correspondence of categories and syntactic structure, biasing toward simpler categories encourages simpler structures.",
                "In previous work, we were able to incorporate this preference into a Bayesian parsing model, biasing PCFG productions toward sim-pler categories by encoding a notion of category simplicity into a prior (#REF).",
                "Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable.",
                "We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #TARGET_REF ) ."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:Baldridge observed is that, cross-linguistically, grammars prefer simpler syntactic structures when possible, and that due to the natural correspondence of categories and syntactic structure, biasing toward simpler categories encourages simpler structures. In previous work, we were able to incorporate this preference into a Bayesian parsing model, biasing PCFG productions toward sim-pler categories by encoding a notion of category simplicity into a prior (#REF). Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable. We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #TARGET_REF ) .",
        "output": "introduction:Baldridge observed is that, cross-linguistically, grammars prefer simpler syntactic structures when possible, and that due to the natural correspondence of categories and syntactic structure, biasing toward simpler categories encourages simpler structures. In previous work, we were able to incorporate this preference into a Bayesian parsing model, biasing PCFG productions toward sim-pler categories by encoding a notion of category simplicity into a prior (#REF). Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable. We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #TARGET_REF<COMP/> ) ."
    },
    {
        "gold": {
            "text": [
                "â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #TARGET_REF , â¢ a `` conditioning '' facility as described by #REF , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #TARGET_REF , â¢ a `` conditioning '' facility as described by #REF , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns .",
        "output": "conclusion:â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #TARGET_REF<MOT/> , â¢ a `` conditioning '' facility as described by #REF , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns ."
    },
    {
        "gold": {
            "text": [
                "We work with sentences, clauses, phrases and words, using syntactic structures generated by a parser.",
                "Our system incrementally processes a text, and extracts pairs of text units: two clauses, a verb and each of its arguments, a noun and each of its modifiers.",
                "For each pair of units, the system builds a syntactic graph surrounding the main element (main clause, head verb, head noun).",
                "It then tries to find among the previously processed instances another main element with a matching syntactic graph.",
                "If such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph.",
                "We have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases.",
                "The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent ( #TARGET_REFa ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:We work with sentences, clauses, phrases and words, using syntactic structures generated by a parser. Our system incrementally processes a text, and extracts pairs of text units: two clauses, a verb and each of its arguments, a noun and each of its modifiers. For each pair of units, the system builds a syntactic graph surrounding the main element (main clause, head verb, head noun). It then tries to find among the previously processed instances another main element with a matching syntactic graph. If such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph. We have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases. The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent ( #TARGET_REFa ) .",
        "output": "introduction:We work with sentences, clauses, phrases and words, using syntactic structures generated by a parser. Our system incrementally processes a text, and extracts pairs of text units: two clauses, a verb and each of its arguments, a noun and each of its modifiers. For each pair of units, the system builds a syntactic graph surrounding the main element (main clause, head verb, head noun). It then tries to find among the previously processed instances another main element with a matching syntactic graph. If such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph. We have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases. The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent ( #TARGET_REF<EXT/>a ) ."
    },
    {
        "gold": {
            "text": [
                "In a final processing stage , we generalize over the marker lexicon following a process found in #TARGET_REF .",
                "In Block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool.",
                "In a subsequent stage, chunk pairs are extracted, which are then generalized to produce a set of translation templates for each source, target segment."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:In a final processing stage , we generalize over the marker lexicon following a process found in #TARGET_REF . In Block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool. In a subsequent stage, chunk pairs are extracted, which are then generalized to produce a set of translation templates for each source, target segment.",
        "output": "introduction:In a final processing stage , we generalize over the marker lexicon following a process found in #TARGET_REF<FUT/> . In Block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool. In a subsequent stage, chunk pairs are extracted, which are then generalized to produce a set of translation templates for each source, target segment."
    },
    {
        "gold": {
            "text": [
                "The idea of introducing constraints over a model to better guide the learning process has appeared before.",
                "In the context of word alignment , #TARGET_REF use a state-duration HMM in order to model word-to-phrase translations .",
                "The fertility of each source word is implicitly encoded in the durations of the HMM states.",
                "Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant η > 1.",
                "This encourages more transitions and hence shorter phrases.",
                "For the task of unsupervised dependency parsing, #REF add a constraint of the form \"the average length of dependencies should be X\" to capture the locality of syntax (at least half of the dependencies are between adjacent words), using a scheme they call structural annealing.",
                "They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e.",
                "The factor δ changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:The idea of introducing constraints over a model to better guide the learning process has appeared before. In the context of word alignment , #TARGET_REF use a state-duration HMM in order to model word-to-phrase translations . The fertility of each source word is implicitly encoded in the durations of the HMM states. Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant η > 1. This encourages more transitions and hence shorter phrases. For the task of unsupervised dependency parsing, #REF add a constraint of the form \"the average length of dependencies should be X\" to capture the locality of syntax (at least half of the dependencies are between adjacent words), using a scheme they call structural annealing. They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e. The factor δ changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.",
        "output": "related work:The idea of introducing constraints over a model to better guide the learning process has appeared before. In the context of word alignment , #TARGET_REF<BACK/> use a state-duration HMM in order to model word-to-phrase translations . The fertility of each source word is implicitly encoded in the durations of the HMM states. Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant η > 1. This encourages more transitions and hence shorter phrases. For the task of unsupervised dependency parsing, #REF add a constraint of the form \"the average length of dependencies should be X\" to capture the locality of syntax (at least half of the dependencies are between adjacent words), using a scheme they call structural annealing. They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e. The factor δ changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training."
    },
    {
        "gold": {
            "text": [
                "For each interaction definition we can therefore check which of the flame clauses are applicable and discard the non-applicable ones.",
                "We thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates.",
                "The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques (#REF).",
                "29 The unfolding transformation is also referred to as partial execution, for example, by #REF.",
                "Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.",
                "As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #TARGET_REF ) .",
                "Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section.",
                "To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates.",
                "3° The successive unfolding steps are schematically represented in Figure 20."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:For each interaction definition we can therefore check which of the flame clauses are applicable and discard the non-applicable ones. We thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates. The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques (#REF). 29 The unfolding transformation is also referred to as partial execution, for example, by #REF. Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time. As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #TARGET_REF ) . Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section. To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates. 3° The successive unfolding steps are schematically represented in Figure 20.",
        "output": "introduction:For each interaction definition we can therefore check which of the flame clauses are applicable and discard the non-applicable ones. We thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates. The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques (#REF). 29 The unfolding transformation is also referred to as partial execution, for example, by #REF. Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time. As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #TARGET_REF<USE/> ) . Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section. To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates. 3° The successive unfolding steps are schematically represented in Figure 20."
    },
    {
        "gold": {
            "text": [
                "Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #REF ; #TARGET_REF ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #REF ) to compare it to other shallow parsers .",
                "Table 1 shows that it ranks among the top shallow parsers evaluated there 1 ."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #REF ; #TARGET_REF ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #REF ) to compare it to other shallow parsers . Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .",
        "output": "experiments:Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #REF ; #TARGET_REF<COMP/> ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #REF ) to compare it to other shallow parsers . Table 1 shows that it ranks among the top shallow parsers evaluated there 1 ."
    },
    {
        "gold": {
            "text": [
                "Division and subtraction are also possible: −(p, v) = (−p, −v) and (p, v) −1 = (p −1 , −p −1 vp −1 ).",
                "Division is commonly used in defining f θ (for normalization). 19",
                "Multiple edges from j to k are summed into a single edge.",
                "(#REF).",
                "Efficient hardware implementation is also possible via chip-level parallelism ( #TARGET_REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:Division and subtraction are also possible: −(p, v) = (−p, −v) and (p, v) −1 = (p −1 , −p −1 vp −1 ). Division is commonly used in defining f θ (for normalization). 19 Multiple edges from j to k are summed into a single edge. (#REF). Efficient hardware implementation is also possible via chip-level parallelism ( #TARGET_REF ) .",
        "output": "nan:Division and subtraction are also possible: −(p, v) = (−p, −v) and (p, v) −1 = (p −1 , −p −1 vp −1 ). Division is commonly used in defining f θ (for normalization). 19 Multiple edges from j to k are summed into a single edge. (#REF). Efficient hardware implementation is also possible via chip-level parallelism ( #TARGET_REF<MOT/> ) ."
    },
    {
        "gold": {
            "text": [
                "Learning algorithms.",
                "We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #REF , motivated by its success in the related tasks of word sense disambiguation ( #TARGET_REF ) and NE classification ( #REF ) .",
                "We apply add-one smoothing to smooth the class posteriors."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:Learning algorithms. We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #REF , motivated by its success in the related tasks of word sense disambiguation ( #TARGET_REF ) and NE classification ( #REF ) . We apply add-one smoothing to smooth the class posteriors.",
        "output": "introduction:Learning algorithms. We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #REF , motivated by its success in the related tasks of word sense disambiguation ( #TARGET_REF<EXT/> ) and NE classification ( #REF ) . We apply add-one smoothing to smooth the class posteriors."
    },
    {
        "gold": {
            "text": [
                "results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition.",
                "In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions.",
                "This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations.",
                "The result holds for both the MaltParser ( #REF ) and the Easy-First Parser ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition. In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions. This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations. The result holds for both the MaltParser ( #REF ) and the Easy-First Parser ( #TARGET_REF ) .",
        "output": "introduction:results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition. In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions. This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations. The result holds for both the MaltParser ( #REF ) and the Easy-First Parser ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #TARGET_REF ; also Section 8.1 of the present article ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #TARGET_REF ; also Section 8.1 of the present article ) .",
        "output": "introduction:2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #TARGET_REF<BACK/> ; also Section 8.1 of the present article ) ."
    },
    {
        "gold": {
            "text": [
                "In this paper, we show that the strongly equivalent grammars enable the sharing of \"parsing techniques\", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.",
                "We apply our system to the latest version of the XTAG English grammar (The XTAG Research #REF), which is a large-scale FB-LTAG grammar.",
                "A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #TARGET_REF ) .",
                "This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.",
                "We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:In this paper, we show that the strongly equivalent grammars enable the sharing of \"parsing techniques\", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities. We apply our system to the latest version of the XTAG English grammar (The XTAG Research #REF), which is a large-scale FB-LTAG grammar. A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #TARGET_REF ) . This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.",
        "output": "introduction:In this paper, we show that the strongly equivalent grammars enable the sharing of \"parsing techniques\", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities. We apply our system to the latest version of the XTAG English grammar (The XTAG Research #REF), which is a large-scale FB-LTAG grammar. A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #TARGET_REF<USE/> ) . This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing."
    },
    {
        "gold": {
            "text": [
                "The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.",
                "Kuhn and de #REF proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model.",
                "Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.",
                "The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.",
                "But unlike the cache model, it uses a multipass strategy.",
                "#TARGET_REF developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last",
                "In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.",
                "Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.",
                "For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition. Kuhn and de #REF proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model. Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage. The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. #TARGET_REF developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.",
        "output": "nan:The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition. Kuhn and de #REF proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model. Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage. The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. #TARGET_REF<COMP/> developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed."
    },
    {
        "gold": {
            "text": [
                "Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.",
                "On one side, we plan to explore alternative ways to build phrase and paraphrase tables.",
                "One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #TARGET_REF ) .",
                "Another interesting direction is to investigate the potential of paraphrase patterns (i.e.",
                "patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (#REF).",
                "On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge.",
                "As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment.",
                "1343"
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE. On one side, we plan to explore alternative ways to build phrase and paraphrase tables. One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #TARGET_REF ) . Another interesting direction is to investigate the potential of paraphrase patterns (i.e. patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (#REF). On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge. As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment. 1343",
        "output": "conclusion:Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE. On one side, we plan to explore alternative ways to build phrase and paraphrase tables. One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #TARGET_REF<MOT/> ) . Another interesting direction is to investigate the potential of paraphrase patterns (i.e. patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (#REF). On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge. As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment. 1343"
    },
    {
        "gold": {
            "text": [
                "The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.",
                "This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (#REF;#REF) that do not include nonlexicalized fragments.",
                "However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.",
                "While the models of #REF and #REF restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (#REF;#REFa).",
                "The importance of including nonheadwords has become uncontroversial (e.g.",
                "#REF;#REF;#REF).",
                "And #REF argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #TARGET_REF who use exactly the same set of ( all ) tree fragments as proposed in #REF ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "['M Collins', 'N Duffy']:The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset. This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (#REF;#REF) that do not include nonlexicalized fragments. However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions. While the models of #REF and #REF restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (#REF;#REFa). The importance of including nonheadwords has become uncontroversial (e.g. #REF;#REF;#REF). And #REF argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #TARGET_REF who use exactly the same set of ( all ) tree fragments as proposed in #REF .",
        "output": "['M Collins', 'N Duffy']:The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset. This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (#REF;#REF) that do not include nonlexicalized fragments. However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions. While the models of #REF and #REF restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (#REF;#REFa). The importance of including nonheadwords has become uncontroversial (e.g. #REF;#REF;#REF). And #REF argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #TARGET_REF<EXT/> who use exactly the same set of ( all ) tree fragments as proposed in #REF ."
    },
    {
        "gold": {
            "text": [
                "Out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.",
                "We measure the inter annotator agreement using the Fleiss Kappa ( #TARGET_REF ) measure ( x ) where the agreement lies around 0.79 .",
                "Next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 V1+V2 pairs and presented them to 36 native Bangla speakers.",
                "We ask each subjects to give a compositionality score of each verb sequences under 1-10 point scale, 10 being highly compositional and 1 for noncompositional.",
                "We found an agreement of κ=0.69 among the subjects.",
                "We also observe a continuum of compositionality score among the verb sequences.",
                "This reflects that it is difficult to classify Bangla verb sequences discretely into the classes of CV and not a CV.",
                "We then, compare the compositionality score with that of the expert user's annotation.",
                "We found a significant correlation between the expert annotation and the compositionality score."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:Out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping. We measure the inter annotator agreement using the Fleiss Kappa ( #TARGET_REF ) measure ( x ) where the agreement lies around 0.79 . Next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 V1+V2 pairs and presented them to 36 native Bangla speakers. We ask each subjects to give a compositionality score of each verb sequences under 1-10 point scale, 10 being highly compositional and 1 for noncompositional. We found an agreement of κ=0.69 among the subjects. We also observe a continuum of compositionality score among the verb sequences. This reflects that it is difficult to classify Bangla verb sequences discretely into the classes of CV and not a CV. We then, compare the compositionality score with that of the expert user's annotation. We found a significant correlation between the expert annotation and the compositionality score.",
        "output": "method:Out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping. We measure the inter annotator agreement using the Fleiss Kappa ( #TARGET_REF<FUT/> ) measure ( x ) where the agreement lies around 0.79 . Next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 V1+V2 pairs and presented them to 36 native Bangla speakers. We ask each subjects to give a compositionality score of each verb sequences under 1-10 point scale, 10 being highly compositional and 1 for noncompositional. We found an agreement of κ=0.69 among the subjects. We also observe a continuum of compositionality score among the verb sequences. This reflects that it is difficult to classify Bangla verb sequences discretely into the classes of CV and not a CV. We then, compare the compositionality score with that of the expert user's annotation. We found a significant correlation between the expert annotation and the compositionality score."
    },
    {
        "gold": {
            "text": [
                "Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #REF ; #REF ; #REF ; #REFa ; #REFb ; #TARGET_REF ) .",
                "Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the \"meaning of words is entirely given by other words\" (#REFb)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #REF ; #REF ; #REF ; #REFa ; #REFb ; #TARGET_REF ) . Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the \"meaning of words is entirely given by other words\" (#REFb).",
        "output": "introduction:Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #REF ; #REF ; #REF ; #REFa ; #REFb ; #TARGET_REF<BACK/> ) . Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the \"meaning of words is entirely given by other words\" (#REFb)."
    },
    {
        "gold": {
            "text": [
                "The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #TARGET_REF ) .",
                "We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in #REF)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #TARGET_REF ) . We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in #REF).",
        "output": "related work:The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #TARGET_REF<USE/> ) . We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in #REF)."
    },
    {
        "gold": {
            "text": [
                "The reordering models we describe follow our previous work using function word models for translation ( #TARGET_REF ; #REF ) .",
                "The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.",
                "To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.",
                "This section provides a high level overview of our reordering model, which attempts to leverage this information."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:The reordering models we describe follow our previous work using function word models for translation ( #TARGET_REF ; #REF ) . The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them. To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases. This section provides a high level overview of our reordering model, which attempts to leverage this information.",
        "output": "nan:The reordering models we describe follow our previous work using function word models for translation ( #TARGET_REF<COMP/> ; #REF ) . The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them. To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases. This section provides a high level overview of our reordering model, which attempts to leverage this information."
    },
    {
        "gold": {
            "text": [
                "The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.",
                "The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student's answer.",
                "At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #TARGET_REF ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "experiments:The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness. The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student's answer. At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #TARGET_REF .",
        "output": "experiments:The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness. The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student's answer. At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #TARGET_REF<MOT/> ."
    },
    {
        "gold": {
            "text": [
                "The implementation has been inspired by experience in extracting information from very large corpora ( #REF ) and performing experiments on maximum entropy sequence tagging ( #TARGET_REF ; #REF ) .",
                "We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.",
                "These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.",
                "These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (#REF).",
                "We expect even faster training times when we move to conjugate gradient methods."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:The implementation has been inspired by experience in extracting information from very large corpora ( #REF ) and performing experiments on maximum entropy sequence tagging ( #TARGET_REF ; #REF ) . We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (#REF). We expect even faster training times when we move to conjugate gradient methods.",
        "output": "experiments:The implementation has been inspired by experience in extracting information from very large corpora ( #REF ) and performing experiments on maximum entropy sequence tagging ( #TARGET_REF<EXT/> ; #REF ) . We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (#REF). We expect even faster training times when we move to conjugate gradient methods."
    },
    {
        "gold": {
            "text": [
                "â¢ The transition probability a is 0.7 using the EM algorithm ( #TARGET_REF ) on the TREC4 ad-hoc query set ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:â¢ The transition probability a is 0.7 using the EM algorithm ( #TARGET_REF ) on the TREC4 ad-hoc query set .",
        "output": "nan:â¢ The transition probability a is 0.7 using the EM algorithm ( #TARGET_REF<FUT/> ) on the TREC4 ad-hoc query set ."
    },
    {
        "gold": {
            "text": [
                "Compared with retraining mode, incremental training can improve the training efficiency.",
                "In the field of machine learning research , incremental training has been employed in the work ( #TARGET_REF ; #REF ) , but there is little work for tuning parameters of statistical machine translation .",
                "The biggest difficulty lies in that the fea- ture vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.",
                "In this section, we will investigate the incremental trainingmethodsinSMTscenario."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Compared with retraining mode, incremental training can improve the training efficiency. In the field of machine learning research , incremental training has been employed in the work ( #TARGET_REF ; #REF ) , but there is little work for tuning parameters of statistical machine translation . The biggest difficulty lies in that the fea- ture vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable. In this section, we will investigate the incremental trainingmethodsinSMTscenario.",
        "output": "nan:Compared with retraining mode, incremental training can improve the training efficiency. In the field of machine learning research , incremental training has been employed in the work ( #TARGET_REF<BACK/> ; #REF ) , but there is little work for tuning parameters of statistical machine translation . The biggest difficulty lies in that the fea- ture vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable. In this section, we will investigate the incremental trainingmethodsinSMTscenario."
    },
    {
        "gold": {
            "text": [
                "However, these methodologies have not used historical information at the dialogue level as described here.",
                "In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.",
                "The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.",
                "Thus, an error in this work has no pattern but occurs probabilistically.",
                "A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.",
                "The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #TARGET_REF where program flowcharts were constructed from traces of their behaviors .",
                "However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.",
                "Another dialogue acquisition system has been developed by #REF.",
                "However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.",
                "The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.",
                "It self activates to bias recognition toward historically observed patterns but is not otherwise observable."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #TARGET_REF where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #REF. However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user. It self activates to bias recognition toward historically observed patterns but is not otherwise observable.",
        "output": "nan:However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #TARGET_REF<USE/> where program flowcharts were constructed from traces of their behaviors . However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #REF. However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user. It self activates to bias recognition toward historically observed patterns but is not otherwise observable."
    },
    {
        "gold": {
            "text": [
                "In previous work ( #TARGET_REF ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #REF ) .",
                "The system generated phrase boundaries using information derived from the syntactic structure of a sentence.",
                "While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.",
                "Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.",
                "This paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.",
                "Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:In previous work ( #TARGET_REF ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #REF ) . The system generated phrase boundaries using information derived from the syntactic structure of a sentence. While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system. Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing. This paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version. Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries.",
        "output": "introduction:In previous work ( #TARGET_REF<COMP/> ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #REF ) . The system generated phrase boundaries using information derived from the syntactic structure of a sentence. While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system. Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing. This paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version. Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries."
    },
    {
        "gold": {
            "text": [
                "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #TARGET_REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #TARGET_REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) .",
        "output": "conclusion:Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #TARGET_REF<MOT/>a ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "DA classification using words is based on the observation that different DAs use distinctive word strings.",
                "It is known that certain cue words and phrases ( #TARGET_REF ) can serve as explicit indicators of discourse structure .",
                "Similarly, we find distinctive correlations between certain phrases and DA types.",
                "For example, 92.4% of the uh-huh's occur in BACKCHANNELS, and 88.4% of the trigrams \"<start> do you\" occur in YES-NO-QUESTIONS.",
                "To leverage this information source, without hand-coding knowledge about which words are indicative of which DAs, we will use statistical language models that model the full word sequences associated with each DA type.",
                "5.1.1",
                "Classification from True Words.",
                "Assuming that the true (hand-transcribed) words of utterances are given as evidence, we can compute word-based likelihoods P(WIU ) in a straightforward way, by building a statistical language model for each of the 42 DAs.",
                "All DAs of a particular type found in the training corpus were pooled, and a DA-specific trigram model was estimated using standard techniques (Katz backoff [#REF] with Witten-Bell discounting [#REF])."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:DA classification using words is based on the observation that different DAs use distinctive word strings. It is known that certain cue words and phrases ( #TARGET_REF ) can serve as explicit indicators of discourse structure . Similarly, we find distinctive correlations between certain phrases and DA types. For example, 92.4% of the uh-huh's occur in BACKCHANNELS, and 88.4% of the trigrams \"<start> do you\" occur in YES-NO-QUESTIONS. To leverage this information source, without hand-coding knowledge about which words are indicative of which DAs, we will use statistical language models that model the full word sequences associated with each DA type. 5.1.1 Classification from True Words. Assuming that the true (hand-transcribed) words of utterances are given as evidence, we can compute word-based likelihoods P(WIU ) in a straightforward way, by building a statistical language model for each of the 42 DAs. All DAs of a particular type found in the training corpus were pooled, and a DA-specific trigram model was estimated using standard techniques (Katz backoff [#REF] with Witten-Bell discounting [#REF]).",
        "output": "nan:DA classification using words is based on the observation that different DAs use distinctive word strings. It is known that certain cue words and phrases ( #TARGET_REF<EXT/> ) can serve as explicit indicators of discourse structure . Similarly, we find distinctive correlations between certain phrases and DA types. For example, 92.4% of the uh-huh's occur in BACKCHANNELS, and 88.4% of the trigrams \"<start> do you\" occur in YES-NO-QUESTIONS. To leverage this information source, without hand-coding knowledge about which words are indicative of which DAs, we will use statistical language models that model the full word sequences associated with each DA type. 5.1.1 Classification from True Words. Assuming that the true (hand-transcribed) words of utterances are given as evidence, we can compute word-based likelihoods P(WIU ) in a straightforward way, by building a statistical language model for each of the 42 DAs. All DAs of a particular type found in the training corpus were pooled, and a DA-specific trigram model was estimated using standard techniques (Katz backoff [#REF] with Witten-Bell discounting [#REF])."
    },
    {
        "gold": {
            "text": [
                "For our experiments we used the standard division of the WSJ ( #TARGET_REF ) , with sections 2 through 21 for training ( approx .",
                "40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.",
                "As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks.",
                "Without loss of generality, all trees were converted to binary branching (and were reconverted to n-ary trees after parsing).",
                "We employed the same unknown (category) word model as in #REF, based on statistics on word-endings, hyphenation and capitalization in combination with Good-Turing (#REF: 85 -87).",
                "We used \"evalb\" 4 to compute the standard PARSEVAL scores for our results (Manning & #REF).",
                "We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:For our experiments we used the standard division of the WSJ ( #TARGET_REF ) , with sections 2 through 21 for training ( approx . 40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set. As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks. Without loss of generality, all trees were converted to binary branching (and were reconverted to n-ary trees after parsing). We employed the same unknown (category) word model as in #REF, based on statistics on word-endings, hyphenation and capitalization in combination with Good-Turing (#REF: 85 -87). We used \"evalb\" 4 to compute the standard PARSEVAL scores for our results (Manning & #REF). We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems.",
        "output": "experiments:For our experiments we used the standard division of the WSJ ( #TARGET_REF<FUT/> ) , with sections 2 through 21 for training ( approx . 40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set. As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks. Without loss of generality, all trees were converted to binary branching (and were reconverted to n-ary trees after parsing). We employed the same unknown (category) word model as in #REF, based on statistics on word-endings, hyphenation and capitalization in combination with Good-Turing (#REF: 85 -87). We used \"evalb\" 4 to compute the standard PARSEVAL scores for our results (Manning & #REF). We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems."
    },
    {
        "gold": {
            "text": [
                "Many different features have been used to represent documents where an ambiguous name is mentioned.",
                "The most basic is a Bag of Words (BoW) representation of the document text.",
                "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (#REF;#REF).",
                "Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #REF ) and sometimes in combination with others see for instance ( #REF ; #TARGET_REF ) - .",
                "Other representations use the link structure (#REF) or generate graph representations of the extracted features (#REF).",
                "Some researchers (#REF;#REF) have explored the use of Wikipedia information to improve the disambiguation process.",
                "Wikipedia provides candidate entities that are linked to specific mentions in a text.",
                "The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.",
                "These approaches are yet to be applied to the specific task of grouping search results."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (#REF;#REF). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #REF ) and sometimes in combination with others see for instance ( #REF ; #TARGET_REF ) - . Other representations use the link structure (#REF) or generate graph representations of the extracted features (#REF). Some researchers (#REF;#REF) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results.",
        "output": "related work:Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (#REF;#REF). Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #REF ) and sometimes in combination with others see for instance ( #REF ; #TARGET_REF<BACK/> ) - . Other representations use the link structure (#REF) or generate graph representations of the extracted features (#REF). Some researchers (#REF;#REF) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results."
    },
    {
        "gold": {
            "text": [
                "This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #TARGET_REF .",
                "It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #TARGET_REF . It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations.",
        "output": "nan:This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #TARGET_REF<USE/> . It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations."
    },
    {
        "gold": {
            "text": [
                "In our previous work ( #TARGET_REF ) , we started an initial investigation on conversation entailment .",
                "We have collected a dataset of 875 instances.",
                "Each instance consists of a conversation segment and a hypothesis (as described in Section 1).",
                "The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions.",
                "We developed an approach that is motivated by previous work on textual entailment.",
                "We use clauses in the logic-based approaches as the underlying representation of our system.",
                "Based on this representation, we apply a two stage entailment process similar to #REF developed for textual entailment: an alignment stage followed by an entailment stage."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "related work:In our previous work ( #TARGET_REF ) , we started an initial investigation on conversation entailment . We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1). The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions. We developed an approach that is motivated by previous work on textual entailment. We use clauses in the logic-based approaches as the underlying representation of our system. Based on this representation, we apply a two stage entailment process similar to #REF developed for textual entailment: an alignment stage followed by an entailment stage.",
        "output": "related work:In our previous work ( #TARGET_REF<COMP/> ) , we started an initial investigation on conversation entailment . We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1). The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions. We developed an approach that is motivated by previous work on textual entailment. We use clauses in the logic-based approaches as the underlying representation of our system. Based on this representation, we apply a two stage entailment process similar to #REF developed for textual entailment: an alignment stage followed by an entailment stage."
    },
    {
        "gold": {
            "text": [
                "The Generative Programming approach to NLP infrastructure development will allow tools such as sentence boundary detectors, POS taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components.",
                "For instance , implementing an efficient version of the MXPOST POS tagger ( #TARGET_REF ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:The Generative Programming approach to NLP infrastructure development will allow tools such as sentence boundary detectors, POS taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components. For instance , implementing an efficient version of the MXPOST POS tagger ( #TARGET_REF ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .",
        "output": "conclusion:The Generative Programming approach to NLP infrastructure development will allow tools such as sentence boundary detectors, POS taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components. For instance , implementing an efficient version of the MXPOST POS tagger ( #TARGET_REF<MOT/> ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component ."
    },
    {
        "gold": {
            "text": [
                "If the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail.",
                "In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #TARGET_REF ) .",
                "The only practical alternative is to provide the generator with \"crisp\" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database.",
                "It is this avenue that we have explored in this article, in combination with various (incremental and other) approaches to GRE.",
                "Far from being a peculiarity of a few adjectives, vagueness is widespread.",
                "We believe that our approach can be applied to a variety of situations in which vagueness affects referring expressions including, for example, r color terms (Section 9.3); r nouns that allow different degrees of strictness (Section 9.5); r degrees of salience (Section 9.4); and r imprecise pointing (Section 9.5)."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:If the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail. In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #TARGET_REF ) . The only practical alternative is to provide the generator with \"crisp\" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database. It is this avenue that we have explored in this article, in combination with various (incremental and other) approaches to GRE. Far from being a peculiarity of a few adjectives, vagueness is widespread. We believe that our approach can be applied to a variety of situations in which vagueness affects referring expressions including, for example, r color terms (Section 9.3); r nouns that allow different degrees of strictness (Section 9.5); r degrees of salience (Section 9.4); and r imprecise pointing (Section 9.5).",
        "output": "experiments:If the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail. In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #TARGET_REF<EXT/> ) . The only practical alternative is to provide the generator with \"crisp\" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database. It is this avenue that we have explored in this article, in combination with various (incremental and other) approaches to GRE. Far from being a peculiarity of a few adjectives, vagueness is widespread. We believe that our approach can be applied to a variety of situations in which vagueness affects referring expressions including, for example, r color terms (Section 9.3); r nouns that allow different degrees of strictness (Section 9.5); r degrees of salience (Section 9.4); and r imprecise pointing (Section 9.5)."
    },
    {
        "gold": {
            "text": [
                "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) #TARGET_REF , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) #TARGET_REF , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .",
        "output": "nan:Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) #TARGET_REF<FUT/> , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
    },
    {
        "gold": {
            "text": [
                "Applying an absolute threshold of five occurrences, we still generate 162 frame types  from Penn-II and 221 from Penn-III.",
                "#TARGET_REF , by comparison , employ 163 distinct predefined frames ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:Applying an absolute threshold of five occurrences, we still generate 162 frame types  from Penn-II and 221 from Penn-III. #TARGET_REF , by comparison , employ 163 distinct predefined frames .",
        "output": "experiments:Applying an absolute threshold of five occurrences, we still generate 162 frame types  from Penn-II and 221 from Penn-III. #TARGET_REF<BACK/> , by comparison , employ 163 distinct predefined frames ."
    },
    {
        "gold": {
            "text": [
                "The design and implementation of our current system leaves many open avenues for future exploration, one of which concerns our assumptions about the query interface.",
                "Previously , a user study ( #TARGET_REF ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .",
                "We have argued that a query interface based on structured PICO frames will yield better-formulated queries, although it is unclear whether physicians would invest the upfront effort necessary to accomplish this.",
                "Issuing extremely short queries appears to be an ingrained habit of information seekers today, and the dominance of World Wide Web searches reinforce this behavior.",
                "Given these trends, physicians may actually prefer the rapid back-and-forth interaction style that comes with short queries.",
                "We believe that if systems can produce noticeably better results with richer queries, users will make more of an effort to formulate them.",
                "This, however, presents a chicken-and-egg problem: One possible solution is to develop models that can automatically fill query frames given a couple of keywords-this would serve to kick-start the query generation process."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:The design and implementation of our current system leaves many open avenues for future exploration, one of which concerns our assumptions about the query interface. Previously , a user study ( #TARGET_REF ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance . We have argued that a query interface based on structured PICO frames will yield better-formulated queries, although it is unclear whether physicians would invest the upfront effort necessary to accomplish this. Issuing extremely short queries appears to be an ingrained habit of information seekers today, and the dominance of World Wide Web searches reinforce this behavior. Given these trends, physicians may actually prefer the rapid back-and-forth interaction style that comes with short queries. We believe that if systems can produce noticeably better results with richer queries, users will make more of an effort to formulate them. This, however, presents a chicken-and-egg problem: One possible solution is to develop models that can automatically fill query frames given a couple of keywords-this would serve to kick-start the query generation process.",
        "output": "conclusion:The design and implementation of our current system leaves many open avenues for future exploration, one of which concerns our assumptions about the query interface. Previously , a user study ( #TARGET_REF<USE/> ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance . We have argued that a query interface based on structured PICO frames will yield better-formulated queries, although it is unclear whether physicians would invest the upfront effort necessary to accomplish this. Issuing extremely short queries appears to be an ingrained habit of information seekers today, and the dominance of World Wide Web searches reinforce this behavior. Given these trends, physicians may actually prefer the rapid back-and-forth interaction style that comes with short queries. We believe that if systems can produce noticeably better results with richer queries, users will make more of an effort to formulate them. This, however, presents a chicken-and-egg problem: One possible solution is to develop models that can automatically fill query frames given a couple of keywords-this would serve to kick-start the query generation process."
    },
    {
        "gold": {
            "text": [
                "#REF claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.",
                "#REF and #REF take a similar approach, but within a different theoretical framework.",
                "Previous versions of our work , as described in #TARGET_REF also assume that phrasing is dependent on predicate-argument structure .",
                "The problem here is that the phrasing in observed data often ignores the argument status of constituents.",
                "In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.",
                "All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.",
                "(The complement in 17a and the adjuncts in 17b-f are italicized.)",
                "The relation between discourse and prosodic phrasing has been examined in some detail by #REF, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.",
                "Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:#REF claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct. #REF and #REF take a similar approach, but within a different theoretical framework. Previous versions of our work , as described in #TARGET_REF also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts. All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others. (The complement in 17a and the adjuncts in 17b-f are italicized.) The relation between discourse and prosodic phrasing has been examined in some detail by #REF, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse. Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.",
        "output": "introduction:#REF claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct. #REF and #REF take a similar approach, but within a different theoretical framework. Previous versions of our work , as described in #TARGET_REF<COMP/> also assume that phrasing is dependent on predicate-argument structure . The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts. All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others. (The complement in 17a and the adjuncts in 17b-f are italicized.) The relation between discourse and prosodic phrasing has been examined in some detail by #REF, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse. Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer."
    },
    {
        "gold": {
            "text": [
                "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) .",
        "output": "conclusion:Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #TARGET_REF<MOT/> ; #REF ; #REF ; #REF ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "In our experiments we work with a set of English-Japanese reordering rules 1 and gold reorderings based on human generated correct reordering of an aligned target sentences.",
                "We use a reordering score based on the reordering penalty from the METEOR scoring metric.",
                "Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( #TARGET_REF ) and is simpler to measure .",
                "Results for three augmented-loss schedules are shown: 0.5 where for every two treebank updates we make one augmented-loss update, 1 is a 1-to-1 mix, and 2 is where we make twice as many augmented-loss updates as treebank updates."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:In our experiments we work with a set of English-Japanese reordering rules 1 and gold reorderings based on human generated correct reordering of an aligned target sentences. We use a reordering score based on the reordering penalty from the METEOR scoring metric. Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( #TARGET_REF ) and is simpler to measure . Results for three augmented-loss schedules are shown: 0.5 where for every two treebank updates we make one augmented-loss update, 1 is a 1-to-1 mix, and 2 is where we make twice as many augmented-loss updates as treebank updates.",
        "output": "experiments:In our experiments we work with a set of English-Japanese reordering rules 1 and gold reorderings based on human generated correct reordering of an aligned target sentences. We use a reordering score based on the reordering penalty from the METEOR scoring metric. Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( #TARGET_REF<EXT/> ) and is simpler to measure . Results for three augmented-loss schedules are shown: 0.5 where for every two treebank updates we make one augmented-loss update, 1 is a 1-to-1 mix, and 2 is where we make twice as many augmented-loss updates as treebank updates."
    },
    {
        "gold": {
            "text": [
                "Semantic filters can also be used to prevent multiple versions of the same case frame ( #TARGET_REF ) showing up as complements .",
                "For instance, the set of complements [from-place], [to-place], and [at-time] are freely ordered following a movement verb such as \"leave.\"",
                "Thus a flight can \"leave for Chicago from Boston at nine,\" or, equivalently, \"leave at nine for Chicago from Boston.\"",
                "If these complements are each allowed to follow the other, then in TINA an infinite sequence of [from-place]s, [to-place]s and [at-time]s is possible.",
                "This is of course unacceptable, but it is straightforward to have each node, as it occurs, or in a semantic bit specifying its case frame, and, in turn, fail if that bit has already been set.",
                "We have found that this strategy, in conjunction with the capability of erasing all semantic bits whenever a new clause is entered (through the meta level \"detach\" operation mentioned previously) serves the desired goal of eliminating the unwanted redundancies."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Semantic filters can also be used to prevent multiple versions of the same case frame ( #TARGET_REF ) showing up as complements . For instance, the set of complements [from-place], [to-place], and [at-time] are freely ordered following a movement verb such as \"leave.\" Thus a flight can \"leave for Chicago from Boston at nine,\" or, equivalently, \"leave at nine for Chicago from Boston.\" If these complements are each allowed to follow the other, then in TINA an infinite sequence of [from-place]s, [to-place]s and [at-time]s is possible. This is of course unacceptable, but it is straightforward to have each node, as it occurs, or in a semantic bit specifying its case frame, and, in turn, fail if that bit has already been set. We have found that this strategy, in conjunction with the capability of erasing all semantic bits whenever a new clause is entered (through the meta level \"detach\" operation mentioned previously) serves the desired goal of eliminating the unwanted redundancies.",
        "output": "nan:Semantic filters can also be used to prevent multiple versions of the same case frame ( #TARGET_REF<FUT/> ) showing up as complements . For instance, the set of complements [from-place], [to-place], and [at-time] are freely ordered following a movement verb such as \"leave.\" Thus a flight can \"leave for Chicago from Boston at nine,\" or, equivalently, \"leave at nine for Chicago from Boston.\" If these complements are each allowed to follow the other, then in TINA an infinite sequence of [from-place]s, [to-place]s and [at-time]s is possible. This is of course unacceptable, but it is straightforward to have each node, as it occurs, or in a semantic bit specifying its case frame, and, in turn, fail if that bit has already been set. We have found that this strategy, in conjunction with the capability of erasing all semantic bits whenever a new clause is entered (through the meta level \"detach\" operation mentioned previously) serves the desired goal of eliminating the unwanted redundancies."
    },
    {
        "gold": {
            "text": [
                "This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ.",
                "We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.",
                "Bod (2001Bod ( , 2003.",
                "But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #TARGET_REF and #REF , Bonnema et al. 's estimator performs worse and is comparable to #REF ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ. We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001Bod ( , 2003. But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #TARGET_REF and #REF , Bonnema et al. 's estimator performs worse and is comparable to #REF .",
        "output": "introduction:This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ. We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001Bod ( , 2003. But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #TARGET_REF<BACK/> and #REF , Bonnema et al. 's estimator performs worse and is comparable to #REF ."
    },
    {
        "gold": {
            "text": [
                "Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12.",
                "But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.",
                "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.",
                "This is roughly an 11% relative reduction in error rate over #REF and Bod's PCFG-reduction reported in Table 1.",
                "Compared to the reranking technique in #TARGET_REF , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .",
                "While SL-DOP and LS-DOP have been compared before in"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12. But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11% relative reduction in error rate over #REF and Bod's PCFG-reduction reported in Table 1. Compared to the reranking technique in #TARGET_REF , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction . While SL-DOP and LS-DOP have been compared before in",
        "output": "conclusion:Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12. But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11% relative reduction in error rate over #REF and Bod's PCFG-reduction reported in Table 1. Compared to the reranking technique in #TARGET_REF<USE/> , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction . While SL-DOP and LS-DOP have been compared before in"
    },
    {
        "gold": {
            "text": [
                "In our previous work ( #TARGET_REF ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â§ ... â§ dm , and a hypothesis H represented by another set of clauses H = h1 â§ ... â§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .",
                "This is based on a simple as- sumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:In our previous work ( #TARGET_REF ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â§ ... â§ dm , and a hypothesis H represented by another set of clauses H = h1 â§ ... â§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows . This is based on a simple as- sumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses.",
        "output": "nan:In our previous work ( #TARGET_REF<COMP/> ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â§ ... â§ dm , and a hypothesis H represented by another set of clauses H = h1 â§ ... â§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows . This is based on a simple as- sumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses."
    },
    {
        "gold": {
            "text": [
                "In a similar vain to #REF and #TARGET_REF , the method extends an existing flat shallow-parsing method to handle composite structures ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:In a similar vain to #REF and #TARGET_REF , the method extends an existing flat shallow-parsing method to handle composite structures .",
        "output": "conclusion:In a similar vain to #REF and #TARGET_REF<MOT/> , the method extends an existing flat shallow-parsing method to handle composite structures ."
    },
    {
        "gold": {
            "text": [
                "In this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization.",
                "The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #TARGET_REF ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:In this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization. The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #TARGET_REF ) .",
        "output": "introduction:In this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization. The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #TARGET_REF<EXT/> ) ."
    },
    {
        "gold": {
            "text": [
                "13 We also employed sequence-based measures using the ROUGE tool set ( #TARGET_REF ) , with similar results to those obtained with the word-by-word measures ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:13 We also employed sequence-based measures using the ROUGE tool set ( #TARGET_REF ) , with similar results to those obtained with the word-by-word measures .",
        "output": "method:13 We also employed sequence-based measures using the ROUGE tool set ( #TARGET_REF<FUT/> ) , with similar results to those obtained with the word-by-word measures ."
    },
    {
        "gold": {
            "text": [
                "More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.",
                "Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.",
                "For instance , ( #TARGET_REF ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .",
                "Similarly, (#REF) and (#REF) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.",
                "And (#REF) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content. Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. For instance , ( #TARGET_REF ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning . Similarly, (#REF) and (#REF) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source. And (#REF) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.",
        "output": "introduction:More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content. Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques. For instance , ( #TARGET_REF<BACK/> ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning . Similarly, (#REF) and (#REF) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source. And (#REF) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts."
    },
    {
        "gold": {
            "text": [
                "Although not the first to employ a generative approach to directly model content , the seminal work of #TARGET_REF is a noteworthy point of reference and comparison .",
                "However, our study differs in several important respects.",
                "Barzilay and Lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.",
                "In contrast, because the discourse structure of medical abstracts is welldefined and training data is relatively easy to obtain, we were able to apply a supervised approach.",
                "Whereas Barzilay and Lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task.",
                "Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Although not the first to employ a generative approach to directly model content , the seminal work of #TARGET_REF is a noteworthy point of reference and comparison . However, our study differs in several important respects. Barzilay and Lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques. In contrast, because the discourse structure of medical abstracts is welldefined and training data is relatively easy to obtain, we were able to apply a supervised approach. Whereas Barzilay and Lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task. Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here.",
        "output": "related work:Although not the first to employ a generative approach to directly model content , the seminal work of #TARGET_REF<USE/> is a noteworthy point of reference and comparison . However, our study differs in several important respects. Barzilay and Lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques. In contrast, because the discourse structure of medical abstracts is welldefined and training data is relatively easy to obtain, we were able to apply a supervised approach. Whereas Barzilay and Lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task. Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here."
    },
    {
        "gold": {
            "text": [
                "In order to obtain semantic representations of each word , we apply our previous strategy ( #TARGET_REF ) .",
                "Rather than using a termdocument matrix, we had followed an approach akin to that of Schütze (1993), who performed SVD on a Nx2N term-term matrix.",
                "The N here represents the N-1 most-frequent words as well as a glob position to account for all other words not in the top N-1.",
                "The matrix is structured such that for a given word w's row, the first N columns denote words that -NCS (µ,1)"
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "method:In order to obtain semantic representations of each word , we apply our previous strategy ( #TARGET_REF ) . Rather than using a termdocument matrix, we had followed an approach akin to that of Schütze (1993), who performed SVD on a Nx2N term-term matrix. The N here represents the N-1 most-frequent words as well as a glob position to account for all other words not in the top N-1. The matrix is structured such that for a given word w's row, the first N columns denote words that -NCS (µ,1)",
        "output": "method:In order to obtain semantic representations of each word , we apply our previous strategy ( #TARGET_REF<COMP/> ) . Rather than using a termdocument matrix, we had followed an approach akin to that of Schütze (1993), who performed SVD on a Nx2N term-term matrix. The N here represents the N-1 most-frequent words as well as a glob position to account for all other words not in the top N-1. The matrix is structured such that for a given word w's row, the first N columns denote words that -NCS (µ,1)"
    },
    {
        "gold": {
            "text": [
                "There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #TARGET_REF ; #REF ) .",
                "Incorporating such techniques would deo crease the system developer workload.",
                "However, there has been no work on domain-independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which WIT handles well.",
                "Therefore incorporating those techniques remains as a future work."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #TARGET_REF ; #REF ) . Incorporating such techniques would deo crease the system developer workload. However, there has been no work on domain-independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which WIT handles well. Therefore incorporating those techniques remains as a future work.",
        "output": "conclusion:There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #TARGET_REF<MOT/> ; #REF ) . Incorporating such techniques would deo crease the system developer workload. However, there has been no work on domain-independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which WIT handles well. Therefore incorporating those techniques remains as a future work."
    },
    {
        "gold": {
            "text": [
                "Like CCM, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.",
                "Since we are not generating from the model , this does not introduce difficulties ( #TARGET_REF ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "method:Like CCM, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid. Since we are not generating from the model , this does not introduce difficulties ( #TARGET_REF ) .",
        "output": "method:Like CCM, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid. Since we are not generating from the model , this does not introduce difficulties ( #TARGET_REF<EXT/> ) ."
    },
    {
        "gold": {
            "text": [
                "Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( #TARGET_REF ) .",
                "The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts of the answer.",
                "Once the complete answer has been accumulated, the system accepts it and moves on.",
                "Tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( #TARGET_REF ) . The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts of the answer. Once the complete answer has been accumulated, the system accepts it and moves on. Tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student.",
        "output": "experiments:Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( #TARGET_REF<FUT/> ) . The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts of the answer. Once the complete answer has been accumulated, the system accepts it and moves on. Tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student."
    },
    {
        "gold": {
            "text": [
                "First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and #REF).",
                "Second, software for utilizing this ontology already exists: MetaMap (#REF) identifies concepts in free text, and SemRep (#REF) extracts relations between the concepts.",
                "Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.",
                "#REF version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.",
                "The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.",
                "Third, the paradigm of evidence-based medicine (#REF) provides a task-based model of the clinical information-seeking process.",
                "The PICO framework ( #TARGET_REF ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .",
                "The confluence of these many factors makes clinical question answering a very exciting area of research."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and #REF). Second, software for utilizing this ontology already exists: MetaMap (#REF) identifies concepts in free text, and SemRep (#REF) extracts relations between the concepts. Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon. #REF version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies. The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third, the paradigm of evidence-based medicine (#REF) provides a task-based model of the clinical information-seeking process. The PICO framework ( #TARGET_REF ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system . The confluence of these many factors makes clinical question answering a very exciting area of research.",
        "output": "introduction:First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and #REF). Second, software for utilizing this ontology already exists: MetaMap (#REF) identifies concepts in free text, and SemRep (#REF) extracts relations between the concepts. Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon. #REF version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies. The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third, the paradigm of evidence-based medicine (#REF) provides a task-based model of the clinical information-seeking process. The PICO framework ( #TARGET_REF<BACK/> ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system . The confluence of these many factors makes clinical question answering a very exciting area of research."
    },
    {
        "gold": {
            "text": [
                "We then go on to compare the current approach with that of some other theories with similar aims : the `` standard  version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #TARGET_REF and #REF ; underspecified Discourse Representation Theory ( #REF ) ; and the `` glue language  approach of #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:We then go on to compare the current approach with that of some other theories with similar aims : the `` standard  version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #TARGET_REF and #REF ; underspecified Discourse Representation Theory ( #REF ) ; and the `` glue language  approach of #REF .",
        "output": "introduction:We then go on to compare the current approach with that of some other theories with similar aims : the `` standard  version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #TARGET_REF<USE/> and #REF ; underspecified Discourse Representation Theory ( #REF ) ; and the `` glue language  approach of #REF ."
    },
    {
        "gold": {
            "text": [
                "For our experiments, we use naive Bayes as the learning algorithm.",
                "The knowledge sources we use include parts-of-speech, local collocations, and surrounding words.",
                "These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #TARGET_REF ) ."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:For our experiments, we use naive Bayes as the learning algorithm. The knowledge sources we use include parts-of-speech, local collocations, and surrounding words. These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #TARGET_REF ) .",
        "output": "nan:For our experiments, we use naive Bayes as the learning algorithm. The knowledge sources we use include parts-of-speech, local collocations, and surrounding words. These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #TARGET_REF<COMP/> ) ."
    },
    {
        "gold": {
            "text": [
                "The function α(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task.",
                "Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #TARGET_REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:The function α(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task. Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #TARGET_REF ) .",
        "output": "nan:The function α(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task. Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #TARGET_REF<MOT/> ) ."
    },
    {
        "gold": {
            "text": [
                "We also compute GIST vectors (#REF) for every image using LearGIST (#REF).",
                "Unlike SURF descriptors, GIST produces a single vector representation for an image.",
                "The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall \"gist\" of the whole image.",
                "It is frequently used in tasks like scene identification , and #TARGET_REF shows that distance in GIST space correlates well with semantic distance in WordNet .",
                "After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:We also compute GIST vectors (#REF) for every image using LearGIST (#REF). Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall \"gist\" of the whole image. It is frequently used in tasks like scene identification , and #TARGET_REF shows that distance in GIST space correlates well with semantic distance in WordNet . After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.",
        "output": "experiments:We also compute GIST vectors (#REF) for every image using LearGIST (#REF). Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall \"gist\" of the whole image. It is frequently used in tasks like scene identification , and #TARGET_REF<EXT/> shows that distance in GIST space correlates well with semantic distance in WordNet . After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality."
    },
    {
        "gold": {
            "text": [
                "To create the baseline system , we use the opensource Joshua 4.0 system ( #REF ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #TARGET_REF ) respectively ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:To create the baseline system , we use the opensource Joshua 4.0 system ( #REF ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #TARGET_REF ) respectively .",
        "output": "experiments:To create the baseline system , we use the opensource Joshua 4.0 system ( #REF ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #TARGET_REF<FUT/> ) respectively ."
    },
    {
        "gold": {
            "text": [
                "Once the word \"up\" is given its meaning relative to our experience with gravity, it is not free to \"slip\" into its opposite.",
                "\"Up\" means up and not down ....",
                "We have a model that men and women couple to produce offspring who are similar to their parents, and this model is grounded in genetics, and the semantics of kinship metaphor is grounded in this model.",
                "Mothers have a different role than fathers in this model, and thus there is a reason why \"Death is the father of beauty\" fails poetically while \"Death is the mother of beauty\" succeeds ....",
                "It is precisely this \"grounding\" of logical predicates in other conceptual structures that we would like to capture.",
                "We investigate here only the \"grounding\" in logical theories.",
                "However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #TARGET_REF ) .",
                "#REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Once the word \"up\" is given its meaning relative to our experience with gravity, it is not free to \"slip\" into its opposite. \"Up\" means up and not down .... We have a model that men and women couple to produce offspring who are similar to their parents, and this model is grounded in genetics, and the semantics of kinship metaphor is grounded in this model. Mothers have a different role than fathers in this model, and thus there is a reason why \"Death is the father of beauty\" fails poetically while \"Death is the mother of beauty\" succeeds .... It is precisely this \"grounding\" of logical predicates in other conceptual structures that we would like to capture. We investigate here only the \"grounding\" in logical theories. However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #TARGET_REF ) . #REF).",
        "output": "introduction:Once the word \"up\" is given its meaning relative to our experience with gravity, it is not free to \"slip\" into its opposite. \"Up\" means up and not down .... We have a model that men and women couple to produce offspring who are similar to their parents, and this model is grounded in genetics, and the semantics of kinship metaphor is grounded in this model. Mothers have a different role than fathers in this model, and thus there is a reason why \"Death is the father of beauty\" fails poetically while \"Death is the mother of beauty\" succeeds .... It is precisely this \"grounding\" of logical predicates in other conceptual structures that we would like to capture. We investigate here only the \"grounding\" in logical theories. However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #TARGET_REF<BACK/> ) . #REF)."
    },
    {
        "gold": {
            "text": [
                "Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.",
                "To copy otherwise, or to republish, requires a fee and/or specific permission.",
                "0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (#REFb).",
                "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.",
                "#TARGET_REF ; #REF ) consult relatively small lexicons , typically generated by hand .",
                "Two exceptions to this generalisation are the Linguistic String Project (#REF) and the IBM CRITIQUE (formerly EPISTLE) Project (#REF;#REF); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources.",
                "In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see #REF for details).",
                "However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (#REFb). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #TARGET_REF ; #REF ) consult relatively small lexicons , typically generated by hand . Two exceptions to this generalisation are the Linguistic String Project (#REF) and the IBM CRITIQUE (formerly EPISTLE) Project (#REF;#REF); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see #REF for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.",
        "output": "introduction:Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (#REFb). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #TARGET_REF<USE/> ; #REF ) consult relatively small lexicons , typically generated by hand . Two exceptions to this generalisation are the Linguistic String Project (#REF) and the IBM CRITIQUE (formerly EPISTLE) Project (#REF;#REF); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see #REF for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons."
    },
    {
        "gold": {
            "text": [
                "For both languages, we resolved coreference by using SWIZZLE,our implementation of a bilingual coreference resolver.",
                "SWIZZLE is a multilingual enhancement of COCKTAIL ( #TARGET_REF ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  .",
                "When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively.",
                "When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuris- tics for coreference.",
                "Our experiments show that SWIZZLEoutperformed COCKTAILon both English and Romanian test documents."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:For both languages, we resolved coreference by using SWIZZLE,our implementation of a bilingual coreference resolver. SWIZZLE is a multilingual enhancement of COCKTAIL ( #TARGET_REF ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  . When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively. When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuris- tics for coreference. Our experiments show that SWIZZLEoutperformed COCKTAILon both English and Romanian test documents.",
        "output": "introduction:For both languages, we resolved coreference by using SWIZZLE,our implementation of a bilingual coreference resolver. SWIZZLE is a multilingual enhancement of COCKTAIL ( #TARGET_REF<COMP/> ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  . When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively. When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuris- tics for coreference. Our experiments show that SWIZZLEoutperformed COCKTAILon both English and Romanian test documents."
    },
    {
        "gold": {
            "text": [
                "The second area where the methods described here could be improved is in the way that multiple information sources are integrated.",
                "The technique method described in section 3.7 is a fairly crude method for combining frequency information with symbolic data.",
                "It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (#REF).",
                "In particular , boosting ( #REF ; #TARGET_REF ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:The second area where the methods described here could be improved is in the way that multiple information sources are integrated. The technique method described in section 3.7 is a fairly crude method for combining frequency information with symbolic data. It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (#REF). In particular , boosting ( #REF ; #TARGET_REF ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .",
        "output": "conclusion:The second area where the methods described here could be improved is in the way that multiple information sources are integrated. The technique method described in section 3.7 is a fairly crude method for combining frequency information with symbolic data. It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (#REF). In particular , boosting ( #REF ; #TARGET_REF<MOT/> ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly ."
    },
    {
        "gold": {
            "text": [
                "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.",
                "They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #TARGET_REF ) .",
                "One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF).",
                "With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.",
                "After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #TARGET_REF ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases.",
        "output": "nan:Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #TARGET_REF<EXT/> ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases."
    },
    {
        "gold": {
            "text": [
                "We run GIZA + + ( #TARGET_REF ) on the training corpus in both directions ( #REF ) to obtain the word alignment for each sentence pair .",
                "We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (#REF) with modified Kneser-Ney smoothing (#REF).",
                "In our experiments the translation performances are measured by case-insensitive BLEU4 metric (#REF) and we use mteval-v13a.pl",
                "as the evaluation tool.",
                "The significance testing is performed by paired bootstrap re-sampling (#REF)."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We run GIZA + + ( #TARGET_REF ) on the training corpus in both directions ( #REF ) to obtain the word alignment for each sentence pair . We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (#REF) with modified Kneser-Ney smoothing (#REF). In our experiments the translation performances are measured by case-insensitive BLEU4 metric (#REF) and we use mteval-v13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling (#REF).",
        "output": "experiments:We run GIZA + + ( #TARGET_REF<FUT/> ) on the training corpus in both directions ( #REF ) to obtain the word alignment for each sentence pair . We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (#REF) with modified Kneser-Ney smoothing (#REF). In our experiments the translation performances are measured by case-insensitive BLEU4 metric (#REF) and we use mteval-v13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling (#REF)."
    },
    {
        "gold": {
            "text": [
                "In addition to the different response-generation methods, we have proposed a metalevel strategy to combine them.",
                "This kind of meta-learning is referred to as stacking by the Data Mining community (#REF).",
                "#REF implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version.",
                "They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #TARGET_REF as follows .",
                "The merging category corresponds to techniques where the individual methods affect each other in different ways (this category encompasses Burke's feature combination, cascade, feature augmentation, and meta-level sub-categories).",
                "The ensemble category corresponds to techniques where the predictions of the individual methods are combined to produce a final prediction (this category encompasses Burke's weighted, switching, and mixed sub-categories)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:In addition to the different response-generation methods, we have proposed a metalevel strategy to combine them. This kind of meta-learning is referred to as stacking by the Data Mining community (#REF). #REF implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version. They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #TARGET_REF as follows . The merging category corresponds to techniques where the individual methods affect each other in different ways (this category encompasses Burke's feature combination, cascade, feature augmentation, and meta-level sub-categories). The ensemble category corresponds to techniques where the predictions of the individual methods are combined to produce a final prediction (this category encompasses Burke's weighted, switching, and mixed sub-categories).",
        "output": "nan:In addition to the different response-generation methods, we have proposed a metalevel strategy to combine them. This kind of meta-learning is referred to as stacking by the Data Mining community (#REF). #REF implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version. They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #TARGET_REF<BACK/> as follows . The merging category corresponds to techniques where the individual methods affect each other in different ways (this category encompasses Burke's feature combination, cascade, feature augmentation, and meta-level sub-categories). The ensemble category corresponds to techniques where the predictions of the individual methods are combined to produce a final prediction (this category encompasses Burke's weighted, switching, and mixed sub-categories)."
    },
    {
        "gold": {
            "text": [
                "It is not aimed at handling dependencies , which require heavy use of lexical information ( #TARGET_REF , for PP attachment ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:It is not aimed at handling dependencies , which require heavy use of lexical information ( #TARGET_REF , for PP attachment ) .",
        "output": "conclusion:It is not aimed at handling dependencies , which require heavy use of lexical information ( #TARGET_REF<USE/> , for PP attachment ) ."
    },
    {
        "gold": {
            "text": [
                "In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.",
                "The model we rely on was originally developed by #REF and is based on a generalization of Latent Dirichlet Allocation.",
                "This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #TARGET_REF ; #REF ) .",
                "While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning.",
                "We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.",
                "Finally, we describe two ways to extend the model by incorporating three or more modalities.",
                "We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.",
                "We release both our code and data to the community for future research. 1"
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The model we rely on was originally developed by #REF and is based on a generalization of Latent Dirichlet Allocation. This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #TARGET_REF ; #REF ) . While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning. We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features. Finally, we describe two ways to extend the model by incorporating three or more modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning. We release both our code and data to the community for future research. 1",
        "output": "introduction:In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The model we rely on was originally developed by #REF and is based on a generalization of Latent Dirichlet Allocation. This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #TARGET_REF<COMP/> ; #REF ) . While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning. We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features. Finally, we describe two ways to extend the model by incorporating three or more modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning. We release both our code and data to the community for future research. 1"
    },
    {
        "gold": {
            "text": [
                "Table 2 depicts that testing each sentence with local training method takes 2.9 seconds, which is comparable to the testing time 2.0 seconds with global training method 4 .",
                "This shows that the local method is efficient.",
                "Further, compared to the retrieval, the local training is not the bottleneck.",
                "Actually , if we use LSH technique ( #TARGET_REF ) in retrieval process , the local method can be easily scaled to a larger training data ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "experiments:Table 2 depicts that testing each sentence with local training method takes 2.9 seconds, which is comparable to the testing time 2.0 seconds with global training method 4 . This shows that the local method is efficient. Further, compared to the retrieval, the local training is not the bottleneck. Actually , if we use LSH technique ( #TARGET_REF ) in retrieval process , the local method can be easily scaled to a larger training data .",
        "output": "experiments:Table 2 depicts that testing each sentence with local training method takes 2.9 seconds, which is comparable to the testing time 2.0 seconds with global training method 4 . This shows that the local method is efficient. Further, compared to the retrieval, the local training is not the bottleneck. Actually , if we use LSH technique ( #TARGET_REF<MOT/> ) in retrieval process , the local method can be easily scaled to a larger training data ."
    },
    {
        "gold": {
            "text": [
                "The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages.",
                "Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment.",
                "Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF).",
                "But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #TARGET_REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #TARGET_REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) .",
        "output": "introduction:The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #TARGET_REF<EXT/> ) ; and joint unsupervised POS and parser induction across languages ( #REF ) ."
    },
    {
        "gold": {
            "text": [
                "• If arc probabilities (or even λ, ν, µ, ρ) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f θ whose (input, output) matches (x, y).",
                "The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #TARGET_REF ; Chen and",
                "For globally normalized, joint models, the predicted vector is ec f (Σ * , ∆ * ).",
                "If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute). 13"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:• If arc probabilities (or even λ, ν, µ, ρ) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f θ whose (input, output) matches (x, y). The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #TARGET_REF ; Chen and For globally normalized, joint models, the predicted vector is ec f (Σ * , ∆ * ). If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute). 13",
        "output": "nan:• If arc probabilities (or even λ, ν, µ, ρ) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f θ whose (input, output) matches (x, y). The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #TARGET_REF<FUT/> ; Chen and For globally normalized, joint models, the predicted vector is ec f (Σ * , ∆ * ). If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute). 13"
    },
    {
        "gold": {
            "text": [
                "As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #REF ; #REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #REF ) and the CATiB ( #TARGET_REF ) .",
                "Recently, #REF analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.",
                "#REF reports experiments on Arabic parsing using his MaltParser (#REF), trained on the PADT.",
                "His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #REF ; #REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #REF ) and the CATiB ( #TARGET_REF ) . Recently, #REF analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. #REF reports experiments on Arabic parsing using his MaltParser (#REF), trained on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.",
        "output": "related work:As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #REF ; #REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #REF ) and the CATiB ( #TARGET_REF<BACK/> ) . Recently, #REF analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. #REF reports experiments on Arabic parsing using his MaltParser (#REF), trained on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser."
    },
    {
        "gold": {
            "text": [
                "However, these methodologies have not used historical information at the dialogue level as described here.",
                "In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.",
                "The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.",
                "Thus, an error in this work has no pattern but occurs probabilistically.",
                "A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.",
                "The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors.",
                "However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.",
                "Another dialogue acquisition system has been developed by #TARGET_REF .",
                "However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.",
                "The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.",
                "It self activates to bias recognition toward historically observed patterns but is not otherwise observable."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #TARGET_REF . However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user. It self activates to bias recognition toward historically observed patterns but is not otherwise observable.",
        "output": "nan:However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #TARGET_REF<USE/> . However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user. It self activates to bias recognition toward historically observed patterns but is not otherwise observable."
    },
    {
        "gold": {
            "text": [
                "WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #TARGET_REFb ) , a video-recording programming system , a schedule management system ( #REFa ) , and a weather infomiation system ( #REF ) .",
                "The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.",
                "A sample dialogue between this system and a naive user is shown in Figure 2.",
                "This system employs HTK as the speech recognition engine.",
                "The weather information system can answer the user's questions about weather forecasts in Japan.",
                "The vocabulary size is around 500, and the number of phrase structure rules is 31.",
                "The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #TARGET_REFb ) , a video-recording programming system , a schedule management system ( #REFa ) , and a weather infomiation system ( #REF ) . The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue between this system and a naive user is shown in Figure 2. This system employs HTK as the speech recognition engine. The weather information system can answer the user's questions about weather forecasts in Japan. The vocabulary size is around 500, and the number of phrase structure rules is 31. The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000.",
        "output": "experiments:WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #TARGET_REF<COMP/>b ) , a video-recording programming system , a schedule management system ( #REFa ) , and a weather infomiation system ( #REF ) . The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue between this system and a naive user is shown in Figure 2. This system employs HTK as the speech recognition engine. The weather information system can answer the user's questions about weather forecasts in Japan. The vocabulary size is around 500, and the number of phrase structure rules is 31. The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000."
    },
    {
        "gold": {
            "text": [
                "In a similar vain to #TARGET_REF and #REF , the method extends an existing flat shallow-parsing method to handle composite structures ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:In a similar vain to #TARGET_REF and #REF , the method extends an existing flat shallow-parsing method to handle composite structures .",
        "output": "conclusion:In a similar vain to #TARGET_REF<MOT/> and #REF , the method extends an existing flat shallow-parsing method to handle composite structures ."
    },
    {
        "gold": {
            "text": [
                "It can be shown ( #TARGET_REF ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "method:It can be shown ( #TARGET_REF ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .",
        "output": "method:It can be shown ( #TARGET_REF<EXT/> ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events ."
    },
    {
        "gold": {
            "text": [
                "The idea behind the Doc-Pred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected.",
                "In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #REF ; #TARGET_REF ) .",
                "We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2).",
                "The input to Snob is a set of binary vectors, one vector per response document.",
                "The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency).",
                "The predictive model is a Decision Graph (#REF), which, like Snob, is based on the MML principle."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:The idea behind the Doc-Pred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #REF ; #TARGET_REF ) . We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency). The predictive model is a Decision Graph (#REF), which, like Snob, is based on the MML principle.",
        "output": "method:The idea behind the Doc-Pred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #REF ; #TARGET_REF<FUT/> ) . We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency). The predictive model is a Decision Graph (#REF), which, like Snob, is based on the MML principle."
    },
    {
        "gold": {
            "text": [
                "Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.",
                "For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #TARGET_REF ) , ( Al-Adhaileh & #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #TARGET_REF ) , ( Al-Adhaileh & #REF ) .",
        "output": "nan:Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #TARGET_REF<BACK/> ) , ( Al-Adhaileh & #REF ) ."
    },
    {
        "gold": {
            "text": [
                "Our reordering model is closely related to the model proposed by Zhang and Gildea (2005;2007a), with respect to conditioning the reordering predictions on lexical items.",
                "These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words.",
                "With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( #TARGET_REF ) .",
                "However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Our reordering model is closely related to the model proposed by Zhang and Gildea (2005;2007a), with respect to conditioning the reordering predictions on lexical items. These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words. With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( #TARGET_REF ) . However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model.",
        "output": "related work:Our reordering model is closely related to the model proposed by Zhang and Gildea (2005;2007a), with respect to conditioning the reordering predictions on lexical items. These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words. With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( #TARGET_REF<USE/> ) . However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model."
    },
    {
        "gold": {
            "text": [
                "Definite relations are a convenient way of encoding the interaction of lexical rules, as they readily support various program transformations to improve the encoding: We show that the definite relations produced by the compiler can be refined by program transformation techniques to increase efficiency.",
                "The resulting encoding allows the execution of lexical rules on-the-fly, i.e., coroutined with other constraints at some time after lexical lookup.",
                "The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #TARGET_REF , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program ."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:Definite relations are a convenient way of encoding the interaction of lexical rules, as they readily support various program transformations to improve the encoding: We show that the definite relations produced by the compiler can be refined by program transformation techniques to increase efficiency. The resulting encoding allows the execution of lexical rules on-the-fly, i.e., coroutined with other constraints at some time after lexical lookup. The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #TARGET_REF , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program .",
        "output": "introduction:Definite relations are a convenient way of encoding the interaction of lexical rules, as they readily support various program transformations to improve the encoding: We show that the definite relations produced by the compiler can be refined by program transformation techniques to increase efficiency. The resulting encoding allows the execution of lexical rules on-the-fly, i.e., coroutined with other constraints at some time after lexical lookup. The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #TARGET_REF<COMP/> , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program ."
    },
    {
        "gold": {
            "text": [
                "Although exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.",
                "In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled.",
                "The true utility of content models is to structure abstracts that have no structure to begin with.",
                "Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance.",
                "Such a component would serve as the first stage of a clinical question answering system ( #TARGET_REF ) or summarization system ( #REF ) .",
                "We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Although exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process. In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled. The true utility of content models is to structure abstracts that have no structure to begin with. Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. Such a component would serve as the first stage of a clinical question answering system ( #TARGET_REF ) or summarization system ( #REF ) . We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured.",
        "output": "conclusion:Although exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process. In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled. The true utility of content models is to structure abstracts that have no structure to begin with. Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. Such a component would serve as the first stage of a clinical question answering system ( #TARGET_REF<MOT/> ) or summarization system ( #REF ) . We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured."
    },
    {
        "gold": {
            "text": [
                "One important type of lexical information is the subcategorization requirements of an entry (i.e., the arguments a predicate must take in order to form a grammatical construction).",
                "Lexicons, including subcategorization details, were traditionally produced by hand.",
                "However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component.",
                "In addition, subcategorization requirements may vary across linguistic domain or genre (#REF).",
                "#TARGET_REF argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .",
                "Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:One important type of lexical information is the subcategorization requirements of an entry (i.e., the arguments a predicate must take in order to form a grammatical construction). Lexicons, including subcategorization details, were traditionally produced by hand. However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component. In addition, subcategorization requirements may vary across linguistic domain or genre (#REF). #TARGET_REF argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature . Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue.",
        "output": "introduction:One important type of lexical information is the subcategorization requirements of an entry (i.e., the arguments a predicate must take in order to form a grammatical construction). Lexicons, including subcategorization details, were traditionally produced by hand. However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component. In addition, subcategorization requirements may vary across linguistic domain or genre (#REF). #TARGET_REF<EXT/> argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature . Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue."
    },
    {
        "gold": {
            "text": [
                "We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #TARGET_REF ; #REF ) .4",
                "This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags.",
                "These counts are then combined with estimates of the �openness� of each tag in order to assess its likelihood of appearing with new words."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #TARGET_REF ; #REF ) .4 This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags. These counts are then combined with estimates of the �openness� of each tag in order to assess its likelihood of appearing with new words.",
        "output": "method:We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #TARGET_REF<FUT/> ; #REF ) .4 This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags. These counts are then combined with estimates of the �openness� of each tag in order to assess its likelihood of appearing with new words."
    },
    {
        "gold": {
            "text": [
                "Surveys and articles on the topic include #REF , de #REF , and #TARGET_REF .",
                "Still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self-contained."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Surveys and articles on the topic include #REF , de #REF , and #TARGET_REF . Still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self-contained.",
        "output": "nan:Surveys and articles on the topic include #REF , de #REF , and #TARGET_REF<BACK/> . Still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self-contained."
    },
    {
        "gold": {
            "text": [
                "We see that the image modalities are much more useful than they are in compositionality prediction.",
                "The SURF modality does extremely well in particular, but the GIST features also provide statistically significant improvements over the text-only model.",
                "Since the SURF and GIST image features tend to capture object-likeness and scene-likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them.",
                "This seems to provide additional evidence of #TARGET_REFb ) 's suggestion that something like a distributional hypothesis of images is plausible ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:We see that the image modalities are much more useful than they are in compositionality prediction. The SURF modality does extremely well in particular, but the GIST features also provide statistically significant improvements over the text-only model. Since the SURF and GIST image features tend to capture object-likeness and scene-likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them. This seems to provide additional evidence of #TARGET_REFb ) 's suggestion that something like a distributional hypothesis of images is plausible .",
        "output": "experiments:We see that the image modalities are much more useful than they are in compositionality prediction. The SURF modality does extremely well in particular, but the GIST features also provide statistically significant improvements over the text-only model. Since the SURF and GIST image features tend to capture object-likeness and scene-likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them. This seems to provide additional evidence of #TARGET_REF<USE/>b ) 's suggestion that something like a distributional hypothesis of images is plausible ."
    },
    {
        "gold": {
            "text": [
                "The shape-based metric.",
                "The only disambiguation metric that we used in our previous work ( #TARGET_REFb ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right .",
                "The explanation for this metric is that text processing is, essentially, a left-to-right process.",
                "In many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels.",
                "I The more text writers add, the more they elaborate on the text that went before: as a consequence, incremental discourse building consists mostly of expansion of the tight branches.",
                "According to the shape-based metric, we consider that a discourse tree A is \"better\" than another discourse tree B if A is more skewed to the right than B (see Marcu (1997c) for a mathematical formulation of the notion of skewedness)."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:The shape-based metric. The only disambiguation metric that we used in our previous work ( #TARGET_REFb ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right . The explanation for this metric is that text processing is, essentially, a left-to-right process. In many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels. I The more text writers add, the more they elaborate on the text that went before: as a consequence, incremental discourse building consists mostly of expansion of the tight branches. According to the shape-based metric, we consider that a discourse tree A is \"better\" than another discourse tree B if A is more skewed to the right than B (see Marcu (1997c) for a mathematical formulation of the notion of skewedness).",
        "output": "nan:The shape-based metric. The only disambiguation metric that we used in our previous work ( #TARGET_REF<COMP/>b ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right . The explanation for this metric is that text processing is, essentially, a left-to-right process. In many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels. I The more text writers add, the more they elaborate on the text that went before: as a consequence, incremental discourse building consists mostly of expansion of the tight branches. According to the shape-based metric, we consider that a discourse tree A is \"better\" than another discourse tree B if A is more skewed to the right than B (see Marcu (1997c) for a mathematical formulation of the notion of skewedness)."
    },
    {
        "gold": {
            "text": [
                "Although exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.",
                "In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled.",
                "The true utility of content models is to structure abstracts that have no structure to begin with.",
                "Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance.",
                "Such a component would serve as the first stage of a clinical question answering system ( #REF ) or summarization system ( #TARGET_REF ) .",
                "We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Although exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process. In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled. The true utility of content models is to structure abstracts that have no structure to begin with. Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. Such a component would serve as the first stage of a clinical question answering system ( #REF ) or summarization system ( #TARGET_REF ) . We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured.",
        "output": "conclusion:Although exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process. In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled. The true utility of content models is to structure abstracts that have no structure to begin with. Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. Such a component would serve as the first stage of a clinical question answering system ( #REF ) or summarization system ( #TARGET_REF<MOT/> ) . We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured."
    },
    {
        "gold": {
            "text": [
                "Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors.",
                "In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.",
                "For example, modeling CASE in Czech improves Czech parsing (#REF): CASE is relevant, not redundant, and can be predicted with sufficient accuracy.",
                "It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and #REF ; Nivre , Boguslavsky , and #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors. In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. For example, modeling CASE in Czech improves Czech parsing (#REF): CASE is relevant, not redundant, and can be predicted with sufficient accuracy. It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and #REF ; Nivre , Boguslavsky , and #REF ; #TARGET_REF ) .",
        "output": "introduction:Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors. In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. For example, modeling CASE in Czech improves Czech parsing (#REF): CASE is relevant, not redundant, and can be predicted with sufficient accuracy. It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and #REF ; Nivre , Boguslavsky , and #REF ; #TARGET_REF<EXT/> ) ."
    },
    {
        "gold": {
            "text": [
                "To date, four distinct domain-specific versions of TINA have been implemented.",
                "The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #TARGET_REF ) .",
                "The second version (RM) concerns the Resource Management task (#REF) that has been popular within the DARPA community in recent years.",
                "The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (#REF).",
                "The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.",
                "A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:To date, four distinct domain-specific versions of TINA have been implemented. The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #TARGET_REF ) . The second version (RM) concerns the Resource Management task (#REF) that has been popular within the DARPA community in recent years. The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (#REF). The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region. A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community.",
        "output": "nan:To date, four distinct domain-specific versions of TINA have been implemented. The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #TARGET_REF<FUT/> ) . The second version (RM) concerns the Resource Management task (#REF) that has been popular within the DARPA community in recent years. The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (#REF). The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region. A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community."
    },
    {
        "gold": {
            "text": [
                "Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #TARGET_REF for an overview ) .",
                "Others have looked at the application of machine learning algorithms to annotated multimodal corpora.",
                "For example, #REF and #REF find that machine learning algorithms can be trained to recognise some of the functions of head movements, while #REF show that there is a dependence between focus of attention and assignment of dialogue act labels.",
                "Related are also the studies by Rieks op den #REF and #REF: both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #TARGET_REF for an overview ) . Others have looked at the application of machine learning algorithms to annotated multimodal corpora. For example, #REF and #REF find that machine learning algorithms can be trained to recognise some of the functions of head movements, while #REF show that there is a dependence between focus of attention and assignment of dialogue act labels. Related are also the studies by Rieks op den #REF and #REF: both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus.",
        "output": "introduction:Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #TARGET_REF<BACK/> for an overview ) . Others have looked at the application of machine learning algorithms to annotated multimodal corpora. For example, #REF and #REF find that machine learning algorithms can be trained to recognise some of the functions of head movements, while #REF show that there is a dependence between focus of attention and assignment of dialogue act labels. Related are also the studies by Rieks op den #REF and #REF: both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus."
    },
    {
        "gold": {
            "text": [
                "The problem of handling ill-formed input has been studied by #REF , #REF , #REF , #REF , #REF , #TARGET_REF , #REF , and #REF .",
                "A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.",
                "However, these methodologies have not used historical information at the dialogue level as described here.",
                "In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.",
                "The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.",
                "Thus, an error in this work has no pattern but occurs probabilistically.",
                "A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.",
                "The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors.",
                "However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.",
                "Another dialogue acquisition system has been developed by #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The problem of handling ill-formed input has been studied by #REF , #REF , #REF , #REF , #REF , #TARGET_REF , #REF , and #REF . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #REF.",
        "output": "nan:The problem of handling ill-formed input has been studied by #REF , #REF , #REF , #REF , #REF , #TARGET_REF<USE/> , #REF , and #REF . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #REF."
    },
    {
        "gold": {
            "text": [
                "The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.",
                "The diagnoser , based on #TARGET_REFb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer .",
                "At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to #REF."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness. The diagnoser , based on #TARGET_REFb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer . At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to #REF.",
        "output": "experiments:The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness. The diagnoser , based on #TARGET_REF<COMP/>b ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer . At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to #REF."
    },
    {
        "gold": {
            "text": [
                "We plan to apply our method to wider range of classifiers used in various NLP tasks.",
                "To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs.",
                "When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( #TARGET_REF ) or a memory-efficient trie implementation based on a succinct data structure ( #REF ; #REF ) to reduce required memory usage ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:We plan to apply our method to wider range of classifiers used in various NLP tasks. To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs. When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( #TARGET_REF ) or a memory-efficient trie implementation based on a succinct data structure ( #REF ; #REF ) to reduce required memory usage .",
        "output": "conclusion:We plan to apply our method to wider range of classifiers used in various NLP tasks. To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs. When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( #TARGET_REF<MOT/> ) or a memory-efficient trie implementation based on a succinct data structure ( #REF ; #REF ) to reduce required memory usage ."
    },
    {
        "gold": {
            "text": [
                "The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.",
                "Labeled attachment score is used as our base model loss function.",
                "In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i.",
                "The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).",
                "The score is normalized by the summed arc lengths for the sentence.",
                "The labeled version of this score requires that the labels of the arc are also correct.",
                "Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #TARGET_REF ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (#REF) and textual entailment (#REF)."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well. Labeled attachment score is used as our base model loss function. In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i. The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise). The score is normalized by the summed arc lengths for the sentence. The labeled version of this score requires that the labels of the arc are also correct. Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #TARGET_REF ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (#REF) and textual entailment (#REF).",
        "output": "experiments:The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well. Labeled attachment score is used as our base model loss function. In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i. The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise). The score is normalized by the summed arc lengths for the sentence. The labeled version of this score requires that the labels of the arc are also correct. Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #TARGET_REF<EXT/> ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (#REF) and textual entailment (#REF)."
    },
    {
        "gold": {
            "text": [
                "the mention sub-type , which is a sub-category of the mention type ( #TARGET_REF ) ( e.g. OrgGovernmental , FacilityPath , etc. ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:the mention sub-type , which is a sub-category of the mention type ( #TARGET_REF ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .",
        "output": "nan:the mention sub-type , which is a sub-category of the mention type ( #TARGET_REF<FUT/> ) ( e.g. OrgGovernmental , FacilityPath , etc. ) ."
    },
    {
        "gold": {
            "text": [
                "There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.",
                "Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #TARGET_REF ) , such as the relation between syntax and semantic ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation. Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #TARGET_REF ) , such as the relation between syntax and semantic .",
        "output": "nan:There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation. Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #TARGET_REF<BACK/> ) , such as the relation between syntax and semantic ."
    },
    {
        "gold": {
            "text": [
                "The problem of handling ill-formed input has been studied by #REF , #REF , #TARGET_REF , #REF , #REF , #REF , #REF , and #REF .",
                "A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.",
                "However, these methodologies have not used historical information at the dialogue level as described here.",
                "In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.",
                "The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.",
                "Thus, an error in this work has no pattern but occurs probabilistically.",
                "A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.",
                "The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors.",
                "However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.",
                "Another dialogue acquisition system has been developed by #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The problem of handling ill-formed input has been studied by #REF , #REF , #TARGET_REF , #REF , #REF , #REF , #REF , and #REF . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #REF.",
        "output": "nan:The problem of handling ill-formed input has been studied by #REF , #REF , #TARGET_REF<USE/> , #REF , #REF , #REF , #REF , and #REF . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #REF."
    },
    {
        "gold": {
            "text": [
                "The research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations.",
                "Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #TARGET_REF ) .",
                "However, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans.",
                "In addition, because that work was based on features synthesized over the entire dialogues, the hypotheses that were learned could not be used for prediction during runtime."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "conclusion:The research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations. Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #TARGET_REF ) . However, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans. In addition, because that work was based on features synthesized over the entire dialogues, the hypotheses that were learned could not be used for prediction during runtime.",
        "output": "conclusion:The research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations. Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #TARGET_REF<COMP/> ) . However, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans. In addition, because that work was based on features synthesized over the entire dialogues, the hypotheses that were learned could not be used for prediction during runtime."
    },
    {
        "gold": {
            "text": [
                "The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #TARGET_REFa ) .",
                "A fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge-poor methods are.",
                "In particular, more research should be carried out on the factors influencing the performance of these algorithms.",
                "One of the impediments to the evaluation or fuller utilization of machine learning techniques is the lack of widely available corpora annotated for anaphoric or coreferential links.",
                "More work toward the proposal of consistent and comprehensive evaluation is necessary; so too is work in multilingual contexts.",
                "Some of these challenges have been addressed in the papers published in this issue, but ongoing research will continue to address them in the near future."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #TARGET_REFa ) . A fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge-poor methods are. In particular, more research should be carried out on the factors influencing the performance of these algorithms. One of the impediments to the evaluation or fuller utilization of machine learning techniques is the lack of widely available corpora annotated for anaphoric or coreferential links. More work toward the proposal of consistent and comprehensive evaluation is necessary; so too is work in multilingual contexts. Some of these challenges have been addressed in the papers published in this issue, but ongoing research will continue to address them in the near future.",
        "output": "nan:The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #TARGET_REF<MOT/>a ) . A fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge-poor methods are. In particular, more research should be carried out on the factors influencing the performance of these algorithms. One of the impediments to the evaluation or fuller utilization of machine learning techniques is the lack of widely available corpora annotated for anaphoric or coreferential links. More work toward the proposal of consistent and comprehensive evaluation is necessary; so too is work in multilingual contexts. Some of these challenges have been addressed in the papers published in this issue, but ongoing research will continue to address them in the near future."
    },
    {
        "gold": {
            "text": [
                "In this paper , inspired by KNN-SVM ( #TARGET_REF ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .",
                "Compared with global training methods, such as MERT, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing.",
                "This online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences.",
                "Similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency.",
                "To put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:In this paper , inspired by KNN-SVM ( #TARGET_REF ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems . Compared with global training methods, such as MERT, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing. This online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences. Similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency. To put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set.",
        "output": "introduction:In this paper , inspired by KNN-SVM ( #TARGET_REF<EXT/> ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems . Compared with global training methods, such as MERT, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing. This online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences. Similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency. To put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set."
    },
    {
        "gold": {
            "text": [
                "Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Gr�nnum (2006).",
                "The Praat tool was used ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Gr�nnum (2006). The Praat tool was used ( #TARGET_REF ) .",
        "output": "introduction:Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Gr�nnum (2006). The Praat tool was used ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (#REF; #REF).",
                "These include, just to mention the most popular ones , DIRT ( #REF ) , VerbOcean ( #TARGET_REF ) , FrameNet ( #REF ) , and Wikipedia ( #REF ; #REF ) .",
                "DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.",
                "VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.",
                "FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.",
                "It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.",
                "Wikipedia is often used to extract probabilistic entailment rules based word similar- ity/relatedness scores."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (#REF; #REF). These include, just to mention the most popular ones , DIRT ( #REF ) , VerbOcean ( #TARGET_REF ) , FrameNet ( #REF ) , and Wikipedia ( #REF ; #REF ) . DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses. Wikipedia is often used to extract probabilistic entailment rules based word similar- ity/relatedness scores.",
        "output": "introduction:Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (#REF; #REF). These include, just to mention the most popular ones , DIRT ( #REF ) , VerbOcean ( #TARGET_REF<BACK/> ) , FrameNet ( #REF ) , and Wikipedia ( #REF ; #REF ) . DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses. Wikipedia is often used to extract probabilistic entailment rules based word similar- ity/relatedness scores."
    },
    {
        "gold": {
            "text": [
                "Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.",
                "In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF ; #REF ; #REF ; #REF; #REF ; #TARGET_REF ) .",
                "The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF ; #REF ; #REF ; #REF; #REF ; #TARGET_REF ) . The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.",
        "output": "related work:Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF ; #REF ; #REF ; #REF; #REF ; #TARGET_REF<USE/> ) . The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."
    },
    {
        "gold": {
            "text": [
                "7 In ISNotes, 71% of NP antecedents occur in the same or up to two sentences prior to the anaphor.",
                "Initial experiments show that increasing the window size more than two sentences decreases the performance. 8",
                "To deal with data imbalance, the SVM light parameter is set according to the ratio between positive and negative instances in the training set. 9",
                "To compare the learning-based approach to the rulebased system described in Section 3 directly, we report the mlSystem ruleFeats We provide mlSystem ruleFeats with the same knowledge resources as the rule-based system.",
                "All rules from the rule-based system are incorporated into mlSystem ruleFeats as the features.",
                "mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( #REF ; #TARGET_REFa ; #REFb ) on bridging anaphora recognition and antecedent selection .",
                "Some of these features overlap with the atomic features used in the rule-based system."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:7 In ISNotes, 71% of NP antecedents occur in the same or up to two sentences prior to the anaphor. Initial experiments show that increasing the window size more than two sentences decreases the performance. 8 To deal with data imbalance, the SVM light parameter is set according to the ratio between positive and negative instances in the training set. 9 To compare the learning-based approach to the rulebased system described in Section 3 directly, we report the mlSystem ruleFeats We provide mlSystem ruleFeats with the same knowledge resources as the rule-based system. All rules from the rule-based system are incorporated into mlSystem ruleFeats as the features. mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( #REF ; #TARGET_REFa ; #REFb ) on bridging anaphora recognition and antecedent selection . Some of these features overlap with the atomic features used in the rule-based system.",
        "output": "experiments:7 In ISNotes, 71% of NP antecedents occur in the same or up to two sentences prior to the anaphor. Initial experiments show that increasing the window size more than two sentences decreases the performance. 8 To deal with data imbalance, the SVM light parameter is set according to the ratio between positive and negative instances in the training set. 9 To compare the learning-based approach to the rulebased system described in Section 3 directly, we report the mlSystem ruleFeats We provide mlSystem ruleFeats with the same knowledge resources as the rule-based system. All rules from the rule-based system are incorporated into mlSystem ruleFeats as the features. mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( #REF ; #TARGET_REF<COMP/>a ; #REFb ) on bridging anaphora recognition and antecedent selection . Some of these features overlap with the atomic features used in the rule-based system."
    },
    {
        "gold": {
            "text": [
                "Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.",
                "For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #REF ; #REF ) and history-based parsing ( #TARGET_REF ) .",
                "We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF)."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #REF ; #REF ) and history-based parsing ( #TARGET_REF ) . We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF).",
        "output": "conclusion:Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #REF ; #REF ) and history-based parsing ( #TARGET_REF<MOT/> ) . We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF)."
    },
    {
        "gold": {
            "text": [
                "In this section we describe in detail the baseline NER system we use.",
                "It is inspired by the system described in #TARGET_REF .",
                "Because NER annotations are commonly not nested (for example, in the text \"the US Army\", \"US Army\" is treated as a single entity, instead of the location \"US\" and the organization \"US Army\") it is possible to treat NER as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity.",
                "Following #REF we use the BILOU encoding, where each token can either BEGIN an entity, be INSIDE an entity, be the LAST token in an entity, be OUTSIDE an entity, or be the single UNIQUE token in an entity."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:In this section we describe in detail the baseline NER system we use. It is inspired by the system described in #TARGET_REF . Because NER annotations are commonly not nested (for example, in the text \"the US Army\", \"US Army\" is treated as a single entity, instead of the location \"US\" and the organization \"US Army\") it is possible to treat NER as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity. Following #REF we use the BILOU encoding, where each token can either BEGIN an entity, be INSIDE an entity, be the LAST token in an entity, be OUTSIDE an entity, or be the single UNIQUE token in an entity.",
        "output": "introduction:In this section we describe in detail the baseline NER system we use. It is inspired by the system described in #TARGET_REF<EXT/> . Because NER annotations are commonly not nested (for example, in the text \"the US Army\", \"US Army\" is treated as a single entity, instead of the location \"US\" and the organization \"US Army\") it is possible to treat NER as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity. Following #REF we use the BILOU encoding, where each token can either BEGIN an entity, be INSIDE an entity, be the LAST token in an entity, be OUTSIDE an entity, or be the single UNIQUE token in an entity."
    },
    {
        "gold": {
            "text": [
                "For compound splitting, we follow #REF, using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the ge- ometric mean of word part frequencies.",
                "Other approaches use less deep linguistic resources (e.g., POS-tags #REF) or are (almost) knowledge-free (e.g., #REF). Compound merging is less well studied.",
                "#REF used a simple, list-based merging ap- proach, merging all consecutive words included in a merging list.",
                "This approach resulted in too many compounds.",
                "We follow #TARGET_REF , for compound merging .",
                "We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merg- ing) on one of our two test sets."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:For compound splitting, we follow #REF, using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the ge- ometric mean of word part frequencies. Other approaches use less deep linguistic resources (e.g., POS-tags #REF) or are (almost) knowledge-free (e.g., #REF). Compound merging is less well studied. #REF used a simple, list-based merging ap- proach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow #TARGET_REF , for compound merging . We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merg- ing) on one of our two test sets.",
        "output": "related work:For compound splitting, we follow #REF, using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the ge- ometric mean of word part frequencies. Other approaches use less deep linguistic resources (e.g., POS-tags #REF) or are (almost) knowledge-free (e.g., #REF). Compound merging is less well studied. #REF used a simple, list-based merging ap- proach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow #TARGET_REF<FUT/> , for compound merging . We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merg- ing) on one of our two test sets."
    },
    {
        "gold": {
            "text": [
                "We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #REF , #TARGET_REF ) .",
                "Before we do this, consider the tractability of the original IA.",
                "If the running time of FindBestValue(r, A i ) is a constant times the number of Values of the Attribute A i , then the worst-case running time of IA (and IA Plur ) is O(n v n a ), where n a equals the number of Attributes in the language and n v the average number of Values of all Attributes.",
                "This is because, in the worst case, all Values of all Attributes need to be attempted (#REF).",
                "As for the new algorithm, we focus on the crucial phases 2, 4, and 5."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #REF , #TARGET_REF ) . Before we do this, consider the tractability of the original IA. If the running time of FindBestValue(r, A i ) is a constant times the number of Values of the Attribute A i , then the worst-case running time of IA (and IA Plur ) is O(n v n a ), where n a equals the number of Attributes in the language and n v the average number of Values of all Attributes. This is because, in the worst case, all Values of all Attributes need to be attempted (#REF). As for the new algorithm, we focus on the crucial phases 2, 4, and 5.",
        "output": "introduction:We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #REF , #TARGET_REF<BACK/> ) . Before we do this, consider the tractability of the original IA. If the running time of FindBestValue(r, A i ) is a constant times the number of Values of the Attribute A i , then the worst-case running time of IA (and IA Plur ) is O(n v n a ), where n a equals the number of Attributes in the language and n v the average number of Values of all Attributes. This is because, in the worst case, all Values of all Attributes need to be attempted (#REF). As for the new algorithm, we focus on the crucial phases 2, 4, and 5."
    },
    {
        "gold": {
            "text": [
                "1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.",
                "This step significantly reduces the computational burden of the algorithm.",
                "It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #REF ; #REF ; #TARGET_REF ) .",
                "To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #REF ; #REF ; #TARGET_REF ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.",
        "output": "method:1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #REF ; #REF ; #TARGET_REF<USE/> ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly."
    },
    {
        "gold": {
            "text": [
                "In P2, on the other hand, we recast SC as a se- quence labeling task.",
                "In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance la- bel for each post in the input post sequence.",
                "This choice is motivated by an observation we made previously ( #TARGET_REFa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3"
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:In P2, on the other hand, we recast SC as a se- quence labeling task. In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance la- bel for each post in the input post sequence. This choice is motivated by an observation we made previously ( #TARGET_REFa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3",
        "output": "experiments:In P2, on the other hand, we recast SC as a se- quence labeling task. In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance la- bel for each post in the input post sequence. This choice is motivated by an observation we made previously ( #TARGET_REF<COMP/>a ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3"
    },
    {
        "gold": {
            "text": [
                "In the longer term, it is clear that neither the contents nor form of any existing published dictionary meet all the requirements of a natural language processing system.",
                "A substantial component of the research reported above has been devoted to restructuring LDOCE to make it more suitable for automatic analysis.",
                "However, even after this process much of the information in LDOCE remains difficult to access, essentially because it is aimed at a human reader, as opposed to a computer system.",
                "This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis.",
                "In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #TARGET_REF ) .",
                "However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:In the longer term, it is clear that neither the contents nor form of any existing published dictionary meet all the requirements of a natural language processing system. A substantial component of the research reported above has been devoted to restructuring LDOCE to make it more suitable for automatic analysis. However, even after this process much of the information in LDOCE remains difficult to access, essentially because it is aimed at a human reader, as opposed to a computer system. This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis. In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #TARGET_REF ) . However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly.",
        "output": "conclusion:In the longer term, it is clear that neither the contents nor form of any existing published dictionary meet all the requirements of a natural language processing system. A substantial component of the research reported above has been devoted to restructuring LDOCE to make it more suitable for automatic analysis. However, even after this process much of the information in LDOCE remains difficult to access, essentially because it is aimed at a human reader, as opposed to a computer system. This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis. In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #TARGET_REF<MOT/> ) . However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly."
    },
    {
        "gold": {
            "text": [
                "The DM features also include running tallies for the number of reprompts (num-reprompts), number of confirmation prompts (num.confirms), and number of subdialogue prompts (num-subdials), that had been played up to each point in the diMogue, as well as running percentages (percent-reprompts, percentconfirms, percent-subdials).",
                "The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #TARGET_REF ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:The DM features also include running tallies for the number of reprompts (num-reprompts), number of confirmation prompts (num.confirms), and number of subdialogue prompts (num-subdials), that had been played up to each point in the diMogue, as well as running percentages (percent-reprompts, percentconfirms, percent-subdials). The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #TARGET_REF ) .",
        "output": "nan:The DM features also include running tallies for the number of reprompts (num-reprompts), number of confirmation prompts (num.confirms), and number of subdialogue prompts (num-subdials), that had been played up to each point in the diMogue, as well as running percentages (percent-reprompts, percentconfirms, percent-subdials). The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #TARGET_REF<EXT/> ) ."
    },
    {
        "gold": {
            "text": [
                "The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #TARGET_REF ) .",
                "It is constituted of two parts.",
                "One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.",
                "The other is a dictionary which lists the frames of all the labeled predicates.",
                "Figure 1 is an example from the PropBank 1 .",
                "We put the word-by-word translation and the translation of the whole sentence below the example.",
                "It is quite a complex sentence, as there are many semantic roles in it.",
                "In this sentence, all the semantic roles of the verb 提供 (provide) are presented in the syntactic tree.",
                "We can separate the semantic roles into two groups."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #TARGET_REF ) . It is constituted of two parts. One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank. The other is a dictionary which lists the frames of all the labeled predicates. Figure 1 is an example from the PropBank 1 . We put the word-by-word translation and the translation of the whole sentence below the example. It is quite a complex sentence, as there are many semantic roles in it. In this sentence, all the semantic roles of the verb 提供 (provide) are presented in the syntactic tree. We can separate the semantic roles into two groups.",
        "output": "nan:The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #TARGET_REF<FUT/> ) . It is constituted of two parts. One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank. The other is a dictionary which lists the frames of all the labeled predicates. Figure 1 is an example from the PropBank 1 . We put the word-by-word translation and the translation of the whole sentence below the example. It is quite a complex sentence, as there are many semantic roles in it. In this sentence, all the semantic roles of the verb 提供 (provide) are presented in the syntactic tree. We can separate the semantic roles into two groups."
    },
    {
        "gold": {
            "text": [
                "Shallow parsing is studied as an alternative to full-sentence parsing.",
                "Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (#REF;#REF;#REF).",
                "A lot of recent work on shallow parsing has been influenced by Abney's work (#REF), who has suggested to \"chunk\" sentences to base level phrases.",
                "For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September .\"",
                "would be chunked as follows (Tjong Kim #REF): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.",
                "Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (#REF;#REF;#REF). A lot of recent work on shallow parsing has been influenced by Abney's work (#REF), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September .\" would be chunked as follows (Tjong Kim #REF): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) .",
        "output": "introduction:Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (#REF;#REF;#REF). A lot of recent work on shallow parsing has been influenced by Abney's work (#REF), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September .\" would be chunked as follows (Tjong Kim #REF): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #TARGET_REF<BACK/> ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) ."
    },
    {
        "gold": {
            "text": [
                "9 We do not relate to specific results in their study because it has been brought to our attention that #TARGET_REF are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:9 We do not relate to specific results in their study because it has been brought to our attention that #TARGET_REF are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) .",
        "output": "related work:9 We do not relate to specific results in their study because it has been brought to our attention that #TARGET_REF<USE/> are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) ."
    },
    {
        "gold": {
            "text": [
                "Following our previous work ( #TARGET_REF ; Althaus , Karamanis , and #REF ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists .",
                "A set of candidate orderings is produced by creating different permutations of these lists.",
                "A metric of coherence uses features from centering to compute a score for each candidate ordering and select the highest scoring ordering as the output. 9",
                "A wide range of metrics of coherence can be defined in centering's terms, simply on the basis of the work we reviewed in Section 3. To exemplify this, let us first assume that the ordering in Example (3), which is analyzed as a sequence of CF lists in Table 5, is a candidate ordering.",
                "Table 6 summarizes the NOCBs, the violations of COHERENCE, SALIENCE, and CHEAPNESS, and the centering transitions for this ordering. 10",
                "he candidate ordering contains two NOCBs in sentences (3e) and (3f)."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:Following our previous work ( #TARGET_REF ; Althaus , Karamanis , and #REF ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists . A set of candidate orderings is produced by creating different permutations of these lists. A metric of coherence uses features from centering to compute a score for each candidate ordering and select the highest scoring ordering as the output. 9 A wide range of metrics of coherence can be defined in centering's terms, simply on the basis of the work we reviewed in Section 3. To exemplify this, let us first assume that the ordering in Example (3), which is analyzed as a sequence of CF lists in Table 5, is a candidate ordering. Table 6 summarizes the NOCBs, the violations of COHERENCE, SALIENCE, and CHEAPNESS, and the centering transitions for this ordering. 10 he candidate ordering contains two NOCBs in sentences (3e) and (3f).",
        "output": "nan:Following our previous work ( #TARGET_REF<COMP/> ; Althaus , Karamanis , and #REF ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists . A set of candidate orderings is produced by creating different permutations of these lists. A metric of coherence uses features from centering to compute a score for each candidate ordering and select the highest scoring ordering as the output. 9 A wide range of metrics of coherence can be defined in centering's terms, simply on the basis of the work we reviewed in Section 3. To exemplify this, let us first assume that the ordering in Example (3), which is analyzed as a sequence of CF lists in Table 5, is a candidate ordering. Table 6 summarizes the NOCBs, the violations of COHERENCE, SALIENCE, and CHEAPNESS, and the centering transitions for this ordering. 10 he candidate ordering contains two NOCBs in sentences (3e) and (3f)."
    },
    {
        "gold": {
            "text": [
                "Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.",
                "For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #REF ; #TARGET_REF ) and history-based parsing ( #REF ) .",
                "We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF)."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #REF ; #TARGET_REF ) and history-based parsing ( #REF ) . We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF).",
        "output": "conclusion:Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #REF ; #TARGET_REF<MOT/> ) and history-based parsing ( #REF ) . We could also introduce new variables, e.g., nonterminal refinements (#REF), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (#REF;#REF)."
    },
    {
        "gold": {
            "text": [
                "To deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of unknown word.",
                "For example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.",
                "Each component will return a confidence measure of the reliability of its prediction , c.f. ( #TARGET_REF ) .",
                "The results from each component are evaluated to determine the final category of the word."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:To deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of unknown word. For example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc. Each component will return a confidence measure of the reliability of its prediction , c.f. ( #TARGET_REF ) . The results from each component are evaluated to determine the final category of the word.",
        "output": "experiments:To deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of unknown word. For example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc. Each component will return a confidence measure of the reliability of its prediction , c.f. ( #TARGET_REF<EXT/> ) . The results from each component are evaluated to determine the final category of the word."
    },
    {
        "gold": {
            "text": [
                "The strategy decision made by the tutorial planner, together with relevant semantic content from the student's answer (e.g., part of the answer to confirm), is passed to content planning and generation.",
                "The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( #TARGET_REF ) generation system to produce the appropriate text .",
                "Templates are used to generate some stock phrases such as \"When you are ready, go on to the next slide.\""
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The strategy decision made by the tutorial planner, together with relevant semantic content from the student's answer (e.g., part of the answer to confirm), is passed to content planning and generation. The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( #TARGET_REF ) generation system to produce the appropriate text . Templates are used to generate some stock phrases such as \"When you are ready, go on to the next slide.\"",
        "output": "experiments:The strategy decision made by the tutorial planner, together with relevant semantic content from the student's answer (e.g., part of the answer to confirm), is passed to content planning and generation. The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( #TARGET_REF<FUT/> ) generation system to produce the appropriate text . Templates are used to generate some stock phrases such as \"When you are ready, go on to the next slide.\""
    },
    {
        "gold": {
            "text": [
                "Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.",
                "#REF relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.",
                "The frames do not include details of specific prepositions.",
                "Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.",
                "#TARGET_REF run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .",
                "The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.",
                "#REF employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames.",
                "#REF attempts to improve on the approach of #REF by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon. #REF relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames. The frames do not include details of specific prepositions. Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames. #TARGET_REF run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes . The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. #REF employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #REF attempts to improve on the approach of #REF by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.",
        "output": "related work:Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon. #REF relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames. The frames do not include details of specific prepositions. Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames. #TARGET_REF<BACK/> run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes . The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. #REF employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #REF attempts to improve on the approach of #REF by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur."
    },
    {
        "gold": {
            "text": [
                "Viewing and and his as inflection is problematic since a mapping from the English phrase and his brother to the Arabic stem for brother is required.",
                "The situation is worse if there are English words (e.g., adjectives) separating his and brother.",
                "This required mapping is a significant problem for generalization.",
                "We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection).",
                "We apply a �split in preprocessing and resynthesize in postprocessing� approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al.",
                "The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.",
                "There has been other work on solving inflection.",
                "#TARGET_REF introduced factored SMT .",
                "We use more complex context features.",
                "#REF tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.",
                "#REF improved on this by marking prepositions with the case they mark (one of the most important markups in our system).",
                "Both efforts were ineffective on large data sets."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Viewing and and his as inflection is problematic since a mapping from the English phrase and his brother to the Arabic stem for brother is required. The situation is worse if there are English words (e.g., adjectives) separating his and brother. This required mapping is a significant problem for generalization. We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection). We apply a �split in preprocessing and resynthesize in postprocessing� approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al. The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. #TARGET_REF introduced factored SMT . We use more complex context features. #REF tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. #REF improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets.",
        "output": "related work:Viewing and and his as inflection is problematic since a mapping from the English phrase and his brother to the Arabic stem for brother is required. The situation is worse if there are English words (e.g., adjectives) separating his and brother. This required mapping is a significant problem for generalization. We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection). We apply a �split in preprocessing and resynthesize in postprocessing� approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al. The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. #TARGET_REF<USE/> introduced factored SMT . We use more complex context features. #REF tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. #REF improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets."
    },
    {
        "gold": {
            "text": [
                "A brief version of this work, with some additional material, first appeared as ( #TARGET_REFa ) .",
                "A leisurely journal-length version with more details has been prepared and is available."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:A brief version of this work, with some additional material, first appeared as ( #TARGET_REFa ) . A leisurely journal-length version with more details has been prepared and is available.",
        "output": "introduction:A brief version of this work, with some additional material, first appeared as ( #TARGET_REF<COMP/>a ) . A leisurely journal-length version with more details has been prepared and is available."
    },
    {
        "gold": {
            "text": [
                "When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.",
                "In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.",
                "Note that this ensures that greater importance is attributed to longer chunks, as is usual in most EBMT systems (cf.",
                "#REF;#REF;#REF). 7",
                "s an example, consider the translation into French of the house collapsed.",
                "Assume the conditional probabilities in ( 33  These mistranslations are all caused by boundary friction.",
                "Each of the translations in ( 37) and ( 38) would be output with an associated weight and ranked by the system.",
                "We would like to incorporate into our model a procedure whereby translation chunks extracted from the phrasal and marker lexicons are more highly regarded than those constructed by inserting words from the word-level lexicon into generalized marker chunks.",
                "That is, we want to allocate a larger portion of the probability space to the phrasal and marker lexicons than to the generalized or wordlevel lexicons.",
                "We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( #TARGET_REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "introduction:When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string. In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. Note that this ensures that greater importance is attributed to longer chunks, as is usual in most EBMT systems (cf. #REF;#REF;#REF). 7 s an example, consider the translation into French of the house collapsed. Assume the conditional probabilities in ( 33  These mistranslations are all caused by boundary friction. Each of the translations in ( 37) and ( 38) would be output with an associated weight and ranked by the system. We would like to incorporate into our model a procedure whereby translation chunks extracted from the phrasal and marker lexicons are more highly regarded than those constructed by inserting words from the word-level lexicon into generalized marker chunks. That is, we want to allocate a larger portion of the probability space to the phrasal and marker lexicons than to the generalized or wordlevel lexicons. We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( #TARGET_REF ) .",
        "output": "introduction:When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string. In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. Note that this ensures that greater importance is attributed to longer chunks, as is usual in most EBMT systems (cf. #REF;#REF;#REF). 7 s an example, consider the translation into French of the house collapsed. Assume the conditional probabilities in ( 33  These mistranslations are all caused by boundary friction. Each of the translations in ( 37) and ( 38) would be output with an associated weight and ranked by the system. We would like to incorporate into our model a procedure whereby translation chunks extracted from the phrasal and marker lexicons are more highly regarded than those constructed by inserting words from the word-level lexicon into generalized marker chunks. That is, we want to allocate a larger portion of the probability space to the phrasal and marker lexicons than to the generalized or wordlevel lexicons. We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( #TARGET_REF<MOT/> ) ."
    },
    {
        "gold": {
            "text": [
                "Based on the research results reported in #TARGET_REF , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG .",
                "We developed a compiler that takes as its input a set of lexical rules, deduces the nec- essary transfer of properties not changed by the individual lexical rules, and encodes the set of lexical rules and their interaction into definite relations constraining lexical entries.",
                "Each lexical entry is automatically extended with a definite clause encoding of the lexical rule applications which the entry can undergo.",
                "The definite clauses thereby introduce what we refer to as systematic covariationinlexicalentries."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:Based on the research results reported in #TARGET_REF , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG . We developed a compiler that takes as its input a set of lexical rules, deduces the nec- essary transfer of properties not changed by the individual lexical rules, and encodes the set of lexical rules and their interaction into definite relations constraining lexical entries. Each lexical entry is automatically extended with a definite clause encoding of the lexical rule applications which the entry can undergo. The definite clauses thereby introduce what we refer to as systematic covariationinlexicalentries.",
        "output": "introduction:Based on the research results reported in #TARGET_REF<EXT/> , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG . We developed a compiler that takes as its input a set of lexical rules, deduces the nec- essary transfer of properties not changed by the individual lexical rules, and encodes the set of lexical rules and their interaction into definite relations constraining lexical entries. Each lexical entry is automatically extended with a definite clause encoding of the lexical rule applications which the entry can undergo. The definite clauses thereby introduce what we refer to as systematic covariationinlexicalentries."
    },
    {
        "gold": {
            "text": [
                "2The algorithm was implemented by the the authors , following the description in #TARGET_REF ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:2The algorithm was implemented by the the authors , following the description in #TARGET_REF .",
        "output": "method:2The algorithm was implemented by the the authors , following the description in #TARGET_REF<FUT/> ."
    },
    {
        "gold": {
            "text": [
                "The example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response.",
                "This situation suggests a response-automation approach that follows the document retrieval paradigm ( #TARGET_REF ) , where a new request is matched with existing response documents ( e-mails ) .",
                "However, specific words in the request do not always match a response well, and sometimes do not match a response at all, as demonstrated by the examples in Figures 1(a) and 1(c), respectively."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:The example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response. This situation suggests a response-automation approach that follows the document retrieval paradigm ( #TARGET_REF ) , where a new request is matched with existing response documents ( e-mails ) . However, specific words in the request do not always match a response well, and sometimes do not match a response at all, as demonstrated by the examples in Figures 1(a) and 1(c), respectively.",
        "output": "nan:The example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response. This situation suggests a response-automation approach that follows the document retrieval paradigm ( #TARGET_REF<BACK/> ) , where a new request is matched with existing response documents ( e-mails ) . However, specific words in the request do not always match a response well, and sometimes do not match a response at all, as demonstrated by the examples in Figures 1(a) and 1(c), respectively."
    },
    {
        "gold": {
            "text": [
                "Although there are other discussions of the paragraph as a central element of discourse ( e.g. #REF , #TARGET_REF , #REF , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .",
                "Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.",
                "Our interest, however, lies precisely in that area."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Although there are other discussions of the paragraph as a central element of discourse ( e.g. #REF , #TARGET_REF , #REF , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area.",
        "output": "introduction:Although there are other discussions of the paragraph as a central element of discourse ( e.g. #REF , #TARGET_REF<USE/> , #REF , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area."
    },
    {
        "gold": {
            "text": [
                "The tutorial policy makes a high-level decision as to which strategy to use (for example, \"acknowledge the correct part and give a high specificity hint\") based on the answer analysis and dialogue context.",
                "At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy .",
                "Since the system accepts unrestricted input, interpretation errors are unavoidable.",
                "Our recovery policy is modeled on the TargetedHelp ( #TARGET_REF ) policy used in task-oriented dialogue .",
                "If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I'm sorry, I'm having a problem understanding.",
                "I don't know the word power.\"",
                "The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:The tutorial policy makes a high-level decision as to which strategy to use (for example, \"acknowledge the correct part and give a high specificity hint\") based on the answer analysis and dialogue context. At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp ( #TARGET_REF ) policy used in task-oriented dialogue . If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I'm sorry, I'm having a problem understanding. I don't know the word power.\" The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.",
        "output": "experiments:The tutorial policy makes a high-level decision as to which strategy to use (for example, \"acknowledge the correct part and give a high specificity hint\") based on the answer analysis and dialogue context. At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp ( #TARGET_REF<COMP/> ) policy used in task-oriented dialogue . If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I'm sorry, I'm having a problem understanding. I don't know the word power.\" The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers."
    },
    {
        "gold": {
            "text": [
                "Finally, our system associates conditional probabilities with the frames we extract.",
                "Making the distinction between the behavior of verbs in active and passive contexts is particularly important for the accurate assignment of probabilities to semantic forms.",
                "We carried out an extensive evaluation of the complete induced lexicon against the full COMLEX resource.",
                "To our knowledge, this is the most extensive qualitative evaluation of subcategorization extraction in English.",
                "The only evaluation of a similar scale is that carried out by Schulte im Walde (2002b) for German.",
                "The results reported here for Penn-II compare favorably against the baseline and, in fact, are an improvement on those reported in O' #REF.",
                "The results for the larger, more domain-diverse Penn-III lexicon are very encouraging, in some cases almost 15% above the baseline.",
                "We believe our semantic forms are fine-grained, and by choosing to evaluate against COMLEX, we set our sights high: COMLEX is considerably more detailed than the OALD or LDOCE used for other earlier evaluations.",
                "Our error analysis also revealed some interesting issues associated with using an external standard such as COMLEX.",
                "In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( #TARGET_REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:Finally, our system associates conditional probabilities with the frames we extract. Making the distinction between the behavior of verbs in active and passive contexts is particularly important for the accurate assignment of probabilities to semantic forms. We carried out an extensive evaluation of the complete induced lexicon against the full COMLEX resource. To our knowledge, this is the most extensive qualitative evaluation of subcategorization extraction in English. The only evaluation of a similar scale is that carried out by Schulte im Walde (2002b) for German. The results reported here for Penn-II compare favorably against the baseline and, in fact, are an improvement on those reported in O' #REF. The results for the larger, more domain-diverse Penn-III lexicon are very encouraging, in some cases almost 15% above the baseline. We believe our semantic forms are fine-grained, and by choosing to evaluate against COMLEX, we set our sights high: COMLEX is considerably more detailed than the OALD or LDOCE used for other earlier evaluations. Our error analysis also revealed some interesting issues associated with using an external standard such as COMLEX. In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( #TARGET_REF ) .",
        "output": "nan:Finally, our system associates conditional probabilities with the frames we extract. Making the distinction between the behavior of verbs in active and passive contexts is particularly important for the accurate assignment of probabilities to semantic forms. We carried out an extensive evaluation of the complete induced lexicon against the full COMLEX resource. To our knowledge, this is the most extensive qualitative evaluation of subcategorization extraction in English. The only evaluation of a similar scale is that carried out by Schulte im Walde (2002b) for German. The results reported here for Penn-II compare favorably against the baseline and, in fact, are an improvement on those reported in O' #REF. The results for the larger, more domain-diverse Penn-III lexicon are very encouraging, in some cases almost 15% above the baseline. We believe our semantic forms are fine-grained, and by choosing to evaluate against COMLEX, we set our sights high: COMLEX is considerably more detailed than the OALD or LDOCE used for other earlier evaluations. Our error analysis also revealed some interesting issues associated with using an external standard such as COMLEX. In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( #TARGET_REF<MOT/> ) ."
    },
    {
        "gold": {
            "text": [
                "A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon.",
                "Most current HPSG analyses of Dutch, German, Italian, and French fall into that category.",
                "1 Furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run-time.",
                "Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry.",
                "This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned.",
                "As shown in #TARGET_REF this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon. Most current HPSG analyses of Dutch, German, Italian, and French fall into that category. 1 Furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run-time. Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry. This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned. As shown in #TARGET_REF this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .",
        "output": "introduction:A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon. Most current HPSG analyses of Dutch, German, Italian, and French fall into that category. 1 Furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run-time. Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry. This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned. As shown in #TARGET_REF<EXT/> this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries ."
    },
    {
        "gold": {
            "text": [
                "When we first integrated this recognizer with TINA, we used a \"wire\" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing.",
                "A simple word-pair grammar constrained the search space.",
                "If the parse failed, then the sentence was rejected.",
                "We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #TARGET_REF ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( #REF , #REF ) .",
                "Both the A* and the Viterbi search are left-to-right search algorithms.",
                "However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time.",
                "That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:When we first integrated this recognizer with TINA, we used a \"wire\" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing. A simple word-pair grammar constrained the search space. If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #TARGET_REF ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( #REF , #REF ) . Both the A* and the Viterbi search are left-to-right search algorithms. However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time. That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold.",
        "output": "nan:When we first integrated this recognizer with TINA, we used a \"wire\" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing. A simple word-pair grammar constrained the search space. If the parse failed, then the sentence was rejected. We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #TARGET_REF<FUT/> ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( #REF , #REF ) . Both the A* and the Viterbi search are left-to-right search algorithms. However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time. That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold."
    },
    {
        "gold": {
            "text": [
                "Much work has been done on the use of morphological features for parsing of morphologically rich languages.",
                "#REF report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable).",
                "This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size �3000+).",
                "They also report that the use of gender, number, and person features did not yield any improvements.",
                "The results for Czech are the opposite of our results for Arabic, as we will see.",
                "This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic� and Vidov�-Hladk� 1998) compared with Arabic (�14.0%, see Table 3).",
                "Similarly , #TARGET_REF report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .",
                "Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.",
                "We also find that the number feature helps for Arabic."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Much work has been done on the use of morphological features for parsing of morphologically rich languages. #REF report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable). This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size �3000+). They also report that the use of gender, number, and person features did not yield any improvements. The results for Czech are the opposite of our results for Arabic, as we will see. This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic� and Vidov�-Hladk� 1998) compared with Arabic (�14.0%, see Table 3). Similarly , #TARGET_REF report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations . Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic.",
        "output": "related work:Much work has been done on the use of morphological features for parsing of morphologically rich languages. #REF report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable). This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size �3000+). They also report that the use of gender, number, and person features did not yield any improvements. The results for Czech are the opposite of our results for Arabic, as we will see. This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic� and Vidov�-Hladk� 1998) compared with Arabic (�14.0%, see Table 3). Similarly , #TARGET_REF<BACK/> report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations . Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic."
    },
    {
        "gold": {
            "text": [
                "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF.",
                "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF , assertional statements as in #REF , or semantic nets as in #TARGET_REF .",
                "That is, the current system learns procedures rather than data structures.",
                "There is some literature on procedure acquisition such as the LISP synthesis work described in #REF and the PROLOG synthesis method of #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF. The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF , assertional statements as in #REF , or semantic nets as in #TARGET_REF . That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in #REF and the PROLOG synthesis method of #REF.",
        "output": "nan:The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF. The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF , assertional statements as in #REF , or semantic nets as in #TARGET_REF<USE/> . That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in #REF and the PROLOG synthesis method of #REF."
    },
    {
        "gold": {
            "text": [
                "The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (#REF).",
                "The first direct application of parse forest in translation is our previous work ( #TARGET_REF ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .",
                "This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "related work:The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (#REF). The first direct application of parse forest in translation is our previous work ( #TARGET_REF ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) . This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding.",
        "output": "related work:The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (#REF). The first direct application of parse forest in translation is our previous work ( #TARGET_REF<COMP/> ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) . This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding."
    },
    {
        "gold": {
            "text": [
                "Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.",
                "This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.",
                "Future research should apply the work of #REF and #TARGET_REF , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations. This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension. Future research should apply the work of #REF and #TARGET_REF , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .",
        "output": "conclusion:Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations. This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension. Future research should apply the work of #REF and #TARGET_REF<MOT/> , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars ."
    },
    {
        "gold": {
            "text": [
                "Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #TARGET_REF .",
                "We use the automatically built thesaurus of #REF to find words similar to each constituent, in order to automatically generate variants.",
                "Variants are generated by replacing either the noun or the verb constituent of a pair with a semantically (and syntactically) similar word. 3",
                "xamples of automatically generated variants for the pair spill, bean are pour, bean , stream, bean , spill, corn , and spill, rice ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #TARGET_REF . We use the automatically built thesaurus of #REF to find words similar to each constituent, in order to automatically generate variants. Variants are generated by replacing either the noun or the verb constituent of a pair with a semantically (and syntactically) similar word. 3 xamples of automatically generated variants for the pair spill, bean are pour, bean , stream, bean , spill, corn , and spill, rice .",
        "output": "nan:Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #TARGET_REF<EXT/> . We use the automatically built thesaurus of #REF to find words similar to each constituent, in order to automatically generate variants. Variants are generated by replacing either the noun or the verb constituent of a pair with a semantically (and syntactically) similar word. 3 xamples of automatically generated variants for the pair spill, bean are pour, bean , stream, bean , spill, corn , and spill, rice ."
    },
    {
        "gold": {
            "text": [
                "These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #TARGET_REF Arabic data .",
                "The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.",
                "In addition, we also report results on the official test data."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "conclusion:These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #TARGET_REF Arabic data . The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future. In addition, we also report results on the official test data.",
        "output": "conclusion:These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #TARGET_REF<FUT/> Arabic data . The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future. In addition, we also report results on the official test data."
    },
    {
        "gold": {
            "text": [
                "The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.",
                "For example , ( #TARGET_REF ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents. For example , ( #TARGET_REF ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .",
        "output": "related work:The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents. For example , ( #TARGET_REF<BACK/> ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces ."
    },
    {
        "gold": {
            "text": [
                "Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.",
                "In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #REF; #REF; #REF; #TARGET_REF ; #REF).",
                "The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #REF; #REF; #REF; #TARGET_REF ; #REF). The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.",
        "output": "related work:Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #REF; #REF; #REF; #TARGET_REF<USE/> ; #REF). The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."
    },
    {
        "gold": {
            "text": [
                "1. Char-n-grams (G): We start with a character n-gram-based approach (#REF), which is most common and followed by many language identification researchers.",
                "Following the work of #REF, we select character n-grams (n=1 to 5) and the word as the features in our experiments.",
                "2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments.",
                "raw length value as a feature , we follow our previous work ( #REF ; #TARGET_REF ) and create multiple features for length using a decision tree ( J48 ) .",
                "We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.",
                "4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:1. Char-n-grams (G): We start with a character n-gram-based approach (#REF), which is most common and followed by many language identification researchers. Following the work of #REF, we select character n-grams (n=1 to 5) and the word as the features in our experiments. 2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. raw length value as a feature , we follow our previous work ( #REF ; #TARGET_REF ) and create multiple features for length using a decision tree ( J48 ) . We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.",
        "output": "experiments:1. Char-n-grams (G): We start with a character n-gram-based approach (#REF), which is most common and followed by many language identification researchers. Following the work of #REF, we select character n-grams (n=1 to 5) and the word as the features in our experiments. 2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments. raw length value as a feature , we follow our previous work ( #REF ; #TARGET_REF<COMP/> ) and create multiple features for length using a decision tree ( J48 ) . We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized."
    },
    {
        "gold": {
            "text": [
                "Other factors such as student confidence could be considered as well ( #TARGET_REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "experiments:Other factors such as student confidence could be considered as well ( #TARGET_REF ) .",
        "output": "experiments:Other factors such as student confidence could be considered as well ( #TARGET_REF<MOT/> ) ."
    },
    {
        "gold": {
            "text": [
                "A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.",
                "For instance , #TARGET_REF , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''",
                "The pairing of these two sentences may be said to create a small paragraph.",
                "Our point is that an acceptable structure can be assigned to the utterance \"Reagan thinks bananas\" only within the paragraph in which this utterance occurs.",
                "We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task.",
                "That is, we can ask in the first sentence of a paragraph about Kissinger's favorite fruit, elaborate the question and the circumstances in the next few sentences, and give the above answer at the end.",
                "We do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism (although it may be); rather, it has the grammatical role of providing functional structures that can be assigned to strings."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings. For instance , #TARGET_REF , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? '' The pairing of these two sentences may be said to create a small paragraph. Our point is that an acceptable structure can be assigned to the utterance \"Reagan thinks bananas\" only within the paragraph in which this utterance occurs. We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task. That is, we can ask in the first sentence of a paragraph about Kissinger's favorite fruit, elaborate the question and the circumstances in the next few sentences, and give the above answer at the end. We do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism (although it may be); rather, it has the grammatical role of providing functional structures that can be assigned to strings.",
        "output": "introduction:A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings. For instance , #TARGET_REF<EXT/> , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? '' The pairing of these two sentences may be said to create a small paragraph. Our point is that an acceptable structure can be assigned to the utterance \"Reagan thinks bananas\" only within the paragraph in which this utterance occurs. We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task. That is, we can ask in the first sentence of a paragraph about Kissinger's favorite fruit, elaborate the question and the circumstances in the next few sentences, and give the above answer at the end. We do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism (although it may be); rather, it has the grammatical role of providing functional structures that can be assigned to strings."
    },
    {
        "gold": {
            "text": [
                "The base distribution 0 ( | ) P r N is designed to assign prior probabilities to the STSG production rules.",
                "Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #TARGET_REF and decompose the prior probability P0 ( r | N ) into two factors as follows :"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:The base distribution 0 ( | ) P r N is designed to assign prior probabilities to the STSG production rules. Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #TARGET_REF and decompose the prior probability P0 ( r | N ) into two factors as follows :",
        "output": "method:The base distribution 0 ( | ) P r N is designed to assign prior probabilities to the STSG production rules. Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #TARGET_REF<FUT/> and decompose the prior probability P0 ( r | N ) into two factors as follows :"
    },
    {
        "gold": {
            "text": [
                "One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2 We extended the left-corner parsing model in a few minor ways using grammar transforms.",
                "We replace Chomsky adjunction structures (i.e.",
                "structures of the form [X [X ...] [Y ...]]) with a special \"modifier\" link in the tree (becoming [X ... [mod Y • requiring nodes which are popped from the stack to choose between attaching with a normal link or a modifier link.",
                "We also compiled some frequent chains of non-branching nodes (such as [S [VP ...1]) into a single node with a new label (becoming [S-VP ...]).",
                "These transforms are undone before any evaluation is performed on the output trees.",
                "We do not believe these transforms have a major impact on performance, but we have not currently run tests without them.",
                "feature sets, but then efficiency becomes a problem.",
                "#TARGET_REF define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2 We extended the left-corner parsing model in a few minor ways using grammar transforms. We replace Chomsky adjunction structures (i.e. structures of the form [X [X ...] [Y ...]]) with a special \"modifier\" link in the tree (becoming [X ... [mod Y • requiring nodes which are popped from the stack to choose between attaching with a normal link or a modifier link. We also compiled some frequent chains of non-branching nodes (such as [S [VP ...1]) into a single node with a new label (becoming [S-VP ...]). These transforms are undone before any evaluation is performed on the output trees. We do not believe these transforms have a major impact on performance, but we have not currently run tests without them. feature sets, but then efficiency becomes a problem. #TARGET_REF define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( #REF ) .",
        "output": "nan:One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2 We extended the left-corner parsing model in a few minor ways using grammar transforms. We replace Chomsky adjunction structures (i.e. structures of the form [X [X ...] [Y ...]]) with a special \"modifier\" link in the tree (becoming [X ... [mod Y • requiring nodes which are popped from the stack to choose between attaching with a normal link or a modifier link. We also compiled some frequent chains of non-branching nodes (such as [S [VP ...1]) into a single node with a new label (becoming [S-VP ...]). These transforms are undone before any evaluation is performed on the output trees. We do not believe these transforms have a major impact on performance, but we have not currently run tests without them. feature sets, but then efficiency becomes a problem. #TARGET_REF<BACK/> define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( #REF ) ."
    },
    {
        "gold": {
            "text": [
                "The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #TARGET_REF .",
                "Their study also illustrates the importance of semantic classes and relations.",
                "However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope).",
                "Because secondary sources are often more consistently organized, it is possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE abstracts in general).",
                "Our study tackles outcome identification in primary medical sources and demonstrates that respectable performance is possible with a feature-combination approach."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #TARGET_REF . Their study also illustrates the importance of semantic classes and relations. However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope). Because secondary sources are often more consistently organized, it is possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE abstracts in general). Our study tackles outcome identification in primary medical sources and demonstrates that respectable performance is possible with a feature-combination approach.",
        "output": "related work:The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #TARGET_REF<USE/> . Their study also illustrates the importance of semantic classes and relations. However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope). Because secondary sources are often more consistently organized, it is possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE abstracts in general). Our study tackles outcome identification in primary medical sources and demonstrates that respectable performance is possible with a feature-combination approach."
    },
    {
        "gold": {
            "text": [
                "We also included two user-based features, gender and pretest score.",
                "Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( #REFa ; #TARGET_REF ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( #REF ; #REFb ) and made freely available in the openSMILE Toolkit ( #REF ) .",
                "To date, however, these features have only decreased the crossvalidation performance of our models.",
                "8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise)."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:We also included two user-based features, gender and pretest score. Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( #REFa ; #TARGET_REF ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( #REF ; #REFb ) and made freely available in the openSMILE Toolkit ( #REF ) . To date, however, these features have only decreased the crossvalidation performance of our models. 8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise).",
        "output": "nan:We also included two user-based features, gender and pretest score. Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( #REFa ; #TARGET_REF<COMP/> ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( #REF ; #REFb ) and made freely available in the openSMILE Toolkit ( #REF ) . To date, however, these features have only decreased the crossvalidation performance of our models. 8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise)."
    },
    {
        "gold": {
            "text": [
                "â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #REF , â¢ a `` conditioning '' facility as described by #TARGET_REF , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #REF , â¢ a `` conditioning '' facility as described by #TARGET_REF , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns .",
        "output": "conclusion:â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #REF , â¢ a `` conditioning '' facility as described by #TARGET_REF<MOT/> , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns ."
    },
    {
        "gold": {
            "text": [
                "However, there are at least three arguments against iterating PT.",
                "First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time.",
                "Secondly, the cooperative principle of Grice (1975Grice ( , 1978, under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader.",
                "Finally , it has been shown by #REF that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #TARGET_REF strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .",
                "Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly, the cooperative principle of Grice (1975Grice ( , 1978, under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally , it has been shown by #REF that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #TARGET_REF strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph . Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.",
        "output": "introduction:However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly, the cooperative principle of Grice (1975Grice ( , 1978, under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally , it has been shown by #REF that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #TARGET_REF<EXT/> strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph . Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions."
    },
    {
        "gold": {
            "text": [
                "The problem of boundary friction is clearly visible here: We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP.",
                "However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #TARGET_REF .",
                "Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often.",
                "Rather than search for competing candidates, we select the \"best\" translation and have its morphological variants searched for on-line.",
                "In the example above, namely, the personal computers, we search for les ordinateurs personnels versus the wrong alternatives le/la/l'ordinateurs personnels.",
                "Interestingly, using Lycos, and setting the search language to French, the correct form les ordinateurs personnels is uniquely preferred over the other alternatives, as it is found 2,454 times, whereas the others are not found at all.",
                "In this case, this translation overrides the highest-ranked translation (50) and is output as the final translation.",
                "In fact, in checking the translations obtained for NPs using system combination ABC, we noted that 251 NPs out of the test set of 500 could be improved."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The problem of boundary friction is clearly visible here: We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP. However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #TARGET_REF . Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often. Rather than search for competing candidates, we select the \"best\" translation and have its morphological variants searched for on-line. In the example above, namely, the personal computers, we search for les ordinateurs personnels versus the wrong alternatives le/la/l'ordinateurs personnels. Interestingly, using Lycos, and setting the search language to French, the correct form les ordinateurs personnels is uniquely preferred over the other alternatives, as it is found 2,454 times, whereas the others are not found at all. In this case, this translation overrides the highest-ranked translation (50) and is output as the final translation. In fact, in checking the translations obtained for NPs using system combination ABC, we noted that 251 NPs out of the test set of 500 could be improved.",
        "output": "experiments:The problem of boundary friction is clearly visible here: We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP. However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #TARGET_REF<FUT/> . Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often. Rather than search for competing candidates, we select the \"best\" translation and have its morphological variants searched for on-line. In the example above, namely, the personal computers, we search for les ordinateurs personnels versus the wrong alternatives le/la/l'ordinateurs personnels. Interestingly, using Lycos, and setting the search language to French, the correct form les ordinateurs personnels is uniquely preferred over the other alternatives, as it is found 2,454 times, whereas the others are not found at all. In this case, this translation overrides the highest-ranked translation (50) and is output as the final translation. In fact, in checking the translations obtained for NPs using system combination ABC, we noted that 251 NPs out of the test set of 500 could be improved."
    },
    {
        "gold": {
            "text": [
                "Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.",
                "For example, #REF consider the case in which acquiring additional human-annotated training data is not possible.",
                "They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.",
                "This approach assumes that there are enough existing labeled data to train the individual parsers.",
                "Another technique for making better use of unlabeled data is cotraining (#REF), in which two sufficiently different learners help each other learn by labeling training data for one another.",
                "The work of #TARGET_REF and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .",
                "#REF have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.",
                "Similar approaches are being explored for parsing #REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing. For example, #REF consider the case in which acquiring additional human-annotated training data is not possible. They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers. This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining (#REF), in which two sufficiently different learners help each other learn by labeling training data for one another. The work of #TARGET_REF and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing . #REF have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing #REF).",
        "output": "related work:Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing. For example, #REF consider the case in which acquiring additional human-annotated training data is not possible. They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers. This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining (#REF), in which two sufficiently different learners help each other learn by labeling training data for one another. The work of #TARGET_REF<BACK/> and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing . #REF have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing #REF)."
    },
    {
        "gold": {
            "text": [
                "Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.",
                "So we will use a very simple formalism, like the one above, resembling the standard first order language.",
                "But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of #REF, which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora.",
                "The logical notation of #REF is more sophisticated, and may be considered another possibility.",
                "Jackendoff's (1983) formalism is richer and resembles more closely an English grammar.",
                "#TARGET_REF , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''",
                "Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.",
                "It will also be a model for our simplified logical notation (cf.",
                "Section 5).",
                "We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of #REF, which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of #REF is more sophisticated, and may be considered another possibility. Jackendoff's (1983) formalism is richer and resembles more closely an English grammar. #TARGET_REF , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . '' Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics. It will also be a model for our simplified logical notation (cf. Section 5). We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.",
        "output": "introduction:Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of #REF, which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of #REF is more sophisticated, and may be considered another possibility. Jackendoff's (1983) formalism is richer and resembles more closely an English grammar. #TARGET_REF<USE/> , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . '' Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics. It will also be a model for our simplified logical notation (cf. Section 5). We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences."
    },
    {
        "gold": {
            "text": [
                "We do not claim that Gla is the best or unique way of expressing the rule \"assume that the writer did not say too much.\"",
                "Rather, we stress the possibility that one can axiomatize and productively use such a rule.",
                "We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #TARGET_REF ."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:We do not claim that Gla is the best or unique way of expressing the rule \"assume that the writer did not say too much.\" Rather, we stress the possibility that one can axiomatize and productively use such a rule. We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #TARGET_REF .",
        "output": "introduction:We do not claim that Gla is the best or unique way of expressing the rule \"assume that the writer did not say too much.\" Rather, we stress the possibility that one can axiomatize and productively use such a rule. We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #TARGET_REF<COMP/> ."
    },
    {
        "gold": {
            "text": [
                "Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (#REF).",
                "Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.",
                "For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #REF ; #TARGET_REF ) .",
                "techniques."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (#REF). Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries. For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #REF ; #TARGET_REF ) . techniques.",
        "output": "nan:Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (#REF). Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries. For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #REF ; #TARGET_REF<MOT/> ) . techniques."
    },
    {
        "gold": {
            "text": [
                "Its value is the syntactic category of the lexical item whose lemma gives rise to the PRED value at that particular level of embedding.",
                "This makes it possible to classify words and their semantic forms based on their syntactic category and reduces the risk of inaccurate assignment of subcategorization frame frequencies due to POS ambiguity, distinguishing, for example, between the nominal and verbal occurrences of the lemma fight.",
                "With this, the output for the verb impose in Figure 4 is impose (v,[subj, obj, obl:on]).",
                "For some of our experiments, we conflate the different verbal (and other) tags used in the Penn Treebanks to a single verbal marker (Table 4).",
                "As a further extension, the extraction procedure reads off the syntactic category of the head of each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]). 3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details.",
                "#TARGET_REF argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "method:Its value is the syntactic category of the lexical item whose lemma gives rise to the PRED value at that particular level of embedding. This makes it possible to classify words and their semantic forms based on their syntactic category and reduces the risk of inaccurate assignment of subcategorization frame frequencies due to POS ambiguity, distinguishing, for example, between the nominal and verbal occurrences of the lemma fight. With this, the output for the verb impose in Figure 4 is impose (v,[subj, obj, obl:on]). For some of our experiments, we conflate the different verbal (and other) tags used in the Penn Treebanks to a single verbal marker (Table 4). As a further extension, the extraction procedure reads off the syntactic category of the head of each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]). 3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details. #TARGET_REF argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .",
        "output": "method:Its value is the syntactic category of the lexical item whose lemma gives rise to the PRED value at that particular level of embedding. This makes it possible to classify words and their semantic forms based on their syntactic category and reduces the risk of inaccurate assignment of subcategorization frame frequencies due to POS ambiguity, distinguishing, for example, between the nominal and verbal occurrences of the lemma fight. With this, the output for the verb impose in Figure 4 is impose (v,[subj, obj, obl:on]). For some of our experiments, we conflate the different verbal (and other) tags used in the Penn Treebanks to a single verbal marker (Table 4). As a further extension, the extraction procedure reads off the syntactic category of the head of each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]). 3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details. #TARGET_REF<EXT/> argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization ."
    },
    {
        "gold": {
            "text": [
                "The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques.",
                "Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research.",
                "Specifically , we used Decision Graphs ( #TARGET_REF ) for Doc-Pred , and SVMs ( #REF ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .",
                "Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.",
                "These methodological variations are summarized in Table 2."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques. Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research. Specifically , we used Decision Graphs ( #TARGET_REF ) for Doc-Pred , and SVMs ( #REF ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) . Because this variation was uniformly implemented for both approaches, it does not affect their relative performance. These methodological variations are summarized in Table 2.",
        "output": "method:The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques. Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research. Specifically , we used Decision Graphs ( #TARGET_REF<FUT/> ) for Doc-Pred , and SVMs ( #REF ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) . Because this variation was uniformly implemented for both approaches, it does not affect their relative performance. These methodological variations are summarized in Table 2."
    },
    {
        "gold": {
            "text": [
                "The availability of toolkits for this weighted case ( #TARGET_REF ; #REF ) promises to unify much of statistical NLP .",
                "Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy-channel decoding, 1 including classic models for speech recognition (#REF) and machine translation (#REF).",
                "Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The availability of toolkits for this weighted case ( #TARGET_REF ; #REF ) promises to unify much of statistical NLP . Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy-channel decoding, 1 including classic models for speech recognition (#REF) and machine translation (#REF). Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.",
        "output": "introduction:The availability of toolkits for this weighted case ( #TARGET_REF<BACK/> ; #REF ) promises to unify much of statistical NLP . Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy-channel decoding, 1 including classic models for speech recognition (#REF) and machine translation (#REF). Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources."
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF , 1997 ) assumes that words ending in - ed are verbs .",
                "However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.",
                "Instead, we take advantage of the fact that the initial phrasal chunks correspond to rule right-hand sides.",
                "That is, for a rule in the Penn Treebank VP −→ VBG, NP, PP, we are certain (if the annotators have done their job correctly) that the first word in each of the strings corresponding to this right-hand side is a VBG, that is, a present participle.",
                "Given this information, in such cases we tag such words with the <LEX> tag.",
                "Taking expanding the board to 14 members −→ augmente le conseilà 14 membres as an example, we extract the chunks in ( 24 We ignore here the trivially true lexical chunk \"<QUANT> 14 : 14.\""
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:#TARGET_REF , 1997 ) assumes that words ending in - ed are verbs . However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category. Instead, we take advantage of the fact that the initial phrasal chunks correspond to rule right-hand sides. That is, for a rule in the Penn Treebank VP −→ VBG, NP, PP, we are certain (if the annotators have done their job correctly) that the first word in each of the strings corresponding to this right-hand side is a VBG, that is, a present participle. Given this information, in such cases we tag such words with the <LEX> tag. Taking expanding the board to 14 members −→ augmente le conseilà 14 membres as an example, we extract the chunks in ( 24 We ignore here the trivially true lexical chunk \"<QUANT> 14 : 14.\"",
        "output": "introduction:#TARGET_REF<USE/> , 1997 ) assumes that words ending in - ed are verbs . However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category. Instead, we take advantage of the fact that the initial phrasal chunks correspond to rule right-hand sides. That is, for a rule in the Penn Treebank VP −→ VBG, NP, PP, we are certain (if the annotators have done their job correctly) that the first word in each of the strings corresponding to this right-hand side is a VBG, that is, a present participle. Given this information, in such cases we tag such words with the <LEX> tag. Taking expanding the board to 14 members −→ augmente le conseilà 14 membres as an example, we extract the chunks in ( 24 We ignore here the trivially true lexical chunk \"<QUANT> 14 : 14.\""
    },
    {
        "gold": {
            "text": [
                "In this paper we present a linguistically motivated framework for uniform lexicostructural processing.",
                "It has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation (NLG) and for transfer in machine translation (MT).",
                "Our work extends directions taken in systems such as Ariane ( #REF ) , FoG ( #REF ) , JOYCE ( Rambow and #TARGET_REF ) , and LFS ( #REF ) .",
                "Although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics-based approaches to MT."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:In this paper we present a linguistically motivated framework for uniform lexicostructural processing. It has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation (NLG) and for transfer in machine translation (MT). Our work extends directions taken in systems such as Ariane ( #REF ) , FoG ( #REF ) , JOYCE ( Rambow and #TARGET_REF ) , and LFS ( #REF ) . Although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics-based approaches to MT.",
        "output": "introduction:In this paper we present a linguistically motivated framework for uniform lexicostructural processing. It has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation (NLG) and for transfer in machine translation (MT). Our work extends directions taken in systems such as Ariane ( #REF ) , FoG ( #REF ) , JOYCE ( Rambow and #TARGET_REF<COMP/> ) , and LFS ( #REF ) . Although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics-based approaches to MT."
    },
    {
        "gold": {
            "text": [
                "For shuffling paraphrases, french alternations are partially described in (#REF) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.",
                "For complementing this database and for converse constructions, the LADL tables (#REF) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.",
                "In particular , ( #TARGET_REF ) lists the converses of some 3 500 predicative nouns ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:For shuffling paraphrases, french alternations are partially described in (#REF) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs. For complementing this database and for converse constructions, the LADL tables (#REF) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions. In particular , ( #TARGET_REF ) lists the converses of some 3 500 predicative nouns .",
        "output": "nan:For shuffling paraphrases, french alternations are partially described in (#REF) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs. For complementing this database and for converse constructions, the LADL tables (#REF) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions. In particular , ( #TARGET_REF<MOT/> ) lists the converses of some 3 500 predicative nouns ."
    },
    {
        "gold": {
            "text": [
                "Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) .",
                "Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) . Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.",
        "output": "introduction:Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #TARGET_REF<EXT/> ; #REF ; #REF ; #REF ) . Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation."
    },
    {
        "gold": {
            "text": [
                "Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions (#REF), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.",
                "Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.",
                "We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #TARGET_REF ) .",
                "Ultimately we want to incorporate TINA'S probabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions (#REF), the tightly coupled system allows the parser to discard partial theories that have no way of continuing. Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer. We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #TARGET_REF ) . Ultimately we want to incorporate TINA'S probabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.",
        "output": "nan:Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions (#REF), the tightly coupled system allows the parser to discard partial theories that have no way of continuing. Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer. We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #TARGET_REF<FUT/> ) . Ultimately we want to incorporate TINA'S probabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model."
    },
    {
        "gold": {
            "text": [
                "In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.",
                "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., #REF, #REF).",
                "While these approaches have been reasonably successful ( see #REF ) , #TARGET_REF speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .",
                "In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.",
                "As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., #REF), their semantic similarity as computed using WordNet (e.g., #REF) or Wikipedia (#REF), and the contextual role played by an NP (see #REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document. In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., #REF, #REF). While these approaches have been reasonably successful ( see #REF ) , #TARGET_REF speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance . In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., #REF), their semantic similarity as computed using WordNet (e.g., #REF) or Wikipedia (#REF), and the contextual role played by an NP (see #REF).",
        "output": "introduction:In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document. In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., #REF, #REF). While these approaches have been reasonably successful ( see #REF ) , #TARGET_REF<BACK/> speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance . In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., #REF), their semantic similarity as computed using WordNet (e.g., #REF) or Wikipedia (#REF), and the contextual role played by an NP (see #REF)."
    },
    {
        "gold": {
            "text": [
                "The problem of handling ill-formed input has been studied by #REF , #TARGET_REF , #REF , #REF , #REF , #REF , #REF , and #REF .",
                "A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.",
                "However, these methodologies have not used historical information at the dialogue level as described here.",
                "In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.",
                "The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.",
                "Thus, an error in this work has no pattern but occurs probabilistically.",
                "A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.",
                "The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors.",
                "However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.",
                "Another dialogue acquisition system has been developed by #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The problem of handling ill-formed input has been studied by #REF , #TARGET_REF , #REF , #REF , #REF , #REF , #REF , and #REF . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #REF.",
        "output": "nan:The problem of handling ill-formed input has been studied by #REF , #TARGET_REF<USE/> , #REF , #REF , #REF , #REF , #REF , and #REF . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #REF."
    },
    {
        "gold": {
            "text": [
                "To address this limitation , our previous work ( #TARGET_REF ) has initiated an investigation on the problem of conversation entailment .",
                "The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H.",
                "For instance, as in Example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot.",
                "While our previous work has provided some interesting preliminary observations, it mostly focused on data collection and initial experiments and analysis using a small set of development data.",
                "It is not clear whether the previous results are generally applicable, how different components in the entailment framework interact with each other, and how different representations may influence the entailment outcome."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:To address this limitation , our previous work ( #TARGET_REF ) has initiated an investigation on the problem of conversation entailment . The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H. For instance, as in Example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot. While our previous work has provided some interesting preliminary observations, it mostly focused on data collection and initial experiments and analysis using a small set of development data. It is not clear whether the previous results are generally applicable, how different components in the entailment framework interact with each other, and how different representations may influence the entailment outcome.",
        "output": "introduction:To address this limitation , our previous work ( #TARGET_REF<COMP/> ) has initiated an investigation on the problem of conversation entailment . The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H. For instance, as in Example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot. While our previous work has provided some interesting preliminary observations, it mostly focused on data collection and initial experiments and analysis using a small set of development data. It is not clear whether the previous results are generally applicable, how different components in the entailment framework interact with each other, and how different representations may influence the entailment outcome."
    },
    {
        "gold": {
            "text": [
                "We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.",
                "Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #TARGET_REF ; #REF ) for self training .",
                "Self-training should also benefit other discriminatively trained parsers with latent annotations (#REF), although training would be much slower compared to using generative models, as in our case."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #TARGET_REF ; #REF ) for self training . Self-training should also benefit other discriminatively trained parsers with latent annotations (#REF), although training would be much slower compared to using generative models, as in our case.",
        "output": "conclusion:We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #TARGET_REF<MOT/> ; #REF ) for self training . Self-training should also benefit other discriminatively trained parsers with latent annotations (#REF), although training would be much slower compared to using generative models, as in our case."
    },
    {
        "gold": {
            "text": [
                "The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages.",
                "Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment.",
                "Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF).",
                "But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #TARGET_REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #TARGET_REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) .",
        "output": "introduction:The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #TARGET_REF<EXT/> ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) ."
    },
    {
        "gold": {
            "text": [
                "For unbiased evaluation of our approach, we used a home-grown search engine (implemented in the PYTHON script language).",
                "It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf-idf metric.",
                "The retrieval process relies on the vector space model ( #TARGET_REF ) , with the cosine measure expressing the similarity between a query and a document .",
                "The search engine produces a ranked output of documents."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:For unbiased evaluation of our approach, we used a home-grown search engine (implemented in the PYTHON script language). It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf-idf metric. The retrieval process relies on the vector space model ( #TARGET_REF ) , with the cosine measure expressing the similarity between a query and a document . The search engine produces a ranked output of documents.",
        "output": "experiments:For unbiased evaluation of our approach, we used a home-grown search engine (implemented in the PYTHON script language). It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf-idf metric. The retrieval process relies on the vector space model ( #TARGET_REF<FUT/> ) , with the cosine measure expressing the similarity between a query and a document . The search engine produces a ranked output of documents."
    },
    {
        "gold": {
            "text": [
                "Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #REF ; #REF ; #TARGET_REF ; #REF ) , which was difficult both to represent and to process , and which required considerable human input .",
                "However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.",
                "A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ("
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #REF ; #REF ; #TARGET_REF ; #REF ) , which was difficult both to represent and to process , and which required considerable human input . However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (",
        "output": "nan:Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #REF ; #REF ; #TARGET_REF<BACK/> ; #REF ) , which was difficult both to represent and to process , and which required considerable human input . However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ("
    },
    {
        "gold": {
            "text": [
                "The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations.",
                "First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%.",
                "This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.",
                "Second , in line with the findings of ( #TARGET_REF ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .",
                "63.50%).",
                "Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).",
                "In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations. First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%. This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. Second , in line with the findings of ( #TARGET_REF ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%). Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%). In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation.",
        "output": "experiments:The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations. First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%. This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. Second , in line with the findings of ( #TARGET_REF<USE/> ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) . 63.50%). Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%). In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation."
    },
    {
        "gold": {
            "text": [
                "The reordering models we describe follow our previous work using function word models for translation ( #REF ; #TARGET_REF ) .",
                "The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.",
                "To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.",
                "This section provides a high level overview of our reordering model, which attempts to leverage this information."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:The reordering models we describe follow our previous work using function word models for translation ( #REF ; #TARGET_REF ) . The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them. To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases. This section provides a high level overview of our reordering model, which attempts to leverage this information.",
        "output": "nan:The reordering models we describe follow our previous work using function word models for translation ( #REF ; #TARGET_REF<COMP/> ) . The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them. To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases. This section provides a high level overview of our reordering model, which attempts to leverage this information."
    },
    {
        "gold": {
            "text": [
                "We have presented a comparative evaluation of our approach on datasets from four different domains.",
                "In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets.",
                "Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.",
                "Our CRF-based approach also yields promising results in the crossdomain setting.",
                "The features we employ scale well across domains, given that the opinion target vocabularies are substantially different.",
                "For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #REF ; #TARGET_REF ) , perform in comparison to our approach .",
                "Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.",
                "We observe similar challenges as #REF regarding the analysis of complex sentences.",
                "Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.",
                "It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:We have presented a comparative evaluation of our approach on datasets from four different domains. In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets. Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction. Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #REF ; #TARGET_REF ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as #REF regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality. It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse.",
        "output": "conclusion:We have presented a comparative evaluation of our approach on datasets from four different domains. In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets. Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction. Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #REF ; #TARGET_REF<MOT/> ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as #REF regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality. It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse."
    },
    {
        "gold": {
            "text": [
                "#REF; #REF).",
                "The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.",
                "This approach has now gained wide usage , as exemplified by the work of #TARGET_REF , 1999 ) , Charniak ( 1996 , 1997 ) , #REF , #REF , and many others ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:#REF; #REF). The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar. This approach has now gained wide usage , as exemplified by the work of #TARGET_REF , 1999 ) , Charniak ( 1996 , 1997 ) , #REF , #REF , and many others .",
        "output": "introduction:#REF; #REF). The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar. This approach has now gained wide usage , as exemplified by the work of #TARGET_REF<EXT/> , 1999 ) , Charniak ( 1996 , 1997 ) , #REF , #REF , and many others ."
    },
    {
        "gold": {
            "text": [
                "For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form. 14",
                "The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees.",
                "The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities).",
                "We follow the notation convention of #TARGET_REF ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form. 14 The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees. The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities). We follow the notation convention of #TARGET_REF .",
        "output": "nan:For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form. 14 The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees. The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities). We follow the notation convention of #TARGET_REF<FUT/> ."
    },
    {
        "gold": {
            "text": [
                "An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.",
                "before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.",
                "If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.",
                "Typical examples are Bulgarian ( #REF ; #REF ) , Chinese ( #TARGET_REF ) , Danish ( #REF ) , and Swedish ( #REF ) .",
                "Japanese (#REF), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6. Typical examples are Bulgarian ( #REF ; #REF ) , Chinese ( #TARGET_REF ) , Danish ( #REF ) , and Swedish ( #REF ) . Japanese (#REF), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.",
        "output": "experiments:An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6. Typical examples are Bulgarian ( #REF ; #REF ) , Chinese ( #TARGET_REF<BACK/> ) , Danish ( #REF ) , and Swedish ( #REF ) . Japanese (#REF), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances."
    },
    {
        "gold": {
            "text": [
                "The directional nature of the generative models used to recover word alignments conflicts with their interpretation as translations.",
                "In practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model (the first row of panels in Figure 1).",
                "The standard approach is to train two models independently and then intersect their predictions ( #TARGET_REF ) .",
                "However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree.",
                "Let the directional models be defined as: − → p ( − → z ) (source-target) and ← − p ( ← − z ) (target-source).",
                "We suppress dependence on x and y for brevity.",
                "Define z to range over the union of all possible directional alignments − → Z ∪ ← − Z .",
                "We define a mixture model p"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The directional nature of the generative models used to recover word alignments conflicts with their interpretation as translations. In practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model (the first row of panels in Figure 1). The standard approach is to train two models independently and then intersect their predictions ( #TARGET_REF ) . However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree. Let the directional models be defined as: − → p ( − → z ) (source-target) and ← − p ( ← − z ) (target-source). We suppress dependence on x and y for brevity. Define z to range over the union of all possible directional alignments − → Z ∪ ← − Z . We define a mixture model p",
        "output": "nan:The directional nature of the generative models used to recover word alignments conflicts with their interpretation as translations. In practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model (the first row of panels in Figure 1). The standard approach is to train two models independently and then intersect their predictions ( #TARGET_REF<USE/> ) . However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree. Let the directional models be defined as: − → p ( − → z ) (source-target) and ← − p ( ← − z ) (target-source). We suppress dependence on x and y for brevity. Define z to range over the union of all possible directional alignments − → Z ∪ ← − Z . We define a mixture model p"
    },
    {
        "gold": {
            "text": [
                "Once the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing.",
                "The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #TARGET_REF , 1985 ) .",
                "The codes are doubly articulated; capital letters represent the grammatical relations which hold between a verb and its arguments and numbers represent subcategorisation frames which a verb can appear in.",
                "Most of the subcategorisation frames are specified by syntactic category, but some are very ill-specified; for instance, 9 is defined as \"needs a descriptive word or phrase\".",
                "In practice many adverbial and predicative complements will satisfy this code, when attached to a verb; for example, put [xg] where the code marks a locative adverbial prepositional phrase vs. make under sense 14 (hereafter written make( 14)) is coded IX9] where it marks a predicative noun phrase or prepositional phrase."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:Once the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing. The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #TARGET_REF , 1985 ) . The codes are doubly articulated; capital letters represent the grammatical relations which hold between a verb and its arguments and numbers represent subcategorisation frames which a verb can appear in. Most of the subcategorisation frames are specified by syntactic category, but some are very ill-specified; for instance, 9 is defined as \"needs a descriptive word or phrase\". In practice many adverbial and predicative complements will satisfy this code, when attached to a verb; for example, put [xg] where the code marks a locative adverbial prepositional phrase vs. make under sense 14 (hereafter written make( 14)) is coded IX9] where it marks a predicative noun phrase or prepositional phrase.",
        "output": "nan:Once the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing. The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #TARGET_REF<COMP/> , 1985 ) . The codes are doubly articulated; capital letters represent the grammatical relations which hold between a verb and its arguments and numbers represent subcategorisation frames which a verb can appear in. Most of the subcategorisation frames are specified by syntactic category, but some are very ill-specified; for instance, 9 is defined as \"needs a descriptive word or phrase\". In practice many adverbial and predicative complements will satisfy this code, when attached to a verb; for example, put [xg] where the code marks a locative adverbial prepositional phrase vs. make under sense 14 (hereafter written make( 14)) is coded IX9] where it marks a predicative noun phrase or prepositional phrase."
    },
    {
        "gold": {
            "text": [
                "This small experiment demonstrates a number of points.",
                "Firstly, it seems reasonable to conclude that the assignment of individual codes to verbs is on the whole relatively accurate in LDOCE.",
                "Of the 139 verbs tested, we only found code omissions in 10 cases.",
                "Secondly though, when we consider the interaction between the assignments of codes and word sense classification, LDOCE appears less reliable.",
                "This is the primary source of error in the case of the Object Raising rule.",
                "Thirdly, it seems clear that the Object Raising rule is straining the limits of what can be reliably extracted from the LDOCE coding system.",
                "Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( #TARGET_REF:460 ff . )",
                "However, only two of these criteria are explicit in the coding system."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:This small experiment demonstrates a number of points. Firstly, it seems reasonable to conclude that the assignment of individual codes to verbs is on the whole relatively accurate in LDOCE. Of the 139 verbs tested, we only found code omissions in 10 cases. Secondly though, when we consider the interaction between the assignments of codes and word sense classification, LDOCE appears less reliable. This is the primary source of error in the case of the Object Raising rule. Thirdly, it seems clear that the Object Raising rule is straining the limits of what can be reliably extracted from the LDOCE coding system. Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( #TARGET_REF:460 ff . ) However, only two of these criteria are explicit in the coding system.",
        "output": "nan:This small experiment demonstrates a number of points. Firstly, it seems reasonable to conclude that the assignment of individual codes to verbs is on the whole relatively accurate in LDOCE. Of the 139 verbs tested, we only found code omissions in 10 cases. Secondly though, when we consider the interaction between the assignments of codes and word sense classification, LDOCE appears less reliable. This is the primary source of error in the case of the Object Raising rule. Thirdly, it seems clear that the Object Raising rule is straining the limits of what can be reliably extracted from the LDOCE coding system. Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( #TARGET_REF<MOT/>:460 ff . ) However, only two of these criteria are explicit in the coding system."
    },
    {
        "gold": {
            "text": [
                "Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.",
                "Many investigators (e.g.",
                "Many investigators ( e.g. #REF ; #REF ; #TARGET_REF ; #REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .",
                "And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in #REF and O'#REF---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem.",
                "As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.",
                "We believe that one reason for the improvement has to do with the increased pitch range that our system uses.",
                "Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. #REF ; #REF ; #TARGET_REF ; #REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in #REF and O'#REF---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses. Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.",
        "output": "experiments:Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. #REF ; #REF ; #TARGET_REF<EXT/> ; #REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in #REF and O'#REF---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses. Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations."
    },
    {
        "gold": {
            "text": [
                "OAK 7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc).",
                "It provides a fine grained NE recognition covering 100 different NE types ( #TARGET_REF ) .",
                "Given the sparseness of most of these fine-grained NE types, we have merged them in coarser groups: event, facility, location, person, organisation, product, periodx, timex and numex."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:OAK 7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc). It provides a fine grained NE recognition covering 100 different NE types ( #TARGET_REF ) . Given the sparseness of most of these fine-grained NE types, we have merged them in coarser groups: event, facility, location, person, organisation, product, periodx, timex and numex.",
        "output": "experiments:OAK 7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc). It provides a fine grained NE recognition covering 100 different NE types ( #TARGET_REF<FUT/> ) . Given the sparseness of most of these fine-grained NE types, we have merged them in coarser groups: event, facility, location, person, organisation, product, periodx, timex and numex."
    },
    {
        "gold": {
            "text": [
                "Much of the earlier work in anaphora resolution heavily exploited domain and lin- guistic knowledge (#REF; #REF; #REF; #REF), which was difficult both to represent and to process, and which required considerable human input.",
                "However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.",
                "A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( #REF , 1991 ; #REF ; #TARGET_REF ; #REF ; Williams , Harvey , and #REF ; #REF ; #REF , 1998b ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Much of the earlier work in anaphora resolution heavily exploited domain and lin- guistic knowledge (#REF; #REF; #REF; #REF), which was difficult both to represent and to process, and which required considerable human input. However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( #REF , 1991 ; #REF ; #TARGET_REF ; #REF ; Williams , Harvey , and #REF ; #REF ; #REF , 1998b ) .",
        "output": "nan:Much of the earlier work in anaphora resolution heavily exploited domain and lin- guistic knowledge (#REF; #REF; #REF; #REF), which was difficult both to represent and to process, and which required considerable human input. However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( #REF , 1991 ; #REF ; #TARGET_REF<BACK/> ; #REF ; Williams , Harvey , and #REF ; #REF ; #REF , 1998b ) ."
    },
    {
        "gold": {
            "text": [
                "Such a rich graphical model can represent many dependencies but there are two dangers-one is that the computational complexity of training the model and searching for the most likely labeling given the tree can be prohibitive, and the other is that if too many dependencies are encoded, the model will over-fit the training data and will not generalize well.",
                "We propose a model which circumvents these two dangers and achieves significant performance gains over a similar local model that does not add any dependency arcs among the random variables.",
                "To tackle the efficiency problem, we adopt dynamic programming and re-ranking algorithms.",
                "To avoid overfitting we encode only a small set of linguistically motivated dependencies in features over sets of the random variables.",
                "Our re-ranking approach , like the approach to parse re-ranking of #TARGET_REF , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .",
                "The joint model is restricted to these n assignments and does not have to search the exponentially large space of all possible joint labelings."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Such a rich graphical model can represent many dependencies but there are two dangers-one is that the computational complexity of training the model and searching for the most likely labeling given the tree can be prohibitive, and the other is that if too many dependencies are encoded, the model will over-fit the training data and will not generalize well. We propose a model which circumvents these two dangers and achieves significant performance gains over a similar local model that does not add any dependency arcs among the random variables. To tackle the efficiency problem, we adopt dynamic programming and re-ranking algorithms. To avoid overfitting we encode only a small set of linguistically motivated dependencies in features over sets of the random variables. Our re-ranking approach , like the approach to parse re-ranking of #TARGET_REF , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes . The joint model is restricted to these n assignments and does not have to search the exponentially large space of all possible joint labelings.",
        "output": "introduction:Such a rich graphical model can represent many dependencies but there are two dangers-one is that the computational complexity of training the model and searching for the most likely labeling given the tree can be prohibitive, and the other is that if too many dependencies are encoded, the model will over-fit the training data and will not generalize well. We propose a model which circumvents these two dangers and achieves significant performance gains over a similar local model that does not add any dependency arcs among the random variables. To tackle the efficiency problem, we adopt dynamic programming and re-ranking algorithms. To avoid overfitting we encode only a small set of linguistically motivated dependencies in features over sets of the random variables. Our re-ranking approach , like the approach to parse re-ranking of #TARGET_REF<USE/> , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes . The joint model is restricted to these n assignments and does not have to search the exponentially large space of all possible joint labelings."
    },
    {
        "gold": {
            "text": [
                "Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (#REF).",
                "Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) .",
                "For the referring expression generation task here, we also need a lexicon with grounded semantics."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (#REF). Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) . For the referring expression generation task here, we also need a lexicon with grounded semantics.",
        "output": "nan:Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (#REF). Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF<COMP/> ) . For the referring expression generation task here, we also need a lexicon with grounded semantics."
    },
    {
        "gold": {
            "text": [
                "The sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach.",
                "In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( #REF ) , class n-grams ( #TARGET_REF ) , grammatical features ( #REF ) , etc ' ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:The sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach. In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( #REF ) , class n-grams ( #TARGET_REF ) , grammatical features ( #REF ) , etc ' .",
        "output": "conclusion:The sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach. In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( #REF ) , class n-grams ( #TARGET_REF<MOT/> ) , grammatical features ( #REF ) , etc ' ."
    },
    {
        "gold": {
            "text": [
                "For our features we used large-margin classifiers trained using the online algorithm described in (#REF).",
                "The code for the classifier was generously provided by Daisuke Okanohara.",
                "This code was extensively optimized to take advantage of the very sparse sentence representation described above.",
                "As shown in ( #TARGET_REF ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .",
                "Therefore, we used a 3rd order polynomial kernel, which was found to give good results.",
                "No special effort was otherwise made in order to optimize the parameters of the classifiers."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:For our features we used large-margin classifiers trained using the online algorithm described in (#REF). The code for the classifier was generously provided by Daisuke Okanohara. This code was extensively optimized to take advantage of the very sparse sentence representation described above. As shown in ( #TARGET_REF ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences . Therefore, we used a 3rd order polynomial kernel, which was found to give good results. No special effort was otherwise made in order to optimize the parameters of the classifiers.",
        "output": "experiments:For our features we used large-margin classifiers trained using the online algorithm described in (#REF). The code for the classifier was generously provided by Daisuke Okanohara. This code was extensively optimized to take advantage of the very sparse sentence representation described above. As shown in ( #TARGET_REF<EXT/> ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences . Therefore, we used a 3rd order polynomial kernel, which was found to give good results. No special effort was otherwise made in order to optimize the parameters of the classifiers."
    },
    {
        "gold": {
            "text": [
                "Developing a calculus for reasoning with QLFs is too large a task to be undertaken here.",
                "But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #TARGET_REF ) work to our own framework .",
                "Reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context or disambiguation is.",
                "His example is:"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Developing a calculus for reasoning with QLFs is too large a task to be undertaken here. But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #TARGET_REF ) work to our own framework . Reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context or disambiguation is. His example is:",
        "output": "nan:Developing a calculus for reasoning with QLFs is too large a task to be undertaken here. But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #TARGET_REF<FUT/> ) work to our own framework . Reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context or disambiguation is. His example is:"
    },
    {
        "gold": {
            "text": [
                "Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.",
                "dictionary-based ( #TARGET_REF ) , ontology-based ( #REF ; #REF ) , information-based ( #REF ; #REF ) or distributional ( #REF ) .",
                "The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based ( #TARGET_REF ) , ontology-based ( #REF ; #REF ) , information-based ( #REF ; #REF ) or distributional ( #REF ) . The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.",
        "output": "nan:Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based ( #TARGET_REF<BACK/> ) , ontology-based ( #REF ; #REF ) , information-based ( #REF ; #REF ) or distributional ( #REF ) . The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora."
    },
    {
        "gold": {
            "text": [
                "The head gestures in the DanPASS data have been coded by non expert annotators (one annotator per video) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators.",
                "The annotations of this video were then used to measure inter-coder agreement in ANVIL as it was the case for the annotations on feedback expressions.",
                "In the case of gestures we also measured agreement on gesture segmentation.",
                "The figures obtained are given in Table 3.",
                "These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #TARGET_REF ) , but are still sat -isfactory given the high number of categories provided by the scheme."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The head gestures in the DanPASS data have been coded by non expert annotators (one annotator per video) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators. The annotations of this video were then used to measure inter-coder agreement in ANVIL as it was the case for the annotations on feedback expressions. In the case of gestures we also measured agreement on gesture segmentation. The figures obtained are given in Table 3. These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #TARGET_REF ) , but are still sat -isfactory given the high number of categories provided by the scheme.",
        "output": "nan:The head gestures in the DanPASS data have been coded by non expert annotators (one annotator per video) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators. The annotations of this video were then used to measure inter-coder agreement in ANVIL as it was the case for the annotations on feedback expressions. In the case of gestures we also measured agreement on gesture segmentation. The figures obtained are given in Table 3. These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #TARGET_REF<USE/> ) , but are still sat -isfactory given the high number of categories provided by the scheme."
    },
    {
        "gold": {
            "text": [
                "The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG ( #REF ) , LFS ( #REF ) , and JOYCE ( Rambow and #TARGET_REF ) .",
                "The framework was originally developed for the realization of deep-syntactic structures in NLG ."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG ( #REF ) , LFS ( #REF ) , and JOYCE ( Rambow and #TARGET_REF ) . The framework was originally developed for the realization of deep-syntactic structures in NLG .",
        "output": "nan:The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG ( #REF ) , LFS ( #REF ) , and JOYCE ( Rambow and #TARGET_REF<COMP/> ) . The framework was originally developed for the realization of deep-syntactic structures in NLG ."
    },
    {
        "gold": {
            "text": [
                "Even better accuracy can be achieved with a more fine-grained link class structure.",
                "Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .",
                "Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #TARGET_REF ) .",
                "If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.",
                "In this manner, the model can account for a wider range of translation phenomena."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy . Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #TARGET_REF ) . If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena.",
        "output": "conclusion:Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy . Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #TARGET_REF<MOT/> ) . If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena."
    },
    {
        "gold": {
            "text": [
                "Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.",
                "As noted above , it is well documented ( #TARGET_REF ) that subcategorization frames ( and their frequencies ) vary across domains .",
                "We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ.",
                "For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.",
                "It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases.",
                "This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.",
                "Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data. As noted above , it is well documented ( #TARGET_REF ) that subcategorization frames ( and their frequencies ) vary across domains . We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ. For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon. It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases. This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.",
        "output": "nan:Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data. As noted above , it is well documented ( #TARGET_REF<EXT/> ) that subcategorization frames ( and their frequencies ) vary across domains . We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ. For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon. It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases. This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples."
    },
    {
        "gold": {
            "text": [
                "We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #REF ; #REF ; #TARGET_REF ; #REF ) for Bangla morphologically complex words .",
                "Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming).",
                "The subjects were asked to make a lexical decision whether the given target is a valid word in that language.",
                "The same target word is again probed but with a different audio or visual probe called the control word.",
                "The control shows no relationship with the target.",
                "For example, baYaska (aged) and baYasa (age) is a prime-target pair, for which the corresponding control-target pair could be naYana (eye) and baYasa (age)."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #REF ; #REF ; #TARGET_REF ; #REF ) for Bangla morphologically complex words . Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming). The subjects were asked to make a lexical decision whether the given target is a valid word in that language. The same target word is again probed but with a different audio or visual probe called the control word. The control shows no relationship with the target. For example, baYaska (aged) and baYasa (age) is a prime-target pair, for which the corresponding control-target pair could be naYana (eye) and baYasa (age).",
        "output": "method:We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #REF ; #REF ; #TARGET_REF<FUT/> ; #REF ) for Bangla morphologically complex words . Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming). The subjects were asked to make a lexical decision whether the given target is a valid word in that language. The same target word is again probed but with a different audio or visual probe called the control word. The control shows no relationship with the target. For example, baYaska (aged) and baYasa (age) is a prime-target pair, for which the corresponding control-target pair could be naYana (eye) and baYasa (age)."
    },
    {
        "gold": {
            "text": [
                "Clinical question answering is an emerging area of research that has only recently begun to receive serious attention.",
                "As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated.",
                "In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems.",
                "For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( #TARGET_REF ) .",
                "In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Clinical question answering is an emerging area of research that has only recently begun to receive serious attention. As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated. In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( #TARGET_REF ) . In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview.",
        "output": "related work:Clinical question answering is an emerging area of research that has only recently begun to receive serious attention. As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated. In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( #TARGET_REF<BACK/> ) . In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview."
    },
    {
        "gold": {
            "text": [
                "The first step is to label each node as either a head, complement, or adjunct based on the approaches of #REF and #REF.",
                "Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.",
                "The algorithm handles \"like\" coordination and exploits the traces used in the treebank in order to interpret LDDs.",
                "Unlike our approach , those of #TARGET_REF and Hockenmaier , Bierner , and #REF include a substantial initial correction and clean-up of the Penn-II trees .",
                "Miyao, Ninomiya, and #REF and Nakanishi, Miyao, and #REF describe a methodology for acquiring an English HPSG from the Penn-II Treebank.",
                "Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on #REF and #REF; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:The first step is to label each node as either a head, complement, or adjunct based on the approaches of #REF and #REF. Each node is subsequently assigned the relevant category based on its constituent type and surface configuration. The algorithm handles \"like\" coordination and exploits the traces used in the treebank in order to interpret LDDs. Unlike our approach , those of #TARGET_REF and Hockenmaier , Bierner , and #REF include a substantial initial correction and clean-up of the Penn-II trees . Miyao, Ninomiya, and #REF and Nakanishi, Miyao, and #REF describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on #REF and #REF; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.",
        "output": "related work:The first step is to label each node as either a head, complement, or adjunct based on the approaches of #REF and #REF. Each node is subsequently assigned the relevant category based on its constituent type and surface configuration. The algorithm handles \"like\" coordination and exploits the traces used in the treebank in order to interpret LDDs. Unlike our approach , those of #TARGET_REF<USE/> and Hockenmaier , Bierner , and #REF include a substantial initial correction and clean-up of the Penn-II trees . Miyao, Ninomiya, and #REF and Nakanishi, Miyao, and #REF describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on #REF and #REF; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category."
    },
    {
        "gold": {
            "text": [
                "In selecting features for Korean, we have to ac- count for relatively free word order (#REF).",
                "We follow our previous work ( #TARGET_REF ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also #REF ) .",
                "Each word is broken down into: stem, affixes, stem POS, and affixes POS.",
                "We also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties.",
                "Although these are relatively shallow features, they provide enough lexical and grammatical context to help select better or worse training data (section 3) and to provide a basis for a preliminary system (section 4)."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:In selecting features for Korean, we have to ac- count for relatively free word order (#REF). We follow our previous work ( #TARGET_REF ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also #REF ) . Each word is broken down into: stem, affixes, stem POS, and affixes POS. We also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties. Although these are relatively shallow features, they provide enough lexical and grammatical context to help select better or worse training data (section 3) and to provide a basis for a preliminary system (section 4).",
        "output": "nan:In selecting features for Korean, we have to ac- count for relatively free word order (#REF). We follow our previous work ( #TARGET_REF<COMP/> ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also #REF ) . Each word is broken down into: stem, affixes, stem POS, and affixes POS. We also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties. Although these are relatively shallow features, they provide enough lexical and grammatical context to help select better or worse training data (section 3) and to provide a basis for a preliminary system (section 4)."
    },
    {
        "gold": {
            "text": [
                "Even better accuracy can be achieved with a more fine-grained link class structure.",
                "Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #TARGET_REF ) .",
                "Another interesting extension is to broaden the definition of a \"word\" to include multi-word lexical units (#REF).",
                "If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.",
                "In this manner, the model can account for a wider range of translation phenomena."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #TARGET_REF ) . Another interesting extension is to broaden the definition of a \"word\" to include multi-word lexical units (#REF). If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena.",
        "output": "conclusion:Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #TARGET_REF<MOT/> ) . Another interesting extension is to broaden the definition of a \"word\" to include multi-word lexical units (#REF). If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena."
    },
    {
        "gold": {
            "text": [
                "Note: In our translation from English to logic we are assuming that \"it\" is anaphoric (with the pronoun following the element that it refers to), not cataphoric (the other way around).",
                "This means that the \"it\" that brought the disease in P1 will not be considered to refer to the infection \"i\" or the death \"d\" in P3.",
                "This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #TARGET_REF , p. 329 ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:Note: In our translation from English to logic we are assuming that \"it\" is anaphoric (with the pronoun following the element that it refers to), not cataphoric (the other way around). This means that the \"it\" that brought the disease in P1 will not be considered to refer to the infection \"i\" or the death \"d\" in P3. This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #TARGET_REF , p. 329 ) .",
        "output": "introduction:Note: In our translation from English to logic we are assuming that \"it\" is anaphoric (with the pronoun following the element that it refers to), not cataphoric (the other way around). This means that the \"it\" that brought the disease in P1 will not be considered to refer to the infection \"i\" or the death \"d\" in P3. This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #TARGET_REF<EXT/> , p. 329 ) ."
    },
    {
        "gold": {
            "text": [
                "The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.",
                "One approach to this more general problem , taken by the ` Nitrogen ' generator ( #TARGET_REFa ; #REFb ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .",
                "Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word.",
                "It also should be straightforwardly applicable to the more specific problem we are addressing here.",
                "To determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability.",
                "This has the advantage of reducing the problem of adjective ordering to the problem of estimating n-gram probabilities, something which is relatively well understood."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system. One approach to this more general problem , taken by the ` Nitrogen ' generator ( #TARGET_REFa ; #REFb ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model . Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word. It also should be straightforwardly applicable to the more specific problem we are addressing here. To determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability. This has the advantage of reducing the problem of adjective ordering to the problem of estimating n-gram probabilities, something which is relatively well understood.",
        "output": "method:The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system. One approach to this more general problem , taken by the ` Nitrogen ' generator ( #TARGET_REF<FUT/>a ; #REFb ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model . Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word. It also should be straightforwardly applicable to the more specific problem we are addressing here. To determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability. This has the advantage of reducing the problem of adjective ordering to the problem of estimating n-gram probabilities, something which is relatively well understood."
    },
    {
        "gold": {
            "text": [
                "As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.",
                "The first work to do this with topic models is Feng and Lapata (2010b).",
                "They use a Bag of Visual Words ( BoVW ) model ( #TARGET_REF ) to create a bimodal vocabulary describing documents .",
                "The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.",
                "Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.",
                "Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.",
                "More recently, #REF show that visual attribute classifiers, which have been immensely successful in object recognition (#REF), act as excellent substitutes for feature norms.",
                "Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (#REF;#REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words ( BoVW ) model ( #TARGET_REF ) to create a bimodal vocabulary describing documents . The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, #REF show that visual attribute classifiers, which have been immensely successful in object recognition (#REF), act as excellent substitutes for feature norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (#REF;#REF).",
        "output": "related work:As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words ( BoVW ) model ( #TARGET_REF<BACK/> ) to create a bimodal vocabulary describing documents . The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, #REF show that visual attribute classifiers, which have been immensely successful in object recognition (#REF), act as excellent substitutes for feature norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (#REF;#REF)."
    },
    {
        "gold": {
            "text": [
                "Table 1 shows our results for each of our selected models with our compositionality evaluation.",
                "The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).",
                "This result is consistent with other works using this model with these features ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Table 1 shows our results for each of our selected models with our compositionality evaluation. The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works using this model with these features ( #TARGET_REF ; #REF ) .",
        "output": "experiments:Table 1 shows our results for each of our selected models with our compositionality evaluation. The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works using this model with these features ( #TARGET_REF<USE/> ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "Using the implicit modeling of argument consistency , we follow the same approach as in our previous work ( #TARGET_REF ) and trained a logistic regression model to predict verb alignment based on the features in Table 1 ."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "method:Using the implicit modeling of argument consistency , we follow the same approach as in our previous work ( #TARGET_REF ) and trained a logistic regression model to predict verb alignment based on the features in Table 1 .",
        "output": "method:Using the implicit modeling of argument consistency , we follow the same approach as in our previous work ( #TARGET_REF<COMP/> ) and trained a logistic regression model to predict verb alignment based on the features in Table 1 ."
    },
    {
        "gold": {
            "text": [
                "• Site based corpus annotation -in which the user can specify a web site to annotate • Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate • Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger).",
                "Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #TARGET_REFb ) and shingling techniques described by #REF ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "method:• Site based corpus annotation -in which the user can specify a web site to annotate • Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate • Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger). Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #TARGET_REFb ) and shingling techniques described by #REF .",
        "output": "method:• Site based corpus annotation -in which the user can specify a web site to annotate • Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate • Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger). Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #TARGET_REF<MOT/>b ) and shingling techniques described by #REF ."
    },
    {
        "gold": {
            "text": [
                "is the probability of producing the target tree fragment frag.",
                "To generate frag,  used a geometric prior to decide how many child nodes to assign each node.",
                "Differently, we require that each multi-word non-terminal node must have two child nodes.",
                "This is because the binary structure has been verified to be very effective for tree-based translation ( #TARGET_REF ; #REFa ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "method:is the probability of producing the target tree fragment frag. To generate frag,  used a geometric prior to decide how many child nodes to assign each node. Differently, we require that each multi-word non-terminal node must have two child nodes. This is because the binary structure has been verified to be very effective for tree-based translation ( #TARGET_REF ; #REFa ) .",
        "output": "method:is the probability of producing the target tree fragment frag. To generate frag,  used a geometric prior to decide how many child nodes to assign each node. Differently, we require that each multi-word non-terminal node must have two child nodes. This is because the binary structure has been verified to be very effective for tree-based translation ( #TARGET_REF<EXT/> ; #REFa ) ."
    },
    {
        "gold": {
            "text": [
                "We have used the testbeds from WePS-1 ( #TARGET_REF , 2007)2 and WePS-2 (#REF) evaluation campaigns 3."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:We have used the testbeds from WePS-1 ( #TARGET_REF , 2007)2 and WePS-2 (#REF) evaluation campaigns 3.",
        "output": "related work:We have used the testbeds from WePS-1 ( #TARGET_REF<FUT/> , 2007)2 and WePS-2 (#REF) evaluation campaigns 3."
    },
    {
        "gold": {
            "text": [
                "We adopt the three-level semantics as a formal tool for the analysis of paragraphs.",
                "This semantics was constructed (#REFa, 1987b) as a formal framework for default and commonsense reasoning.",
                "It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.",
                "For instance , relating \"they\" to \"apples\" in the sentence ( cfXXX #TARGET_REF p. 195 ; #REFa ) : We bought the boys apples because they were so cheap"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:We adopt the three-level semantics as a formal tool for the analysis of paragraphs. This semantics was constructed (#REFa, 1987b) as a formal framework for default and commonsense reasoning. It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates. For instance , relating \"they\" to \"apples\" in the sentence ( cfXXX #TARGET_REF p. 195 ; #REFa ) : We bought the boys apples because they were so cheap",
        "output": "introduction:We adopt the three-level semantics as a formal tool for the analysis of paragraphs. This semantics was constructed (#REFa, 1987b) as a formal framework for default and commonsense reasoning. It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates. For instance , relating \"they\" to \"apples\" in the sentence ( cfXXX #TARGET_REF<BACK/> p. 195 ; #REFa ) : We bought the boys apples because they were so cheap"
    },
    {
        "gold": {
            "text": [
                "Two applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and #REF; #REF), and answer extraction in FAQs (Frequently Asked Questions) (#REF; #REF; Jijkoun and de #REF;  #TARGET_REF ) .",
                "An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.",
                "In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.",
                "Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Two applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and #REF; #REF), and answer extraction in FAQs (Frequently Asked Questions) (#REF; #REF; Jijkoun and de #REF;  #TARGET_REF ) . An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information. Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses.",
        "output": "nan:Two applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and #REF; #REF), and answer extraction in FAQs (Frequently Asked Questions) (#REF; #REF; Jijkoun and de #REF;  #TARGET_REF<USE/> ) . An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information. Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses."
    },
    {
        "gold": {
            "text": [
                "How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #TARGET_REF ) .",
                "In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.",
                "Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue.",
                "Robots have much lower perceptual capabilities of the environment than humans.",
                "How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to?"
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #TARGET_REF ) . In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment. Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue. Robots have much lower perceptual capabilities of the environment than humans. How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to?",
        "output": "introduction:How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #TARGET_REF<COMP/> ) . In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment. Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue. Robots have much lower perceptual capabilities of the environment than humans. How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to?"
    },
    {
        "gold": {
            "text": [
                "However, synonym mapping is still incomplete in the current state of our subword dictionary.",
                "A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.",
                "We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5).",
                "It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.",
                "This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system.",
                "Alternatively , we may think of user-centered comparative studies ( #TARGET_REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "experiments:However, synonym mapping is still incomplete in the current state of our subword dictionary. A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing. We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5). It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents. This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system. Alternatively , we may think of user-centered comparative studies ( #TARGET_REF ) .",
        "output": "experiments:However, synonym mapping is still incomplete in the current state of our subword dictionary. A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing. We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5). It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents. This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system. Alternatively , we may think of user-centered comparative studies ( #TARGET_REF<MOT/> ) ."
    },
    {
        "gold": {
            "text": [
                "Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.",
                "Many investigators (e.g.",
                "Many investigators ( e.g. #TARGET_REF ; #REF ; #REF ; #REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .",
                "And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in #REF and O'#REF---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem.",
                "As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.",
                "We believe that one reason for the improvement has to do with the increased pitch range that our system uses.",
                "Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. #TARGET_REF ; #REF ; #REF ; #REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in #REF and O'#REF---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses. Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.",
        "output": "experiments:Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Many investigators ( e.g. #TARGET_REF<EXT/> ; #REF ; #REF ; #REF ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech . And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in #REF and O'#REF---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses. Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations."
    },
    {
        "gold": {
            "text": [
                "Although our problem extractor returns a list of clinical problems, we only evaluate performance on identification of the primary problem.",
                "For some abstracts, MeSH headings can be used as ground truth, because one of the human indexers' tasks in assigning terms is to identify the main topic of the article (sometimes a disorder).",
                "For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #TARGET_REF ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Although our problem extractor returns a list of clinical problems, we only evaluate performance on identification of the primary problem. For some abstracts, MeSH headings can be used as ground truth, because one of the human indexers' tasks in assigning terms is to identify the main topic of the article (sometimes a disorder). For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #TARGET_REF .",
        "output": "nan:Although our problem extractor returns a list of clinical problems, we only evaluate performance on identification of the primary problem. For some abstracts, MeSH headings can be used as ground truth, because one of the human indexers' tasks in assigning terms is to identify the main topic of the article (sometimes a disorder). For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #TARGET_REF<FUT/> ."
    },
    {
        "gold": {
            "text": [
                "#REF did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.",
                "They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.",
                "This paper made the first attempt on Chinese SRL and produced promising results.",
                "After the PropBank (#REF) was built,  and #REF have produced more complete and systematic research on Chinese SRL.",
                "#REF has made some preliminary attempt on the idea of hierarchical semantic role labeling.",
                "However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.",
                "So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.",
                "#REF did very encouraging work on the feature calibration of semantic role labeling.",
                "They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.",
                "For semantic analysis, developing features that capture the right kind of information is crucial.",
                "Experiments on Chinese SRL ( #REF , #TARGET_REF ) reassured these findings ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:#REF did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (#REF) was built,  and #REF have produced more complete and systematic research on Chinese SRL. #REF has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #REF did very encouraging work on the feature calibration of semantic role labeling. They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL ( #REF , #TARGET_REF ) reassured these findings .",
        "output": "introduction:#REF did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (#REF) was built,  and #REF have produced more complete and systematic research on Chinese SRL. #REF has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #REF did very encouraging work on the feature calibration of semantic role labeling. They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL ( #REF , #TARGET_REF<BACK/> ) reassured these findings ."
    },
    {
        "gold": {
            "text": [
                "So far, we have only evaluated models trained on gold POS tag set and morphological feature values.",
                "Some researchers , however , including #TARGET_REF , train on predicted feature values instead .",
                "It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test.",
                "But we argue that it also makes sense that training on a combination of gold and predicted features (one copy of each) might do even better, because good predictions of feature values are reinforced (since they repeat the gold patterns), whereas noisy predicted feature values are still represented in training (in patterns that do not repeat the gold). 21",
                "To test our hypothesis, we start this section by comparing three variations:"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:So far, we have only evaluated models trained on gold POS tag set and morphological feature values. Some researchers , however , including #TARGET_REF , train on predicted feature values instead . It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test. But we argue that it also makes sense that training on a combination of gold and predicted features (one copy of each) might do even better, because good predictions of feature values are reinforced (since they repeat the gold patterns), whereas noisy predicted feature values are still represented in training (in patterns that do not repeat the gold). 21 To test our hypothesis, we start this section by comparing three variations:",
        "output": "experiments:So far, we have only evaluated models trained on gold POS tag set and morphological feature values. Some researchers , however , including #TARGET_REF<USE/> , train on predicted feature values instead . It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test. But we argue that it also makes sense that training on a combination of gold and predicted features (one copy of each) might do even better, because good predictions of feature values are reinforced (since they repeat the gold patterns), whereas noisy predicted feature values are still represented in training (in patterns that do not repeat the gold). 21 To test our hypothesis, we start this section by comparing three variations:"
    },
    {
        "gold": {
            "text": [
                "WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #REFb ) , a video-recording programming system , a schedule management system ( #REFa ) , and a weather infomiation system ( #TARGET_REF ) .",
                "The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.",
                "A sample dialogue between this system and a naive user is shown in Figure 2.",
                "This system employs HTK as the speech recognition engine.",
                "The weather information system can answer the user's questions about weather forecasts in Japan.",
                "The vocabulary size is around 500, and the number of phrase structure rules is 31.",
                "The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #REFb ) , a video-recording programming system , a schedule management system ( #REFa ) , and a weather infomiation system ( #TARGET_REF ) . The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue between this system and a naive user is shown in Figure 2. This system employs HTK as the speech recognition engine. The weather information system can answer the user's questions about weather forecasts in Japan. The vocabulary size is around 500, and the number of phrase structure rules is 31. The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000.",
        "output": "experiments:WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #REFb ) , a video-recording programming system , a schedule management system ( #REFa ) , and a weather infomiation system ( #TARGET_REF<COMP/> ) . The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue between this system and a naive user is shown in Figure 2. This system employs HTK as the speech recognition engine. The weather information system can answer the user's questions about weather forecasts in Japan. The vocabulary size is around 500, and the number of phrase structure rules is 31. The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000."
    },
    {
        "gold": {
            "text": [
                "For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeillé FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above.",
                "Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising.",
                "To address this problem , we are currently working on developing a metagrammar in the sense of ( #TARGET_REF ) .",
                "This metagrammar allows us to factorise both syntactic and semantic information.",
                "Syntactic information is factorised in the usual way.",
                "For instance, there will be a class NOVN1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeillé FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above. Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. To address this problem , we are currently working on developing a metagrammar in the sense of ( #TARGET_REF ) . This metagrammar allows us to factorise both syntactic and semantic information. Syntactic information is factorised in the usual way. For instance, there will be a class NOVN1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur.",
        "output": "nan:For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeillé FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above. Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. To address this problem , we are currently working on developing a metagrammar in the sense of ( #TARGET_REF<MOT/> ) . This metagrammar allows us to factorise both syntactic and semantic information. Syntactic information is factorised in the usual way. For instance, there will be a class NOVN1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur."
    },
    {
        "gold": {
            "text": [
                "The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages.",
                "Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment.",
                "Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF).",
                "But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #TARGET_REF ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #TARGET_REF ) .",
        "output": "introduction:The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #TARGET_REF<EXT/> ) ."
    },
    {
        "gold": {
            "text": [
                "The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( #TARGET_REF ) .",
                "The parser, written in Prolog, implements a classic constituency English grammar from #REF.",
                "Pairs of syntactic units connected by grammatical relations are extracted from the parse trees.",
                "A dependency parser would produce a similar output, but DIPETT also provides verb subcategorization information (such as, for example, subject-verb-object or subject-verb-objectindirect object), which we use to select the (best) matching syntactic structures."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( #TARGET_REF ) . The parser, written in Prolog, implements a classic constituency English grammar from #REF. Pairs of syntactic units connected by grammatical relations are extracted from the parse trees. A dependency parser would produce a similar output, but DIPETT also provides verb subcategorization information (such as, for example, subject-verb-object or subject-verb-objectindirect object), which we use to select the (best) matching syntactic structures.",
        "output": "experiments:The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( #TARGET_REF<FUT/> ) . The parser, written in Prolog, implements a classic constituency English grammar from #REF. Pairs of syntactic units connected by grammatical relations are extracted from the parse trees. A dependency parser would produce a similar output, but DIPETT also provides verb subcategorization information (such as, for example, subject-verb-object or subject-verb-objectindirect object), which we use to select the (best) matching syntactic structures."
    },
    {
        "gold": {
            "text": [
                "ment ( #TARGET_REF ; #REF ; #REF ) .",
                "These works are re- stricted to each closed community, and the rela- tion between them is not well discussed.",
                "Investi- gating the relation will be apparently valuable for both communities."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:ment ( #TARGET_REF ; #REF ; #REF ) . These works are re- stricted to each closed community, and the rela- tion between them is not well discussed. Investi- gating the relation will be apparently valuable for both communities.",
        "output": "introduction:ment ( #TARGET_REF<BACK/> ; #REF ; #REF ) . These works are re- stricted to each closed community, and the rela- tion between them is not well discussed. Investi- gating the relation will be apparently valuable for both communities."
    },
    {
        "gold": {
            "text": [
                "Another common approach to lexical rules is to encode them as unary phrase structure rules.",
                "This approach is taken, for example, in LKB (#REF) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (#REF, 31).",
                "A similar method is included in PATR-II ( #TARGET_REF ) and can be used to encode lexical rules as binary relations in the CUF system ( #REF ; #REFb ) or the TFS system ( #REF ; #REF ) .",
                "The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken, for example, in LKB (#REF) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (#REF, 31). A similar method is included in PATR-II ( #TARGET_REF ) and can be used to encode lexical rules as binary relations in the CUF system ( #REF ; #REFb ) or the TFS system ( #REF ; #REF ) . The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.",
        "output": "related work:Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken, for example, in LKB (#REF) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (#REF, 31). A similar method is included in PATR-II ( #TARGET_REF<USE/> ) and can be used to encode lexical rules as binary relations in the CUF system ( #REF ; #REFb ) or the TFS system ( #REF ; #REF ) . The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."
    },
    {
        "gold": {
            "text": [
                "In this paper, we describes a lexicon organized around systematic polysemy.",
                "The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (#REF).",
                "In our previous work ( #TARGET_REF ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .",
                "In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy.",
                "We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (#REF) and DSO (#REF).",
                "The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better values (#REF) than arbitrary sense groupings on the agreement data."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:In this paper, we describes a lexicon organized around systematic polysemy. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (#REF). In our previous work ( #TARGET_REF ) , we applied this method to a small subset of WordNet nouns and showed potential applicability . In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy. We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (#REF) and DSO (#REF). The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better values (#REF) than arbitrary sense groupings on the agreement data.",
        "output": "introduction:In this paper, we describes a lexicon organized around systematic polysemy. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (#REF). In our previous work ( #TARGET_REF<COMP/> ) , we applied this method to a small subset of WordNet nouns and showed potential applicability . In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy. We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (#REF) and DSO (#REF). The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better values (#REF) than arbitrary sense groupings on the agreement data."
    },
    {
        "gold": {
            "text": [
                "We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.",
                "Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #REF ; #TARGET_REF ) for self training .",
                "Self-training should also benefit other discriminatively trained parsers with latent annotations (#REF), although training would be much slower compared to using generative models, as in our case."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #REF ; #TARGET_REF ) for self training . Self-training should also benefit other discriminatively trained parsers with latent annotations (#REF), although training would be much slower compared to using generative models, as in our case.",
        "output": "conclusion:We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #REF ; #TARGET_REF<MOT/> ) for self training . Self-training should also benefit other discriminatively trained parsers with latent annotations (#REF), although training would be much slower compared to using generative models, as in our case."
    },
    {
        "gold": {
            "text": [
                "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.",
                "They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #TARGET_REF ) , and TE ( #REF ) .",
                "One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF).",
                "With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.",
                "After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #TARGET_REF ) , and TE ( #REF ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases.",
        "output": "nan:Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #TARGET_REF<EXT/> ) , and TE ( #REF ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases."
    },
    {
        "gold": {
            "text": [
                "Following the notations in Algorithm 2, W b is the baseline weight, D i = { f i j , c i j , r i j } K j=1 denotes training examples for t i .",
                "For the sake of brevity, we will drop the index i, D i = { f j , c j , r j } K j=1 , in the rest of this paper.",
                "Our goal is to find an optimal weight, denoted by W i , which is a local weight and used for decoding the sentence t i .",
                "Unlike the global method which performs tuning on the whole development set Dev + D i as in Algorithm 1, W i can be incrementally learned by optimizing on D i based on W b .",
                "We employ the idea of ultraconservative update ( #TARGET_REF ; #REF ) to propose two incremental methods for local training in Algorithm 2 as follows ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Following the notations in Algorithm 2, W b is the baseline weight, D i = { f i j , c i j , r i j } K j=1 denotes training examples for t i . For the sake of brevity, we will drop the index i, D i = { f j , c j , r j } K j=1 , in the rest of this paper. Our goal is to find an optimal weight, denoted by W i , which is a local weight and used for decoding the sentence t i . Unlike the global method which performs tuning on the whole development set Dev + D i as in Algorithm 1, W i can be incrementally learned by optimizing on D i based on W b . We employ the idea of ultraconservative update ( #TARGET_REF ; #REF ) to propose two incremental methods for local training in Algorithm 2 as follows .",
        "output": "nan:Following the notations in Algorithm 2, W b is the baseline weight, D i = { f i j , c i j , r i j } K j=1 denotes training examples for t i . For the sake of brevity, we will drop the index i, D i = { f j , c j , r j } K j=1 , in the rest of this paper. Our goal is to find an optimal weight, denoted by W i , which is a local weight and used for decoding the sentence t i . Unlike the global method which performs tuning on the whole development set Dev + D i as in Algorithm 1, W i can be incrementally learned by optimizing on D i based on W b . We employ the idea of ultraconservative update ( #TARGET_REF<FUT/> ; #REF ) to propose two incremental methods for local training in Algorithm 2 as follows ."
    },
    {
        "gold": {
            "text": [
                "A number of stand-alone tools have also been developed.",
                "For example, the suite of LT tools (#REF;#REF) perform tokenization, tagging and chunking on XML marked-up text directly.",
                "These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.",
                "This gives a greater flexibility but the tradeoff is that these tools can run very slowly.",
                "Other tools have been designed around particular techniques , such as finite state machines ( #REF ; #TARGET_REF ) .",
                "However, the source code for these tools is not freely available, so they cannot be extended."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:A number of stand-alone tools have also been developed. For example, the suite of LT tools (#REF;#REF) perform tokenization, tagging and chunking on XML marked-up text directly. These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques , such as finite state machines ( #REF ; #TARGET_REF ) . However, the source code for these tools is not freely available, so they cannot be extended.",
        "output": "experiments:A number of stand-alone tools have also been developed. For example, the suite of LT tools (#REF;#REF) perform tokenization, tagging and chunking on XML marked-up text directly. These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files. This gives a greater flexibility but the tradeoff is that these tools can run very slowly. Other tools have been designed around particular techniques , such as finite state machines ( #REF ; #TARGET_REF<BACK/> ) . However, the source code for these tools is not freely available, so they cannot be extended."
    },
    {
        "gold": {
            "text": [
                "written as fully specified relations between words, rather, only what is supposed to be changed is specified.",
                "Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #TARGET_REF , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .",
                "This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 ° The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.",
                "The index that the subject bore in the input is assigned to an optional prepositional complement in the output."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:written as fully specified relations between words, rather, only what is supposed to be changed is specified. Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #TARGET_REF , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch . This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 ° The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output. The index that the subject bore in the input is assigned to an optional prepositional complement in the output.",
        "output": "introduction:written as fully specified relations between words, rather, only what is supposed to be changed is specified. Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #TARGET_REF<USE/> , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch . This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 ° The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output. The index that the subject bore in the input is assigned to an optional prepositional complement in the output."
    },
    {
        "gold": {
            "text": [
                "The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.",
                "This section , which elaborates on preliminary results reported in #TARGET_REF , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .",
                "For an example of a completely annotated abstract, see Figure 2.",
                "Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering. This section , which elaborates on preliminary results reported in #TARGET_REF , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence . For an example of a completely annotated abstract, see Figure 2. Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases.",
        "output": "nan:The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering. This section , which elaborates on preliminary results reported in #TARGET_REF<COMP/> , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence . For an example of a completely annotated abstract, see Figure 2. Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases."
    },
    {
        "gold": {
            "text": [
                "We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.",
                "Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (#REF;#REF) for self training.",
                "Self-training should also benefit other discriminatively trained parsers with latent annotations ( #TARGET_REF ) , although training would be much slower compared to using generative models , as in our case ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (#REF;#REF) for self training. Self-training should also benefit other discriminatively trained parsers with latent annotations ( #TARGET_REF ) , although training would be much slower compared to using generative models , as in our case .",
        "output": "conclusion:We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (#REF;#REF) for self training. Self-training should also benefit other discriminatively trained parsers with latent annotations ( #TARGET_REF<MOT/> ) , although training would be much slower compared to using generative models , as in our case ."
    },
    {
        "gold": {
            "text": [
                "Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.",
                "An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.",
                "We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.",
                "( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #TARGET_REFa ) ) .",
                "Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun. We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type. ( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #TARGET_REFa ) ) . Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP.",
        "output": "introduction:Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun. We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type. ( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #TARGET_REF<EXT/>a ) ) . Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."
    },
    {
        "gold": {
            "text": [
                "For better comparison with work of others , we adopt the suggestion made by #TARGET_REF to evaluate the parsing quality on sentences up to 70 tokens long .",
                "We report these filtered results in Table 14.",
                "Filtered results are consistently higher (as expected).",
                "Results are about 0.9% absolute higher on the development set, and about 0.6% higher on the test set.",
                "The contribution of the RAT feature across sets is negligible (or small and unstable), resulting in less than 0.1% absolute loss on the dev set, but about 0.15% gain on the test set.",
                "For clarity and conciseness, we only show the best model (with RAT) in Table 14."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "conclusion:For better comparison with work of others , we adopt the suggestion made by #TARGET_REF to evaluate the parsing quality on sentences up to 70 tokens long . We report these filtered results in Table 14. Filtered results are consistently higher (as expected). Results are about 0.9% absolute higher on the development set, and about 0.6% higher on the test set. The contribution of the RAT feature across sets is negligible (or small and unstable), resulting in less than 0.1% absolute loss on the dev set, but about 0.15% gain on the test set. For clarity and conciseness, we only show the best model (with RAT) in Table 14.",
        "output": "conclusion:For better comparison with work of others , we adopt the suggestion made by #TARGET_REF<FUT/> to evaluate the parsing quality on sentences up to 70 tokens long . We report these filtered results in Table 14. Filtered results are consistently higher (as expected). Results are about 0.9% absolute higher on the development set, and about 0.6% higher on the test set. The contribution of the RAT feature across sets is negligible (or small and unstable), resulting in less than 0.1% absolute loss on the dev set, but about 0.15% gain on the test set. For clarity and conciseness, we only show the best model (with RAT) in Table 14."
    },
    {
        "gold": {
            "text": [
                "Many different features have been used to represent documents where an ambiguous name is mentioned.",
                "The most basic is a Bag of Words (BoW) representation of the document text.",
                "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (#REF;#REF).",
                "Nevertheless, the full document text is present in most systems, sometimes as the only feature (#REF) and sometimes in combination with otherssee for instance (#REF;#REF)-.",
                "Other representations use the link structure (#REF) or generate graph representations of the extracted features (#REF).",
                "Some researchers ( #TARGET_REF ; #REF ) have explored the use of Wikipedia information to improve the disambiguation process .",
                "Wikipedia provides candidate entities that are linked to specific mentions in a text.",
                "The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.",
                "These approaches are yet to be applied to the specific task of grouping search results."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (#REF;#REF). Nevertheless, the full document text is present in most systems, sometimes as the only feature (#REF) and sometimes in combination with otherssee for instance (#REF;#REF)-. Other representations use the link structure (#REF) or generate graph representations of the extracted features (#REF). Some researchers ( #TARGET_REF ; #REF ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results.",
        "output": "related work:Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (#REF;#REF). Nevertheless, the full document text is present in most systems, sometimes as the only feature (#REF) and sometimes in combination with otherssee for instance (#REF;#REF)-. Other representations use the link structure (#REF) or generate graph representations of the extracted features (#REF). Some researchers ( #TARGET_REF<BACK/> ; #REF ) have explored the use of Wikipedia information to improve the disambiguation process . Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results."
    },
    {
        "gold": {
            "text": [
                "Several works have proposed discriminative tech- niques to train log-linear model for SMT.",
                "(#REF; #REF) used maximum likelihood estimation to learn weights for MT.",
                "(#REF; #REF; #REF; #TARGET_REF ) employed an evaluation metric as a loss function and directly optimized it.",
                "(#REF; #REF; #REF) proposed other optimiza- tion objectives by introducing a margin-based and ranking-based indirect loss functions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Several works have proposed discriminative tech- niques to train log-linear model for SMT. (#REF; #REF) used maximum likelihood estimation to learn weights for MT. (#REF; #REF; #REF; #TARGET_REF ) employed an evaluation metric as a loss function and directly optimized it. (#REF; #REF; #REF) proposed other optimiza- tion objectives by introducing a margin-based and ranking-based indirect loss functions.",
        "output": "related work:Several works have proposed discriminative tech- niques to train log-linear model for SMT. (#REF; #REF) used maximum likelihood estimation to learn weights for MT. (#REF; #REF; #REF; #TARGET_REF<USE/> ) employed an evaluation metric as a loss function and directly optimized it. (#REF; #REF; #REF) proposed other optimiza- tion objectives by introducing a margin-based and ranking-based indirect loss functions."
    },
    {
        "gold": {
            "text": [
                "We instead propose a novel approach that extracts rules from packed forests (Section 3), which compactly encodes many more alternatives than kbest lists.",
                "Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( #REF ; #TARGET_REF ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .",
                "When combined with our previous orthogonal work on forest-based decoding (#REF), the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:We instead propose a novel approach that extracts rules from packed forests (Section 3), which compactly encodes many more alternatives than kbest lists. Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( #REF ; #TARGET_REF ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses . When combined with our previous orthogonal work on forest-based decoding (#REF), the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date.",
        "output": "introduction:We instead propose a novel approach that extracts rules from packed forests (Section 3), which compactly encodes many more alternatives than kbest lists. Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( #REF ; #TARGET_REF<COMP/> ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses . When combined with our previous orthogonal work on forest-based decoding (#REF), the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date."
    },
    {
        "gold": {
            "text": [
                "It is clear that the type of answers desired by physicians require a level of semantic analysis that is beyond the current state of the art, even with the aid of existing medical ontologies.",
                "For example, even the seemingly straightforward task of identifying similarities and differences in outcome statements is rendered exceedingly complex by the tremendous amount of background medical knowledge that must be brought to bear in interpreting clinical results and subtle differences in study design, objectives, and results; the closest analogous task in computational linguistics-redundancy detection for multi-document summarization-seems easy by comparison.",
                "Furthermore, it is unclear if textual strings make \"good answers.\"",
                "Perhaps a graphical rendering of the semantic predicates present in relevant abstracts might more effectively convey the desired information; see, for example, Fiszman, Rindflesch, and #REF.",
                "Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #TARGET_REF ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:It is clear that the type of answers desired by physicians require a level of semantic analysis that is beyond the current state of the art, even with the aid of existing medical ontologies. For example, even the seemingly straightforward task of identifying similarities and differences in outcome statements is rendered exceedingly complex by the tremendous amount of background medical knowledge that must be brought to bear in interpreting clinical results and subtle differences in study design, objectives, and results; the closest analogous task in computational linguistics-redundancy detection for multi-document summarization-seems easy by comparison. Furthermore, it is unclear if textual strings make \"good answers.\" Perhaps a graphical rendering of the semantic predicates present in relevant abstracts might more effectively convey the desired information; see, for example, Fiszman, Rindflesch, and #REF. Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #TARGET_REF .",
        "output": "nan:It is clear that the type of answers desired by physicians require a level of semantic analysis that is beyond the current state of the art, even with the aid of existing medical ontologies. For example, even the seemingly straightforward task of identifying similarities and differences in outcome statements is rendered exceedingly complex by the tremendous amount of background medical knowledge that must be brought to bear in interpreting clinical results and subtle differences in study design, objectives, and results; the closest analogous task in computational linguistics-redundancy detection for multi-document summarization-seems easy by comparison. Furthermore, it is unclear if textual strings make \"good answers.\" Perhaps a graphical rendering of the semantic predicates present in relevant abstracts might more effectively convey the desired information; see, for example, Fiszman, Rindflesch, and #REF. Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #TARGET_REF<MOT/> ."
    },
    {
        "gold": {
            "text": [
                "P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.",
                "Inspired by ( #REF ) and ( #TARGET_REF ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "method:P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables. Inspired by ( #REF ) and ( #TARGET_REF ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .",
        "output": "method:P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables. Inspired by ( #REF ) and ( #TARGET_REF<EXT/> ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string ."
    },
    {
        "gold": {
            "text": [
                "In an attempt to further boost performance, we employed Linear Discriminant Analysis (LDA) to find a linear projection of the four-dimensional vec- tors that maximizes the separation of the Gaussians (corresponding to the HMM states).",
                "#TARGET_REF describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space .",
                "Each sentence/vector is then mul- tiplied by this matrix, and new HMM models are re-computed from the projected data."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:In an attempt to further boost performance, we employed Linear Discriminant Analysis (LDA) to find a linear projection of the four-dimensional vec- tors that maximizes the separation of the Gaussians (corresponding to the HMM states). #TARGET_REF describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space . Each sentence/vector is then mul- tiplied by this matrix, and new HMM models are re-computed from the projected data.",
        "output": "method:In an attempt to further boost performance, we employed Linear Discriminant Analysis (LDA) to find a linear projection of the four-dimensional vec- tors that maximizes the separation of the Gaussians (corresponding to the HMM states). #TARGET_REF<FUT/> describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space . Each sentence/vector is then mul- tiplied by this matrix, and new HMM models are re-computed from the projected data."
    },
    {
        "gold": {
            "text": [
                "Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (#REF;#REF).",
                "These include, just to mention the most popular ones , DIRT ( #REF ) , VerbOcean ( #REF ) , FrameNet ( #REF ) , and Wikipedia ( #TARGET_REF ; #REF ) .",
                "DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.",
                "VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.",
                "FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.",
                "It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.",
                "Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (#REF;#REF). These include, just to mention the most popular ones , DIRT ( #REF ) , VerbOcean ( #REF ) , FrameNet ( #REF ) , and Wikipedia ( #TARGET_REF ; #REF ) . DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses. Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.",
        "output": "introduction:Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (#REF;#REF). These include, just to mention the most popular ones , DIRT ( #REF ) , VerbOcean ( #REF ) , FrameNet ( #REF ) , and Wikipedia ( #TARGET_REF<BACK/> ; #REF ) . DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses. Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores."
    },
    {
        "gold": {
            "text": [
                "The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX #TARGET_REF ) .",
                "#REF).",
                "KRYPTON's A-box, encoding the object theory as a set of assertions, uses standard first order logic; the T-box contains information expressed in a frame-based language equivalent to a fragment of FOL.",
                "However, the distinction between the two parts is purely functional--that is, characterized in terms of the system's behavior.",
                "From the logical point of view, the knowledge base is the union of the two boxes, i.e. a theory, and the entailment is standard.",
                "In our system, we also distinguish between the \"definitional\" and factual information, but the \"definitional\" part contains collections of mutually excluding theories, not just of formulas describing a semantic network.",
                "Moreover, in addition to proposing this structure of R, we have described the two mechanisms for exploiting it, \"coherence\" and \"dominance,\" which are not variants of the standard first order entailment, but abduction."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX #TARGET_REF ) . #REF). KRYPTON's A-box, encoding the object theory as a set of assertions, uses standard first order logic; the T-box contains information expressed in a frame-based language equivalent to a fragment of FOL. However, the distinction between the two parts is purely functional--that is, characterized in terms of the system's behavior. From the logical point of view, the knowledge base is the union of the two boxes, i.e. a theory, and the entailment is standard. In our system, we also distinguish between the \"definitional\" and factual information, but the \"definitional\" part contains collections of mutually excluding theories, not just of formulas describing a semantic network. Moreover, in addition to proposing this structure of R, we have described the two mechanisms for exploiting it, \"coherence\" and \"dominance,\" which are not variants of the standard first order entailment, but abduction.",
        "output": "introduction:The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX #TARGET_REF<USE/> ) . #REF). KRYPTON's A-box, encoding the object theory as a set of assertions, uses standard first order logic; the T-box contains information expressed in a frame-based language equivalent to a fragment of FOL. However, the distinction between the two parts is purely functional--that is, characterized in terms of the system's behavior. From the logical point of view, the knowledge base is the union of the two boxes, i.e. a theory, and the entailment is standard. In our system, we also distinguish between the \"definitional\" and factual information, but the \"definitional\" part contains collections of mutually excluding theories, not just of formulas describing a semantic network. Moreover, in addition to proposing this structure of R, we have described the two mechanisms for exploiting it, \"coherence\" and \"dominance,\" which are not variants of the standard first order entailment, but abduction."
    },
    {
        "gold": {
            "text": [
                "In any subcategorisation frame which involves a predicate complement there will be a non-transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence.",
                "In these situations the parser can use the semantic type of the verb to compute this relationship.",
                "Expanding on a suggestion of #TARGET_REF , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .",
                "These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation.",
                "For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement.",
                "Michiels proposed rules for doing this for infinitive complement codes; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP, AP and PP predication (see #REF, for further discussion)."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:In any subcategorisation frame which involves a predicate complement there will be a non-transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence. In these situations the parser can use the semantic type of the verb to compute this relationship. Expanding on a suggestion of #TARGET_REF , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it . These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation. For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement. Michiels proposed rules for doing this for infinitive complement codes; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP, AP and PP predication (see #REF, for further discussion).",
        "output": "nan:In any subcategorisation frame which involves a predicate complement there will be a non-transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence. In these situations the parser can use the semantic type of the verb to compute this relationship. Expanding on a suggestion of #TARGET_REF<COMP/> , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it . These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation. For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement. Michiels proposed rules for doing this for infinitive complement codes; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP, AP and PP predication (see #REF, for further discussion)."
    },
    {
        "gold": {
            "text": [
                "Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.",
                "This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.",
                "Future research should apply the work of #TARGET_REF and #REF , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations. This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension. Future research should apply the work of #TARGET_REF and #REF , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .",
        "output": "conclusion:Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations. This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension. Future research should apply the work of #TARGET_REF<MOT/> and #REF , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars ."
    },
    {
        "gold": {
            "text": [
                "Self-supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling.",
                "Rather, Welling at al. demonstrated its effectiveness in modeling hand-written digits and on synthetic data.",
                "Đn both cases essentially linear classifiers were used as features.",
                "As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples.",
                "Unfortunately , as shown in ( #TARGET_REF ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .",
                "While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data.",
                "This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications.",
                "For this reason we use a different sampling scheme which we refer to as rejection sampling."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:Self-supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling. Rather, Welling at al. demonstrated its effectiveness in modeling hand-written digits and on synthetic data. Đn both cases essentially linear classifiers were used as features. As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately , as shown in ( #TARGET_REF ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data. This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications. For this reason we use a different sampling scheme which we refer to as rejection sampling.",
        "output": "nan:Self-supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling. Rather, Welling at al. demonstrated its effectiveness in modeling hand-written digits and on synthetic data. Đn both cases essentially linear classifiers were used as features. As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately , as shown in ( #TARGET_REF<EXT/> ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) . While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data. This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications. For this reason we use a different sampling scheme which we refer to as rejection sampling."
    },
    {
        "gold": {
            "text": [
                "(2) .",
                "We define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.",
                "Each punctuation symbol is considered a separate token.",
                "Character classes , such as punctuation , are defined according to the Unicode Standard ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:(2) . We define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters. Each punctuation symbol is considered a separate token. Character classes , such as punctuation , are defined according to the Unicode Standard ( #TARGET_REF ) .",
        "output": "nan:(2) . We define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters. Each punctuation symbol is considered a separate token. Character classes , such as punctuation , are defined according to the Unicode Standard ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #TARGET_REF ; #REF ; #REFa ) .",
                "However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.",
                "Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-#REF), certain machine-assisted translation tools (e.g.",
                "(#REF;#REFb)), concordancing for bilingual lexicography (#REF;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &#REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #TARGET_REF ; #REF ; #REFa ) . However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-#REF), certain machine-assisted translation tools (e.g. (#REF;#REFb)), concordancing for bilingual lexicography (#REF;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &#REF).",
        "output": "introduction:Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #TARGET_REF<BACK/> ; #REF ; #REFa ) . However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-#REF), certain machine-assisted translation tools (e.g. (#REF;#REFb)), concordancing for bilingual lexicography (#REF;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &#REF)."
    },
    {
        "gold": {
            "text": [
                "Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and #REF;#REF), and answer extraction in FAQs (Frequently Asked Questions) ( #TARGET_REF; #REF).",
                "An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.",
                "In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.",
                "Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and #REF;#REF), and answer extraction in FAQs (Frequently Asked Questions) ( #TARGET_REF; #REF). An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information. Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses.",
        "output": "nan:Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and #REF;#REF), and answer extraction in FAQs (Frequently Asked Questions) ( #TARGET_REF<USE/>; #REF). An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information. Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses."
    },
    {
        "gold": {
            "text": [
                "Unfortunately, there is no technical documentation or specification concerning this Arabic categorizer.",
                "Sakhr's marketing literature claims that this categorizer is based on Arabic morphology and some research that has been carried out on natural language processing.",
                "The present work evaluates the performance on Arabic documents of the Naïve Bayes algorithm (NB) -one of the simplest algorithms applied to English document categorization (#REF).",
                "The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets.",
                "This work is a continuation of that initiated in ( #TARGET_REF ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .",
                "A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).",
                "The present work expands the work in (#REF) by experimenting with the use of a better root extraction algorithm (El #REF) for document preprocessing, and using a different data set, collected from the largest Arabic site on the web: aljazeera.net."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "related work:Unfortunately, there is no technical documentation or specification concerning this Arabic categorizer. Sakhr's marketing literature claims that this categorizer is based on Arabic morphology and some research that has been carried out on natural language processing. The present work evaluates the performance on Arabic documents of the Naïve Bayes algorithm (NB) -one of the simplest algorithms applied to English document categorization (#REF). The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets. This work is a continuation of that initiated in ( #TARGET_REF ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) . A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories). The present work expands the work in (#REF) by experimenting with the use of a better root extraction algorithm (El #REF) for document preprocessing, and using a different data set, collected from the largest Arabic site on the web: aljazeera.net.",
        "output": "related work:Unfortunately, there is no technical documentation or specification concerning this Arabic categorizer. Sakhr's marketing literature claims that this categorizer is based on Arabic morphology and some research that has been carried out on natural language processing. The present work evaluates the performance on Arabic documents of the Naïve Bayes algorithm (NB) -one of the simplest algorithms applied to English document categorization (#REF). The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets. This work is a continuation of that initiated in ( #TARGET_REF<COMP/> ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) . A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories). The present work expands the work in (#REF) by experimenting with the use of a better root extraction algorithm (El #REF) for document preprocessing, and using a different data set, collected from the largest Arabic site on the web: aljazeera.net."
    },
    {
        "gold": {
            "text": [
                "Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.",
                "However , the method we are currently using in the ATIS domain ( #TARGET_REF ) represents our most promising approach to this problem .",
                "We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).",
                "The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.",
                "Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to specifications based on its broad-class identity (as one of noun, noun-phrase, predicate, quantifier, etc.).",
                "For example, a [subject] is a noun-phrase node with the label \"topic.\"",
                "During the top-down cycle, it creates a blank frame and inserts it into a \"topic\" slot in the frame that was handed to it.",
                "It passes the blank frame to its children, who will then fill it appropriately, labeling it as a QSET or as a REFERENCE."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group. However , the method we are currently using in the ATIS domain ( #TARGET_REF ) represents our most promising approach to this problem . We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects). The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree. Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to specifications based on its broad-class identity (as one of noun, noun-phrase, predicate, quantifier, etc.). For example, a [subject] is a noun-phrase node with the label \"topic.\" During the top-down cycle, it creates a blank frame and inserts it into a \"topic\" slot in the frame that was handed to it. It passes the blank frame to its children, who will then fill it appropriately, labeling it as a QSET or as a REFERENCE.",
        "output": "nan:Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group. However , the method we are currently using in the ATIS domain ( #TARGET_REF<MOT/> ) represents our most promising approach to this problem . We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects). The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree. Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to specifications based on its broad-class identity (as one of noun, noun-phrase, predicate, quantifier, etc.). For example, a [subject] is a noun-phrase node with the label \"topic.\" During the top-down cycle, it creates a blank frame and inserts it into a \"topic\" slot in the frame that was handed to it. It passes the blank frame to its children, who will then fill it appropriately, labeling it as a QSET or as a REFERENCE."
    },
    {
        "gold": {
            "text": [
                "We developed a web-based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair.",
                "Test subjects were invited via email to participate in the experiment.",
                "Thus, they were not supervised during the experiment.",
                "#TARGET_REF observed that some annotators were not familiar with the exact definition of semantic relatedness .",
                "Their results differed particularly in cases of antonymy or distributionally related pairs.",
                "We created a manual with a detailed introduction to SR stressing the crucial points.",
                "The manual was presented to the subjects before the experiment and could be re-accessed at any time.",
                "During the experiment, one concept pair at a time was presented to the test subjects in random ordering.",
                "Subjects had to assign a discrete relatedness value {0,1,2,3,4} to each pair.",
                "Figure 2 shows the system's GUI."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:We developed a web-based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair. Test subjects were invited via email to participate in the experiment. Thus, they were not supervised during the experiment. #TARGET_REF observed that some annotators were not familiar with the exact definition of semantic relatedness . Their results differed particularly in cases of antonymy or distributionally related pairs. We created a manual with a detailed introduction to SR stressing the crucial points. The manual was presented to the subjects before the experiment and could be re-accessed at any time. During the experiment, one concept pair at a time was presented to the test subjects in random ordering. Subjects had to assign a discrete relatedness value {0,1,2,3,4} to each pair. Figure 2 shows the system's GUI.",
        "output": "experiments:We developed a web-based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair. Test subjects were invited via email to participate in the experiment. Thus, they were not supervised during the experiment. #TARGET_REF<EXT/> observed that some annotators were not familiar with the exact definition of semantic relatedness . Their results differed particularly in cases of antonymy or distributionally related pairs. We created a manual with a detailed introduction to SR stressing the crucial points. The manual was presented to the subjects before the experiment and could be re-accessed at any time. During the experiment, one concept pair at a time was presented to the test subjects in random ordering. Subjects had to assign a discrete relatedness value {0,1,2,3,4} to each pair. Figure 2 shows the system's GUI."
    },
    {
        "gold": {
            "text": [
                "We attempted two approaches for assigning these weights. The first method relied on ad hoc weight selection based on intuition.",
                "The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification ( #TARGET_REF ) , which can be described by the following equation:"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:We attempted two approaches for assigning these weights. The first method relied on ad hoc weight selection based on intuition. The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification ( #TARGET_REF ) , which can be described by the following equation:",
        "output": "nan:We attempted two approaches for assigning these weights. The first method relied on ad hoc weight selection based on intuition. The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification ( #TARGET_REF<FUT/> ) , which can be described by the following equation:"
    },
    {
        "gold": {
            "text": [
                "In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #TARGET_REF ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .",
                "The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation.",
                "Such correspondence is defined in a way that is able to handle some non-standard cases (e.g.",
                "non-projective correspondence)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #TARGET_REF ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two . The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation. Such correspondence is defined in a way that is able to handle some non-standard cases (e.g. non-projective correspondence).",
        "output": "introduction:In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #TARGET_REF<BACK/> ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two . The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation. Such correspondence is defined in a way that is able to handle some non-standard cases (e.g. non-projective correspondence)."
    },
    {
        "gold": {
            "text": [
                "The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).",
                "The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.",
                "Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.",
                "The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1). The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( #REF ; #TARGET_REF ; #REF ) .",
        "output": "nan:The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1). The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( #REF ; #TARGET_REF<USE/> ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "Bridging or associative anaphora has been widely discussed in the linguistic literature (#REF;#REF;#REF;Löbner, 1998).",
                "#REF and #REF include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference).",
                "We follow our previous work ( #TARGET_REFb ) and restrict bridging to non-coreferential cases .",
                "We also exclude comparative anaphora (#REF)."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:Bridging or associative anaphora has been widely discussed in the linguistic literature (#REF;#REF;#REF;Löbner, 1998). #REF and #REF include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work ( #TARGET_REFb ) and restrict bridging to non-coreferential cases . We also exclude comparative anaphora (#REF).",
        "output": "introduction:Bridging or associative anaphora has been widely discussed in the linguistic literature (#REF;#REF;#REF;Löbner, 1998). #REF and #REF include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work ( #TARGET_REF<COMP/>b ) and restrict bridging to non-coreferential cases . We also exclude comparative anaphora (#REF)."
    },
    {
        "gold": {
            "text": [
                "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) .",
        "output": "conclusion:Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #TARGET_REF<MOT/> ; #REF ; #REF ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors.",
                "In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.",
                "For example , modeling CASE in Czech improves Czech parsing ( #TARGET_REF ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .",
                "It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and #REF;Nivre, Boguslavsky, and #REF;."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors. In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. For example , modeling CASE in Czech improves Czech parsing ( #TARGET_REF ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy . It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and #REF;Nivre, Boguslavsky, and #REF;.",
        "output": "introduction:Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors. In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. For example , modeling CASE in Czech improves Czech parsing ( #TARGET_REF<EXT/> ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy . It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and #REF;Nivre, Boguslavsky, and #REF;."
    },
    {
        "gold": {
            "text": [
                "word hypotheses.",
                "As the recogn/fion engine, either VoiceRex, developed by NTI\" (#REF), or HTK from Entropic Research can be used.",
                "Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #TARGET_REF ) .",
                "This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (#REF) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses.",
                "This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.",
                "This module continuously runs and outputs recognition results when it detects a speech interval.",
                "This enables the language generation module to react immediately to user interruptions while the system is speaking.",
                "The language model for speech recognition is a network (regular) grammar, and it allows each speech interval to be an arbitrary number of phrases.",
                "A phrase is a sequence of words, which is to be defined in a domain-dependent way.",
                "Sentences can be decomposed into a couple of phrases."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:word hypotheses. As the recogn/fion engine, either VoiceRex, developed by NTI\" (#REF), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #TARGET_REF ) . This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (#REF) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses. This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously runs and outputs recognition results when it detects a speech interval. This enables the language generation module to react immediately to user interruptions while the system is speaking. The language model for speech recognition is a network (regular) grammar, and it allows each speech interval to be an arbitrary number of phrases. A phrase is a sequence of words, which is to be defined in a domain-dependent way. Sentences can be decomposed into a couple of phrases.",
        "output": "nan:word hypotheses. As the recogn/fion engine, either VoiceRex, developed by NTI\" (#REF), or HTK from Entropic Research can be used. Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #TARGET_REF<FUT/> ) . This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (#REF) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses. This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible. This module continuously runs and outputs recognition results when it detects a speech interval. This enables the language generation module to react immediately to user interruptions while the system is speaking. The language model for speech recognition is a network (regular) grammar, and it allows each speech interval to be an arbitrary number of phrases. A phrase is a sequence of words, which is to be defined in a domain-dependent way. Sentences can be decomposed into a couple of phrases."
    },
    {
        "gold": {
            "text": [
                "The ICA system ( #TARGET_REF ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:The ICA system ( #TARGET_REF ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .",
        "output": "nan:The ICA system ( #TARGET_REF<BACK/> ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance ."
    },
    {
        "gold": {
            "text": [
                "In the first experiment, we use an induction algorithm (#REFa) based on the expectation-maximization (EM) principle that induces parsers for PLTIGs.",
                "The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data.",
                "In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing.",
                "Here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units.",
                "For example, the sentence Several fund managers expect a rough market this morning before prices stabilize.",
                "would be labeled as \"((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)\"",
                "Our algorithm is similar to the approach taken by #TARGET_REF for inducing PCFG parsers ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:In the first experiment, we use an induction algorithm (#REFa) based on the expectation-maximization (EM) principle that induces parsers for PLTIGs. The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data. In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing. Here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units. For example, the sentence Several fund managers expect a rough market this morning before prices stabilize. would be labeled as \"((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)\" Our algorithm is similar to the approach taken by #TARGET_REF for inducing PCFG parsers .",
        "output": "nan:In the first experiment, we use an induction algorithm (#REFa) based on the expectation-maximization (EM) principle that induces parsers for PLTIGs. The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data. In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing. Here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units. For example, the sentence Several fund managers expect a rough market this morning before prices stabilize. would be labeled as \"((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)\" Our algorithm is similar to the approach taken by #TARGET_REF<USE/> for inducing PCFG parsers ."
    },
    {
        "gold": {
            "text": [
                "Our approach to extract and classify social events builds on our previous work ( #TARGET_REF ) , which in turn builds on work from the relation extraction community ( #REF ) .",
                "Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.",
                "Researchers have used other notions of semantics in the literature such as latent semantic analysis (#REF) and relation-specific semantics (#REF;#REF).",
                "To the best of our knowledge, there is only one work that uses frame semantics for relation extraction (#REF).",
                "#REF propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity between two dependency trees.",
                "They, however, do not propose data representations that are based on frame parses and the resulting arborescent structures, instead adding features to syntactic trees.",
                "We believe the implicit feature space of kernels based on our data representation encode a richer and larger feature space than the one proposed by #REF."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "related work:Our approach to extract and classify social events builds on our previous work ( #TARGET_REF ) , which in turn builds on work from the relation extraction community ( #REF ) . Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper. Researchers have used other notions of semantics in the literature such as latent semantic analysis (#REF) and relation-specific semantics (#REF;#REF). To the best of our knowledge, there is only one work that uses frame semantics for relation extraction (#REF). #REF propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity between two dependency trees. They, however, do not propose data representations that are based on frame parses and the resulting arborescent structures, instead adding features to syntactic trees. We believe the implicit feature space of kernels based on our data representation encode a richer and larger feature space than the one proposed by #REF.",
        "output": "related work:Our approach to extract and classify social events builds on our previous work ( #TARGET_REF<COMP/> ) , which in turn builds on work from the relation extraction community ( #REF ) . Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper. Researchers have used other notions of semantics in the literature such as latent semantic analysis (#REF) and relation-specific semantics (#REF;#REF). To the best of our knowledge, there is only one work that uses frame semantics for relation extraction (#REF). #REF propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity between two dependency trees. They, however, do not propose data representations that are based on frame parses and the resulting arborescent structures, instead adding features to syntactic trees. We believe the implicit feature space of kernels based on our data representation encode a richer and larger feature space than the one proposed by #REF."
    },
    {
        "gold": {
            "text": [
                "In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification.",
                "Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.",
                "#REF have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #REF ; #TARGET_REF ) .",
                "The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification. Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes. #REF have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #REF ; #TARGET_REF ) . The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation.",
        "output": "conclusion:In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification. Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes. #REF have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #REF ; #TARGET_REF<MOT/> ) . The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation."
    },
    {
        "gold": {
            "text": [
                "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.",
                "They proved to be useful in a number of NLP applications such as natural language generation ( #TARGET_REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) .",
                "One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF).",
                "With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.",
                "After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #TARGET_REF ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases.",
        "output": "nan:Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation ( #TARGET_REF<EXT/> ) , multidocument summarization ( #REF ) , automatic evaluation of MT ( #REF ) , and TE ( #REF ) . One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (#REF) can be applied to increase the precision of the extracted paraphrases."
    },
    {
        "gold": {
            "text": [
                "A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #TARGET_REF ) .",
        "output": "introduction:A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g.",
                "word sense disambiguation (#REF) or malapropism detection (#REF).",
                "#TARGET_REF argue for application-specific evaluation of similarity measures , because measures are always used for some task .",
                "But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.",
                "A certain measure may work well in one application, but not in another.",
                "Application-based evaluation can only state the fact, but give little explanation about the reasons."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g. word sense disambiguation (#REF) or malapropism detection (#REF). #TARGET_REF argue for application-specific evaluation of similarity measures , because measures are always used for some task . But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality. A certain measure may work well in one application, but not in another. Application-based evaluation can only state the fact, but give little explanation about the reasons.",
        "output": "nan:The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g. word sense disambiguation (#REF) or malapropism detection (#REF). #TARGET_REF<BACK/> argue for application-specific evaluation of similarity measures , because measures are always used for some task . But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality. A certain measure may work well in one application, but not in another. Application-based evaluation can only state the fact, but give little explanation about the reasons."
    },
    {
        "gold": {
            "text": [
                "According to #TARGET_REF , p. 67 ) , these two sentences are incoherent .",
                "However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid.)",
                "suddenly (for Hobbs) becomes coherent.",
                "It seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change; after all, the first two sentences didn't change when the third one was added.",
                "On the other hand, this change is easily explained when we treat the first two sentences as a paragraph: if the third sentence is not a part of the background knowledge, the paragraph is incoherent.",
                "And the paragraph obtained by adding the third sentence is coherent.",
                "Moreover, coherence here is clearly the result of the existence of the topic \"John likes spinach.\""
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:According to #TARGET_REF , p. 67 ) , these two sentences are incoherent . However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid.) suddenly (for Hobbs) becomes coherent. It seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change; after all, the first two sentences didn't change when the third one was added. On the other hand, this change is easily explained when we treat the first two sentences as a paragraph: if the third sentence is not a part of the background knowledge, the paragraph is incoherent. And the paragraph obtained by adding the third sentence is coherent. Moreover, coherence here is clearly the result of the existence of the topic \"John likes spinach.\"",
        "output": "introduction:According to #TARGET_REF<USE/> , p. 67 ) , these two sentences are incoherent . However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid.) suddenly (for Hobbs) becomes coherent. It seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change; after all, the first two sentences didn't change when the third one was added. On the other hand, this change is easily explained when we treat the first two sentences as a paragraph: if the third sentence is not a part of the background knowledge, the paragraph is incoherent. And the paragraph obtained by adding the third sentence is coherent. Moreover, coherence here is clearly the result of the existence of the topic \"John likes spinach.\""
    },
    {
        "gold": {
            "text": [
                "10 Both algorithms are provided by Weka (http://www.cs.waikato.ac.nz/ml/weka/).",
                "provide complementary perspectives.",
                "While the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features.",
                "To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.",
                "Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.",
                "( Details of how the average-expert model performs can be found in our prior work ( #TARGET_REF ) . )"
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:10 Both algorithms are provided by Weka (http://www.cs.waikato.ac.nz/ml/weka/). provide complementary perspectives. While the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features. To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below. Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings. ( Details of how the average-expert model performs can be found in our prior work ( #TARGET_REF ) . )",
        "output": "experiments:10 Both algorithms are provided by Weka (http://www.cs.waikato.ac.nz/ml/weka/). provide complementary perspectives. While the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features. To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below. Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings. ( Details of how the average-expert model performs can be found in our prior work ( #TARGET_REF<COMP/> ) . )"
    },
    {
        "gold": {
            "text": [
                "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) .",
        "output": "conclusion:Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #REFa ; #REFb ; #REF ; #REF ) or shallow semantic trees , ( #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF<MOT/> ) ."
    },
    {
        "gold": {
            "text": [
                "Word reordering between German and English is a complex problem.",
                "Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #TARGET_REF ) , we tried to adapt the same approach to the German-English language pair .",
                "It turned out that there is a larger variety of long reordering patterns in this case.",
                "Nevertheless, some experiments performed after the official evaluation showed promising results.",
                "We plan to pursue this work in several directions: Defining a lattice weighting scheme that distinguishes between original word order and reordering paths could help the decoder select the more promising path through the lattice.",
                "Applying similar reordering rules to the training corpus would reduce the mismatch between the training data and the reordered input sentences.",
                "Finally, it would be useful to explore the impact of different distortion limits on the decoding of reordering lattices in order to find an optimal trade-off between decoderdriven short-range and lattice-driven long-range reordering."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "conclusion:Word reordering between German and English is a complex problem. Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #TARGET_REF ) , we tried to adapt the same approach to the German-English language pair . It turned out that there is a larger variety of long reordering patterns in this case. Nevertheless, some experiments performed after the official evaluation showed promising results. We plan to pursue this work in several directions: Defining a lattice weighting scheme that distinguishes between original word order and reordering paths could help the decoder select the more promising path through the lattice. Applying similar reordering rules to the training corpus would reduce the mismatch between the training data and the reordered input sentences. Finally, it would be useful to explore the impact of different distortion limits on the decoding of reordering lattices in order to find an optimal trade-off between decoderdriven short-range and lattice-driven long-range reordering.",
        "output": "conclusion:Word reordering between German and English is a complex problem. Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #TARGET_REF<EXT/> ) , we tried to adapt the same approach to the German-English language pair . It turned out that there is a larger variety of long reordering patterns in this case. Nevertheless, some experiments performed after the official evaluation showed promising results. We plan to pursue this work in several directions: Defining a lattice weighting scheme that distinguishes between original word order and reordering paths could help the decoder select the more promising path through the lattice. Applying similar reordering rules to the training corpus would reduce the mismatch between the training data and the reordered input sentences. Finally, it would be useful to explore the impact of different distortion limits on the decoding of reordering lattices in order to find an optimal trade-off between decoderdriven short-range and lattice-driven long-range reordering."
    },
    {
        "gold": {
            "text": [
                "Following #REF and #TARGET_REF , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts .",
                "The four states in our HMMs correspond to the information that characterizes each section (\"introduction\", \"methods\", \"results\", and \"conclusions\") and state transitions capture the discourse flow from section to section."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:Following #REF and #TARGET_REF , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts . The four states in our HMMs correspond to the information that characterizes each section (\"introduction\", \"methods\", \"results\", and \"conclusions\") and state transitions capture the discourse flow from section to section.",
        "output": "method:Following #REF and #TARGET_REF<FUT/> , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts . The four states in our HMMs correspond to the information that characterizes each section (\"introduction\", \"methods\", \"results\", and \"conclusions\") and state transitions capture the discourse flow from section to section."
    },
    {
        "gold": {
            "text": [
                "The first step in the application of our methodology is the production of a treebank annotated with LFG f-structure information.",
                "F-structures are attribute-value structures which represent abstract syntactic information, approximating to basic predicate-argument-modifier structures.",
                "Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and #REF ; #TARGET_REF ; Sadler , van Genabith , and #REF ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .",
                "However, more recent work (#REF;Cahill, McCarthy, et al. 2004) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank (#REF), containing more than 1,000,000 words and 49,000 sentences."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "method:The first step in the application of our methodology is the production of a treebank annotated with LFG f-structure information. F-structures are attribute-value structures which represent abstract syntactic information, approximating to basic predicate-argument-modifier structures. Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and #REF ; #TARGET_REF ; Sadler , van Genabith , and #REF ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept . However, more recent work (#REF;Cahill, McCarthy, et al. 2004) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank (#REF), containing more than 1,000,000 words and 49,000 sentences.",
        "output": "method:The first step in the application of our methodology is the production of a treebank annotated with LFG f-structure information. F-structures are attribute-value structures which represent abstract syntactic information, approximating to basic predicate-argument-modifier structures. Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and #REF ; #TARGET_REF<BACK/> ; Sadler , van Genabith , and #REF ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept . However, more recent work (#REF;Cahill, McCarthy, et al. 2004) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank (#REF), containing more than 1,000,000 words and 49,000 sentences."
    },
    {
        "gold": {
            "text": [
                "The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) .",
                "The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.",
                "The best current model (#REF) has only 6% less precision error and only 11% less recall error than the lexicalized model.",
                "The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.",
                "It is also achieved without any explicit notion of lexical head."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) . The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. The best current model (#REF) has only 6% less precision error and only 11% less recall error than the lexicalized model. The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words. It is also achieved without any explicit notion of lexical head.",
        "output": "experiments:The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( #REF ; #REF ; #REF ; #TARGET_REF<USE/> ; #REF ) . The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. The best current model (#REF) has only 6% less precision error and only 11% less recall error than the lexicalized model. The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words. It is also achieved without any explicit notion of lexical head."
    },
    {
        "gold": {
            "text": [
                "The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #TARGET_REF ) .",
                "(#REF).",
                "From all 4,832 user turns, 68.5% were non-angry, 14.3% slightly angry, 5.0% very angry and 12.2% contained garbage, i.e. nonspeech events.",
                "In total, the number of interaction parameters servings as input variables for the model amounts to 52."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #TARGET_REF ) . (#REF). From all 4,832 user turns, 68.5% were non-angry, 14.3% slightly angry, 5.0% very angry and 12.2% contained garbage, i.e. nonspeech events. In total, the number of interaction parameters servings as input variables for the model amounts to 52.",
        "output": "nan:The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #TARGET_REF<COMP/> ) . (#REF). From all 4,832 user turns, 68.5% were non-angry, 14.3% slightly angry, 5.0% very angry and 12.2% contained garbage, i.e. nonspeech events. In total, the number of interaction parameters servings as input variables for the model amounts to 52."
    },
    {
        "gold": {
            "text": [
                "For shuffling paraphrases, french alternations are partially described in (#REF) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.",
                "For complementing this database and for converse constructions , the LADL tables ( #TARGET_REF ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .",
                "In particular, (#REF) lists the converses of some 3 500 predicative nouns."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:For shuffling paraphrases, french alternations are partially described in (#REF) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs. For complementing this database and for converse constructions , the LADL tables ( #TARGET_REF ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions . In particular, (#REF) lists the converses of some 3 500 predicative nouns.",
        "output": "nan:For shuffling paraphrases, french alternations are partially described in (#REF) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs. For complementing this database and for converse constructions , the LADL tables ( #TARGET_REF<MOT/> ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions . In particular, (#REF) lists the converses of some 3 500 predicative nouns."
    },
    {
        "gold": {
            "text": [
                "Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #TARGET_REF ) and Memory-based learning ( MBL ) ( #REF ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .",
                "We will base these components on the design of Weka (#REF)."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #TARGET_REF ) and Memory-based learning ( MBL ) ( #REF ) have been applied to many different problems , so a single interchangeable component should be used to represent each method . We will base these components on the design of Weka (#REF).",
        "output": "nan:Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #TARGET_REF<EXT/> ) and Memory-based learning ( MBL ) ( #REF ) have been applied to many different problems , so a single interchangeable component should be used to represent each method . We will base these components on the design of Weka (#REF)."
    },
    {
        "gold": {
            "text": [
                "We use the TRIPS dialogue parser (#REF) to parse the utterances.",
                "The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.",
                "The contextual interpreter then uses a reference resolution approach similar to #REF , and an ontology mapping mechanism ( #TARGET_REFa ) to produce a domain-specific semantic representation of the student 's output .",
                "Utterance content is represented as a set of extracted objects and relations between them.",
                "Negation is supported, together with a heuristic scoping algorithm.",
                "The interpreter also performs basic ellipsis resolution.",
                "For example, it can determine that in the answer to the question \"Which bulbs will be on and which bulbs will be off in this diagram?\",",
                "\"off\" can be taken to mean \"all bulbs in the di-agram will be off.\"",
                "The resulting output is then passed on to the domain reasoning and diagnosis components."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We use the TRIPS dialogue parser (#REF) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to #REF , and an ontology mapping mechanism ( #TARGET_REFa ) to produce a domain-specific semantic representation of the student 's output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question \"Which bulbs will be on and which bulbs will be off in this diagram?\", \"off\" can be taken to mean \"all bulbs in the di-agram will be off.\" The resulting output is then passed on to the domain reasoning and diagnosis components.",
        "output": "experiments:We use the TRIPS dialogue parser (#REF) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to #REF , and an ontology mapping mechanism ( #TARGET_REF<FUT/>a ) to produce a domain-specific semantic representation of the student 's output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question \"Which bulbs will be on and which bulbs will be off in this diagram?\", \"off\" can be taken to mean \"all bulbs in the di-agram will be off.\" The resulting output is then passed on to the domain reasoning and diagnosis components."
    },
    {
        "gold": {
            "text": [
                "Many approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.",
                "Some works abstract perception via the usage of symbolic logic representations ( #TARGET_REF ; #REF ; #REF ; #REF ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .",
                "Within the latter cat- egory, the two most common representations have been association norms, where subjects are given a cue word and name the first (or several) associated words that come to mind (e.g., #REF), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., #REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Many approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead. Some works abstract perception via the usage of symbolic logic representations ( #TARGET_REF ; #REF ; #REF ; #REF ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Within the latter cat- egory, the two most common representations have been association norms, where subjects are given a cue word and name the first (or several) associated words that come to mind (e.g., #REF), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., #REF).",
        "output": "related work:Many approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead. Some works abstract perception via the usage of symbolic logic representations ( #TARGET_REF<BACK/> ; #REF ; #REF ; #REF ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies . Within the latter cat- egory, the two most common representations have been association norms, where subjects are given a cue word and name the first (or several) associated words that come to mind (e.g., #REF), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., #REF)."
    },
    {
        "gold": {
            "text": [
                "To prove that our method is effective , we also make a comparison between the performances of our system and #TARGET_REF , #REF .",
                "#REF is the best SRL system until now and it has the same data setting with ours.",
                "The results are presented in Table 6  We have to point out that all the three systems are based on Gold standard parsing.",
                "From the table 6, we can find that our system is better than both of the related systems.",
                "Our system has outperformed #REF with a relative error reduction rate of 9.8%."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:To prove that our method is effective , we also make a comparison between the performances of our system and #TARGET_REF , #REF . #REF is the best SRL system until now and it has the same data setting with ours. The results are presented in Table 6  We have to point out that all the three systems are based on Gold standard parsing. From the table 6, we can find that our system is better than both of the related systems. Our system has outperformed #REF with a relative error reduction rate of 9.8%.",
        "output": "experiments:To prove that our method is effective , we also make a comparison between the performances of our system and #TARGET_REF<USE/> , #REF . #REF is the best SRL system until now and it has the same data setting with ours. The results are presented in Table 6  We have to point out that all the three systems are based on Gold standard parsing. From the table 6, we can find that our system is better than both of the related systems. Our system has outperformed #REF with a relative error reduction rate of 9.8%."
    },
    {
        "gold": {
            "text": [
                "magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.",
                "In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv£:;G#REF).",
                "Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #REF ) as discussed in ( #TARGET_REFa ) and ( #REF ) .",
                "Typed feature grammar constraints that are inexpensive to resolve are dealt with using the top-down interpreter of the ConTroll grammar development system (G#REFb) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements. In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv£:;G#REF). Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #REF ) as discussed in ( #TARGET_REFa ) and ( #REF ) . Typed feature grammar constraints that are inexpensive to resolve are dealt with using the top-down interpreter of the ConTroll grammar development system (G#REFb) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation.",
        "output": "introduction:magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements. In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv£:;G#REF). Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #REF ) as discussed in ( #TARGET_REF<COMP/>a ) and ( #REF ) . Typed feature grammar constraints that are inexpensive to resolve are dealt with using the top-down interpreter of the ConTroll grammar development system (G#REFb) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation."
    },
    {
        "gold": {
            "text": [
                "In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.",
                "The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( #TARGET_REF ) .",
                "Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (#REF).",
                "Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment. The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( #TARGET_REF ) . Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (#REF). Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.",
        "output": "conclusion:In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment. The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( #TARGET_REF<MOT/> ) . Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (#REF). Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail."
    },
    {
        "gold": {
            "text": [
                "The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form.",
                "This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.",
                "In this work, we use the Arabic root extraction technique in (El #REF).",
                "It compares favorably to other stemming or root extraction algorithms ( #TARGET_REF ; #REF ; and #REF ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .",
                "In the remainder of this paper, we will use the term \"root\" and \"term\" interchangeably to refer to canonical forms obtained through this root extraction process."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form. This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root. In this work, we use the Arabic root extraction technique in (El #REF). It compares favorably to other stemming or root extraction algorithms ( #TARGET_REF ; #REF ; and #REF ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs . In the remainder of this paper, we will use the term \"root\" and \"term\" interchangeably to refer to canonical forms obtained through this root extraction process.",
        "output": "nan:The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form. This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root. In this work, we use the Arabic root extraction technique in (El #REF). It compares favorably to other stemming or root extraction algorithms ( #TARGET_REF<EXT/> ; #REF ; and #REF ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs . In the remainder of this paper, we will use the term \"root\" and \"term\" interchangeably to refer to canonical forms obtained through this root extraction process."
    },
    {
        "gold": {
            "text": [
                "As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #TARGET_REF ) thus ensuring an additional level of abstraction .",
                "The metagrammar is an abstract specification of the linguistic properties (phrase structure, valency, realisation of grammatical functions etc.) encoded in the grammar basic units.",
                "This specification is then compiled to automatically produce a specific grammar."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #TARGET_REF ) thus ensuring an additional level of abstraction . The metagrammar is an abstract specification of the linguistic properties (phrase structure, valency, realisation of grammatical functions etc.) encoded in the grammar basic units. This specification is then compiled to automatically produce a specific grammar.",
        "output": "introduction:As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #TARGET_REF<FUT/> ) thus ensuring an additional level of abstraction . The metagrammar is an abstract specification of the linguistic properties (phrase structure, valency, realisation of grammatical functions etc.) encoded in the grammar basic units. This specification is then compiled to automatically produce a specific grammar."
    },
    {
        "gold": {
            "text": [
                "Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.",
                "Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #REF ) , neural networks ( #REF ) , and maximum-entropy modeling ( #TARGET_REF ) .",
                "Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.",
                "Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.",
                "This implies an investment in the data annotation phase."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #REF ) , neural networks ( #REF ) , and maximum-entropy modeling ( #TARGET_REF ) . Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign. Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training. This implies an investment in the data annotation phase.",
        "output": "nan:Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #REF ) , neural networks ( #REF ) , and maximum-entropy modeling ( #TARGET_REF<BACK/> ) . Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign. Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training. This implies an investment in the data annotation phase."
    },
    {
        "gold": {
            "text": [
                "Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.",
                "They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state. They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #TARGET_REF .",
        "output": "nan:Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state. They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #TARGET_REF<USE/> ."
    },
    {
        "gold": {
            "text": [
                "Although C++ is extremely efficient, it is not suitable for rapidly gluing components together to form new tools.",
                "To overcome this problem we have implemented an interface to the infrastructure in the Python scripting language.",
                "Python has a number of advantages over other options, such as Java and Perl.",
                "Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.",
                "It has already been used to implement a framework for teaching NLP ( #TARGET_REF ) ."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "nan:Although C++ is extremely efficient, it is not suitable for rapidly gluing components together to form new tools. To overcome this problem we have implemented an interface to the infrastructure in the Python scripting language. Python has a number of advantages over other options, such as Java and Perl. Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation. It has already been used to implement a framework for teaching NLP ( #TARGET_REF ) .",
        "output": "nan:Although C++ is extremely efficient, it is not suitable for rapidly gluing components together to form new tools. To overcome this problem we have implemented an interface to the infrastructure in the Python scripting language. Python has a number of advantages over other options, such as Java and Perl. Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation. It has already been used to implement a framework for teaching NLP ( #TARGET_REF<COMP/> ) ."
    },
    {
        "gold": {
            "text": [
                "In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.",
                "The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn't explicitly explain the reason why different terminology is needed (#REF).",
                "Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #TARGET_REF ) .",
                "Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment. The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn't explicitly explain the reason why different terminology is needed (#REF). Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #TARGET_REF ) . Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.",
        "output": "conclusion:In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment. The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn't explicitly explain the reason why different terminology is needed (#REF). Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #TARGET_REF<MOT/> ) . Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail."
    },
    {
        "gold": {
            "text": [
                "Using the initial target U-trees, source sentences and word alignment, we extract minimal GHKM translation rules 7 in terms of frontier nodes (#REF).",
                "Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.",
                "For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.",
                "In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.",
                "Differently , #TARGET_REF designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .",
                "We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.",
                "This subject will be one of our future work topics."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "method:Using the initial target U-trees, source sentences and word alignment, we extract minimal GHKM translation rules 7 in terms of frontier nodes (#REF). Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment. For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler. Differently , #TARGET_REF designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment . We believe that it is possible to investigate the space of both tree structure and alignment simultaneously. This subject will be one of our future work topics.",
        "output": "method:Using the initial target U-trees, source sentences and word alignment, we extract minimal GHKM translation rules 7 in terms of frontier nodes (#REF). Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment. For example, the bold italic nodes with shadows in Figure 2 are frontier nodes. In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler. Differently , #TARGET_REF<EXT/> designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment . We believe that it is possible to investigate the space of both tree structure and alignment simultaneously. This subject will be one of our future work topics."
    },
    {
        "gold": {
            "text": [
                "Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.",
                "The direction of the slash operator gives the behavior of the function.",
                "A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject.",
                "We follow #REF in allowing a small set of generic, linguistically-plausible unary and binary grammar rules.",
                "We further add rules for combining with punctuation to the left and right and allow for the merge rule X â X X of #TARGET_REF ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1. The direction of the slash operator gives the behavior of the function. A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject. We follow #REF in allowing a small set of generic, linguistically-plausible unary and binary grammar rules. We further add rules for combining with punctuation to the left and right and allow for the merge rule X â X X of #TARGET_REF .",
        "output": "nan:Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1. The direction of the slash operator gives the behavior of the function. A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject. We follow #REF in allowing a small set of generic, linguistically-plausible unary and binary grammar rules. We further add rules for combining with punctuation to the left and right and allow for the merge rule X â X X of #TARGET_REF<FUT/> ."
    },
    {
        "gold": {
            "text": [
                "Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.",
                "In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an \"understanding\" of the intended message.",
                "In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.",
                "Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.",
                "Representative systems are described in #REF , De #REF , #REF , #REF , and #TARGET_REF ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe. In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an \"understanding\" of the intended message. In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in #REF , De #REF , #REF , #REF , and #TARGET_REF .",
        "output": "introduction:Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe. In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an \"understanding\" of the intended message. In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in #REF , De #REF , #REF , #REF , and #TARGET_REF<BACK/> ."
    },
    {
        "gold": {
            "text": [
                "Another common approach is term translation, e.g., via a bilingual lexicon. (#REF; #TARGET_REF ; #REF).",
                "While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.",
                "Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de #REF; #REF.",
                "#REF studied the issue of disarnbiguation for mono-lingual IR."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Another common approach is term translation, e.g., via a bilingual lexicon. (#REF; #TARGET_REF ; #REF). While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de #REF; #REF. #REF studied the issue of disarnbiguation for mono-lingual IR.",
        "output": "related work:Another common approach is term translation, e.g., via a bilingual lexicon. (#REF; #TARGET_REF<USE/> ; #REF). While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de #REF; #REF. #REF studied the issue of disarnbiguation for mono-lingual IR."
    },
    {
        "gold": {
            "text": [
                "In our prior work ( #TARGET_REF ) , we examined whether techniques used for predicting the helpfulness of product reviews ( #REF ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review .",
                "While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating (e.g.",
                "author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model.",
                "In fact, we observe that peer-review helpfulness seems to differ not only between students and experts (example 1), but also between types of experts (example 2)."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "introduction:In our prior work ( #TARGET_REF ) , we examined whether techniques used for predicting the helpfulness of product reviews ( #REF ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review . While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating (e.g. author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model. In fact, we observe that peer-review helpfulness seems to differ not only between students and experts (example 1), but also between types of experts (example 2).",
        "output": "introduction:In our prior work ( #TARGET_REF<COMP/> ) , we examined whether techniques used for predicting the helpfulness of product reviews ( #REF ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review . While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating (e.g. author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model. In fact, we observe that peer-review helpfulness seems to differ not only between students and experts (example 1), but also between types of experts (example 2)."
    },
    {
        "gold": {
            "text": [
                "There are several stategies that might be pursued.",
                "One is to adopt Pinkal's \"radical underspecification\" approach (#REF) and use underspecified representations for all types of ambiguity, even syntactic ambiguity.",
                "The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( #TARGET_REF ) , with the resolution process as described here .",
                "Alternatively, I believe it is worth exploring the approach to disambiguation described in #REF, which would mesh nicely with the theory presented here."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:There are several stategies that might be pursued. One is to adopt Pinkal's \"radical underspecification\" approach (#REF) and use underspecified representations for all types of ambiguity, even syntactic ambiguity. The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( #TARGET_REF ) , with the resolution process as described here . Alternatively, I believe it is worth exploring the approach to disambiguation described in #REF, which would mesh nicely with the theory presented here.",
        "output": "nan:There are several stategies that might be pursued. One is to adopt Pinkal's \"radical underspecification\" approach (#REF) and use underspecified representations for all types of ambiguity, even syntactic ambiguity. The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( #TARGET_REF<MOT/> ) , with the resolution process as described here . Alternatively, I believe it is worth exploring the approach to disambiguation described in #REF, which would mesh nicely with the theory presented here."
    },
    {
        "gold": {
            "text": [
                "Due to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here.",
                "Motivated by (#TARGET_REF, 2003; #REF), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW−WbII2+ A � j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[αW · h(fj, e)] Pα(e|fj; W) = (7) Ee'Ec; exp[αW · h(fj, e')], where α > 0 is a real number valued smoother.",
                "One can see that, in the extreme case, for α —* oc, (6) converges to (5)."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:Due to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here. Motivated by (#TARGET_REF, 2003; #REF), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW−WbII2+ A � j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[αW · h(fj, e)] Pα(e|fj; W) = (7) Ee'Ec; exp[αW · h(fj, e')], where α > 0 is a real number valued smoother. One can see that, in the extreme case, for α —* oc, (6) converges to (5).",
        "output": "introduction:Due to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here. Motivated by (#TARGET_REF<EXT/>, 2003; #REF), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW−WbII2+ A � j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[αW · h(fj, e)] Pα(e|fj; W) = (7) Ee'Ec; exp[αW · h(fj, e')], where α > 0 is a real number valued smoother. One can see that, in the extreme case, for α —* oc, (6) converges to (5)."
    },
    {
        "gold": {
            "text": [
                "This reflects that verb sequences which are not CV shows high degree of compositionality.",
                "In other words non CV verbs can directly interpret from their constituent verbs.",
                "This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning.",
                "In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences.",
                "We followed the same experimental procedure as discussed in ( #TARGET_REF ) for English polymorphemic words .",
                "However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination.",
                "The reaction time (RT) of each subject is recorded.",
                "Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs.",
                "This proves our hypothesis that Bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon and thus follows the full-listing model whereas compositional verb phrases are individually parsed.",
                "However, we do believe that our experiment is composed of a very small set of data and it is premature to conclude anything concrete based only on the current experimental results."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:This reflects that verb sequences which are not CV shows high degree of compositionality. In other words non CV verbs can directly interpret from their constituent verbs. This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning. In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in ( #TARGET_REF ) for English polymorphemic words . However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. The reaction time (RT) of each subject is recorded. Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs. This proves our hypothesis that Bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon and thus follows the full-listing model whereas compositional verb phrases are individually parsed. However, we do believe that our experiment is composed of a very small set of data and it is premature to conclude anything concrete based only on the current experimental results.",
        "output": "method:This reflects that verb sequences which are not CV shows high degree of compositionality. In other words non CV verbs can directly interpret from their constituent verbs. This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning. In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences. We followed the same experimental procedure as discussed in ( #TARGET_REF<FUT/> ) for English polymorphemic words . However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination. The reaction time (RT) of each subject is recorded. Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs. This proves our hypothesis that Bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon and thus follows the full-listing model whereas compositional verb phrases are individually parsed. However, we do believe that our experiment is composed of a very small set of data and it is premature to conclude anything concrete based only on the current experimental results."
    },
    {
        "gold": {
            "text": [
                "Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (#REF).",
                "In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #REF ) or the system ( #TARGET_REF ) .",
                "Such systems extract information from some types of syntactic units (clauses in (#REF;#REF;#REF); noun phrases in (#REF;#REF)).",
                "Lists of semantic relations are designed to capture salient domain information."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (#REF). In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #REF ) or the system ( #TARGET_REF ) . Such systems extract information from some types of syntactic units (clauses in (#REF;#REF;#REF); noun phrases in (#REF;#REF)). Lists of semantic relations are designed to capture salient domain information.",
        "output": "related work:Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (#REF). In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #REF ) or the system ( #TARGET_REF<BACK/> ) . Such systems extract information from some types of syntactic units (clauses in (#REF;#REF;#REF); noun phrases in (#REF;#REF)). Lists of semantic relations are designed to capture salient domain information."
    },
    {
        "gold": {
            "text": [
                "A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #TARGET_REF , #REF , #REF , #REF , #REF , #REF , and #REF ) .",
                "Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.",
                "While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.",
                "A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #TARGET_REF , #REF , #REF , #REF , #REF , #REF , and #REF ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF.",
        "output": "nan:A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #TARGET_REF<USE/> , #REF , #REF , #REF , #REF , #REF , and #REF ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF."
    },
    {
        "gold": {
            "text": [
                "This article represents an extension of our previous work on unsupervised event coreference resolution ( #REF ; #TARGET_REF ) .",
                "In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way.",
                "As data sets, we consider three different resources, including our own corpus (which is the only corpus available that encodes event coreference annotations across and within documents).",
                "In the next section, we provide additional information on how we performed the annotation of this corpus.",
                "Another major contribution of this article is an extended description of the unsupervised models for solving event coreference.",
                "In particular, we focused on providing further explanations about the implementation of the mIBP framework as well as its integration into the HDP and iHMM models.",
                "Finally, in this work, we significantly extended the experimental results section, which also includes a novel set of experiments performed over the OntoNotes English corpus (LDC-ON 2007)."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "related work:This article represents an extension of our previous work on unsupervised event coreference resolution ( #REF ; #TARGET_REF ) . In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way. As data sets, we consider three different resources, including our own corpus (which is the only corpus available that encodes event coreference annotations across and within documents). In the next section, we provide additional information on how we performed the annotation of this corpus. Another major contribution of this article is an extended description of the unsupervised models for solving event coreference. In particular, we focused on providing further explanations about the implementation of the mIBP framework as well as its integration into the HDP and iHMM models. Finally, in this work, we significantly extended the experimental results section, which also includes a novel set of experiments performed over the OntoNotes English corpus (LDC-ON 2007).",
        "output": "related work:This article represents an extension of our previous work on unsupervised event coreference resolution ( #REF ; #TARGET_REF<COMP/> ) . In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way. As data sets, we consider three different resources, including our own corpus (which is the only corpus available that encodes event coreference annotations across and within documents). In the next section, we provide additional information on how we performed the annotation of this corpus. Another major contribution of this article is an extended description of the unsupervised models for solving event coreference. In particular, we focused on providing further explanations about the implementation of the mIBP framework as well as its integration into the HDP and iHMM models. Finally, in this work, we significantly extended the experimental results section, which also includes a novel set of experiments performed over the OntoNotes English corpus (LDC-ON 2007)."
    },
    {
        "gold": {
            "text": [
                "In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification.",
                "Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.",
                "#REF have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #TARGET_REF ; #REF ) .",
                "The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification. Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes. #REF have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #TARGET_REF ; #REF ) . The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation.",
        "output": "conclusion:In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification. Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes. #REF have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #TARGET_REF<MOT/> ; #REF ) . The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation."
    },
    {
        "gold": {
            "text": [
                "The implementation has been inspired by experience in extracting information from very large corpora ( #REF ) and performing experiments on maximum entropy sequence tagging ( #REF ; #TARGET_REF ) .",
                "We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.",
                "These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.",
                "These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (#REF).",
                "We expect even faster training times when we move to conjugate gradient methods."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:The implementation has been inspired by experience in extracting information from very large corpora ( #REF ) and performing experiments on maximum entropy sequence tagging ( #REF ; #TARGET_REF ) . We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (#REF). We expect even faster training times when we move to conjugate gradient methods.",
        "output": "experiments:The implementation has been inspired by experience in extracting information from very large corpora ( #REF ) and performing experiments on maximum entropy sequence tagging ( #REF ; #TARGET_REF<EXT/> ) . We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (#REF). We expect even faster training times when we move to conjugate gradient methods."
    },
    {
        "gold": {
            "text": [
                "The results of this experiment are shown in Table 1.",
                "The first column shows which document retrieval variant is being evaluated.",
                "The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold).",
                "We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request-response pairs yields even more retrieved documents.",
                "For the cases where retrieval took place , we used F-score ( #REF ; #TARGET_REF ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) .",
                "The third column in Table 1 shows the proportion of requests for which this similarity is non-zero.",
                "Again, the third variant (matching on request-response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses.",
                "The fourth column shows the average similarity between the top retrieved response and the real response for the cases where retrieval took place.",
                "Here too the third variant yields the best similarity score (0.52)."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:The results of this experiment are shown in Table 1. The first column shows which document retrieval variant is being evaluated. The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold). We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request-response pairs yields even more retrieved documents. For the cases where retrieval took place , we used F-score ( #REF ; #TARGET_REF ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) . The third column in Table 1 shows the proportion of requests for which this similarity is non-zero. Again, the third variant (matching on request-response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses. The fourth column shows the average similarity between the top retrieved response and the real response for the cases where retrieval took place. Here too the third variant yields the best similarity score (0.52).",
        "output": "method:The results of this experiment are shown in Table 1. The first column shows which document retrieval variant is being evaluated. The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold). We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request-response pairs yields even more retrieved documents. For the cases where retrieval took place , we used F-score ( #REF ; #TARGET_REF<FUT/> ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) . The third column in Table 1 shows the proportion of requests for which this similarity is non-zero. Again, the third variant (matching on request-response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses. The fourth column shows the average similarity between the top retrieved response and the real response for the cases where retrieval took place. Here too the third variant yields the best similarity score (0.52)."
    },
    {
        "gold": {
            "text": [
                "where f and e (e ) are source and target sentences, respectively.",
                "h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .",
                "Some methods are based on likelihood ( #REF ; #REF ) , error rate ( #TARGET_REF ; #REF ; #REF ; #REF ) , margin ( #REF ; #REF ) and ranking ( #REF ) , and among which minimum error rate training ( MERT ) ( #REF ) is the most popular one .",
                "All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.",
                "We call them a global training method.",
                "One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.",
                "However, due to the diversity and uneven distribution of source sentences (#REF), there are some shortcomings in this pipeline."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( #REF ; #REF ) , error rate ( #TARGET_REF ; #REF ; #REF ; #REF ) , margin ( #REF ; #REF ) and ranking ( #REF ) , and among which minimum error rate training ( MERT ) ( #REF ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences (#REF), there are some shortcomings in this pipeline.",
        "output": "introduction:where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( #REF ; #REF ) , error rate ( #TARGET_REF<BACK/> ; #REF ; #REF ; #REF ) , margin ( #REF ; #REF ) and ranking ( #REF ) , and among which minimum error rate training ( MERT ) ( #REF ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences (#REF), there are some shortcomings in this pipeline."
    },
    {
        "gold": {
            "text": [
                "Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.",
                "In a number of proposals , lexical generalizations are captured using lexical underspecification ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF; #REF ; #REF ) .",
                "The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information. In a number of proposals , lexical generalizations are captured using lexical underspecification ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF; #REF ; #REF ) . The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.",
        "output": "related work:Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information. In a number of proposals , lexical generalizations are captured using lexical underspecification ( #TARGET_REF<USE/> ; #REF ; #REF ; #REF ; #REF; #REF ; #REF ) . The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."
    },
    {
        "gold": {
            "text": [
                "That is, we simply take the original mLDA model of #TARGET_REF (2009) and generalize it in the same way they generalize LDA.",
                "At first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a tuple are conditionally independent given the topic assignment zi."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "related work:That is, we simply take the original mLDA model of #TARGET_REF (2009) and generalize it in the same way they generalize LDA. At first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a tuple are conditionally independent given the topic assignment zi.",
        "output": "related work:That is, we simply take the original mLDA model of #TARGET_REF<COMP/> (2009) and generalize it in the same way they generalize LDA. At first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a tuple are conditionally independent given the topic assignment zi."
    },
    {
        "gold": {
            "text": [
                "We have presented a comparative evaluation of our approach on datasets from four different domains.",
                "In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets.",
                "Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.",
                "Our CRF-based approach also yields promising results in the crossdomain setting.",
                "The features we employ scale well across domains, given that the opinion target vocabularies are substantially different.",
                "For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #TARGET_REF ; #REF ) , perform in comparison to our approach .",
                "Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.",
                "We observe similar challenges as #REF regarding the analysis of complex sentences.",
                "Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.",
                "It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "conclusion:We have presented a comparative evaluation of our approach on datasets from four different domains. In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets. Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction. Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #TARGET_REF ; #REF ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as #REF regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality. It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse.",
        "output": "conclusion:We have presented a comparative evaluation of our approach on datasets from four different domains. In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets. Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction. Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #TARGET_REF<MOT/> ; #REF ) , perform in comparison to our approach . Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as #REF regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality. It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse."
    },
    {
        "gold": {
            "text": [
                "Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #TARGET_REF .",
                "The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length chunks.",
                "The problem is then transformed into a simple, but constrained, 5-class classification problem."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #TARGET_REF . The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length chunks. The problem is then transformed into a simple, but constrained, 5-class classification problem.",
        "output": "nan:Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #TARGET_REF<EXT/> . The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length chunks. The problem is then transformed into a simple, but constrained, 5-class classification problem."
    },
    {
        "gold": {
            "text": [
                "The Ruby on #TARGET_REF framework permits us to quickly develop web applications without rewriting common functions and classes ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The Ruby on #TARGET_REF framework permits us to quickly develop web applications without rewriting common functions and classes .",
        "output": "nan:The Ruby on #TARGET_REF<FUT/> framework permits us to quickly develop web applications without rewriting common functions and classes ."
    },
    {
        "gold": {
            "text": [
                "o:e −→, and a:ae −→ share a contextual \"vowel-fronting\" feature, then their weights rise and fall together with the strength of that feature.",
                "The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.",
                "Such approaches have been tried recently in restricted cases ( #REF ; #TARGET_REFb ; #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:o:e −→, and a:ae −→ share a contextual \"vowel-fronting\" feature, then their weights rise and fall together with the strength of that feature. The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired. Such approaches have been tried recently in restricted cases ( #REF ; #TARGET_REFb ; #REF ) .",
        "output": "introduction:o:e −→, and a:ae −→ share a contextual \"vowel-fronting\" feature, then their weights rise and fall together with the strength of that feature. The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired. Such approaches have been tried recently in restricted cases ( #REF ; #TARGET_REF<BACK/>b ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)",
                "We shall see that vague descriptions pose particular challenges to incrementality.",
                "One question emerges when the IA is combined with findings on word order and incremental interpretation.",
                "If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?",
                "This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).",
                "This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (#REF).",
                "A similar problem is discussed in the psycholinguistics of interpretation ( #TARGET_REF ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known .",
                "Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.",
                "Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.) We shall see that vague descriptions pose particular challenges to incrementality. One question emerges when the IA is combined with findings on word order and incremental interpretation. If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected? This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6). This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (#REF). A similar problem is discussed in the psycholinguistics of interpretation ( #TARGET_REF ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known . Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives. Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.",
        "output": "experiments:(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.) We shall see that vague descriptions pose particular challenges to incrementality. One question emerges when the IA is combined with findings on word order and incremental interpretation. If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected? This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6). This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (#REF). A similar problem is discussed in the psycholinguistics of interpretation ( #TARGET_REF<USE/> ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known . Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives. Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said."
    },
    {
        "gold": {
            "text": [
                "The process study most similar to ours is that of Koehn (2009a), who compared scratch, post-edit, and simple interactive modes.",
                "However, he used undergraduate, non-professional subjects, and did not consider re-tuning.",
                "Our experimental design with professional bilingual translators follows our previous work #TARGET_REFa ) comparing scratch translation to post-edit .",
                "Many research translation UIs have been proposed including TransType (#REF), Caitra (#REFb), Thot (Ortiz-Martínez and #REF), TransCenter (#REFb), and CasmaCat (#REF).",
                "However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "related work:The process study most similar to ours is that of Koehn (2009a), who compared scratch, post-edit, and simple interactive modes. However, he used undergraduate, non-professional subjects, and did not consider re-tuning. Our experimental design with professional bilingual translators follows our previous work #TARGET_REFa ) comparing scratch translation to post-edit . Many research translation UIs have been proposed including TransType (#REF), Caitra (#REFb), Thot (Ortiz-Martínez and #REF), TransCenter (#REFb), and CasmaCat (#REF). However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature.",
        "output": "related work:The process study most similar to ours is that of Koehn (2009a), who compared scratch, post-edit, and simple interactive modes. However, he used undergraduate, non-professional subjects, and did not consider re-tuning. Our experimental design with professional bilingual translators follows our previous work #TARGET_REF<COMP/>a ) comparing scratch translation to post-edit . Many research translation UIs have been proposed including TransType (#REF), Caitra (#REFb), Thot (Ortiz-Martínez and #REF), TransCenter (#REFb), and CasmaCat (#REF). However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature."
    },
    {
        "gold": {
            "text": [
                "Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (#REF).",
                "Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.",
                "For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #TARGET_REF ; #REF ) .",
                "techniques."
            ],
            "label": [
                "MOT"
            ]
        },
        "input": "nan:Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (#REF). Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries. For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #TARGET_REF ; #REF ) . techniques.",
        "output": "nan:Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (#REF). Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries. For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #TARGET_REF<MOT/> ; #REF ) . techniques."
    },
    {
        "gold": {
            "text": [
                "Recently we have applied our methodology to the Penn-III Treebank, a more balanced corpus resource with a number of text genres.",
                "Penn-III consists of the WSJ section from Penn-II as well as a parse-annotated subset of the Brown corpus.",
                "The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor.",
                "It has been shown ( #TARGET_REF ) that the subcategorization tendencies of verbs vary across linguistic domains .",
                "Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur.",
                "The f-structure annotation algorithm was extended with only minor amendments to cover the parsed Brown corpus.",
                "The most important of these was the way in which we distinguish between oblique and adjunct.",
                "We noted in Section 4 that our method of assigning an oblique annotation in Penn-II was precise, albeit conservative.",
                "Because of a change of annotation policy in Penn-III, the -CLR tag (indicating a close relationship between a PP and the local syntactic head), information which we had previously exploited, is no longer used."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:Recently we have applied our methodology to the Penn-III Treebank, a more balanced corpus resource with a number of text genres. Penn-III consists of the WSJ section from Penn-II as well as a parse-annotated subset of the Brown corpus. The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor. It has been shown ( #TARGET_REF ) that the subcategorization tendencies of verbs vary across linguistic domains . Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur. The f-structure annotation algorithm was extended with only minor amendments to cover the parsed Brown corpus. The most important of these was the way in which we distinguish between oblique and adjunct. We noted in Section 4 that our method of assigning an oblique annotation in Penn-II was precise, albeit conservative. Because of a change of annotation policy in Penn-III, the -CLR tag (indicating a close relationship between a PP and the local syntactic head), information which we had previously exploited, is no longer used.",
        "output": "nan:Recently we have applied our methodology to the Penn-III Treebank, a more balanced corpus resource with a number of text genres. Penn-III consists of the WSJ section from Penn-II as well as a parse-annotated subset of the Brown corpus. The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor. It has been shown ( #TARGET_REF<EXT/> ) that the subcategorization tendencies of verbs vary across linguistic domains . Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur. The f-structure annotation algorithm was extended with only minor amendments to cover the parsed Brown corpus. The most important of these was the way in which we distinguish between oblique and adjunct. We noted in Section 4 that our method of assigning an oblique annotation in Penn-II was precise, albeit conservative. Because of a change of annotation policy in Penn-III, the -CLR tag (indicating a close relationship between a PP and the local syntactic head), information which we had previously exploited, is no longer used."
    },
    {
        "gold": {
            "text": [
                "The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).",
                "The system is implemented based on (#REF) and ).",
                "In the system , we extract both the minimal GHKM rules ( #TARGET_REF ) , and the rules of SPMT Model 1 ( #REF ) with phrases up to length L = 5 on the source side ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (#REF) and ). In the system , we extract both the minimal GHKM rules ( #TARGET_REF ) , and the rules of SPMT Model 1 ( #REF ) with phrases up to length L = 5 on the source side .",
        "output": "experiments:The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (#REF) and ). In the system , we extract both the minimal GHKM rules ( #TARGET_REF<FUT/> ) , and the rules of SPMT Model 1 ( #REF ) with phrases up to length L = 5 on the source side ."
    },
    {
        "gold": {
            "text": [
                "Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #REF ; #TARGET_REF ; Hockenmaier , Bierner , and #REF ; Nakanishi , Miyao , and #REF ) .",
                "In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).",
                "However, our approach also generalizes to CFG category-based approaches.",
                "In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate.",
                "Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and #REF) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures (#REF;Cahill, McCarthy, et al. 2004).",
                "Our technique requires a treebank annotated with LFG functional schemata."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #REF ; #TARGET_REF ; Hockenmaier , Bierner , and #REF ; Nakanishi , Miyao , and #REF ) . In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems). However, our approach also generalizes to CFG category-based approaches. In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate. Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and #REF) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures (#REF;Cahill, McCarthy, et al. 2004). Our technique requires a treebank annotated with LFG functional schemata.",
        "output": "introduction:Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #REF ; #TARGET_REF<BACK/> ; Hockenmaier , Bierner , and #REF ; Nakanishi , Miyao , and #REF ) . In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems). However, our approach also generalizes to CFG category-based approaches. In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate. Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and #REF) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures (#REF;Cahill, McCarthy, et al. 2004). Our technique requires a treebank annotated with LFG functional schemata."
    },
    {
        "gold": {
            "text": [
                "Note that in our original work ( #TARGET_REF ) , only development data were used to show some initial observations.",
                "Here we trained our mod- els on the development data and results shown are from the testing data."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Note that in our original work ( #TARGET_REF ) , only development data were used to show some initial observations. Here we trained our mod- els on the development data and results shown are from the testing data.",
        "output": "experiments:Note that in our original work ( #TARGET_REF<USE/> ) , only development data were used to show some initial observations. Here we trained our mod- els on the development data and results shown are from the testing data."
    },
    {
        "gold": {
            "text": [
                "Using the tree-cut technique described above , our previous work ( #TARGET_REF ) extracted systematic polysemy from WordNet .",
                "In this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method."
            ],
            "label": [
                "COMP"
            ]
        },
        "input": "experiments:Using the tree-cut technique described above , our previous work ( #TARGET_REF ) extracted systematic polysemy from WordNet . In this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method.",
        "output": "experiments:Using the tree-cut technique described above , our previous work ( #TARGET_REF<COMP/> ) extracted systematic polysemy from WordNet . In this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method."
    },
    {
        "gold": {
            "text": [
                "However, there are at least three arguments against iterating PT.",
                "First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time.",
                "Secondly , the cooperative principle of #TARGET_REF , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader .",
                "Finally, it has been shown by #REF that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by #REF strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.",
                "Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly , the cooperative principle of #TARGET_REF , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader . Finally, it has been shown by #REF that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by #REF strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph. Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.",
        "output": "introduction:However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly , the cooperative principle of #TARGET_REF<EXT/> , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader . Finally, it has been shown by #REF that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by #REF strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph. Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions."
    },
    {
        "gold": {
            "text": [
                "To obtain training and test data for this task, we had a number of naive subjects use the system as if they were trying to obtain actual information.",
                "Their speech was recorded in a simulation mode in which the speech recognition component was excluded.",
                "Instead, an experimenter in a separate room typed in the utterances as spoken by the subject.",
                "Subsequent processing by the natural language and response generation components was done automatically by the computer ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:To obtain training and test data for this task, we had a number of naive subjects use the system as if they were trying to obtain actual information. Their speech was recorded in a simulation mode in which the speech recognition component was excluded. Instead, an experimenter in a separate room typed in the utterances as spoken by the subject. Subsequent processing by the natural language and response generation components was done automatically by the computer ( #TARGET_REF ) .",
        "output": "nan:To obtain training and test data for this task, we had a number of naive subjects use the system as if they were trying to obtain actual information. Their speech was recorded in a simulation mode in which the speech recognition component was excluded. Instead, an experimenter in a separate room typed in the utterances as spoken by the subject. Subsequent processing by the natural language and response generation components was done automatically by the computer ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "We will explore techniques to make the resulting annotated web corpus data available in static form to enable replication and verification of corpus studies based on such data.",
                "The initial solution will be to store the resulting reference 11 http://www.comp.lancs.ac.uk/ucrel/claws/ corpus in the Sketch Engine.",
                "We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web-based corpus studies based in general.",
                "Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust.",
                "Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #TARGET_REFa ) .",
                "Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:We will explore techniques to make the resulting annotated web corpus data available in static form to enable replication and verification of corpus studies based on such data. The initial solution will be to store the resulting reference 11 http://www.comp.lancs.ac.uk/ucrel/claws/ corpus in the Sketch Engine. We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web-based corpus studies based in general. Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust. Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #TARGET_REFa ) . Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here.",
        "output": "nan:We will explore techniques to make the resulting annotated web corpus data available in static form to enable replication and verification of corpus studies based on such data. The initial solution will be to store the resulting reference 11 http://www.comp.lancs.ac.uk/ucrel/claws/ corpus in the Sketch Engine. We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web-based corpus studies based in general. Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust. Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #TARGET_REF<BACK/>a ) . Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here."
    },
    {
        "gold": {
            "text": [
                "There is, however, one difference in the implementation of such a tagger.",
                "Normally, a POS tagger operates on text spans that form a sentence.",
                "This requires resolving sentence boundaries before tagging.",
                "We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #REF ] , Brill 's [ #TARGET_REFa ] , and MaxEnt [ #REF ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .",
                "The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.",
                "This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.",
                "For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.",
                "At the same time since this token is unambiguous, it is not affected by the history."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:There is, however, one difference in the implementation of such a tagger. Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #REF ] , Brill 's [ #TARGET_REFa ] , and MaxEnt [ #REF ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token. At the same time since this token is unambiguous, it is not affected by the history.",
        "output": "nan:There is, however, one difference in the implementation of such a tagger. Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #REF ] , Brill 's [ #TARGET_REF<USE/>a ] , and MaxEnt [ #REF ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token. At the same time since this token is unambiguous, it is not affected by the history."
    },
    {
        "gold": {
            "text": [
                "We extracted word pairs from three different domain-specific corpora (see Table 2).",
                "This is motivated by the aim to enable research in information retrieval incorporating SR measures.",
                "In particular , the `` Semantic Information Retrieval '' project ( SIR #TARGET_REF ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:We extracted word pairs from three different domain-specific corpora (see Table 2). This is motivated by the aim to enable research in information retrieval incorporating SR measures. In particular , the `` Semantic Information Retrieval '' project ( SIR #TARGET_REF ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .",
        "output": "experiments:We extracted word pairs from three different domain-specific corpora (see Table 2). This is motivated by the aim to enable research in information retrieval incorporating SR measures. In particular , the `` Semantic Information Retrieval '' project ( SIR #TARGET_REF<EXT/> ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems ."
    },
    {
        "gold": {
            "text": [
                "We perform our comparison using two state-ofthe-art parsers.",
                "For the full parser , we use the one developed by Michael Collins ( #TARGET_REF ; #REF ) -- one of the most accurate full parsers around .",
                "It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.",
                "Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.",
                "After that, it will choose the candidate parse tree with the highest probability as output.",
                "The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank.",
                "The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (#REF)."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We perform our comparison using two state-ofthe-art parsers. For the full parser , we use the one developed by Michael Collins ( #TARGET_REF ; #REF ) -- one of the most accurate full parsers around . It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them. Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence. After that, it will choose the candidate parse tree with the highest probability as output. The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank. The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (#REF).",
        "output": "experiments:We perform our comparison using two state-ofthe-art parsers. For the full parser , we use the one developed by Michael Collins ( #TARGET_REF<FUT/> ; #REF ) -- one of the most accurate full parsers around . It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them. Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence. After that, it will choose the candidate parse tree with the highest probability as output. The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank. The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (#REF)."
    },
    {
        "gold": {
            "text": [
                "Efficiency has not been a focus for NLP research in general.",
                "However, it will be increasingly important as techniques become more complex and corpus sizes grow.",
                "An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by #REF that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (#REF).",
                "Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #REF ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #TARGET_REF ) .",
                "The TNT POS tagger (#REF) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:Efficiency has not been a focus for NLP research in general. However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by #REF that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (#REF). Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #REF ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #TARGET_REF ) . The TNT POS tagger (#REF) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.",
        "output": "experiments:Efficiency has not been a focus for NLP research in general. However, it will be increasingly important as techniques become more complex and corpus sizes grow. An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by #REF that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (#REF). Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #REF ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #TARGET_REF<BACK/> ) . The TNT POS tagger (#REF) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second."
    },
    {
        "gold": {
            "text": [
                "Disambiguation of capitalized words is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.",
                "As #TARGET_REF rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .",
                "Estimates from the Brown Corpus can be misleading.",
                "For example, the capitalized word 'Acts' is found twice in the Brown Corpus, both times as a proper noun (in a title).",
                "It would be misleading to infer from this evidence that the word 'Acts' is always a proper noun.\""
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Disambiguation of capitalized words is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus. As #TARGET_REF rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not . Estimates from the Brown Corpus can be misleading. For example, the capitalized word 'Acts' is found twice in the Brown Corpus, both times as a proper noun (in a title). It would be misleading to infer from this evidence that the word 'Acts' is always a proper noun.\"",
        "output": "nan:Disambiguation of capitalized words is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus. As #TARGET_REF<USE/> rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not . Estimates from the Brown Corpus can be misleading. For example, the capitalized word 'Acts' is found twice in the Brown Corpus, both times as a proper noun (in a title). It would be misleading to infer from this evidence that the word 'Acts' is always a proper noun.\""
    },
    {
        "gold": {
            "text": [
                "After training, the decision tree classifier is used to select an antecedent for each NP in a test text.",
                "Following #TARGET_REF , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.",
                "If no such NP exists, no antecedent is selected for NPj."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:After training, the decision tree classifier is used to select an antecedent for each NP in a test text. Following #TARGET_REF , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj. If no such NP exists, no antecedent is selected for NPj.",
        "output": "nan:After training, the decision tree classifier is used to select an antecedent for each NP in a test text. Following #TARGET_REF<EXT/> , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj. If no such NP exists, no antecedent is selected for NPj."
    },
    {
        "gold": {
            "text": [
                "To solve these scaling issues , we implement Online Variational Bayesian Inference ( #REF ; #TARGET_REF ) for our models .",
                "In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.",
                "The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors.",
                "Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set.",
                "Online VBI easily scales and quickly converges in all of our experiments.",
                "A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:To solve these scaling issues , we implement Online Variational Bayesian Inference ( #REF ; #TARGET_REF ) for our models . In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables. The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors. Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set. Online VBI easily scales and quickly converges in all of our experiments. A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source.",
        "output": "method:To solve these scaling issues , we implement Online Variational Bayesian Inference ( #REF ; #TARGET_REF<FUT/> ) for our models . In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables. The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors. Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set. Online VBI easily scales and quickly converges in all of our experiments. A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source."
    },
    {
        "gold": {
            "text": [
                "A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #TARGET_REF ; #REF ) .",
                "The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (λ); reading another b (ν); transducing b to p rather than q (µ); starting to transduce p to rather than x (ρ). 4 To prove (1)⇒(3), express f as an FST and apply the well-known Kleene-Schützenberger construction (#REF), taking care to write each regexp in the construction as a constant times a probabilistic regexp.",
                "A full proof is straightforward, as are proofs of (3)⇒( 2), ( 2)⇒(1)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #TARGET_REF ; #REF ) . The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (λ); reading another b (ν); transducing b to p rather than q (µ); starting to transduce p to rather than x (ρ). 4 To prove (1)⇒(3), express f as an FST and apply the well-known Kleene-Schützenberger construction (#REF), taking care to write each regexp in the construction as a constant times a probabilistic regexp. A full proof is straightforward, as are proofs of (3)⇒( 2), ( 2)⇒(1).",
        "output": "introduction:A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #TARGET_REF<BACK/> ; #REF ) . The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (λ); reading another b (ν); transducing b to p rather than q (µ); starting to transduce p to rather than x (ρ). 4 To prove (1)⇒(3), express f as an FST and apply the well-known Kleene-Schützenberger construction (#REF), taking care to write each regexp in the construction as a constant times a probabilistic regexp. A full proof is straightforward, as are proofs of (3)⇒( 2), ( 2)⇒(1)."
    },
    {
        "gold": {
            "text": [
                "A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #REF , #REF , #TARGET_REF , #REF , #REF , #REF , and #REF ) .",
                "Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.",
                "While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.",
                "A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #REF , #REF , #TARGET_REF , #REF , #REF , #REF , and #REF ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF.",
        "output": "nan:A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #REF , #REF , #TARGET_REF<USE/> , #REF , #REF , #REF , and #REF ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF."
    },
    {
        "gold": {
            "text": [
                "The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages.",
                "Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment.",
                "Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF).",
                "But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #TARGET_REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #TARGET_REF ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) .",
        "output": "introduction:The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and #REF] and rules [#REF;#REF]) as well as for MT system combination (Matusov, Ueffing, and #REF). But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #TARGET_REF<EXT/> ; #REF ; Ganchev , Gillenwater , and #REF ) ; discovery of paraphrases ( #REF ) ; and joint unsupervised POS and parser induction across languages ( #REF ) ."
    },
    {
        "gold": {
            "text": [
                "The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).",
                "The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #TARGET_REFb ) , which is an integrated parsing and discourse processing method .",
                "ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.",
                "ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs). The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #TARGET_REFb ) , which is an integrated parsing and discourse processing method . ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence. ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time.",
        "output": "nan:The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs). The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #TARGET_REF<FUT/>b ) , which is an integrated parsing and discourse processing method . ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence. ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time."
    },
    {
        "gold": {
            "text": [
                "Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques.",
                "Traditionally, these techniques have combined extensive syntactic, se- mantic, and discourse knowledge.",
                "The acquisition of such knowledge is time-consuming, difficult, and error-prone.",
                "Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #TARGET_REF ) , ( #REF ) ( #REF ) ) .",
                "For example, CogNIAC (#REF), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.",
                "For this research, we used a coreference resolution sys- tem ((#REF)) that imple- ments different sets of heuristics corresponding to various forms of coreference.",
                "This system, called COCKTAIL,resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text).",
                "These constraints are implemented as a set of heuristics ordered by their priority."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques. Traditionally, these techniques have combined extensive syntactic, se- mantic, and discourse knowledge. The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #TARGET_REF ) , ( #REF ) ( #REF ) ) . For example, CogNIAC (#REF), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research, we used a coreference resolution sys- tem ((#REF)) that imple- ments different sets of heuristics corresponding to various forms of coreference. This system, called COCKTAIL,resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text). These constraints are implemented as a set of heuristics ordered by their priority.",
        "output": "nan:Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques. Traditionally, these techniques have combined extensive syntactic, se- mantic, and discourse knowledge. The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #TARGET_REF<BACK/> ) , ( #REF ) ( #REF ) ) . For example, CogNIAC (#REF), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research, we used a coreference resolution sys- tem ((#REF)) that imple- ments different sets of heuristics corresponding to various forms of coreference. This system, called COCKTAIL,resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text). These constraints are implemented as a set of heuristics ordered by their priority."
    },
    {
        "gold": {
            "text": [
                "The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( #TARGET_REF ) .",
        "output": "nan:The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( #TARGET_REF<USE/> ) ."
    },
    {
        "gold": {
            "text": [
                "The implementation has been inspired by experience in extracting information from very large corpora ( #TARGET_REF ) and performing experiments on maximum entropy sequence tagging ( #REF ; #REF ) .",
                "We have already implemented a P O S tagger, chunker, C C G supertagger and named entity recogniser using the infrastructure.",
                "These tools currently train in less than 10 minutes on the standard training ma- terials and tag faster than T N T , the fastest existing P O S tagger.",
                "These tools use a highly optimised G I S imple- mentation and provide sophisticated Gaussian smoothing (#REF).",
                "We expect even faster train- ing times when we move to conjugate gradient methods."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:The implementation has been inspired by experience in extracting information from very large corpora ( #TARGET_REF ) and performing experiments on maximum entropy sequence tagging ( #REF ; #REF ) . We have already implemented a P O S tagger, chunker, C C G supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training ma- terials and tag faster than T N T , the fastest existing P O S tagger. These tools use a highly optimised G I S imple- mentation and provide sophisticated Gaussian smoothing (#REF). We expect even faster train- ing times when we move to conjugate gradient methods.",
        "output": "experiments:The implementation has been inspired by experience in extracting information from very large corpora ( #TARGET_REF<EXT/> ) and performing experiments on maximum entropy sequence tagging ( #REF ; #REF ) . We have already implemented a P O S tagger, chunker, C C G supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training ma- terials and tag faster than T N T , the fastest existing P O S tagger. These tools use a highly optimised G I S imple- mentation and provide sophisticated Gaussian smoothing (#REF). We expect even faster train- ing times when we move to conjugate gradient methods."
    },
    {
        "gold": {
            "text": [
                "Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #TARGET_REF ) .",
                "For Arabic, since words are morphologically derived from a list of roots (stems), we expected that a feature based on the right and left stems would lead to improvement in system accuracy."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #TARGET_REF ) . For Arabic, since words are morphologically derived from a list of roots (stems), we expected that a feature based on the right and left stems would lead to improvement in system accuracy.",
        "output": "nan:Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #TARGET_REF<FUT/> ) . For Arabic, since words are morphologically derived from a list of roots (stems), we expected that a feature based on the right and left stems would lead to improvement in system accuracy."
    },
    {
        "gold": {
            "text": [
                "Many statistical parsers ( #REF ; #REF ; #REF ) are based on a history-based probability model ( #TARGET_REF ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .",
                "A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.",
                "Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (#REF;#REF;#REF).",
                "In the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.",
                "We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (#REF;#REF).",
                "Because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.",
                "The resulting parser achieves performance far greater than previous approaches to neural network parsing (#REF;#REF), and only marginally below the current state-of-the-art for parsing the Penn Treebank."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Many statistical parsers ( #REF ; #REF ; #REF ) are based on a history-based probability model ( #TARGET_REF ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse . A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated. Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (#REF;#REF;#REF). In the work presented here, we automatically induce a finite set of features to represent the unbounded parse history. We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (#REF;#REF). Because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation. The resulting parser achieves performance far greater than previous approaches to neural network parsing (#REF;#REF), and only marginally below the current state-of-the-art for parsing the Penn Treebank.",
        "output": "introduction:Many statistical parsers ( #REF ; #REF ; #REF ) are based on a history-based probability model ( #TARGET_REF<BACK/> ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse . A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated. Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (#REF;#REF;#REF). In the work presented here, we automatically induce a finite set of features to represent the unbounded parse history. We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (#REF;#REF). Because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation. The resulting parser achieves performance far greater than previous approaches to neural network parsing (#REF;#REF), and only marginally below the current state-of-the-art for parsing the Penn Treebank."
    },
    {
        "gold": {
            "text": [
                "This paper investigates the idea, still unexplored, of a tighter integration of MT and TE algorithms and techniques.",
                "Our aim is to embed cross-lingual processing techniques inside the TE recognition process in order to avoid any dependency on external MT components, and eventually gain full control of the system�s behaviour.",
                "Along this direction, we start from the acquisition and use of lexical knowl- edge, which represents the basic building block of any TE system.",
                "Using the basic solution proposed by ( #TARGET_REF ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:This paper investigates the idea, still unexplored, of a tighter integration of MT and TE algorithms and techniques. Our aim is to embed cross-lingual processing techniques inside the TE recognition process in order to avoid any dependency on external MT components, and eventually gain full control of the system�s behaviour. Along this direction, we start from the acquisition and use of lexical knowl- edge, which represents the basic building block of any TE system. Using the basic solution proposed by ( #TARGET_REF ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :",
        "output": "introduction:This paper investigates the idea, still unexplored, of a tighter integration of MT and TE algorithms and techniques. Our aim is to embed cross-lingual processing techniques inside the TE recognition process in order to avoid any dependency on external MT components, and eventually gain full control of the system�s behaviour. Along this direction, we start from the acquisition and use of lexical knowl- edge, which represents the basic building block of any TE system. Using the basic solution proposed by ( #TARGET_REF<USE/> ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :"
    },
    {
        "gold": {
            "text": [
                "Our motivation for generation of material for language education exists in work such as #REF and #TARGET_REF , which deal with automatic generation of classic fill in the blank questions .",
                "Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "related work:Our motivation for generation of material for language education exists in work such as #REF and #TARGET_REF , which deal with automatic generation of classic fill in the blank questions . Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences.",
        "output": "related work:Our motivation for generation of material for language education exists in work such as #REF and #TARGET_REF<EXT/> , which deal with automatic generation of classic fill in the blank questions . Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences."
    },
    {
        "gold": {
            "text": [
                "Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding ( #TARGET_REF ; Liang , Taskar , and #REF ; Ganchev , and #REF ) .",
                "Using this decoding we include an alignment link i − j if the posterior probability that word i aligns to word j is above some threshold.",
                "This allows the accumulation of probability from several low-scoring alignments that agree on one alignment link.",
                "The threshold is tuned on some small amount of labeled data-in our case the development set-to minimize some loss.",
                "#REF study different loss functions that incorporate linguistic knowledge, and show significant improvement over likelihood decoding.",
                "Note that this could potentially result in an alignment having zero probability under the model, as many-to-many alignments can be produced in this way.",
                "MBR decoding has several advantages over Viterbi decoding.",
                "First, independently of the particular choice of the loss function, by picking a specific threshold we can trade off precision and recall of the predicted word alignments.",
                "In fact, in this work when comparing different alignment sets we do not commit to any loss function but instead compare precision vs recall curves, by generating alignments for different thresholds (0..1).",
                "Second, with this method we can ignore the null word probabilities, which tend to be poorly estimated."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding ( #TARGET_REF ; Liang , Taskar , and #REF ; Ganchev , and #REF ) . Using this decoding we include an alignment link i − j if the posterior probability that word i aligns to word j is above some threshold. This allows the accumulation of probability from several low-scoring alignments that agree on one alignment link. The threshold is tuned on some small amount of labeled data-in our case the development set-to minimize some loss. #REF study different loss functions that incorporate linguistic knowledge, and show significant improvement over likelihood decoding. Note that this could potentially result in an alignment having zero probability under the model, as many-to-many alignments can be produced in this way. MBR decoding has several advantages over Viterbi decoding. First, independently of the particular choice of the loss function, by picking a specific threshold we can trade off precision and recall of the predicted word alignments. In fact, in this work when comparing different alignment sets we do not commit to any loss function but instead compare precision vs recall curves, by generating alignments for different thresholds (0..1). Second, with this method we can ignore the null word probabilities, which tend to be poorly estimated.",
        "output": "introduction:Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding ( #TARGET_REF<FUT/> ; Liang , Taskar , and #REF ; Ganchev , and #REF ) . Using this decoding we include an alignment link i − j if the posterior probability that word i aligns to word j is above some threshold. This allows the accumulation of probability from several low-scoring alignments that agree on one alignment link. The threshold is tuned on some small amount of labeled data-in our case the development set-to minimize some loss. #REF study different loss functions that incorporate linguistic knowledge, and show significant improvement over likelihood decoding. Note that this could potentially result in an alignment having zero probability under the model, as many-to-many alignments can be produced in this way. MBR decoding has several advantages over Viterbi decoding. First, independently of the particular choice of the loss function, by picking a specific threshold we can trade off precision and recall of the predicted word alignments. In fact, in this work when comparing different alignment sets we do not commit to any loss function but instead compare precision vs recall curves, by generating alignments for different thresholds (0..1). Second, with this method we can ignore the null word probabilities, which tend to be poorly estimated."
    },
    {
        "gold": {
            "text": [
                "Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.",
                "dictionary-based ( #REF ) , ontology-based ( #REF ; #REF ) , information-based ( #TARGET_REF ; #REF ) or distributional ( #REF ) .",
                "The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based ( #REF ) , ontology-based ( #REF ; #REF ) , information-based ( #TARGET_REF ; #REF ) or distributional ( #REF ) . The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.",
        "output": "nan:Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based ( #REF ) , ontology-based ( #REF ; #REF ) , information-based ( #TARGET_REF<BACK/> ; #REF ) or distributional ( #REF ) . The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora."
    },
    {
        "gold": {
            "text": [
                "2We could just as easily use other symmetric `` association '' measures , such as 02 ( #TARGET_REF ) or the Dice coefficient ( #REF ) .",
                "co-occur is called a direct association.",
                "Now, suppose that uk and Uk+z often co-occur within their language.",
                "Then vk and uk+l will also co-occur more often than expected by chance.",
                "The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.",
                "Models of translational equivalence that are ignorant of indirect associations have \"a tendency ... to be confused by collocates\" (#REF).",
                "Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (#REFc).",
                "The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest.",
                "The competitive linking algorithm implements this heuristic:"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:2We could just as easily use other symmetric `` association '' measures , such as 02 ( #TARGET_REF ) or the Dice coefficient ( #REF ) . co-occur is called a direct association. Now, suppose that uk and Uk+z often co-occur within their language. Then vk and uk+l will also co-occur more often than expected by chance. The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk. Models of translational equivalence that are ignorant of indirect associations have \"a tendency ... to be confused by collocates\" (#REF). Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (#REFc). The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm implements this heuristic:",
        "output": "method:2We could just as easily use other symmetric `` association '' measures , such as 02 ( #TARGET_REF<USE/> ) or the Dice coefficient ( #REF ) . co-occur is called a direct association. Now, suppose that uk and Uk+z often co-occur within their language. Then vk and uk+l will also co-occur more often than expected by chance. The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk. Models of translational equivalence that are ignorant of indirect associations have \"a tendency ... to be confused by collocates\" (#REF). Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (#REFc). The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm implements this heuristic:"
    },
    {
        "gold": {
            "text": [
                "To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.",
                "In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.",
                "In the latter case, we can also take care of transferring the value of z.",
                "However , as discussed by #TARGET_REF , creating several instances of lexical rules can be avoided .",
                "Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.",
                "This is accomplished by having each lexical rule predicate call a so-called framepredicate,which can have multiple defining clauses.",
                "So for the lexical rule 1, the frame specification is taken care of by extending the predicate in Figure 6 with a call to a frame predicate, as shown in Figure 8.17"
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case. In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value. In the latter case, we can also take care of transferring the value of z. However , as discussed by #TARGET_REF , creating several instances of lexical rules can be avoided . Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule. This is accomplished by having each lexical rule predicate call a so-called framepredicate,which can have multiple defining clauses. So for the lexical rule 1, the frame specification is taken care of by extending the predicate in Figure 6 with a call to a frame predicate, as shown in Figure 8.17",
        "output": "introduction:To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case. In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value. In the latter case, we can also take care of transferring the value of z. However , as discussed by #TARGET_REF<EXT/> , creating several instances of lexical rules can be avoided . Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule. This is accomplished by having each lexical rule predicate call a so-called framepredicate,which can have multiple defining clauses. So for the lexical rule 1, the frame specification is taken care of by extending the predicate in Figure 6 with a call to a frame predicate, as shown in Figure 8.17"
    },
    {
        "gold": {
            "text": [
                "The intent of this experimental section is to evaluate the gains from using constraints during learning, hence the main comparison is between HMM trained with normal EM vs. trained with PR plus constraints.",
                "We also report results for IBM Model 4, because it is often used as the default word alignment model, and can be used as a reference.",
                "However, we would like to note that IBM Model 4 is a more complex model, able to capture more structure, albeit at the cost of intractable inference.",
                "Because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to IBM Model 4 if exact inference was efficient, hopefully yielding similar improvements.",
                "We used a standard implementation of IBM Model 4 ( #TARGET_REF ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves .",
                "We trained IBM Model 4 using the default configuration of the"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The intent of this experimental section is to evaluate the gains from using constraints during learning, hence the main comparison is between HMM trained with normal EM vs. trained with PR plus constraints. We also report results for IBM Model 4, because it is often used as the default word alignment model, and can be used as a reference. However, we would like to note that IBM Model 4 is a more complex model, able to capture more structure, albeit at the cost of intractable inference. Because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to IBM Model 4 if exact inference was efficient, hopefully yielding similar improvements. We used a standard implementation of IBM Model 4 ( #TARGET_REF ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves . We trained IBM Model 4 using the default configuration of the",
        "output": "nan:The intent of this experimental section is to evaluate the gains from using constraints during learning, hence the main comparison is between HMM trained with normal EM vs. trained with PR plus constraints. We also report results for IBM Model 4, because it is often used as the default word alignment model, and can be used as a reference. However, we would like to note that IBM Model 4 is a more complex model, able to capture more structure, albeit at the cost of intractable inference. Because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to IBM Model 4 if exact inference was efficient, hopefully yielding similar improvements. We used a standard implementation of IBM Model 4 ( #TARGET_REF<FUT/> ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves . We trained IBM Model 4 using the default configuration of the"
    },
    {
        "gold": {
            "text": [
                "As discussed earlier, there are two main requirements of the system that are covered by \"high performance\": speed and state of the art accuracy.",
                "Efficiency is required both in training and processing.",
                "Efficient training is required because the amount of data available for training will increase significantly.",
                "Also , advanced methods often require many training iterations , for example active learning ( #REF ) and co-training ( #TARGET_REF ) .",
                "Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:As discussed earlier, there are two main requirements of the system that are covered by \"high performance\": speed and state of the art accuracy. Efficiency is required both in training and processing. Efficient training is required because the amount of data available for training will increase significantly. Also , advanced methods often require many training iterations , for example active learning ( #REF ) and co-training ( #TARGET_REF ) . Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly.",
        "output": "nan:As discussed earlier, there are two main requirements of the system that are covered by \"high performance\": speed and state of the art accuracy. Efficiency is required both in training and processing. Efficient training is required because the amount of data available for training will increase significantly. Also , advanced methods often require many training iterations , for example active learning ( #REF ) and co-training ( #TARGET_REF<BACK/> ) . Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly."
    },
    {
        "gold": {
            "text": [
                "Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.",
                "In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) .",
                "The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) . The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.",
        "output": "related work:Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #TARGET_REF<USE/> ; #REF ; #REF ; #REF ; #REF ) . The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."
    },
    {
        "gold": {
            "text": [
                "#REF did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.",
                "They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.",
                "This paper made the first attempt on Chinese SRL and produced promising results.",
                "After the PropBank (#REF) was built,  and #REF have produced more complete and systematic research on Chinese SRL.",
                "#REF has made some preliminary attempt on the idea of hierarchical semantic role labeling.",
                "However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.",
                "So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.",
                "#REF did very encouraging work on the feature calibration of semantic role labeling.",
                "They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.",
                "For semantic analysis, developing features that capture the right kind of information is crucial.",
                "Experiments on Chinese SRL ( #TARGET_REF , #REF ) reassured these findings ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:#REF did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (#REF) was built,  and #REF have produced more complete and systematic research on Chinese SRL. #REF has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #REF did very encouraging work on the feature calibration of semantic role labeling. They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL ( #TARGET_REF , #REF ) reassured these findings .",
        "output": "introduction:#REF did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (#REF) was built,  and #REF have produced more complete and systematic research on Chinese SRL. #REF has made some preliminary attempt on the idea of hierarchical semantic role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. #REF did very encouraging work on the feature calibration of semantic role labeling. They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL ( #TARGET_REF<EXT/> , #REF ) reassured these findings ."
    },
    {
        "gold": {
            "text": [
                "Our rules for phonological word formation are adopted , for the most part , from G & G , #TARGET_REF , and the account of monosyllabic destressing in #REF .",
                "Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.",
                "If the terminal is a content word, i.e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.",
                "Otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries.",
                "This is accomplished by adjoining a word to its left or right neighbor depending on its lexical category and its position in the tree.",
                "Function words, e.g.",
                "auxiliary verbs, articles, prepositions, pronouns, and conjunctions, are all eligible for adjunction in certain syntactic contexts.",
                "Content words, copular verbs, demonstratives, quantifiers and elements in the complementizer node can serve as hosts for the adjoined material or stand alone."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:Our rules for phonological word formation are adopted , for the most part , from G & G , #TARGET_REF , and the account of monosyllabic destressing in #REF . Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree. If the terminal is a content word, i.e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own. Otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries. This is accomplished by adjoining a word to its left or right neighbor depending on its lexical category and its position in the tree. Function words, e.g. auxiliary verbs, articles, prepositions, pronouns, and conjunctions, are all eligible for adjunction in certain syntactic contexts. Content words, copular verbs, demonstratives, quantifiers and elements in the complementizer node can serve as hosts for the adjoined material or stand alone.",
        "output": "introduction:Our rules for phonological word formation are adopted , for the most part , from G & G , #TARGET_REF<FUT/> , and the account of monosyllabic destressing in #REF . Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree. If the terminal is a content word, i.e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own. Otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries. This is accomplished by adjoining a word to its left or right neighbor depending on its lexical category and its position in the tree. Function words, e.g. auxiliary verbs, articles, prepositions, pronouns, and conjunctions, are all eligible for adjunction in certain syntactic contexts. Content words, copular verbs, demonstratives, quantifiers and elements in the complementizer node can serve as hosts for the adjoined material or stand alone."
    },
    {
        "gold": {
            "text": [
                "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.",
                "They proved to be useful in a number of NLP applications such as natural language generation (#REF), multidocument summarization (#REF), automatic evaluation of MT (#REF), and TE (#REF).",
                "One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF).",
                "With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.",
                "After the extraction , pruning techniques ( #TARGET_REF ) can be applied to increase the precision of the extracted paraphrases ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (#REF), multidocument summarization (#REF), automatic evaluation of MT (#REF), and TE (#REF). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction , pruning techniques ( #TARGET_REF ) can be applied to increase the precision of the extracted paraphrases .",
        "output": "nan:Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (#REF), multidocument summarization (#REF), automatic evaluation of MT (#REF), and TE (#REF). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (#REF). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction , pruning techniques ( #TARGET_REF<BACK/> ) can be applied to increase the precision of the extracted paraphrases ."
    },
    {
        "gold": {
            "text": [
                "Another common approach to lexical rules is to encode them as unary phrase structure rules.",
                "This approach is taken, for example, in LKB (#REF) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (#REF, 31).",
                "A similar method is included in PATR-II ( #REF ) and can be used to encode lexical rules as binary relations in the CUF system ( #REF ; #REFb ) or the TFS system ( #TARGET_REF ; #REF ) .",
                "The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken, for example, in LKB (#REF) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (#REF, 31). A similar method is included in PATR-II ( #REF ) and can be used to encode lexical rules as binary relations in the CUF system ( #REF ; #REFb ) or the TFS system ( #TARGET_REF ; #REF ) . The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.",
        "output": "related work:Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken, for example, in LKB (#REF) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (#REF, 31). A similar method is included in PATR-II ( #REF ) and can be used to encode lexical rules as binary relations in the CUF system ( #REF ; #REFb ) or the TFS system ( #TARGET_REF<USE/> ; #REF ) . The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."
    },
    {
        "gold": {
            "text": [
                "A Brown clustering is a class-based bigram model in which (1) the probability of a document is the product of the probabilities of its bigrams, (2) the probability of each bigram is the product of the probability of a bigram model over latent classes and the probability of each class generating the actual word types in the bigram, and (3) each word type has non-zero probability only on a single class.",
                "Given a one-toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus.",
                "The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest post-merge log-likelihood, doing so until all classes have been merged.",
                "This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #TARGET_REF ; #REF ; #REF ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:A Brown clustering is a class-based bigram model in which (1) the probability of a document is the product of the probabilities of its bigrams, (2) the probability of each bigram is the product of the probability of a bigram model over latent classes and the probability of each class generating the actual word types in the bigram, and (3) each word type has non-zero probability only on a single class. Given a one-toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus. The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest post-merge log-likelihood, doing so until all classes have been merged. This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #TARGET_REF ; #REF ; #REF ) .",
        "output": "introduction:A Brown clustering is a class-based bigram model in which (1) the probability of a document is the product of the probabilities of its bigrams, (2) the probability of each bigram is the product of the probability of a bigram model over latent classes and the probability of each class generating the actual word types in the bigram, and (3) each word type has non-zero probability only on a single class. Given a one-toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus. The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest post-merge log-likelihood, doing so until all classes have been merged. This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #TARGET_REF<EXT/> ; #REF ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "Removing redundant sentences.",
                "After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #TARGET_REF to penalize redundant sentences in cohesive clusters .",
                "This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e., its score is not decremented).",
                "Specifically, given a sentence s k in cluster SC l which contains a sentence with a higher or equal score, the contribution of SC l to Score(s k ) (= Pr(SC l ) × Pr(s k |SC l )) is subtracted from Score(s k ).",
                "After applying these penalties, we retain only the sentences whose adjusted score is greater than zero (for a highly cohesive cluster, typically only one sentence remains)."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:Removing redundant sentences. After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #TARGET_REF to penalize redundant sentences in cohesive clusters . This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e., its score is not decremented). Specifically, given a sentence s k in cluster SC l which contains a sentence with a higher or equal score, the contribution of SC l to Score(s k ) (= Pr(SC l ) × Pr(s k |SC l )) is subtracted from Score(s k ). After applying these penalties, we retain only the sentences whose adjusted score is greater than zero (for a highly cohesive cluster, typically only one sentence remains).",
        "output": "method:Removing redundant sentences. After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #TARGET_REF<FUT/> to penalize redundant sentences in cohesive clusters . This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e., its score is not decremented). Specifically, given a sentence s k in cluster SC l which contains a sentence with a higher or equal score, the contribution of SC l to Score(s k ) (= Pr(SC l ) × Pr(s k |SC l )) is subtracted from Score(s k ). After applying these penalties, we retain only the sentences whose adjusted score is greater than zero (for a highly cohesive cluster, typically only one sentence remains)."
    },
    {
        "gold": {
            "text": [
                "Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of _source, target_ chunks.",
                "Juola (1994, 1997) conducts some small experiments using his METLA system to show the viability of this approach for English __ French and English __ Urdu.",
                "For the English __ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data.",
                "For English â > Urdu , #TARGET_REF , page 213 ) notes that \"the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus .",
                "On novel test sentences, he gives results of 72% correct translation.",
                "In their Gaijin system, #REF give a result of 63% accurate translations obtained for English __ German on a test set of 791 sentences from CorelDRAW manuals."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of _source, target_ chunks. Juola (1994, 1997) conducts some small experiments using his METLA system to show the viability of this approach for English __ French and English __ Urdu. For the English __ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data. For English â > Urdu , #TARGET_REF , page 213 ) notes that \"the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus . On novel test sentences, he gives results of 72% correct translation. In their Gaijin system, #REF give a result of 63% accurate translations obtained for English __ German on a test set of 791 sentences from CorelDRAW manuals.",
        "output": "introduction:Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of _source, target_ chunks. Juola (1994, 1997) conducts some small experiments using his METLA system to show the viability of this approach for English __ French and English __ Urdu. For the English __ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data. For English â > Urdu , #TARGET_REF<BACK/> , page 213 ) notes that \"the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus . On novel test sentences, he gives results of 72% correct translation. In their Gaijin system, #REF give a result of 63% accurate translations obtained for English __ German on a test set of 791 sentences from CorelDRAW manuals."
    },
    {
        "gold": {
            "text": [
                "The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as #REF which produces weather reports from structured data or #REF which generates descriptions of objects from images.",
                "Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #TARGET_REF ) ( #REF ) ( #REF ) or song lyrics ( #REF ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .",
                "In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as #REF which produces weather reports from structured data or #REF which generates descriptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #TARGET_REF ) ( #REF ) ( #REF ) or song lyrics ( #REF ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced . In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.",
        "output": "related work:The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as #REF which produces weather reports from structured data or #REF which generates descriptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #TARGET_REF<USE/> ) ( #REF ) ( #REF ) or song lyrics ( #REF ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced . In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric."
    },
    {
        "gold": {
            "text": [
                "In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #TARGET_REF ) ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "method:In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #TARGET_REF ) .",
        "output": "method:In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #TARGET_REF<EXT/> ) ."
    },
    {
        "gold": {
            "text": [
                "Figure 2 illustrates the sentence \"John picks the box up\" with its corresponding SSTC.",
                "It contains a nonprojective correspondence.",
                "An interval is assigned to each word in the sentence, i.e. (0-1) for \"John\", (1-2) for \"picks\", (2-3) for \"the\", (3-4) for \"box\" and (4-5) for \"up\".",
                "A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #TARGET_REF ) and Boitet & #REF .",
                "and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Figure 2 illustrates the sentence \"John picks the box up\" with its corresponding SSTC. It contains a nonprojective correspondence. An interval is assigned to each word in the sentence, i.e. (0-1) for \"John\", (1-2) for \"picks\", (2-3) for \"the\", (3-4) for \"box\" and (4-5) for \"up\". A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #TARGET_REF ) and Boitet & #REF . and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.",
        "output": "nan:Figure 2 illustrates the sentence \"John picks the box up\" with its corresponding SSTC. It contains a nonprojective correspondence. An interval is assigned to each word in the sentence, i.e. (0-1) for \"John\", (1-2) for \"picks\", (2-3) for \"the\", (3-4) for \"box\" and (4-5) for \"up\". A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #TARGET_REF<FUT/> ) and Boitet & #REF . and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree."
    },
    {
        "gold": {
            "text": [
                "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.",
                "This includes work on question answering ( #TARGET_REF ) , sentiment analysis ( #REF ) , MT reordering ( #REF ) , and many other tasks .",
                "In most cases, the accuracy of parsers degrades when run on out-of-domain data (#REF;#REF;#REF;#REF).",
                "But these accuracies are measured with respect to gold-standard out-of-domain parse trees.",
                "There are few tasks that actually depend on the complete parse tree.",
                "Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.",
                "While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.",
                "The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering ( #TARGET_REF ) , sentiment analysis ( #REF ) , MT reordering ( #REF ) , and many other tasks . In most cases, the accuracy of parsers degrades when run on out-of-domain data (#REF;#REF;#REF;#REF). But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training. The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.",
        "output": "introduction:The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering ( #TARGET_REF<BACK/> ) , sentiment analysis ( #REF ) , MT reordering ( #REF ) , and many other tasks . In most cases, the accuracy of parsers degrades when run on out-of-domain data (#REF;#REF;#REF;#REF). But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training. The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure."
    },
    {
        "gold": {
            "text": [
                "Unless explicitly stated otherwise, we assume that formulas are expressed in a certain (formal) language L without equality; the extension L(=) of L is going to be used only in Section 5 for dealing with noun phrase references.",
                "This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX #TARGET_REF .",
                "#REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Unless explicitly stated otherwise, we assume that formulas are expressed in a certain (formal) language L without equality; the extension L(=) of L is going to be used only in Section 5 for dealing with noun phrase references. This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX #TARGET_REF . #REF.",
        "output": "introduction:Unless explicitly stated otherwise, we assume that formulas are expressed in a certain (formal) language L without equality; the extension L(=) of L is going to be used only in Section 5 for dealing with noun phrase references. This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX #TARGET_REF<USE/> . #REF."
    },
    {
        "gold": {
            "text": [
                "For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus.",
                "The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine.",
                "For the hyperparameters, we set Į to 0.1 and p expand = 1/3 to give a preference to the rules with small fragments.",
                "We built an s2t translation system with the achieved U-trees after the 1000th iteration.",
                "We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11",
                "11 From ( #TARGET_REF ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .",
                "Thus, to be convenient, we only conduct experiments with the SAMT system."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus. The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine. For the hyperparameters, we set Į to 0.1 and p expand = 1/3 to give a preference to the rules with small fragments. We built an s2t translation system with the achieved U-trees after the 1000th iteration. We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11 11 From ( #TARGET_REF ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags . Thus, to be convenient, we only conduct experiments with the SAMT system.",
        "output": "experiments:For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus. The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine. For the hyperparameters, we set Į to 0.1 and p expand = 1/3 to give a preference to the rules with small fragments. We built an s2t translation system with the achieved U-trees after the 1000th iteration. We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11 11 From ( #TARGET_REF<EXT/> ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags . Thus, to be convenient, we only conduct experiments with the SAMT system."
    },
    {
        "gold": {
            "text": [
                "To model o ( Li , S â T ) , o ( Ri , S â T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #TARGET_REF .",
                "Formally, this model takes the form of probability distribution P ori (o(L i,S→T ), o(R i,S→T )|Y i,S→T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases).",
                "In particular, o maps the reordering into one of the following four orientation values (borrowed from #REF) with respect to the function word: Monotone Adjacent (MA), Monotone Gap (MG), Reverse Adjacent (RA) and Reverse Gap (RG).",
                "The Monotone/Reverse distinction indicates whether the projected order follows the original order, while the Adjacent/Gap distinction indicates whether the pro-This heuristic is commonly used in learning phrase pairs from parallel text.",
                "The maximality ensures the uniqueness of L and R."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:To model o ( Li , S â T ) , o ( Ri , S â T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #TARGET_REF . Formally, this model takes the form of probability distribution P ori (o(L i,S→T ), o(R i,S→T )|Y i,S→T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases). In particular, o maps the reordering into one of the following four orientation values (borrowed from #REF) with respect to the function word: Monotone Adjacent (MA), Monotone Gap (MG), Reverse Adjacent (RA) and Reverse Gap (RG). The Monotone/Reverse distinction indicates whether the projected order follows the original order, while the Adjacent/Gap distinction indicates whether the pro-This heuristic is commonly used in learning phrase pairs from parallel text. The maximality ensures the uniqueness of L and R.",
        "output": "nan:To model o ( Li , S â T ) , o ( Ri , S â T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #TARGET_REF<FUT/> . Formally, this model takes the form of probability distribution P ori (o(L i,S→T ), o(R i,S→T )|Y i,S→T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases). In particular, o maps the reordering into one of the following four orientation values (borrowed from #REF) with respect to the function word: Monotone Adjacent (MA), Monotone Gap (MG), Reverse Adjacent (RA) and Reverse Gap (RG). The Monotone/Reverse distinction indicates whether the projected order follows the original order, while the Adjacent/Gap distinction indicates whether the pro-This heuristic is commonly used in learning phrase pairs from parallel text. The maximality ensures the uniqueness of L and R."
    },
    {
        "gold": {
            "text": [
                "The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #REF ; #REF ; #REF ; #TARGET_REF ) .",
                "One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #REF ; #REF ; #REF ; #TARGET_REF ) . One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.",
        "output": "introduction:The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #REF ; #REF ; #REF ; #TARGET_REF<BACK/> ) . One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain."
    },
    {
        "gold": {
            "text": [
                "â¢ The regular TBL , as described in section 2 ; â¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â¢ The FastTBL algorithm ; â¢ The ICA algorithm ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:â¢ The regular TBL , as described in section 2 ; â¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â¢ The FastTBL algorithm ; â¢ The ICA algorithm ( #TARGET_REF ) .",
        "output": "experiments:â¢ The regular TBL , as described in section 2 ; â¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â¢ The FastTBL algorithm ; â¢ The ICA algorithm ( #TARGET_REF<USE/> ) ."
    },
    {
        "gold": {
            "text": [
                "Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) .",
                "Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "introduction:Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ; #REF ) . Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.",
        "output": "introduction:Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , #REF ; Carroll , Briscoe , and #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF<EXT/> ; #REF ) . Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation."
    },
    {
        "gold": {
            "text": [
                "It also shows the structural identity to bilingual grammars as used in ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:It also shows the structural identity to bilingual grammars as used in ( #TARGET_REF ) .",
        "output": "nan:It also shows the structural identity to bilingual grammars as used in ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.",
                "Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model.",
                "The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon (#REF;#REF).",
                "On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.",
                "The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (#REF;#REF;#REF).",
                "Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.",
                "For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages. Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model. The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon (#REF;#REF). On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units. The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (#REF;#REF;#REF). Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately. For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #TARGET_REF ) .",
        "output": "related work:Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages. Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model. The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon (#REF;#REF). On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units. The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (#REF;#REF;#REF). Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately. For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "To account for this difference, we can estimate separate values of X + and A-for different ranges of n(u,v).",
                "Similarly, the hidden parameters can be conditioned on the linked parts of speech.",
                "Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.",
                "Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.",
                "This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #TARGET_REFb ) .",
                "for their models (#REFb).",
                "When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:To account for this difference, we can estimate separate values of X + and A-for different ranges of n(u,v). Similarly, the hidden parameters can be conditioned on the linked parts of speech. Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences. Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not. This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #TARGET_REFb ) . for their models (#REFb). When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class.",
        "output": "method:To account for this difference, we can estimate separate values of X + and A-for different ranges of n(u,v). Similarly, the hidden parameters can be conditioned on the linked parts of speech. Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences. Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not. This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #TARGET_REF<USE/>b ) . for their models (#REFb). When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class."
    },
    {
        "gold": {
            "text": [
                "The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form.",
                "This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.",
                "In this work, we use the Arabic root extraction technique in (El #REF).",
                "It compares favorably to other stemming or root extraction algorithms ( #REF ; #REF ; and #TARGET_REF ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .",
                "In the remainder of this paper, we will use the term \"root\" and \"term\" interchangeably to refer to canonical forms obtained through this root extraction process."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form. This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root. In this work, we use the Arabic root extraction technique in (El #REF). It compares favorably to other stemming or root extraction algorithms ( #REF ; #REF ; and #TARGET_REF ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs . In the remainder of this paper, we will use the term \"root\" and \"term\" interchangeably to refer to canonical forms obtained through this root extraction process.",
        "output": "nan:The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form. This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root. In this work, we use the Arabic root extraction technique in (El #REF). It compares favorably to other stemming or root extraction algorithms ( #REF ; #REF ; and #TARGET_REF<EXT/> ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs . In the remainder of this paper, we will use the term \"root\" and \"term\" interchangeably to refer to canonical forms obtained through this root extraction process."
    },
    {
        "gold": {
            "text": [
                "In our latest implementation of this algorithm, we have recast this segmentation strategy as the composition of three distinct finite state machines.",
                "The first machine, illustrated in Figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations.",
                "The second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries.",
                "The final machine is a trigram language model , specifically a Kneser-Ney ( #TARGET_REF ) based backoff language model .",
                "Differing from (#REF), we have also introduced an explicit model for un-known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.",
                "Using 0.5M words from the combined Arabic Treebanks 1V2, 2V2 and 3V1, the dictionary based segmenter achieves a exact word match 97.8% correct segmentation."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:In our latest implementation of this algorithm, we have recast this segmentation strategy as the composition of three distinct finite state machines. The first machine, illustrated in Figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations. The second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries. The final machine is a trigram language model , specifically a Kneser-Ney ( #TARGET_REF ) based backoff language model . Differing from (#REF), we have also introduced an explicit model for un-known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty. Using 0.5M words from the combined Arabic Treebanks 1V2, 2V2 and 3V1, the dictionary based segmenter achieves a exact word match 97.8% correct segmentation.",
        "output": "nan:In our latest implementation of this algorithm, we have recast this segmentation strategy as the composition of three distinct finite state machines. The first machine, illustrated in Figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations. The second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries. The final machine is a trigram language model , specifically a Kneser-Ney ( #TARGET_REF<FUT/> ) based backoff language model . Differing from (#REF), we have also introduced an explicit model for un-known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty. Using 0.5M words from the combined Arabic Treebanks 1V2, 2V2 and 3V1, the dictionary based segmenter achieves a exact word match 97.8% correct segmentation."
    },
    {
        "gold": {
            "text": [
                "In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #REF ; #REF ; #REF ] , head-driven phrase structure grammar [ HPSG ] [ #TARGET_REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #REF ; #REF ; #REF ] , head-driven phrase structure grammar [ HPSG ] [ #TARGET_REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .",
        "output": "introduction:In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #REF ; #REF ; #REF ] , head-driven phrase structure grammar [ HPSG ] [ #TARGET_REF<BACK/> ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."
    },
    {
        "gold": {
            "text": [
                "Although there are other discussions of the paragraph as a central element of discourse ( e.g. #REF , #REF , #TARGET_REF , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .",
                "Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.",
                "Our interest, however, lies precisely in that area."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Although there are other discussions of the paragraph as a central element of discourse ( e.g. #REF , #REF , #TARGET_REF , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area.",
        "output": "introduction:Although there are other discussions of the paragraph as a central element of discourse ( e.g. #REF , #REF , #TARGET_REF<USE/> , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area."
    },
    {
        "gold": {
            "text": [
                "While the TF measurement concerns the importance of a term in a given document, IDF seeks to measure the relative importance of a term in a collection of documents.",
                "The importance of each term is assumed to be inversely proportional to the number of documents that contain that term.",
                "TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #TARGET_REF ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .",
                "Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t ."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "experiments:While the TF measurement concerns the importance of a term in a given document, IDF seeks to measure the relative importance of a term in a collection of documents. The importance of each term is assumed to be inversely proportional to the number of documents that contain that term. TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #TARGET_REF ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance . Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t .",
        "output": "experiments:While the TF measurement concerns the importance of a term in a given document, IDF seeks to measure the relative importance of a term in a collection of documents. The importance of each term is assumed to be inversely proportional to the number of documents that contain that term. TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #TARGET_REF<EXT/> ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance . Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t ."
    },
    {
        "gold": {
            "text": [
                "The sampler might reinforce the frequent alignment errors (AE), which would harm the translation model (TM).",
                "Actually, the frequent AEs also greatly impair the conventional TM.",
                "Besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent AEs.",
                "Thus, compared with the conventional TMs, we believe that our final TM would not be worse due to AEs.",
                "Our final experiments verify this point and we will conduct a much detailed analysis in future.",
                "9 We only use the minimal GHKM rules ( #TARGET_REF ) here to reduce the complexity of the sampler ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:The sampler might reinforce the frequent alignment errors (AE), which would harm the translation model (TM). Actually, the frequent AEs also greatly impair the conventional TM. Besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent AEs. Thus, compared with the conventional TMs, we believe that our final TM would not be worse due to AEs. Our final experiments verify this point and we will conduct a much detailed analysis in future. 9 We only use the minimal GHKM rules ( #TARGET_REF ) here to reduce the complexity of the sampler .",
        "output": "method:The sampler might reinforce the frequent alignment errors (AE), which would harm the translation model (TM). Actually, the frequent AEs also greatly impair the conventional TM. Besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent AEs. Thus, compared with the conventional TMs, we believe that our final TM would not be worse due to AEs. Our final experiments verify this point and we will conduct a much detailed analysis in future. 9 We only use the minimal GHKM rules ( #TARGET_REF<FUT/> ) here to reduce the complexity of the sampler ."
    },
    {
        "gold": {
            "text": [
                "For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #REF ) , ( 3 ) by compilation of decision trees ( #TARGET_REF ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and #REF ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #REF ) , ( 3 ) by compilation of decision trees ( #TARGET_REF ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and #REF ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .",
        "output": "introduction:For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #REF ) , ( 3 ) by compilation of decision trees ( #TARGET_REF<BACK/> ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and #REF ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below ."
    },
    {
        "gold": {
            "text": [
                "The coreference system system is similar to the Bell tree algorithm as described by ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The coreference system system is similar to the Bell tree algorithm as described by ( #TARGET_REF ) .",
        "output": "nan:The coreference system system is similar to the Bell tree algorithm as described by ( #TARGET_REF<USE/> ) ."
    },
    {
        "gold": {
            "text": [
                "To process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P .",
                "This idea was inspired by #TARGET_REF , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) .",
                "In recent approaches, syntactic information is translated into features which, together with information from FrameNet, WordNet or VerbNet, will be used with ML tools to make predictions for each example in the test set (#REF;#REF)."
            ],
            "label": [
                "EXT"
            ]
        },
        "input": "nan:To process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P . This idea was inspired by #TARGET_REF , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) . In recent approaches, syntactic information is translated into features which, together with information from FrameNet, WordNet or VerbNet, will be used with ML tools to make predictions for each example in the test set (#REF;#REF).",
        "output": "nan:To process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P . This idea was inspired by #TARGET_REF<EXT/> , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) . In recent approaches, syntactic information is translated into features which, together with information from FrameNet, WordNet or VerbNet, will be used with ML tools to make predictions for each example in the test set (#REF;#REF)."
    },
    {
        "gold": {
            "text": [
                "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) #TARGET_REF , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) #TARGET_REF , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .",
        "output": "nan:Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) #TARGET_REF<FUT/> , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
    },
    {
        "gold": {
            "text": [
                "The most used feature for the Web People Search task, however, are NEs.",
                "#REF introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.",
                "In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #REF ; #REF ; #REF ; #TARGET_REF ) .",
                "For instance, #REF uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.",
                "#REF compared the performace of NEs versus BoW features.",
                "In his experiments a only a representation based on Organisation NEs outperformed the word based approach.",
                "Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:The most used feature for the Web People Search task, however, are NEs. #REF introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #REF ; #REF ; #REF ; #TARGET_REF ) . For instance, #REF uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. #REF compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).",
        "output": "related work:The most used feature for the Web People Search task, however, are NEs. #REF introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #REF ; #REF ; #REF ; #TARGET_REF<BACK/> ) . For instance, #REF uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. #REF compared the performace of NEs versus BoW features. In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW)."
    },
    {
        "gold": {
            "text": [
                "Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.",
                "An alternative representation based on #TARGET_REF is presented in #REF , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .",
                "Although a grid may be more descriptively suitable for some aspects of prosody (for example, #REF use the grid representation for their implementation of stress assignment in compound nominals), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience. An alternative representation based on #TARGET_REF is presented in #REF , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree . Although a grid may be more descriptively suitable for some aspects of prosody (for example, #REF use the grid representation for their implementation of stress assignment in compound nominals), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing.",
        "output": "experiments:Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience. An alternative representation based on #TARGET_REF<USE/> is presented in #REF , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree . Although a grid may be more descriptively suitable for some aspects of prosody (for example, #REF use the grid representation for their implementation of stress assignment in compound nominals), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing."
    },
    {
        "gold": {
            "text": [
                "We train the system by clustering the \"experiences\" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).",
                "We then use the program Snob ( #REF ; #TARGET_REF ) to cluster these experiences .",
                "Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.",
                "shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15",
                "These clusters were chosen because they illustrate clearly three situations of interest."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:We train the system by clustering the \"experiences\" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively). We then use the program Snob ( #REF ; #TARGET_REF ) to cluster these experiences . Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data. shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15 These clusters were chosen because they illustrate clearly three situations of interest.",
        "output": "nan:We train the system by clustering the \"experiences\" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively). We then use the program Snob ( #REF ; #TARGET_REF<FUT/> ) to cluster these experiences . Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data. shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15 These clusters were chosen because they illustrate clearly three situations of interest."
    },
    {
        "gold": {
            "text": [
                "Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data.",
                "#TARGET_REF describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .",
                "This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.",
                "Each of these sequences was categorized as a modifier or argument.",
                "Arguments were then mapped to traditional syntactic functions.",
                "For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject.",
                "In general, argumenthood was preferred over adjuncthoood.",
                "As #REF does not include an evaluation, currently it is impossible to say how effective their technique is.",
                "#REF present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank (#REF).",
                "Czech is a language with a freer word order than English and so configurational information cannot be relied upon.",
                "In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data. #TARGET_REF describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank . This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank. Each of these sequences was categorized as a modifier or argument. Arguments were then mapped to traditional syntactic functions. For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject. In general, argumenthood was preferred over adjuncthoood. As #REF does not include an evaluation, currently it is impossible to say how effective their technique is. #REF present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank (#REF). Czech is a language with a freer word order than English and so configurational information cannot be relied upon. In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.",
        "output": "related work:Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data. #TARGET_REF<BACK/> describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank . This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank. Each of these sequences was categorized as a modifier or argument. Arguments were then mapped to traditional syntactic functions. For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject. In general, argumenthood was preferred over adjuncthoood. As #REF does not include an evaluation, currently it is impossible to say how effective their technique is. #REF present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank (#REF). Czech is a language with a freer word order than English and so configurational information cannot be relied upon. In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame."
    },
    {
        "gold": {
            "text": [
                "Several works have proposed discriminative techniques to train log-linear model for SMT.",
                "( #TARGET_REF ; #REF ) used maximum likelihood estimation to learn weights for MT.",
                "(#REF;#REF;#REF; #REF) employed an evaluation metric as a loss function and directly optimized it.",
                "(#REF;#REF;#REF) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Several works have proposed discriminative techniques to train log-linear model for SMT. ( #TARGET_REF ; #REF ) used maximum likelihood estimation to learn weights for MT. (#REF;#REF;#REF; #REF) employed an evaluation metric as a loss function and directly optimized it. (#REF;#REF;#REF) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.",
        "output": "related work:Several works have proposed discriminative techniques to train log-linear model for SMT. ( #TARGET_REF<USE/> ; #REF ) used maximum likelihood estimation to learn weights for MT. (#REF;#REF;#REF; #REF) employed an evaluation metric as a loss function and directly optimized it. (#REF;#REF;#REF) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions."
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF has built a semantic role classifier exploiting the interdependence of semantic roles .",
                "It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.",
                "Se- mantic context features indicates the features ex- tracted from the arguments around the current one.",
                "We can use window size to represent the scope of the context.",
                "Window size [-m, n] means that, in the sequence that all the arguments has constructed, the features of previous m and following n argu- ments will be utilized for the classification of cur- rent semantic role.",
                "There are two kinds of argu- ment sequences in #REF, and we only test the linear sequence.",
                "Take the sentence in fig- ure 1 as an example.",
                "The linear sequence of the arguments in this sentence is: ____(until then), ____ (the insurance company), _ (has), _ ____ (for the Sanxia Project), ____ (in- surance services).",
                "For the argument _ (has), if the semantic context window size is [-1,2], the seman- tic context features e.g. headword, phrase type and etc. of ____ (the insurance company), __ ___ (for the Sanxia Project) and ____ (insurance services) will be utilized to serve the classification task of _ (has)."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:#TARGET_REF has built a semantic role classifier exploiting the interdependence of semantic roles . It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features. Se- mantic context features indicates the features ex- tracted from the arguments around the current one. We can use window size to represent the scope of the context. Window size [-m, n] means that, in the sequence that all the arguments has constructed, the features of previous m and following n argu- ments will be utilized for the classification of cur- rent semantic role. There are two kinds of argu- ment sequences in #REF, and we only test the linear sequence. Take the sentence in fig- ure 1 as an example. The linear sequence of the arguments in this sentence is: ____(until then), ____ (the insurance company), _ (has), _ ____ (for the Sanxia Project), ____ (in- surance services). For the argument _ (has), if the semantic context window size is [-1,2], the seman- tic context features e.g. headword, phrase type and etc. of ____ (the insurance company), __ ___ (for the Sanxia Project) and ____ (insurance services) will be utilized to serve the classification task of _ (has).",
        "output": "nan:#TARGET_REF<FUT/> has built a semantic role classifier exploiting the interdependence of semantic roles . It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features. Se- mantic context features indicates the features ex- tracted from the arguments around the current one. We can use window size to represent the scope of the context. Window size [-m, n] means that, in the sequence that all the arguments has constructed, the features of previous m and following n argu- ments will be utilized for the classification of cur- rent semantic role. There are two kinds of argu- ment sequences in #REF, and we only test the linear sequence. Take the sentence in fig- ure 1 as an example. The linear sequence of the arguments in this sentence is: ____(until then), ____ (the insurance company), _ (has), _ ____ (for the Sanxia Project), ____ (in- surance services). For the argument _ (has), if the semantic context window size is [-1,2], the seman- tic context features e.g. headword, phrase type and etc. of ____ (the insurance company), __ ___ (for the Sanxia Project) and ____ (insurance services) will be utilized to serve the classification task of _ (has)."
    },
    {
        "gold": {
            "text": [
                "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) .",
                "However, most existing systems use pre-authored tutor responses for addressing student errors.",
                "The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.",
                "The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.",
                "It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state. It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.",
        "output": "introduction:Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #TARGET_REF<BACK/> ; #REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state. It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations."
    },
    {
        "gold": {
            "text": [
                "â¢ Only an automatic evaluation was performed , which relied on having model responses ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:â¢ Only an automatic evaluation was performed , which relied on having model responses ( #REF ; #TARGET_REF ) .",
        "output": "method:â¢ Only an automatic evaluation was performed , which relied on having model responses ( #REF ; #TARGET_REF<USE/> ) ."
    },
    {
        "gold": {
            "text": [
                "Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #TARGET_REF ).",
                "It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #TARGET_REF ). It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.",
        "output": "nan:Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #TARGET_REF<FUT/> ). It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall."
    },
    {
        "gold": {
            "text": [
                "A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #TARGET_REF ) .",
                "According to the data available from 1990 U.S. Census Bureau, only 90,000 different names are shared by 100 million people (#REF).",
                "As the amount of information in the WWW grows, more of these people are mentioned in different web pages.",
                "Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #TARGET_REF ) . According to the data available from 1990 U.S. Census Bureau, only 90,000 different names are shared by 100 million people (#REF). As the amount of information in the WWW grows, more of these people are mentioned in different web pages. Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned.",
        "output": "introduction:A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #TARGET_REF<BACK/> ) . According to the data available from 1990 U.S. Census Bureau, only 90,000 different names are shared by 100 million people (#REF). As the amount of information in the WWW grows, more of these people are mentioned in different web pages. Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned."
    },
    {
        "gold": {
            "text": [
                "We can implement a deductive theory of quantifier scope using the conditional equivalence mechanism.",
                "The version proposed here combines a basic insight from #REF with higher-order unification to give an analysis that has a strong resemblance to that proposed in #TARGET_REF , 1991 ) , with some differences that are commented on below .",
                "Like Pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by Lewin.",
                "We analyze quantified NPs at the QLF level as illustrated in the QLF for: We assume that every determiner has its own equivalence, which resolves it as a quantifier: sometimes this can be quite a complicated matter, as with any (#REF), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity.",
                "6"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:We can implement a deductive theory of quantifier scope using the conditional equivalence mechanism. The version proposed here combines a basic insight from #REF with higher-order unification to give an analysis that has a strong resemblance to that proposed in #TARGET_REF , 1991 ) , with some differences that are commented on below . Like Pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by Lewin. We analyze quantified NPs at the QLF level as illustrated in the QLF for: We assume that every determiner has its own equivalence, which resolves it as a quantifier: sometimes this can be quite a complicated matter, as with any (#REF), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity. 6",
        "output": "nan:We can implement a deductive theory of quantifier scope using the conditional equivalence mechanism. The version proposed here combines a basic insight from #REF with higher-order unification to give an analysis that has a strong resemblance to that proposed in #TARGET_REF<USE/> , 1991 ) , with some differences that are commented on below . Like Pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by Lewin. We analyze quantified NPs at the QLF level as illustrated in the QLF for: We assume that every determiner has its own equivalence, which resolves it as a quantifier: sometimes this can be quite a complicated matter, as with any (#REF), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity. 6"
    },
    {
        "gold": {
            "text": [
                "The shallow parser used is the SNoW-based CSCL parser (#REF;#REF).",
                "SNoW ( #REF ; #TARGET_REF ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .",
                "It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.",
                "Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.",
                "However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.",
                "Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).",
                "The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The shallow parser used is the SNoW-based CSCL parser (#REF;#REF). SNoW ( #REF ; #TARGET_REF ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example . It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes. However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference. Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.). The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.",
        "output": "experiments:The shallow parser used is the SNoW-based CSCL parser (#REF;#REF). SNoW ( #REF ; #TARGET_REF<FUT/> ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example . It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes. However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference. Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.). The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes."
    },
    {
        "gold": {
            "text": [
                "We could require that the referent of an evaluative description fall into the correct segment of the relevant dimension.",
                "(For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #TARGET_REF 1999, discussed in Section 7.2).",
                "Be this as it may, we shall henceforth focus on local context, assuming that additional requirements on the global context can be made if necessary."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:We could require that the referent of an evaluative description fall into the correct segment of the relevant dimension. (For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #TARGET_REF 1999, discussed in Section 7.2). Be this as it may, we shall henceforth focus on local context, assuming that additional requirements on the global context can be made if necessary.",
        "output": "experiments:We could require that the referent of an evaluative description fall into the correct segment of the relevant dimension. (For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #TARGET_REF<BACK/> 1999, discussed in Section 7.2). Be this as it may, we shall henceforth focus on local context, assuming that additional requirements on the global context can be made if necessary."
    },
    {
        "gold": {
            "text": [
                "However , most strategies are based on `` internal  or `` external methods  ( #TARGET_REF ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .",
                "(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process.)",
                "The work reported here infers specific semantic relationships based on sets of examples and counterexamples."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:However , most strategies are based on `` internal  or `` external methods  ( #TARGET_REF ) , i.e. methods that rely on the form of terms or on the information gathered from contexts . (In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process.) The work reported here infers specific semantic relationships based on sets of examples and counterexamples.",
        "output": "introduction:However , most strategies are based on `` internal  or `` external methods  ( #TARGET_REF<USE/> ) , i.e. methods that rely on the form of terms or on the information gathered from contexts . (In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process.) The work reported here infers specific semantic relationships based on sets of examples and counterexamples."
    },
    {
        "gold": {
            "text": [
                "In applying sample selection to training a PP-attachment model, we have observed that all effective evaluation functions make use of the model's current hypothesis in estimating the training utility of the candidates.",
                "Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor.",
                "In this section, we investigate whether these observations hold true for training statistical parsing models as well.",
                "Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and #REF ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #TARGET_REF ; #REF ) , and Collins 's Model 2 parser ( 1997 ) .",
                "Although both models are lexicalized, statistical parsers, their learning algorithms are different.",
                "The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data.",
                "In contrast, PLTIG's expectation-maximization-based induction algorithm is partially supervised; the model's parameters are estimated indirectly from the training data."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:In applying sample selection to training a PP-attachment model, we have observed that all effective evaluation functions make use of the model's current hypothesis in estimating the training utility of the candidates. Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor. In this section, we investigate whether these observations hold true for training statistical parsing models as well. Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and #REF ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #TARGET_REF ; #REF ) , and Collins 's Model 2 parser ( 1997 ) . Although both models are lexicalized, statistical parsers, their learning algorithms are different. The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data. In contrast, PLTIG's expectation-maximization-based induction algorithm is partially supervised; the model's parameters are estimated indirectly from the training data.",
        "output": "nan:In applying sample selection to training a PP-attachment model, we have observed that all effective evaluation functions make use of the model's current hypothesis in estimating the training utility of the candidates. Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor. In this section, we investigate whether these observations hold true for training statistical parsing models as well. Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and #REF ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #TARGET_REF<FUT/> ; #REF ) , and Collins 's Model 2 parser ( 1997 ) . Although both models are lexicalized, statistical parsers, their learning algorithms are different. The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data. In contrast, PLTIG's expectation-maximization-based induction algorithm is partially supervised; the model's parameters are estimated indirectly from the training data."
    },
    {
        "gold": {
            "text": [
                "In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £).",
                "In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).",
                "Substitution replaces a substitution node with another initial tree (Figure 3).",
                "Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).",
                "FBLTAG ( #TARGET_REF ; #REF ) is an extension of the LTAG formalism .",
                "In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.",
                "Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research #REF).",
                "The XTAG group (#REF) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.",
                "Development of a large-scale French grammar (Abeillé and #REF) has also started at the University of Pennsylvania and is expanded at University of Paris 7."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £). In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ). Substitution replaces a substitution node with another initial tree (Figure 3). Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4). FBLTAG ( #TARGET_REF ; #REF ) is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research #REF). The XTAG group (#REF) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars. Development of a large-scale French grammar (Abeillé and #REF) has also started at the University of Pennsylvania and is expanded at University of Paris 7.",
        "output": "introduction:In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £). In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ). Substitution replaces a substitution node with another initial tree (Figure 3). Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4). FBLTAG ( #TARGET_REF<BACK/> ; #REF ) is an extension of the LTAG formalism . In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research #REF). The XTAG group (#REF) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars. Development of a large-scale French grammar (Abeillé and #REF) has also started at the University of Pennsylvania and is expanded at University of Paris 7."
    },
    {
        "gold": {
            "text": [
                "Our work on the prosodic phrase status of clause final prepositional phrases, which we discuss below, suggests the existence of a discourse-neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase.",
                "3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #TARGET_REF , which \"are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option\" (p. 251) .",
                "This approach to prosodic phrase boundary determination brings us closer to a framework in which phonological, syntactic, and discourse features all contribute to prosodic phrasing."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Our work on the prosodic phrase status of clause final prepositional phrases, which we discuss below, suggests the existence of a discourse-neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase. 3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #TARGET_REF , which \"are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option\" (p. 251) . This approach to prosodic phrase boundary determination brings us closer to a framework in which phonological, syntactic, and discourse features all contribute to prosodic phrasing.",
        "output": "introduction:Our work on the prosodic phrase status of clause final prepositional phrases, which we discuss below, suggests the existence of a discourse-neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase. 3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #TARGET_REF<USE/> , which \"are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option\" (p. 251) . This approach to prosodic phrase boundary determination brings us closer to a framework in which phonological, syntactic, and discourse features all contribute to prosodic phrasing."
    },
    {
        "gold": {
            "text": [
                "The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #TARGET_REF ) .",
                "29 The unfolding transformation is also referred to as partial execution, for example, by #REF.",
                "Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.",
                "As a result, the literal can be removed from the body of the clause.",
                "Whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #TARGET_REF ) . 29 The unfolding transformation is also referred to as partial execution, for example, by #REF. Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time. As a result, the literal can be removed from the body of the clause. Whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation.",
        "output": "introduction:The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #TARGET_REF<FUT/> ) . 29 The unfolding transformation is also referred to as partial execution, for example, by #REF. Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time. As a result, the literal can be removed from the body of the clause. Whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation."
    },
    {
        "gold": {
            "text": [
                "The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.",
                "This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (#REF; #REF) that do not include non- lexicalized fragments.",
                "However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.",
                "While the models of #REF and #REF restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (#REF; #REFa).",
                "The importance of including nonheadwords has become uncontroversial ( e.g. #REF ; #TARGET_REF ; #REF ) .",
                "And #REF argues for \"keeping track of counts of arbitrary fragments within parse trees\", which has indeed been carried out in #REF who use exactly the same set of (all) tree fragments as proposed in #REF."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset. This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (#REF; #REF) that do not include non- lexicalized fragments. However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions. While the models of #REF and #REF restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (#REF; #REFa). The importance of including nonheadwords has become uncontroversial ( e.g. #REF ; #TARGET_REF ; #REF ) . And #REF argues for \"keeping track of counts of arbitrary fragments within parse trees\", which has indeed been carried out in #REF who use exactly the same set of (all) tree fragments as proposed in #REF.",
        "output": "introduction:The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset. This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (#REF; #REF) that do not include non- lexicalized fragments. However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions. While the models of #REF and #REF restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (#REF; #REFa). The importance of including nonheadwords has become uncontroversial ( e.g. #REF ; #TARGET_REF<BACK/> ; #REF ) . And #REF argues for \"keeping track of counts of arbitrary fragments within parse trees\", which has indeed been carried out in #REF who use exactly the same set of (all) tree fragments as proposed in #REF."
    },
    {
        "gold": {
            "text": [
                "13 For per-state conditional normalization, let Dj,a be the set of arcs from state j with input symbol a ∈ Σ; their weights are normalized to sum to 1.",
                "Besides computing c, the E step must count the expected number dj,a of traversals of arcs in each Dj,a.",
                "Then the predicted vector given θ is j,a dj,a • (expected feature counts on a randomly chosen arc in Dj,a).",
                "Per-state joint normalization ( #TARGET_REFb , Â§ 8.2 ) is similar but drops the dependence on a .",
                "The difficult case is global conditional normalization.",
                "It arises, for example, when training a joint model of the form"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:13 For per-state conditional normalization, let Dj,a be the set of arcs from state j with input symbol a ∈ Σ; their weights are normalized to sum to 1. Besides computing c, the E step must count the expected number dj,a of traversals of arcs in each Dj,a. Then the predicted vector given θ is j,a dj,a • (expected feature counts on a randomly chosen arc in Dj,a). Per-state joint normalization ( #TARGET_REFb , Â§ 8.2 ) is similar but drops the dependence on a . The difficult case is global conditional normalization. It arises, for example, when training a joint model of the form",
        "output": "nan:13 For per-state conditional normalization, let Dj,a be the set of arcs from state j with input symbol a ∈ Σ; their weights are normalized to sum to 1. Besides computing c, the E step must count the expected number dj,a of traversals of arcs in each Dj,a. Then the predicted vector given θ is j,a dj,a • (expected feature counts on a randomly chosen arc in Dj,a). Per-state joint normalization ( #TARGET_REF<USE/>b , Â§ 8.2 ) is similar but drops the dependence on a . The difficult case is global conditional normalization. It arises, for example, when training a joint model of the form"
    },
    {
        "gold": {
            "text": [
                "Some modification of this scheme is necessary when the input stream is not deterministic.",
                "For the A * algorithm ( #TARGET_REF ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .",
                "Unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.",
                "With a deterministic word sequence it seems reasonable to assume probability 1.0 for what has been found."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Some modification of this scheme is necessary when the input stream is not deterministic. For the A * algorithm ( #TARGET_REF ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion . Unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied. With a deterministic word sequence it seems reasonable to assume probability 1.0 for what has been found.",
        "output": "nan:Some modification of this scheme is necessary when the input stream is not deterministic. For the A * algorithm ( #TARGET_REF<FUT/> ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion . Unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied. With a deterministic word sequence it seems reasonable to assume probability 1.0 for what has been found."
    },
    {
        "gold": {
            "text": [
                "One way to think of the direct evidence method is to see that it defines a relation ≺ on the set of English adjectives.",
                "Given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a , then a ≺ b.",
                "If the re-verse is true, and b, a is found more often than a, b , then b ≺ a.",
                "If neither order appears in the training data, then neither a ≺ b nor b ≺ a and an order must be randomly assigned.",
                "#TARGET_REF propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .",
                "That is, if a ≺ c and c ≺ b, we can conclude that a ≺ b.",
                "To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.",
                "However, the pairs large, new and new, green occur fairly frequently.",
                "Therefore, in the face of this evidence we can assign this pair the order large, green , which not coincidently is the correct English word order."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:One way to think of the direct evidence method is to see that it defines a relation ≺ on the set of English adjectives. Given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a , then a ≺ b. If the re-verse is true, and b, a is found more often than a, b , then b ≺ a. If neither order appears in the training data, then neither a ≺ b nor b ≺ a and an order must be randomly assigned. #TARGET_REF propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation . That is, if a ≺ c and c ≺ b, we can conclude that a ≺ b. To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method. However, the pairs large, new and new, green occur fairly frequently. Therefore, in the face of this evidence we can assign this pair the order large, green , which not coincidently is the correct English word order.",
        "output": "experiments:One way to think of the direct evidence method is to see that it defines a relation ≺ on the set of English adjectives. Given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a , then a ≺ b. If the re-verse is true, and b, a is found more often than a, b , then b ≺ a. If neither order appears in the training data, then neither a ≺ b nor b ≺ a and an order must be randomly assigned. #TARGET_REF<BACK/> propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation . That is, if a ≺ c and c ≺ b, we can conclude that a ≺ b. To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method. However, the pairs large, new and new, green occur fairly frequently. Therefore, in the face of this evidence we can assign this pair the order large, green , which not coincidently is the correct English word order."
    },
    {
        "gold": {
            "text": [
                "Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.",
                "An alternative representation based on #REF is presented in #REF, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.",
                "Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #TARGET_REF use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience. An alternative representation based on #REF is presented in #REF, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree. Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #TARGET_REF use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing .",
        "output": "experiments:Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience. An alternative representation based on #REF is presented in #REF, which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree. Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #TARGET_REF<USE/> use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing ."
    },
    {
        "gold": {
            "text": [
                "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) #TARGET_REF ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) #TARGET_REF .",
        "output": "nan:Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) #TARGET_REF<FUT/> ."
    },
    {
        "gold": {
            "text": [
                "Several recent works suggest studying coreference jointly with other tasks.",
                "#REF model entity coreference and event coreference jointly ; #TARGET_REF consider joint coreference and entity-linking .",
                "The work closest to ours is that of #REF, which studies a joint anaphoricity detection and coreference resolution framework.",
                "While their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Several recent works suggest studying coreference jointly with other tasks. #REF model entity coreference and event coreference jointly ; #TARGET_REF consider joint coreference and entity-linking . The work closest to ours is that of #REF, which studies a joint anaphoricity detection and coreference resolution framework. While their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different.",
        "output": "related work:Several recent works suggest studying coreference jointly with other tasks. #REF model entity coreference and event coreference jointly ; #TARGET_REF<BACK/> consider joint coreference and entity-linking . The work closest to ours is that of #REF, which studies a joint anaphoricity detection and coreference resolution framework. While their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different."
    },
    {
        "gold": {
            "text": [
                "Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.",
                "To copy otherwise, or to republish, requires a fee and/or specific permission.",
                "0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (#REFb).",
                "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.",
                "#REF;#REF) consult relatively small lexicons, typically generated by hand.",
                "Two exceptions to this generalisation are the Linguistic String Project ( #REF ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( #REF ; #TARGET_REF ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .",
                "In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see #REF for details).",
                "However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (#REFb). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #REF;#REF) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( #REF ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( #REF ; #TARGET_REF ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see #REF for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.",
        "output": "introduction:Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (#REFb). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #REF;#REF) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( #REF ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( #REF ; #TARGET_REF<USE/> ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see #REF for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons."
    },
    {
        "gold": {
            "text": [
                "From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.",
                "In addition to headwords , dictionary search through the pronunciation field is available ; #TARGET_REF has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #REF ) .",
                "In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (#REF).",
                "Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes. In addition to headwords , dictionary search through the pronunciation field is available ; #TARGET_REF has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #REF ) . In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (#REF). Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample.",
        "output": "nan:From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes. In addition to headwords , dictionary search through the pronunciation field is available ; #TARGET_REF<FUT/> has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #REF ) . In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (#REF). Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample."
    },
    {
        "gold": {
            "text": [
                "Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.",
                "There are two types of selection algorithms: committee-based and single learner.",
                "A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem).",
                "The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and #REF ; #TARGET_REF ) .",
                "For computationally intensive problems, such as parsing, keeping multiple learners may be impractical."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set. There are two types of selection algorithms: committee-based and single learner. A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem). The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and #REF ; #TARGET_REF ) . For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.",
        "output": "nan:Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set. There are two types of selection algorithms: committee-based and single learner. A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem). The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and #REF ; #TARGET_REF<BACK/> ) . For computationally intensive problems, such as parsing, keeping multiple learners may be impractical."
    },
    {
        "gold": {
            "text": [
                "The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).",
                "The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.",
                "Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.",
                "The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1). The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( #REF ; #REF ; #TARGET_REF ) .",
        "output": "nan:The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1). The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( #REF ; #REF ; #TARGET_REF<USE/> ) ."
    },
    {
        "gold": {
            "text": [
                "The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4.",
                "These two sets of data were used for automatic dialogue act classification, which was run in the Weka system (#REF).",
                "We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .",
                "The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #TARGET_REF ) .",
                "Therefore, here we show the results of this classifier.",
                "Ten-folds crossvalidation was applied throughout."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4. These two sets of data were used for automatic dialogue act classification, which was run in the Weka system (#REF). We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table . The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #TARGET_REF ) . Therefore, here we show the results of this classifier. Ten-folds crossvalidation was applied throughout.",
        "output": "nan:The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4. These two sets of data were used for automatic dialogue act classification, which was run in the Weka system (#REF). We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table . The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #TARGET_REF<FUT/> ) . Therefore, here we show the results of this classifier. Ten-folds crossvalidation was applied throughout."
    },
    {
        "gold": {
            "text": [
                "With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #TARGET_REF .",
                "One requirement for NLP is the ability to accurately recognize terms that represent biological entities in free text.",
                "Another requirement is the ability to associate these terms with corresponding biological entities (i.e., records in biological databases) in order to be used by other automated systems for literature mining.",
                "Such task is called biological entity tagging.",
                "Biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely: synonymy (i.e., different terms refer to the same entity), ambiguity (i.e., one term is associated with different entities), and coverage (i.e., entity terms or entities are not present in databases or knowledge bases)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #TARGET_REF . One requirement for NLP is the ability to accurately recognize terms that represent biological entities in free text. Another requirement is the ability to associate these terms with corresponding biological entities (i.e., records in biological databases) in order to be used by other automated systems for literature mining. Such task is called biological entity tagging. Biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely: synonymy (i.e., different terms refer to the same entity), ambiguity (i.e., one term is associated with different entities), and coverage (i.e., entity terms or entities are not present in databases or knowledge bases).",
        "output": "introduction:With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #TARGET_REF<BACK/> . One requirement for NLP is the ability to accurately recognize terms that represent biological entities in free text. Another requirement is the ability to associate these terms with corresponding biological entities (i.e., records in biological databases) in order to be used by other automated systems for literature mining. Such task is called biological entity tagging. Biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely: synonymy (i.e., different terms refer to the same entity), ambiguity (i.e., one term is associated with different entities), and coverage (i.e., entity terms or entities are not present in databases or knowledge bases)."
    },
    {
        "gold": {
            "text": [
                "With all its strong points, there are a number of restrictions to the proposed approach.",
                "First, in its present form it is suitable only for processing of reasonably \"wellbehaved\" texts that consistently use capitalization (mixed case) and do not contain much noisy data.",
                "Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts.",
                "We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.",
                "This is where robust syntactic systems like SATZ ( #REF ) or the POS tagger reported in #TARGET_REF , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:With all its strong points, there are a number of restrictions to the proposed approach. First, in its present form it is suitable only for processing of reasonably \"wellbehaved\" texts that consistently use capitalization (mixed case) and do not contain much noisy data. Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts. We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. This is where robust syntactic systems like SATZ ( #REF ) or the POS tagger reported in #TARGET_REF , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .",
        "output": "conclusion:With all its strong points, there are a number of restrictions to the proposed approach. First, in its present form it is suitable only for processing of reasonably \"wellbehaved\" texts that consistently use capitalization (mixed case) and do not contain much noisy data. Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts. We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. This is where robust syntactic systems like SATZ ( #REF ) or the POS tagger reported in #TARGET_REF<USE/> , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage ."
    },
    {
        "gold": {
            "text": [
                "In this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags.",
                "In order to estimate the parameters of our model , we develop a blocked sampler based on that of #TARGET_REF to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .",
                "However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior.",
                "Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:In this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags. In order to estimate the parameters of our model , we develop a blocked sampler based on that of #TARGET_REF to sample parse trees for sentences in the raw training corpus according to their posterior probabilities . However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior. Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios.",
        "output": "introduction:In this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags. In order to estimate the parameters of our model , we develop a blocked sampler based on that of #TARGET_REF<FUT/> to sample parse trees for sentences in the raw training corpus according to their posterior probabilities . However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior. Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios."
    },
    {
        "gold": {
            "text": [
                "The efforts required for performing morphologi- cal analysis vary from language to language.",
                "For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (#REF; #REF) demonstrably improve retrieval performance.",
                "This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; #REF ; #TARGET_REF ; Ekmekc Â¸ ioglu et al. , 1995 ; #REF ; #REF ) .",
                "When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.",
                "This is particularly true for the medical domain.",
                "From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (#REF; #REF; #REF; #REF; #REF; #REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The efforts required for performing morphologi- cal analysis vary from language to language. For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (#REF; #REF) demonstrably improve retrieval performance. This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; #REF ; #TARGET_REF ; Ekmekc Â¸ ioglu et al. , 1995 ; #REF ; #REF ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain. From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (#REF; #REF; #REF; #REF; #REF; #REF).",
        "output": "introduction:The efforts required for performing morphologi- cal analysis vary from language to language. For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (#REF; #REF) demonstrably improve retrieval performance. This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; #REF ; #TARGET_REF<BACK/> ; Ekmekc Â¸ ioglu et al. , 1995 ; #REF ; #REF ) . When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain. From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (#REF; #REF; #REF; #REF; #REF; #REF)."
    },
    {
        "gold": {
            "text": [
                "The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.",
                "This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.",
                "#REF reported a correlation of r=.9026. 10",
                "The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.",
                "#REF did not report inter-subject correlation for their larger dataset.",
                "#TARGET_REF reported a correlation of r = .69 .",
                "Test subjects were trained students of computational linguistics, and word pairs were selected analytically."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05. This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset. #REF reported a correlation of r=.9026. 10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. #REF did not report inter-subject correlation for their larger dataset. #TARGET_REF reported a correlation of r = .69 . Test subjects were trained students of computational linguistics, and word pairs were selected analytically.",
        "output": "experiments:The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05. This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset. #REF reported a correlation of r=.9026. 10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. #REF did not report inter-subject correlation for their larger dataset. #TARGET_REF<USE/> reported a correlation of r = .69 . Test subjects were trained students of computational linguistics, and word pairs were selected analytically."
    },
    {
        "gold": {
            "text": [
                "The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.",
                "Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #TARGET_REF ) .",
                "When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.",
                "The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.",
                "Two kinds of sentenees can be considered; domain-related ones that express the user's intention about the reser-vafion and dialogue-related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial.",
                "Considering the meeting room reservation system, examples of domain-related sentences are \"I need to book Room 2 on Wednesday\", \"I need to book Room 2\", and \"Room 2\" and dialogue-related ones are \"yes\", \"no\", and \"Okay\"."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules. Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #TARGET_REF ) . When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase. The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences. Two kinds of sentenees can be considered; domain-related ones that express the user's intention about the reser-vafion and dialogue-related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial. Considering the meeting room reservation system, examples of domain-related sentences are \"I need to book Room 2 on Wednesday\", \"I need to book Room 2\", and \"Room 2\" and dialogue-related ones are \"yes\", \"no\", and \"Okay\".",
        "output": "nan:The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules. Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #TARGET_REF<FUT/> ) . When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase. The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences. Two kinds of sentenees can be considered; domain-related ones that express the user's intention about the reser-vafion and dialogue-related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial. Considering the meeting room reservation system, examples of domain-related sentences are \"I need to book Room 2 on Wednesday\", \"I need to book Room 2\", and \"Room 2\" and dialogue-related ones are \"yes\", \"no\", and \"Okay\"."
    },
    {
        "gold": {
            "text": [
                "Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e.",
                "Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.",
                "For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #TARGET_REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #TARGET_REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) .",
        "output": "nan:Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #TARGET_REF<BACK/> ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) ."
    },
    {
        "gold": {
            "text": [
                "Table 1 shows our results for each of our selected models with our compositionality evaluation.",
                "The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).",
                "This result is consistent with other works using this model with these features ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Table 1 shows our results for each of our selected models with our compositionality evaluation. The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works using this model with these features ( #REF ; #TARGET_REF ) .",
        "output": "experiments:Table 1 shows our results for each of our selected models with our compositionality evaluation. The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works using this model with these features ( #REF ; #TARGET_REF<USE/> ) ."
    },
    {
        "gold": {
            "text": [
                "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase #TARGET_REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase #TARGET_REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .",
        "output": "nan:Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase #TARGET_REF<FUT/> , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
    },
    {
        "gold": {
            "text": [
                "Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.",
                "dictionary-based ( #REF ) , ontology-based ( #REF ; #REF ) , information-based ( #REF ; #REF ) or distributional ( #TARGET_REF ) .",
                "The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based ( #REF ) , ontology-based ( #REF ; #REF ) , information-based ( #REF ; #REF ) or distributional ( #TARGET_REF ) . The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.",
        "output": "nan:Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based ( #REF ) , ontology-based ( #REF ; #REF ) , information-based ( #REF ; #REF ) or distributional ( #TARGET_REF<BACK/> ) . The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora."
    },
    {
        "gold": {
            "text": [
                "Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.",
                "State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.",
                "The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #REF ) with the Alembic system ( #TARGET_REF ) : a 0.5 % error rate .",
                "The best performance on the Brown corpus, a 0.2% error rate, was reported by #REF, who trained a decision tree classifier on a 25-million-word corpus.",
                "In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in #REF.",
                "We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #REF ) with the Alembic system ( #TARGET_REF ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by #REF, who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in #REF. We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.",
        "output": "nan:Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #REF ) with the Alembic system ( #TARGET_REF<USE/> ) : a 0.5 % error rate . The best performance on the Brown corpus, a 0.2% error rate, was reported by #REF, who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in #REF. We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus."
    },
    {
        "gold": {
            "text": [
                "We_currently have two application domains that can carry on a spoken dialog with a user.",
                "One , the VOYAGER domain ( #TARGET_REF ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .",
                "The second one, ATIS (#REF), is a system for accessing data in the Official Airline Guide and booking flights.",
                "Work continues on improving all aspects of these domains."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "conclusion:We_currently have two application domains that can carry on a spoken dialog with a user. One , the VOYAGER domain ( #TARGET_REF ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University . The second one, ATIS (#REF), is a system for accessing data in the Official Airline Guide and booking flights. Work continues on improving all aspects of these domains.",
        "output": "conclusion:We_currently have two application domains that can carry on a spoken dialog with a user. One , the VOYAGER domain ( #TARGET_REF<FUT/> ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University . The second one, ATIS (#REF), is a system for accessing data in the Official Airline Guide and booking flights. Work continues on improving all aspects of these domains."
    },
    {
        "gold": {
            "text": [
                "The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.",
                "He lists, classifies, and discusses various types of inference, by which he means, generally, \"the linguistic-logical notions of consequent and presupposition\" Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences).",
                "These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs.\"",
                "According to #TARGET_REF , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases .",
                "Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types.",
                "The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation).",
                "For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs. He lists, classifies, and discusses various types of inference, by which he means, generally, \"the linguistic-logical notions of consequent and presupposition\" Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences). These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs.\" According to #TARGET_REF , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types. The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation). For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined).",
        "output": "introduction:The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs. He lists, classifies, and discusses various types of inference, by which he means, generally, \"the linguistic-logical notions of consequent and presupposition\" Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences). These chunks are sometimes called \"episodes,\" and sometimes \"paragraphs.\" According to #TARGET_REF<BACK/> , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases . Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types. The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation). For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined)."
    },
    {
        "gold": {
            "text": [
                "The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, I5, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear.",
                "On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic.",
                "#REF and #REF provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description.",
                "#TARGET_REF comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, I5, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear. On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic. #REF and #REF provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description. #TARGET_REF comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .",
        "output": "nan:The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, I5, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear. On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic. #REF and #REF provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description. #TARGET_REF<USE/> comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated ."
    },
    {
        "gold": {
            "text": [
                "To construct this test set, we have focused our attention on ten domain-specific terms: commande (command), configuration, fichier (file), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), syst�me (system), utilisateur (user ).",
                "The terms have been identified as the most specific to our corpus by a program developed by #REF and called TermoStat.",
                "The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #TARGET_REF ) .",
                "Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.",
                "(They were removed from the example set.)"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:To construct this test set, we have focused our attention on ten domain-specific terms: commande (command), configuration, fichier (file), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), syst�me (system), utilisateur (user ). The terms have been identified as the most specific to our corpus by a program developed by #REF and called TermoStat. The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #TARGET_REF ) . Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step. (They were removed from the example set.)",
        "output": "nan:To construct this test set, we have focused our attention on ten domain-specific terms: commande (command), configuration, fichier (file), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), syst�me (system), utilisateur (user ). The terms have been identified as the most specific to our corpus by a program developed by #REF and called TermoStat. The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #TARGET_REF<FUT/> ) . Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step. (They were removed from the example set.)"
    },
    {
        "gold": {
            "text": [
                "The most used feature for the Web People Search task, however, are NEs.",
                "#REF introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.",
                "In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents -see for instance (#REF; #REF; #REF; #REF)- .",
                "For instance, #REF uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.",
                "#TARGET_REF compared the performace of NEs versus BoW features .",
                "In his experiments a only a representation based on Organisation NEs outperformed the word based approach.",
                "Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:The most used feature for the Web People Search task, however, are NEs. #REF introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents -see for instance (#REF; #REF; #REF; #REF)- . For instance, #REF uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. #TARGET_REF compared the performace of NEs versus BoW features . In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).",
        "output": "related work:The most used feature for the Web People Search task, however, are NEs. #REF introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names. In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents -see for instance (#REF; #REF; #REF; #REF)- . For instance, #REF uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process. #TARGET_REF<BACK/> compared the performace of NEs versus BoW features . In his experiments a only a representation based on Organisation NEs outperformed the word based approach. Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW)."
    },
    {
        "gold": {
            "text": [
                "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF.",
                "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF, assertional statements as in #REF, or semantic nets as in #REF.",
                "That is, the current system learns procedures rather than data structures.",
                "There is some literature on procedure acquisition such as the LISP synthesis work described in #REF and the PROLOG synthesis method of #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF. The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF, assertional statements as in #REF, or semantic nets as in #REF. That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in #REF and the PROLOG synthesis method of #TARGET_REF .",
        "output": "nan:The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF. The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF, assertional statements as in #REF, or semantic nets as in #REF. That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in #REF and the PROLOG synthesis method of #TARGET_REF<USE/> ."
    },
    {
        "gold": {
            "text": [
                "We prepare the training data by splitting compounds in two steps , following the technique of #TARGET_REF .",
                "First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies.",
                "Training data is then stemmed as described in Section 2.3.",
                "The formerly modifying words of the compound (in our example the words to the left of the rightmost word) do not have a stem markup assigned, except for two cases: i) they are nouns themselves or ii) they are particles separated from a verb.",
                "In these cases, former modifiers are represented identically to their individual occurring counterparts, which helps generalization."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We prepare the training data by splitting compounds in two steps , following the technique of #TARGET_REF . First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies. Training data is then stemmed as described in Section 2.3. The formerly modifying words of the compound (in our example the words to the left of the rightmost word) do not have a stem markup assigned, except for two cases: i) they are nouns themselves or ii) they are particles separated from a verb. In these cases, former modifiers are represented identically to their individual occurring counterparts, which helps generalization.",
        "output": "experiments:We prepare the training data by splitting compounds in two steps , following the technique of #TARGET_REF<FUT/> . First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies. Training data is then stemmed as described in Section 2.3. The formerly modifying words of the compound (in our example the words to the left of the rightmost word) do not have a stem markup assigned, except for two cases: i) they are nouns themselves or ii) they are particles separated from a verb. In these cases, former modifiers are represented identically to their individual occurring counterparts, which helps generalization."
    },
    {
        "gold": {
            "text": [
                "As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #REF ; #TARGET_REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #REF ) and the CATiB ( #REF ) .",
                "Recently, #REF analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.",
                "#REF reports experiments on Arabic parsing using his MaltParser (#REF), trained on the PADT.",
                "His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #REF ; #TARGET_REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #REF ) and the CATiB ( #REF ) . Recently, #REF analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. #REF reports experiments on Arabic parsing using his MaltParser (#REF), trained on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.",
        "output": "related work:As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #REF ; #TARGET_REF<BACK/> ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #REF ) and the CATiB ( #REF ) . Recently, #REF analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. #REF reports experiments on Arabic parsing using his MaltParser (#REF), trained on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser."
    },
    {
        "gold": {
            "text": [
                "In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.",
                "Consequently a POS tagger can naturally treat them similarly to any other ambiguous words.",
                "There is, however, one difference in the implementation of such a tagger.",
                "Normally, a POS tagger operates on text spans that form a sentence.",
                "This requires resolving sentence boundaries before tagging.",
                "We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #REF ] , Brill 's [ #REFa ] , and MaxEnt [ #TARGET_REF ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .",
                "The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.",
                "This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations. Consequently a POS tagger can naturally treat them similarly to any other ambiguous words. There is, however, one difference in the implementation of such a tagger. Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #REF ] , Brill 's [ #REFa ] , and MaxEnt [ #TARGET_REF ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.",
        "output": "nan:In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations. Consequently a POS tagger can naturally treat them similarly to any other ambiguous words. There is, however, one difference in the implementation of such a tagger. Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #REF ] , Brill 's [ #REFa ] , and MaxEnt [ #TARGET_REF<USE/> ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions."
    },
    {
        "gold": {
            "text": [
                "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #TARGET_REF , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #TARGET_REF , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .",
        "output": "nan:Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #TARGET_REF<FUT/> , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- #REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
    },
    {
        "gold": {
            "text": [
                "Such questions are typically answered by designing appropriate priming experiments ( #TARGET_REF ) or other lexical decision tasks .",
                "The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.",
                "(See Sec. 2 for models of morphological organization and access and related experiments)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Such questions are typically answered by designing appropriate priming experiments ( #TARGET_REF ) or other lexical decision tasks . The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain. (See Sec. 2 for models of morphological organization and access and related experiments).",
        "output": "introduction:Such questions are typically answered by designing appropriate priming experiments ( #TARGET_REF<BACK/> ) or other lexical decision tasks . The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain. (See Sec. 2 for models of morphological organization and access and related experiments)."
    },
    {
        "gold": {
            "text": [
                "Other studies which view lR as a query generation process include #REF ; #REF ; #TARGET_REF ; Miller et al , 1999 .",
                "Our work has focused on cross-lingual retrieval."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Other studies which view lR as a query generation process include #REF ; #REF ; #TARGET_REF ; Miller et al , 1999 . Our work has focused on cross-lingual retrieval.",
        "output": "related work:Other studies which view lR as a query generation process include #REF ; #REF ; #TARGET_REF<USE/> ; Miller et al , 1999 . Our work has focused on cross-lingual retrieval."
    },
    {
        "gold": {
            "text": [
                "Vague or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees.",
                "This article focuses on gradable adjectives, also called degree adjectives. 1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program.",
                "Following #TARGET_REF , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .",
                "It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective.",
                "Vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning; as a result, they allow us to make inroads into the difficult area of research on vagueness.",
                "Generation offers an interesting perspective because it forces one to ask when it is a good idea to use these descriptions, in addition to asking what they mean."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Vague or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees. This article focuses on gradable adjectives, also called degree adjectives. 1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Following #TARGET_REF , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole . It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective. Vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning; as a result, they allow us to make inroads into the difficult area of research on vagueness. Generation offers an interesting perspective because it forces one to ask when it is a good idea to use these descriptions, in addition to asking what they mean.",
        "output": "nan:Vague or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees. This article focuses on gradable adjectives, also called degree adjectives. 1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program. Following #TARGET_REF<FUT/> , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole . It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective. Vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning; as a result, they allow us to make inroads into the difficult area of research on vagueness. Generation offers an interesting perspective because it forces one to ask when it is a good idea to use these descriptions, in addition to asking what they mean."
    },
    {
        "gold": {
            "text": [
                "The most specific generalization does not necessarily provide additional constrain- ing information.",
                "However, usually it is the case that lexical entries resulting from lexical rule application differ in very few specifications compared to the number of specifica- tions in a base lexical entry.",
                "Most of the specifications of a lexical entry are assumed to be passed unchanged via the automatically generated frame specification.",
                "Therefore, after lifting the common information into the extended lexical entry, the out-argument in many cases contains enough information to permit a postponed execution of the interaction predicate.",
                "When C is the common information, and D1, ..., Dk are the definitions of the interaction predicate called, we use distributivity to factor out C in (C A D1) V -.. V (C A Dk): We compute C A (D1 V ... V Dk), where the r) are assumed to contain no further common factors.",
                "Once we have computed c, we use it to make the extended lexical entry more specific.",
                "This technique closely resembles the off-line constraint propagation technique described by Marriott, Naish, and #REF.",
                "The reader is referred to #TARGET_REF for a more detailed discussion of our use of constraint propagation."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The most specific generalization does not necessarily provide additional constrain- ing information. However, usually it is the case that lexical entries resulting from lexical rule application differ in very few specifications compared to the number of specifica- tions in a base lexical entry. Most of the specifications of a lexical entry are assumed to be passed unchanged via the automatically generated frame specification. Therefore, after lifting the common information into the extended lexical entry, the out-argument in many cases contains enough information to permit a postponed execution of the interaction predicate. When C is the common information, and D1, ..., Dk are the definitions of the interaction predicate called, we use distributivity to factor out C in (C A D1) V -.. V (C A Dk): We compute C A (D1 V ... V Dk), where the r) are assumed to contain no further common factors. Once we have computed c, we use it to make the extended lexical entry more specific. This technique closely resembles the off-line constraint propagation technique described by Marriott, Naish, and #REF. The reader is referred to #TARGET_REF for a more detailed discussion of our use of constraint propagation.",
        "output": "introduction:The most specific generalization does not necessarily provide additional constrain- ing information. However, usually it is the case that lexical entries resulting from lexical rule application differ in very few specifications compared to the number of specifica- tions in a base lexical entry. Most of the specifications of a lexical entry are assumed to be passed unchanged via the automatically generated frame specification. Therefore, after lifting the common information into the extended lexical entry, the out-argument in many cases contains enough information to permit a postponed execution of the interaction predicate. When C is the common information, and D1, ..., Dk are the definitions of the interaction predicate called, we use distributivity to factor out C in (C A D1) V -.. V (C A Dk): We compute C A (D1 V ... V Dk), where the r) are assumed to contain no further common factors. Once we have computed c, we use it to make the extended lexical entry more specific. This technique closely resembles the off-line constraint propagation technique described by Marriott, Naish, and #REF. The reader is referred to #TARGET_REF<BACK/> for a more detailed discussion of our use of constraint propagation."
    },
    {
        "gold": {
            "text": [
                "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (#REF), \"domain closure assumption\" (ibid.),",
                "\"domain circumscription\" (cf.",
                "#REF), and their kin.",
                "Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #REF , the `` diagnosis from first principles '' of #TARGET_REF , `` explainability '' of #REF , and the subset principle of #REF .",
                "But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.",
                "These connections are being examined elsewhere (Zadrozny forthcoming)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (#REF), \"domain closure assumption\" (ibid.), \"domain circumscription\" (cf. #REF), and their kin. Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #REF , the `` diagnosis from first principles '' of #TARGET_REF , `` explainability '' of #REF , and the subset principle of #REF . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming).",
        "output": "introduction:Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (#REF), \"domain closure assumption\" (ibid.), \"domain circumscription\" (cf. #REF), and their kin. Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #REF , the `` diagnosis from first principles '' of #TARGET_REF<USE/> , `` explainability '' of #REF , and the subset principle of #REF . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming)."
    },
    {
        "gold": {
            "text": [
                "Learning algorithms.",
                "We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #TARGET_REF , motivated by its success in the related tasks of word sense disambiguation ( #REF ) and NE classification ( #REF ) .",
                "We apply add-one smoothing to smooth the class posteriors."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:Learning algorithms. We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #TARGET_REF , motivated by its success in the related tasks of word sense disambiguation ( #REF ) and NE classification ( #REF ) . We apply add-one smoothing to smooth the class posteriors.",
        "output": "introduction:Learning algorithms. We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #TARGET_REF<FUT/> , motivated by its success in the related tasks of word sense disambiguation ( #REF ) and NE classification ( #REF ) . We apply add-one smoothing to smooth the class posteriors."
    },
    {
        "gold": {
            "text": [
                "Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (#REF).",
                "In other methods, lexical resources are specifically tailored to meet the requirements of the domain (#REF) or the system (#REF).",
                "Such systems extract information from some types of syntactic units ( clauses in ( #REF ; #REF ; #TARGET_REF ) ; noun phrases in ( #REF ; #REF ) ) .",
                "Lists of semantic relations are designed to capture salient domain information."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (#REF). In other methods, lexical resources are specifically tailored to meet the requirements of the domain (#REF) or the system (#REF). Such systems extract information from some types of syntactic units ( clauses in ( #REF ; #REF ; #TARGET_REF ) ; noun phrases in ( #REF ; #REF ) ) . Lists of semantic relations are designed to capture salient domain information.",
        "output": "related work:Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (#REF). In other methods, lexical resources are specifically tailored to meet the requirements of the domain (#REF) or the system (#REF). Such systems extract information from some types of syntactic units ( clauses in ( #REF ; #REF ; #TARGET_REF<BACK/> ) ; noun phrases in ( #REF ; #REF ) ) . Lists of semantic relations are designed to capture salient domain information."
    },
    {
        "gold": {
            "text": [
                "Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.",
                "In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) .",
                "The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #REF ; #TARGET_REF ; #REF ; #REF ; #REF ) . The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.",
        "output": "related work:Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #REF ; #TARGET_REF<USE/> ; #REF ; #REF ; #REF ) . The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."
    },
    {
        "gold": {
            "text": [
                "With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #TARGET_REF ; Bentin , S. and #REF ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .",
                "Our cross-modal and masked priming experiment on Bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically/orthographically unrelated.",
                "These observations are similar to those reported for English and indicate that derivationally suffixed words in Bangla are in general accessed through decomposition of the word into its constituent morphemes.",
                "Further, based on the experimental data we have developed a series of computational models that can be used to predict the decomposition of Bangla polymorphemic words.",
                "Our evaluation result shows that decomposition of a polymorphemic word depends on several factors like, frequency, productivity of the suffix and the compositionality between the stem and the suffix."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #TARGET_REF ; Bentin , S. and #REF ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla . Our cross-modal and masked priming experiment on Bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically/orthographically unrelated. These observations are similar to those reported for English and indicate that derivationally suffixed words in Bangla are in general accessed through decomposition of the word into its constituent morphemes. Further, based on the experimental data we have developed a series of computational models that can be used to predict the decomposition of Bangla polymorphemic words. Our evaluation result shows that decomposition of a polymorphemic word depends on several factors like, frequency, productivity of the suffix and the compositionality between the stem and the suffix.",
        "output": "introduction:With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #TARGET_REF<FUT/> ; Bentin , S. and #REF ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla . Our cross-modal and masked priming experiment on Bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically/orthographically unrelated. These observations are similar to those reported for English and indicate that derivationally suffixed words in Bangla are in general accessed through decomposition of the word into its constituent morphemes. Further, based on the experimental data we have developed a series of computational models that can be used to predict the decomposition of Bangla polymorphemic words. Our evaluation result shows that decomposition of a polymorphemic word depends on several factors like, frequency, productivity of the suffix and the compositionality between the stem and the suffix."
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:#TARGET_REF introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :",
        "output": "introduction:#TARGET_REF<BACK/> introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :"
    },
    {
        "gold": {
            "text": [
                "11 #TARGET_REF reports that non-projective and pseudo-projective algorithms outperform the `` eager  projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .",
                "The Nivre �standard� algorithm is also reported there to do better on Arabic, but in a preliminary experimentation, it did slightly worse than the �eager� one, perhaps due to the high percentage of right branching (left headed structures) in our Arabic training set�an observation already noted in #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:11 #TARGET_REF reports that non-projective and pseudo-projective algorithms outperform the `` eager  projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies . The Nivre �standard� algorithm is also reported there to do better on Arabic, but in a preliminary experimentation, it did slightly worse than the �eager� one, perhaps due to the high percentage of right branching (left headed structures) in our Arabic training set�an observation already noted in #REF.",
        "output": "related work:11 #TARGET_REF<USE/> reports that non-projective and pseudo-projective algorithms outperform the `` eager  projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies . The Nivre �standard� algorithm is also reported there to do better on Arabic, but in a preliminary experimentation, it did slightly worse than the �eager� one, perhaps due to the high percentage of right branching (left headed structures) in our Arabic training set�an observation already noted in #REF."
    },
    {
        "gold": {
            "text": [
                "To solve these scaling issues , we implement Online Variational Bayesian Inference ( #TARGET_REF ; #REF ) for our models .",
                "In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.",
                "The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors.",
                "Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set.",
                "Online VBI easily scales and quickly converges in all of our experiments.",
                "A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:To solve these scaling issues , we implement Online Variational Bayesian Inference ( #TARGET_REF ; #REF ) for our models . In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables. The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors. Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set. Online VBI easily scales and quickly converges in all of our experiments. A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source.",
        "output": "method:To solve these scaling issues , we implement Online Variational Bayesian Inference ( #TARGET_REF<FUT/> ; #REF ) for our models . In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables. The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors. Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set. Online VBI easily scales and quickly converges in all of our experiments. A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source."
    },
    {
        "gold": {
            "text": [
                "6.1.1 Was the Use of a Gricean Maxim Necessary?",
                "Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only \"petty conversational implicature\" ( #TARGET_REF ) , or the metarules of Section 5.2?",
                "It seems to us that the answer is no."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:6.1.1 Was the Use of a Gricean Maxim Necessary? Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only \"petty conversational implicature\" ( #TARGET_REF ) , or the metarules of Section 5.2? It seems to us that the answer is no.",
        "output": "introduction:6.1.1 Was the Use of a Gricean Maxim Necessary? Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only \"petty conversational implicature\" ( #TARGET_REF<BACK/> ) , or the metarules of Section 5.2? It seems to us that the answer is no."
    },
    {
        "gold": {
            "text": [
                "1Our rules are similar to those from #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:1Our rules are similar to those from #TARGET_REF .",
        "output": "experiments:1Our rules are similar to those from #TARGET_REF<USE/> ."
    },
    {
        "gold": {
            "text": [
                "In this section, we describe how the memory of our EBMT system is seeded with a set of translations obtained from Web-based MT systems.",
                "From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems.",
                "First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon.",
                "This is then generalized , following a methodology based on #TARGET_REF , to generate the `` generalized marker lexicon . ''",
                "Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www.up.univ-mrs.fr/˜veronis/biblios/ptp.htm.",
                "methodology chosen, we automatically derive a fourth resource, namely, a \"word-level lexicon.\""
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:In this section, we describe how the memory of our EBMT system is seeded with a set of translations obtained from Web-based MT systems. From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems. First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon. This is then generalized , following a methodology based on #TARGET_REF , to generate the `` generalized marker lexicon . '' Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www.up.univ-mrs.fr/˜veronis/biblios/ptp.htm. methodology chosen, we automatically derive a fourth resource, namely, a \"word-level lexicon.\"",
        "output": "introduction:In this section, we describe how the memory of our EBMT system is seeded with a set of translations obtained from Web-based MT systems. From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems. First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon. This is then generalized , following a methodology based on #TARGET_REF<FUT/> , to generate the `` generalized marker lexicon . '' Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www.up.univ-mrs.fr/˜veronis/biblios/ptp.htm. methodology chosen, we automatically derive a fourth resource, namely, a \"word-level lexicon.\""
    },
    {
        "gold": {
            "text": [
                "The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure ( #TARGET_REF ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure ( #TARGET_REF ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs .",
        "output": "nan:The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure ( #TARGET_REF<BACK/> ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs ."
    },
    {
        "gold": {
            "text": [
                "Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.",
                "For MT the most commonly used heuristic is called grow diagonal final ( #TARGET_REF ) .",
                "This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.",
                "The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.",
                "In syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages.",
                "One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more difficult to analyze.",
                "Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance.",
                "In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.",
                "We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.",
                "This heuristic is called soft union (#REF).",
                "Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment. For MT the most commonly used heuristic is called grow diagonal final ( #TARGET_REF ) . This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points. The alignment produced has high recall relative to the intersection and only slightly lower recall than the union. In syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages. One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more difficult to analyze. Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance. In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model. We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union (#REF). Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.",
        "output": "nan:Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment. For MT the most commonly used heuristic is called grow diagonal final ( #TARGET_REF<USE/> ) . This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points. The alignment produced has high recall relative to the intersection and only slightly lower recall than the union. In syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages. One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more difficult to analyze. Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance. In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model. We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union (#REF). Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus."
    },
    {
        "gold": {
            "text": [
                "5 The open source Moses ( #TARGET_REF ) toolkit from www.statmt.org/moses/ ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:5 The open source Moses ( #TARGET_REF ) toolkit from www.statmt.org/moses/ .",
        "output": "nan:5 The open source Moses ( #TARGET_REF<FUT/> ) toolkit from www.statmt.org/moses/ ."
    },
    {
        "gold": {
            "text": [
                "As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #REF ; #REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #TARGET_REF ) and the CATiB ( #REF ) .",
                "Recently, #REF analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.",
                "#REF reports experiments on Arabic parsing using his MaltParser (#REF), trained on the PADT.",
                "His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #REF ; #REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #TARGET_REF ) and the CATiB ( #REF ) . Recently, #REF analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. #REF reports experiments on Arabic parsing using his MaltParser (#REF), trained on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.",
        "output": "related work:As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #REF ; #REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #TARGET_REF<BACK/> ) and the CATiB ( #REF ) . Recently, #REF analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. #REF reports experiments on Arabic parsing using his MaltParser (#REF), trained on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser."
    },
    {
        "gold": {
            "text": [
                "It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #TARGET_REF ) .",
                "We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions.",
                "Yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse (e.g., a fist fight between people of very different sizes).",
                "Luckily, there are no semantic or algorithmic reasons for giving gradables a low ranking.",
                "Let us see how things would work if they were ranked more highly."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #TARGET_REF ) . We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions. Yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse (e.g., a fist fight between people of very different sizes). Luckily, there are no semantic or algorithmic reasons for giving gradables a low ranking. Let us see how things would work if they were ranked more highly.",
        "output": "experiments:It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #TARGET_REF<USE/> ) . We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions. Yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse (e.g., a fist fight between people of very different sizes). Luckily, there are no semantic or algorithmic reasons for giving gradables a low ranking. Let us see how things would work if they were ranked more highly."
    },
    {
        "gold": {
            "text": [
                "The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.",
                "But unlike the cache model, it uses a multipass strategy.",
                "#REF developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence.",
                "In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.",
                "Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.",
                "For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.",
                "#TARGET_REF pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .",
                "They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.",
                "The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. #REF developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence. In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. #TARGET_REF pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names . They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.",
        "output": "nan:The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. #REF developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence. In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. #TARGET_REF<FUT/> pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names . They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists."
    },
    {
        "gold": {
            "text": [
                "The principle we apply when designing D(top,) and f is that we want the inductive bias to reflect structural locality.",
                "For this reason, D(top) includes nodes which are structurally local to top,.",
                "These nodes are the left-corner ancestor of top, (which is below top, on the stack), top 's left-corner child (its leftmost child, if any), and top 's most recent child (which was top,_ 1 , if any).",
                "For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #TARGET_REF ) , as has conditioning on the left-corner child ( #REF ) .",
                "Because these inputs include the history features of both the left- corner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i � 1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features.",
                "Thus this model is making no a priori hard independence assumptions, just a priori soft biases."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:The principle we apply when designing D(top,) and f is that we want the inductive bias to reflect structural locality. For this reason, D(top) includes nodes which are structurally local to top,. These nodes are the left-corner ancestor of top, (which is below top, on the stack), top 's left-corner child (its leftmost child, if any), and top 's most recent child (which was top,_ 1 , if any). For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #TARGET_REF ) , as has conditioning on the left-corner child ( #REF ) . Because these inputs include the history features of both the left- corner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i � 1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features. Thus this model is making no a priori hard independence assumptions, just a priori soft biases.",
        "output": "nan:The principle we apply when designing D(top,) and f is that we want the inductive bias to reflect structural locality. For this reason, D(top) includes nodes which are structurally local to top,. These nodes are the left-corner ancestor of top, (which is below top, on the stack), top 's left-corner child (its leftmost child, if any), and top 's most recent child (which was top,_ 1 , if any). For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #TARGET_REF<BACK/> ) , as has conditioning on the left-corner child ( #REF ) . Because these inputs include the history features of both the left- corner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i � 1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features. Thus this model is making no a priori hard independence assumptions, just a priori soft biases."
    },
    {
        "gold": {
            "text": [
                "Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #TARGET_REF , #REF and others .",
                "Toutanova et.",
                "al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information.",
                "Using additional source side information beyond the markup did not produce a gain in performance."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #TARGET_REF , #REF and others . Toutanova et. al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information. Using additional source side information beyond the markup did not produce a gain in performance.",
        "output": "related work:Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #TARGET_REF<USE/> , #REF and others . Toutanova et. al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information. Using additional source side information beyond the markup did not produce a gain in performance."
    },
    {
        "gold": {
            "text": [
                "Our experimental set-up is designed to evaluate the ability of the different responsegeneration methods to address unseen request e-mails.",
                "In particular, we want to determine the applicability of our methods to different situations, namely, whether different requests are addressed only by some methods, or whether there is a significant overlap between the methods.",
                "Our evaluation is performed by measuring the quality of the generated responses.",
                "Quality is a subjective measure, which is best judged by the users of the system (i.e., the help-desk customers or operators).",
                "In Section 5 , we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system ( #TARGET_REFb ) .",
                "However, our more comprehensive evaluation is an automatic one that treats the responses generated by the help-desk operators as model responses, and performs text-based comparisons between the model responses and the automatically generated ones."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:Our experimental set-up is designed to evaluate the ability of the different responsegeneration methods to address unseen request e-mails. In particular, we want to determine the applicability of our methods to different situations, namely, whether different requests are addressed only by some methods, or whether there is a significant overlap between the methods. Our evaluation is performed by measuring the quality of the generated responses. Quality is a subjective measure, which is best judged by the users of the system (i.e., the help-desk customers or operators). In Section 5 , we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system ( #TARGET_REFb ) . However, our more comprehensive evaluation is an automatic one that treats the responses generated by the help-desk operators as model responses, and performs text-based comparisons between the model responses and the automatically generated ones.",
        "output": "method:Our experimental set-up is designed to evaluate the ability of the different responsegeneration methods to address unseen request e-mails. In particular, we want to determine the applicability of our methods to different situations, namely, whether different requests are addressed only by some methods, or whether there is a significant overlap between the methods. Our evaluation is performed by measuring the quality of the generated responses. Quality is a subjective measure, which is best judged by the users of the system (i.e., the help-desk customers or operators). In Section 5 , we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system ( #TARGET_REF<FUT/>b ) . However, our more comprehensive evaluation is an automatic one that treats the responses generated by the help-desk operators as model responses, and performs text-based comparisons between the model responses and the automatically generated ones."
    },
    {
        "gold": {
            "text": [
                "Semantic Role labeling (SRL) was first defined in #REF.",
                "The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.",
                "The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.",
                "Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.",
                "Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #TARGET_REF ) , Information Extraction ( #REF ) , and Machine Translation ( #REF ) .",
                "With the efforts of many researchers (Carreras and Màrquez 2004, #REF, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Semantic Role labeling (SRL) was first defined in #REF. The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #TARGET_REF ) , Information Extraction ( #REF ) , and Machine Translation ( #REF ) . With the efforts of many researchers (Carreras and Màrquez 2004, #REF, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.",
        "output": "introduction:Semantic Role labeling (SRL) was first defined in #REF. The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #TARGET_REF<BACK/> ) , Information Extraction ( #REF ) , and Machine Translation ( #REF ) . With the efforts of many researchers (Carreras and Màrquez 2004, #REF, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast."
    },
    {
        "gold": {
            "text": [
                "Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.",
                "The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #TARGET_REF : 0.28 % vs. 0.20 % error rate ) .",
                "On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #REF (0.44% vs. 0.5% error rate).",
                "Although these error rates seem to be very small, they are quite significant.",
                "Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000).",
                "This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level.",
                "On the other hand, one error in 200 periods means that there is one error in every two documents in the Brown corpus and one error in every four documents in the WSJ corpus."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections. The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #TARGET_REF : 0.28 % vs. 0.20 % error rate ) . On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #REF (0.44% vs. 0.5% error rate). Although these error rates seem to be very small, they are quite significant. Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000). This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level. On the other hand, one error in 200 periods means that there is one error in every two documents in the Brown corpus and one error in every four documents in the WSJ corpus.",
        "output": "conclusion:Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections. The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #TARGET_REF<USE/> : 0.28 % vs. 0.20 % error rate ) . On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #REF (0.44% vs. 0.5% error rate). Although these error rates seem to be very small, they are quite significant. Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000). This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level. On the other hand, one error in 200 periods means that there is one error in every two documents in the Brown corpus and one error in every four documents in the WSJ corpus."
    },
    {
        "gold": {
            "text": [
                "Each evaluation set contained 80 cases, randomly selected from the corpus in proportion to the size of each data set, where a case contained a request e-mail, the model response, and the responses generated by the two methods being compared.",
                "Each judge was given of these cases, and was asked to assess the generated responses on the four criteria listed previously. 14",
                "e maximized the coverage of this study by allocating different cases to each judge, thus avoiding a situation where a particularly good or bad set of cases is evaluated by all judges.",
                "In addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges' assessments would be comparable.",
                "Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #TARGET_REF ) .",
                "However, it is still necessary to We asked the judges to leave a question unanswered if they felt they did not have the technical knowledge to make a judgment, but this did not occur."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:Each evaluation set contained 80 cases, randomly selected from the corpus in proportion to the size of each data set, where a case contained a request e-mail, the model response, and the responses generated by the two methods being compared. Each judge was given of these cases, and was asked to assess the generated responses on the four criteria listed previously. 14 e maximized the coverage of this study by allocating different cases to each judge, thus avoiding a situation where a particularly good or bad set of cases is evaluated by all judges. In addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges' assessments would be comparable. Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #TARGET_REF ) . However, it is still necessary to We asked the judges to leave a question unanswered if they felt they did not have the technical knowledge to make a judgment, but this did not occur.",
        "output": "method:Each evaluation set contained 80 cases, randomly selected from the corpus in proportion to the size of each data set, where a case contained a request e-mail, the model response, and the responses generated by the two methods being compared. Each judge was given of these cases, and was asked to assess the generated responses on the four criteria listed previously. 14 e maximized the coverage of this study by allocating different cases to each judge, thus avoiding a situation where a particularly good or bad set of cases is evaluated by all judges. In addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges' assessments would be comparable. Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #TARGET_REF<FUT/> ) . However, it is still necessary to We asked the judges to leave a question unanswered if they felt they did not have the technical knowledge to make a judgment, but this did not occur."
    },
    {
        "gold": {
            "text": [
                "Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #TARGET_REF ) .",
                "While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (#REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #TARGET_REF ) . While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (#REF).",
        "output": "introduction:Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #TARGET_REF<BACK/> ) . While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (#REF)."
    },
    {
        "gold": {
            "text": [
                "The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (#REF).",
                "#REF employed a Bayesian method to learn discontinuous SCFG rules.",
                "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.",
                "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.",
                "#REF and #REF focused on joint parsing and alignment.",
                "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.",
                "adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.",
                "#TARGET_REF re-trained the linguistic parsers bilingually based on word alignment .",
                "#REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.",
                "Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.",
                "#REF substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.",
                "#REF further labeled the SCFG rules with POS tags and unsupervised word classes."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (#REF). #REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #REF focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #TARGET_REF re-trained the linguistic parsers bilingually based on word alignment . #REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. #REF substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. #REF further labeled the SCFG rules with POS tags and unsupervised word classes.",
        "output": "related work:The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (#REF). #REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #REF focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #TARGET_REF<USE/> re-trained the linguistic parsers bilingually based on word alignment . #REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. #REF substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. #REF further labeled the SCFG rules with POS tags and unsupervised word classes."
    },
    {
        "gold": {
            "text": [
                "Motivation for Re-Ranking.",
                "For argument identification, the number of possible assignments for a parse tree with n nodes is 2 n .",
                "This number can run into the hundreds of billions for a normal-sized tree.",
                "For argument labeling, the number of possible assignments is ≈ 20 m , if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments.",
                "Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions.",
                "Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #TARGET_REF ) , which selects from likely assignments generated by a model which makes stronger independence assumptions .",
                "We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments.",
                "As can be seen from Figure 8(a), for relatively small values of n, our re-ranking approach does not present a serious bottleneck to performance.",
                "We used a value of n = 10 for training."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Motivation for Re-Ranking. For argument identification, the number of possible assignments for a parse tree with n nodes is 2 n . This number can run into the hundreds of billions for a normal-sized tree. For argument labeling, the number of possible assignments is ≈ 20 m , if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments. Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions. Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #TARGET_REF ) , which selects from likely assignments generated by a model which makes stronger independence assumptions . We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments. As can be seen from Figure 8(a), for relatively small values of n, our re-ranking approach does not present a serious bottleneck to performance. We used a value of n = 10 for training.",
        "output": "nan:Motivation for Re-Ranking. For argument identification, the number of possible assignments for a parse tree with n nodes is 2 n . This number can run into the hundreds of billions for a normal-sized tree. For argument labeling, the number of possible assignments is ≈ 20 m , if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments. Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions. Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #TARGET_REF<FUT/> ) , which selects from likely assignments generated by a model which makes stronger independence assumptions . We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments. As can be seen from Figure 8(a), for relatively small values of n, our re-ranking approach does not present a serious bottleneck to performance. We used a value of n = 10 for training."
    },
    {
        "gold": {
            "text": [
                "where V is a random variable that can take any possible outcome in set V, and p(v) = Pr(V = v) is the density function.",
                "Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #TARGET_REF ) .",
                "Determining the parse tree for a sentence from a set of possible parses can be viewed as assigning a value to a random variable.",
                "Thus, a direct application of the entropy definition to the probability distribution of the parses for sentence w in G computes its tree entropy, TE(w, G), the expected number of bits needed to encode the distribution of possible parses for w.",
                "However, we may not wish to compare two sentences with different numbers of parses by their entropy directly.",
                "If the parse probability distributions for both sentences are uniform, the sentence with more parses will have a higher entropy.",
                "Because longer sentences typically have more parses, using entropy directly would result in a bias toward selecting long sentences.",
                "To normalize for the number of parses, the uncertainty-based evaluation function, f unc , is defined as a measurement of similarity between the actual probability distribution of the parses and a hypothetical uniform distribution for that set of parses.",
                "In particular, we divide the tree entropy by the log of the number of parses: 10"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:where V is a random variable that can take any possible outcome in set V, and p(v) = Pr(V = v) is the density function. Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #TARGET_REF ) . Determining the parse tree for a sentence from a set of possible parses can be viewed as assigning a value to a random variable. Thus, a direct application of the entropy definition to the probability distribution of the parses for sentence w in G computes its tree entropy, TE(w, G), the expected number of bits needed to encode the distribution of possible parses for w. However, we may not wish to compare two sentences with different numbers of parses by their entropy directly. If the parse probability distributions for both sentences are uniform, the sentence with more parses will have a higher entropy. Because longer sentences typically have more parses, using entropy directly would result in a bias toward selecting long sentences. To normalize for the number of parses, the uncertainty-based evaluation function, f unc , is defined as a measurement of similarity between the actual probability distribution of the parses and a hypothetical uniform distribution for that set of parses. In particular, we divide the tree entropy by the log of the number of parses: 10",
        "output": "nan:where V is a random variable that can take any possible outcome in set V, and p(v) = Pr(V = v) is the density function. Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #TARGET_REF<BACK/> ) . Determining the parse tree for a sentence from a set of possible parses can be viewed as assigning a value to a random variable. Thus, a direct application of the entropy definition to the probability distribution of the parses for sentence w in G computes its tree entropy, TE(w, G), the expected number of bits needed to encode the distribution of possible parses for w. However, we may not wish to compare two sentences with different numbers of parses by their entropy directly. If the parse probability distributions for both sentences are uniform, the sentence with more parses will have a higher entropy. Because longer sentences typically have more parses, using entropy directly would result in a bias toward selecting long sentences. To normalize for the number of parses, the uncertainty-based evaluation function, f unc , is defined as a measurement of similarity between the actual probability distribution of the parses and a hypothetical uniform distribution for that set of parses. In particular, we divide the tree entropy by the log of the number of parses: 10"
    },
    {
        "gold": {
            "text": [
                "A common computational treatment of lexical rules adopted , for example , in the ALE system ( #TARGET_REF ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .",
                "While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.",
                "We mentioned in Section 2.2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available.",
                "A related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited.",
                "In the ALE system, for example, a depth bound can be specified for this purpose.",
                "Finally, as shown in Section 6, using an expanded out lexicon can be less time and space efficient than using a lexicon encoding that makes computational use of generalizations over lexical information, as, for example, the covariation encoding."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:A common computational treatment of lexical rules adopted , for example , in the ALE system ( #TARGET_REF ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time . While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation. We mentioned in Section 2.2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available. A related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited. In the ALE system, for example, a depth bound can be specified for this purpose. Finally, as shown in Section 6, using an expanded out lexicon can be less time and space efficient than using a lexicon encoding that makes computational use of generalizations over lexical information, as, for example, the covariation encoding.",
        "output": "related work:A common computational treatment of lexical rules adopted , for example , in the ALE system ( #TARGET_REF<USE/> ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time . While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation. We mentioned in Section 2.2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available. A related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited. In the ALE system, for example, a depth bound can be specified for this purpose. Finally, as shown in Section 6, using an expanded out lexicon can be less time and space efficient than using a lexicon encoding that makes computational use of generalizations over lexical information, as, for example, the covariation encoding."
    },
    {
        "gold": {
            "text": [
                "To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#TARGET_REF ) .",
                "#REF).",
                "FrameNet is an online lexical resource for English based on the principles of Frame Semantics.",
                "In this approach, a word evokes a frame i.e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.",
                "Finally each frame is associated with a set of target words, the words that evoke that frame."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#TARGET_REF ) . #REF). FrameNet is an online lexical resource for English based on the principles of Frame Semantics. In this approach, a word evokes a frame i.e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame. Finally each frame is associated with a set of target words, the words that evoke that frame.",
        "output": "nan:To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#TARGET_REF<FUT/> ) . #REF). FrameNet is an online lexical resource for English based on the principles of Frame Semantics. In this approach, a word evokes a frame i.e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame. Finally each frame is associated with a set of target words, the words that evoke that frame."
    },
    {
        "gold": {
            "text": [
                "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.",
                "This includes work on question answering (#REF), sentiment analysis (#REF), MT reordering (#REF), and many other tasks.",
                "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #REF ; #REF ; #TARGET_REF ; #REF ) .",
                "But these accuracies are measured with respect to gold-standard out-of-domain parse trees.",
                "There are few tasks that actually depend on the complete parse tree.",
                "Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.",
                "While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.",
                "The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (#REF), sentiment analysis (#REF), MT reordering (#REF), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #REF ; #REF ; #TARGET_REF ; #REF ) . But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training. The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.",
        "output": "introduction:The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (#REF), sentiment analysis (#REF), MT reordering (#REF), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #REF ; #REF ; #TARGET_REF<BACK/> ; #REF ) . But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training. The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure."
    },
    {
        "gold": {
            "text": [
                "Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #TARGET_REF ) .",
                "Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).",
                "Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus (#REF), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.",
                "This provides us with a training set that is approximately five times bigger than that of ACE.",
                "More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #REF); and for those that do report performance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #TARGET_REF ) . Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus (#REF), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs. This provides us with a training set that is approximately five times bigger than that of ACE. More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #REF); and for those that do report performance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.",
        "output": "introduction:Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #TARGET_REF<USE/> ) . Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus (#REF), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs. This provides us with a training set that is approximately five times bigger than that of ACE. More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #REF); and for those that do report performance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms."
    },
    {
        "gold": {
            "text": [
                "The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #REF ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #TARGET_REF ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and #REF ] ) , both outperforming it : ( d ) Kulick , Gabbard , and #REF 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #REF ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #TARGET_REF ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and #REF ] ) , both outperforming it : ( d ) Kulick , Gabbard , and #REF 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .",
        "output": "experiments:The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #REF ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #TARGET_REF<FUT/> ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and #REF ] ) , both outperforming it : ( d ) Kulick , Gabbard , and #REF 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 ."
    },
    {
        "gold": {
            "text": [
                "Most DOP models , such as in #REF , #REF , #REF , Sima'an ( 2000 ) and #TARGET_REF , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .",
                "We will refer to these models as Likelihood- DOP models, but in this paper we will specifically mean by \"Likelihood-DOP\" the PCFG-reduction of #REF given in Section 2.2."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Most DOP models , such as in #REF , #REF , #REF , Sima'an ( 2000 ) and #TARGET_REF , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence . We will refer to these models as Likelihood- DOP models, but in this paper we will specifically mean by \"Likelihood-DOP\" the PCFG-reduction of #REF given in Section 2.2.",
        "output": "nan:Most DOP models , such as in #REF , #REF , #REF , Sima'an ( 2000 ) and #TARGET_REF<BACK/> , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence . We will refer to these models as Likelihood- DOP models, but in this paper we will specifically mean by \"Likelihood-DOP\" the PCFG-reduction of #REF given in Section 2.2."
    },
    {
        "gold": {
            "text": [
                "The distribution of averaged human judgments on the whole test set (see Figure 3) is almost balanced with a slight underrepresentation of highly related concepts.",
                "To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used.",
                "However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.",
                "#TARGET_REF pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .",
                "This empty band is not observed here.",
                "However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8).",
                "The plot clearly shows an empty horizontal band with no judgments.",
                "The connection between averaged judgments and standard deviation is plotted in Figure 5."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:The distribution of averaged human judgments on the whole test set (see Figure 3) is almost balanced with a slight underrepresentation of highly related concepts. To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used. However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. #TARGET_REF pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs . This empty band is not observed here. However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8). The plot clearly shows an empty horizontal band with no judgments. The connection between averaged judgments and standard deviation is plotted in Figure 5.",
        "output": "experiments:The distribution of averaged human judgments on the whole test set (see Figure 3) is almost balanced with a slight underrepresentation of highly related concepts. To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used. However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. #TARGET_REF<USE/> pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs . This empty band is not observed here. However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8). The plot clearly shows an empty horizontal band with no judgments. The connection between averaged judgments and standard deviation is plotted in Figure 5."
    },
    {
        "gold": {
            "text": [
                "• Graph-based: An implementation of graphbased parsing algorithms with an arc-factored parameterization (#REF).",
                "We use the non-projective k-best MST algorithm to generate k-best lists ( #TARGET_REF ) , where k = 8 for the experiments in this paper .",
                "The graphbased parser features used in the experiments in this paper are defined over a word, w i at position i; the head of this word w ρ(i) where ρ(i) provides the index of the head word; and partof-speech tags of these words t i .",
                "We use the following set of features similar to #REF:"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:• Graph-based: An implementation of graphbased parsing algorithms with an arc-factored parameterization (#REF). We use the non-projective k-best MST algorithm to generate k-best lists ( #TARGET_REF ) , where k = 8 for the experiments in this paper . The graphbased parser features used in the experiments in this paper are defined over a word, w i at position i; the head of this word w ρ(i) where ρ(i) provides the index of the head word; and partof-speech tags of these words t i . We use the following set of features similar to #REF:",
        "output": "experiments:• Graph-based: An implementation of graphbased parsing algorithms with an arc-factored parameterization (#REF). We use the non-projective k-best MST algorithm to generate k-best lists ( #TARGET_REF<FUT/> ) , where k = 8 for the experiments in this paper . The graphbased parser features used in the experiments in this paper are defined over a word, w i at position i; the head of this word w ρ(i) where ρ(i) provides the index of the head word; and partof-speech tags of these words t i . We use the following set of features similar to #REF:"
    },
    {
        "gold": {
            "text": [
                "The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS (#REF), and JOYCE (#REF).",
                "The framework was originally developed for the realization of deep-syntactic structures in NLG ( #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS (#REF), and JOYCE (#REF). The framework was originally developed for the realization of deep-syntactic structures in NLG ( #TARGET_REF ) .",
        "output": "nan:The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS (#REF), and JOYCE (#REF). The framework was originally developed for the realization of deep-syntactic structures in NLG ( #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/",
                "Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp.",
                "as Bod01 and #REF.",
                "Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.",
                "#TARGET_REF , #REF , #REF and #REF ) .",
                "(1996).",
                "As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.",
                "This corresponds to a speedup of over 60 times.",
                "It should be mentioned that the best precision and recall scores reported in #REF are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).",
                "This may be explained by the fact our best results in #REF were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/ Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp. as Bod01 and #REF. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. #TARGET_REF , #REF , #REF and #REF ) . (1996). As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times. It should be mentioned that the best precision and recall scores reported in #REF are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words). This may be explained by the fact our best results in #REF were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.",
        "output": "experiments:4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/ Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp. as Bod01 and #REF. Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp. #TARGET_REF<USE/> , #REF , #REF and #REF ) . (1996). As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence. This corresponds to a speedup of over 60 times. It should be mentioned that the best precision and recall scores reported in #REF are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words). This may be explained by the fact our best results in #REF were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction."
    },
    {
        "gold": {
            "text": [
                "In general, dialogue act, agreement and turn annotations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.",
                "However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement.",
                "A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL.",
                "Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #TARGET_REF ) 1 and corrected kappa ( #REF ) 2 .",
                "Anvil divides the annotations in slices and compares each slice.",
                "We used slices of 0.04 seconds.",
                "The inter-coder agreement figures obtained for the three types of annotation are given in Table 2."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:In general, dialogue act, agreement and turn annotations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator. However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #TARGET_REF ) 1 and corrected kappa ( #REF ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds. The inter-coder agreement figures obtained for the three types of annotation are given in Table 2.",
        "output": "introduction:In general, dialogue act, agreement and turn annotations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator. However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #TARGET_REF<FUT/> ) 1 and corrected kappa ( #REF ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds. The inter-coder agreement figures obtained for the three types of annotation are given in Table 2."
    },
    {
        "gold": {
            "text": [
                "The psycholinguistic studies of #REF , #TARGET_REF , #REF , #REF , #REF , and #REF , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .",
                "For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.",
                "#REF claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.",
                "Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The psycholinguistic studies of #REF , #TARGET_REF , #REF , #REF , #REF , and #REF , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency . For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings. #REF claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it. Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.",
        "output": "introduction:The psycholinguistic studies of #REF , #TARGET_REF<BACK/> , #REF , #REF , #REF , and #REF , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency . For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings. #REF claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it. Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly."
    },
    {
        "gold": {
            "text": [
                "To copy otherwise, or to republish, requires a fee and/or specific permission.",
                "0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (#REFb).",
                "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.",
                "#REF;#REF) consult relatively small lexicons, typically generated by hand.",
                "Two exceptions to this generalisation are the Linguistic String Project ( #TARGET_REF ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( #REF ; #REF ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .",
                "In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see #REF for details).",
                "However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.",
                "In this paper we provide an evaluation of the LDOCE grammar code system from this perspective."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (#REFb). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #REF;#REF) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( #TARGET_REF ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( #REF ; #REF ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see #REF for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.",
        "output": "introduction:To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (#REFb). Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. #REF;#REF) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project ( #TARGET_REF<USE/> ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( #REF ; #REF ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources . In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see #REF for details). However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective."
    },
    {
        "gold": {
            "text": [
                "Figure 2 illustrates the sentence \"John picks the box up\" with its corresponding SSTC.",
                "It contains a nonprojective correspondence.",
                "An interval is assigned to each word in the sentence, i.e. (0-1) for \"John\", (1-2) for \"picks\", (2-3) for \"the\", (3-4) for \"box\" and (4-5) for \"up\".",
                "A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #REF ) and #TARGET_REF .",
                "and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Figure 2 illustrates the sentence \"John picks the box up\" with its corresponding SSTC. It contains a nonprojective correspondence. An interval is assigned to each word in the sentence, i.e. (0-1) for \"John\", (1-2) for \"picks\", (2-3) for \"the\", (3-4) for \"box\" and (4-5) for \"up\". A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #REF ) and #TARGET_REF . and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.",
        "output": "nan:Figure 2 illustrates the sentence \"John picks the box up\" with its corresponding SSTC. It contains a nonprojective correspondence. An interval is assigned to each word in the sentence, i.e. (0-1) for \"John\", (1-2) for \"picks\", (2-3) for \"the\", (3-4) for \"box\" and (4-5) for \"up\". A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #REF ) and #TARGET_REF<FUT/> . and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree."
    },
    {
        "gold": {
            "text": [
                "In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.",
                "Finding subcategorization frames involves filtering adjuncts from the observed frame.",
                "This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score.",
                "The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more).",
                "#TARGET_REF present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and #REF ) .",
                "In a similar way to that of #REF, Marinov and Hemming's system collects both arguments and adjuncts.",
                "It then uses the binomial log-likelihood ratio to filter incorrect frames.",
                "The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.",
                "The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame. Finding subcategorization frames involves filtering adjuncts from the observed frame. This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score. The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more). #TARGET_REF present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and #REF ) . In a similar way to that of #REF, Marinov and Hemming's system collects both arguments and adjuncts. It then uses the binomial log-likelihood ratio to filter incorrect frames. The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees. The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences.",
        "output": "related work:In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame. Finding subcategorization frames involves filtering adjuncts from the observed frame. This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score. The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more). #TARGET_REF<BACK/> present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and #REF ) . In a similar way to that of #REF, Marinov and Hemming's system collects both arguments and adjuncts. It then uses the binomial log-likelihood ratio to filter incorrect frames. The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees. The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences."
    },
    {
        "gold": {
            "text": [
                "Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #TARGET_REF ) and the coreference resolution system is similar to the one described in ( #REF ) .",
                "Both systems are built around from the maximum-entropy technique (#REF).",
                "We formulate the mention detection task as a sequence classi cation problem.",
                "While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.",
                "The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.",
                "We begin with a segmentation of the written text before starting the classification.",
                "This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens).",
                "The resulting granulariity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions).",
                "Additionally, because the pre xes and su_xes are quite frequent, directly processing unsegmented words results in signi cant data sparseness."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #TARGET_REF ) and the coreference resolution system is similar to the one described in ( #REF ) . Both systems are built around from the maximum-entropy technique (#REF). We formulate the mention detection task as a sequence classi cation problem. While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language. The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes. We begin with a segmentation of the written text before starting the classification. This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens). The resulting granulariity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions). Additionally, because the pre xes and su_xes are quite frequent, directly processing unsegmented words results in signi cant data sparseness.",
        "output": "introduction:Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #TARGET_REF<USE/> ) and the coreference resolution system is similar to the one described in ( #REF ) . Both systems are built around from the maximum-entropy technique (#REF). We formulate the mention detection task as a sequence classi cation problem. While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language. The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes. We begin with a segmentation of the written text before starting the classification. This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens). The resulting granulariity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions). Additionally, because the pre xes and su_xes are quite frequent, directly processing unsegmented words results in signi cant data sparseness."
    },
    {
        "gold": {
            "text": [
                "• abbreviation (as opposed to regular word) These four lists can be acquired completely automatically from raw (unlabeled) texts.",
                "For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #TARGET_REF ) .",
                "We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and #REF) developed for MUC-7.",
                "Although the corpus size of 300,000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on-line sources (including the Internet) makes this resource cheap to obtain."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:• abbreviation (as opposed to regular word) These four lists can be acquired completely automatically from raw (unlabeled) texts. For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #TARGET_REF ) . We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and #REF) developed for MUC-7. Although the corpus size of 300,000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on-line sources (including the Internet) makes this resource cheap to obtain.",
        "output": "nan:• abbreviation (as opposed to regular word) These four lists can be acquired completely automatically from raw (unlabeled) texts. For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #TARGET_REF<FUT/> ) . We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and #REF) developed for MUC-7. Although the corpus size of 300,000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on-line sources (including the Internet) makes this resource cheap to obtain."
    },
    {
        "gold": {
            "text": [
                "Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.",
                "For example, #REF experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.",
                "found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.",
                "This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #TARGET_REF ) .",
                "For a variety of reasons, medicine is an interesting domain of research.",
                "The need for information systems to support physicians at the point of care has been well studied (#REF;#REF;#REF).",
                "Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.",
                "Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, #REF experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #TARGET_REF ) . For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied (#REF;#REF;#REF). Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments.",
        "output": "introduction:Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them. For example, #REF experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score. found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts. This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #TARGET_REF<BACK/> ) . For a variety of reasons, medicine is an interesting domain of research. The need for information systems to support physicians at the point of care has been well studied (#REF;#REF;#REF). Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."
    },
    {
        "gold": {
            "text": [
                "Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.",
                "Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (#REF), minimum description length principal (#REF), and the χ 2 statistic.",
                "(#REF) has found strong correlations between DF, IG and the χ 2 statistic for a term.",
                "On the other hand, (#REF) reports the χ 2 to produce best performance.",
                "In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #TARGET_REF ) .",
                "TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (#REF).",
                "Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (#REF), minimum description length principal (#REF), and the χ 2 statistic. (#REF) has found strong correlations between DF, IG and the χ 2 statistic for a term. On the other hand, (#REF) reports the χ 2 to produce best performance. In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #TARGET_REF ) . TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (#REF). Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.",
        "output": "experiments:Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (#REF), minimum description length principal (#REF), and the χ 2 statistic. (#REF) has found strong correlations between DF, IG and the χ 2 statistic for a term. On the other hand, (#REF) reports the χ 2 to produce best performance. In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #TARGET_REF<USE/> ) . TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (#REF). Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents."
    },
    {
        "gold": {
            "text": [
                "Following the example of #TARGET_REF , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by #REF .",
                "Consequently, a hypertext is a set of lexias.",
                "In hypertexts transitions from one lexia to another are not necessarily sequential, but navigational.",
                "The main problems of hypertexts, acknowledged since the beginning, have been traced as follows (#REF):"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:Following the example of #TARGET_REF , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by #REF . Consequently, a hypertext is a set of lexias. In hypertexts transitions from one lexia to another are not necessarily sequential, but navigational. The main problems of hypertexts, acknowledged since the beginning, have been traced as follows (#REF):",
        "output": "introduction:Following the example of #TARGET_REF<FUT/> , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by #REF . Consequently, a hypertext is a set of lexias. In hypertexts transitions from one lexia to another are not necessarily sequential, but navigational. The main problems of hypertexts, acknowledged since the beginning, have been traced as follows (#REF):"
    },
    {
        "gold": {
            "text": [
                "Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.",
                "Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.",
                "Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.",
                "The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.",
                "This imbalance foils thresholding strategies , clever as they might be ( #TARGET_REF ; Wu & #REF ; #REF ) .",
                "The likelihoods in the word-to-word model remain unnormalized, so they do not compete."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models. Part of the difficulty stems from the implicit assumption in other models that each word has only one sense. Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations. The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation. This imbalance foils thresholding strategies , clever as they might be ( #TARGET_REF ; Wu & #REF ; #REF ) . The likelihoods in the word-to-word model remain unnormalized, so they do not compete.",
        "output": "nan:Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models. Part of the difficulty stems from the implicit assumption in other models that each word has only one sense. Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations. The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation. This imbalance foils thresholding strategies , clever as they might be ( #TARGET_REF<BACK/> ; Wu & #REF ; #REF ) . The likelihoods in the word-to-word model remain unnormalized, so they do not compete."
    },
    {
        "gold": {
            "text": [
                "Furthermore, our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.",
                "Although evaluated on different data sets , this result is consistent with results from previous work ( #REF ; #TARGET_REF ) .",
                "However, what is more interesting here is that while graph-based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40% performance degradation when applied to  the situation where the agent's representation of the shared world is problematic and full of mistakes."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Furthermore, our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception. Although evaluated on different data sets , this result is consistent with results from previous work ( #REF ; #TARGET_REF ) . However, what is more interesting here is that while graph-based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40% performance degradation when applied to  the situation where the agent's representation of the shared world is problematic and full of mistakes.",
        "output": "experiments:Furthermore, our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception. Although evaluated on different data sets , this result is consistent with results from previous work ( #REF ; #TARGET_REF<USE/> ) . However, what is more interesting here is that while graph-based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40% performance degradation when applied to  the situation where the agent's representation of the shared world is problematic and full of mistakes."
    },
    {
        "gold": {
            "text": [
                "To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (#REF).",
                "Then , we binarize the English parse trees using the head binarization approach ( #TARGET_REF ) and use the resulting binary parse trees to build another s2t system ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (#REF). Then , we binarize the English parse trees using the head binarization approach ( #TARGET_REF ) and use the resulting binary parse trees to build another s2t system .",
        "output": "experiments:To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (#REF). Then , we binarize the English parse trees using the head binarization approach ( #TARGET_REF<FUT/> ) and use the resulting binary parse trees to build another s2t system ."
    },
    {
        "gold": {
            "text": [
                "The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.",
                "#TARGET_REF reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .",
                "Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.",
                "For many applications, this is the desired behavior.",
                "The most detailed evaluation of link tokens to date was performed by (Macklovitch & #REF), who trained Brown et al.'s Model 2 on 74 million words of the Canadian Hansards.",
                "These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.",
                "We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.",
                "The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.",
                "Therefore, we ignored English words that were linked to nothing."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports. #TARGET_REF reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % . Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by (Macklovitch & #REF), who trained Brown et al.'s Model 2 on 74 million words of the Canadian Hansards. These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models. The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words. Therefore, we ignored English words that were linked to nothing.",
        "output": "nan:The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports. #TARGET_REF<BACK/> reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % . Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by (Macklovitch & #REF), who trained Brown et al.'s Model 2 on 74 million words of the Canadian Hansards. These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models. The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words. Therefore, we ignored English words that were linked to nothing."
    },
    {
        "gold": {
            "text": [
                "Linguistically, words have associated POS tags, e.g., �verb� or �noun,� which further abstract over morphologically and syntactically similar lexemes.",
                "Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles.",
                "In comparison, the tag set of the Buckwalter Morphological Analyzer ( #TARGET_REF ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8",
                "Cross-linguistically, a core set containing around 12 tags is often assumed as a �universal tag set� (#REF; Petrov, Das, and #REF).",
                "We have adapted the list from #REF for Arabic, and call it here CORE12.",
                "It contains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle (PRT), abbreviation (AB), and punctuation (PNX)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Linguistically, words have associated POS tags, e.g., �verb� or �noun,� which further abstract over morphologically and syntactically similar lexemes. Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles. In comparison, the tag set of the Buckwalter Morphological Analyzer ( #TARGET_REF ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8 Cross-linguistically, a core set containing around 12 tags is often assumed as a �universal tag set� (#REF; Petrov, Das, and #REF). We have adapted the list from #REF for Arabic, and call it here CORE12. It contains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle (PRT), abbreviation (AB), and punctuation (PNX).",
        "output": "experiments:Linguistically, words have associated POS tags, e.g., �verb� or �noun,� which further abstract over morphologically and syntactically similar lexemes. Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles. In comparison, the tag set of the Buckwalter Morphological Analyzer ( #TARGET_REF<USE/> ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8 Cross-linguistically, a core set containing around 12 tags is often assumed as a �universal tag set� (#REF; Petrov, Das, and #REF). We have adapted the list from #REF for Arabic, and call it here CORE12. It contains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle (PRT), abbreviation (AB), and punctuation (PNX)."
    },
    {
        "gold": {
            "text": [
                "In addition, we train an SVM classifier for SC determination by combining the output of five clas- sification methods: DL, 1-NN, ME, NB, and Soon et al.�s method as described in the introduction,8 with the goal of examining whether SC classifica- tion accuracy can be improved by combining the output of individual classifiers in a supervised man- ner.",
                "Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #TARGET_REF )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:In addition, we train an SVM classifier for SC determination by combining the output of five clas- sification methods: DL, 1-NN, ME, NB, and Soon et al.�s method as described in the introduction,8 with the goal of examining whether SC classifica- tion accuracy can be improved by combining the output of individual classifiers in a supervised man- ner. Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #TARGET_REF )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features.",
        "output": "introduction:In addition, we train an SVM classifier for SC determination by combining the output of five clas- sification methods: DL, 1-NN, ME, NB, and Soon et al.�s method as described in the introduction,8 with the goal of examining whether SC classifica- tion accuracy can be improved by combining the output of individual classifiers in a supervised man- ner. Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #TARGET_REF<FUT/> )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features."
    },
    {
        "gold": {
            "text": [
                "The rate of accession may also be represented graphically.",
                "In #TARGET_REF and #REF , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .",
                "We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity.",
                "Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined).",
                "Because of the variation in the size of sections between the Brown and the WSJ, we plotted accession against word count.",
                "The first part of the graph (up to 1,004,414 words)"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:The rate of accession may also be represented graphically. In #TARGET_REF and #REF , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank . We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity. Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined). Because of the variation in the size of sections between the Brown and the WSJ, we plotted accession against word count. The first part of the graph (up to 1,004,414 words)",
        "output": "nan:The rate of accession may also be represented graphically. In #TARGET_REF<BACK/> and #REF , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank . We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity. Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined). Because of the variation in the size of sections between the Brown and the WSJ, we plotted accession against word count. The first part of the graph (up to 1,004,414 words)"
    },
    {
        "gold": {
            "text": [
                "Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (#REF;#REF).",
                "Our task is closer to the work of #TARGET_REF , who looked at the problem of intellectual attribution in scientific texts ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (#REF;#REF). Our task is closer to the work of #TARGET_REF , who looked at the problem of intellectual attribution in scientific texts .",
        "output": "related work:Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (#REF;#REF). Our task is closer to the work of #TARGET_REF<USE/> , who looked at the problem of intellectual attribution in scientific texts ."
    },
    {
        "gold": {
            "text": [
                "We use the Columbia Arabic Treebank ( CATiB ) ( #TARGET_REF ) .",
                "Specifically, we use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information.",
                "CATiB's dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations.",
                "It has a reduced POS tag set consisting of six tags only (henceforth CATIB6).",
                "The tags are: NOM (non-proper nominals including nouns, pronouns, adjectives, and adverbs), PROP (proper nouns), VRB (active-voice verbs), VRB-PASS (passive-voice verbs), PRT (particles such as prepositions or conjunctions), and PNX (punctuation).",
                "CATiB uses a standard set of eight dependency relations: SBJ and OBJ for subject and (direct or indirect) object, respectively (whether they appear pre-or postverbally); IDF for the idafa (possessive) relation; MOD for most other modifications; and other less common relations that we will not discuss here."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We use the Columbia Arabic Treebank ( CATiB ) ( #TARGET_REF ) . Specifically, we use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information. CATiB's dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations. It has a reduced POS tag set consisting of six tags only (henceforth CATIB6). The tags are: NOM (non-proper nominals including nouns, pronouns, adjectives, and adverbs), PROP (proper nouns), VRB (active-voice verbs), VRB-PASS (passive-voice verbs), PRT (particles such as prepositions or conjunctions), and PNX (punctuation). CATiB uses a standard set of eight dependency relations: SBJ and OBJ for subject and (direct or indirect) object, respectively (whether they appear pre-or postverbally); IDF for the idafa (possessive) relation; MOD for most other modifications; and other less common relations that we will not discuss here.",
        "output": "experiments:We use the Columbia Arabic Treebank ( CATiB ) ( #TARGET_REF<FUT/> ) . Specifically, we use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information. CATiB's dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations. It has a reduced POS tag set consisting of six tags only (henceforth CATIB6). The tags are: NOM (non-proper nominals including nouns, pronouns, adjectives, and adverbs), PROP (proper nouns), VRB (active-voice verbs), VRB-PASS (passive-voice verbs), PRT (particles such as prepositions or conjunctions), and PNX (punctuation). CATiB uses a standard set of eight dependency relations: SBJ and OBJ for subject and (direct or indirect) object, respectively (whether they appear pre-or postverbally); IDF for the idafa (possessive) relation; MOD for most other modifications; and other less common relations that we will not discuss here."
    },
    {
        "gold": {
            "text": [
                "Juola's (1994Juola's ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to \"marker-normal form.\"",
                "However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.",
                "Nevertheless , #TARGET_REF , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''",
                "Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated.",
                "Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on #REF to permit a limited form of insertion in the translation process."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Juola's (1994Juola's ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to \"marker-normal form.\" However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless , #TARGET_REF , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . '' Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on #REF to permit a limited form of insertion in the translation process.",
        "output": "introduction:Juola's (1994Juola's ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to \"marker-normal form.\" However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless , #TARGET_REF<BACK/> , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . '' Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on #REF to permit a limited form of insertion in the translation process."
    },
    {
        "gold": {
            "text": [
                "That is, where #TARGET_REF substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.",
                "Given that examples such as ��<DET> a : un� are likely to exist in the word-level lexicon, they may be inserted at the point indicated by the marker tag to form the correct translation un bon homme.",
                "We thus cluster on marker words to improve the coverage of our system (see Section 5 for results that show exactly how clustering on marker words helps); others (notably Brown [2000, 2003]) use clustering techniques to determine equivalence classes of individual words that can occur in the same context, and in so doing derive translation templates from individual translation examples."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:That is, where #TARGET_REF substitutes variables for various words in his templates, we replace certain lexical items with their marker tag. Given that examples such as ��<DET> a : un� are likely to exist in the word-level lexicon, they may be inserted at the point indicated by the marker tag to form the correct translation un bon homme. We thus cluster on marker words to improve the coverage of our system (see Section 5 for results that show exactly how clustering on marker words helps); others (notably Brown [2000, 2003]) use clustering techniques to determine equivalence classes of individual words that can occur in the same context, and in so doing derive translation templates from individual translation examples.",
        "output": "introduction:That is, where #TARGET_REF<USE/> substitutes variables for various words in his templates, we replace certain lexical items with their marker tag. Given that examples such as ��<DET> a : un� are likely to exist in the word-level lexicon, they may be inserted at the point indicated by the marker tag to form the correct translation un bon homme. We thus cluster on marker words to improve the coverage of our system (see Section 5 for results that show exactly how clustering on marker words helps); others (notably Brown [2000, 2003]) use clustering techniques to determine equivalence classes of individual words that can occur in the same context, and in so doing derive translation templates from individual translation examples."
    },
    {
        "gold": {
            "text": [
                "Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #TARGET_REF ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .",
                "Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.",
                "We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (#REF).",
                "Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #TARGET_REF ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing . Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer. We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (#REF). Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.",
        "output": "nan:Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #TARGET_REF<FUT/> ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing . Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer. We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (#REF). Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model."
    },
    {
        "gold": {
            "text": [
                "For shuffling paraphrases , french alternations are partially described in ( #TARGET_REF ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .",
                "For complementing this database and for converse constructions, the LADL tables (#REF) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.",
                "In particular, (#REF) lists the converses of some 3 500 predicative nouns."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:For shuffling paraphrases , french alternations are partially described in ( #TARGET_REF ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs . For complementing this database and for converse constructions, the LADL tables (#REF) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions. In particular, (#REF) lists the converses of some 3 500 predicative nouns.",
        "output": "nan:For shuffling paraphrases , french alternations are partially described in ( #TARGET_REF<BACK/> ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs . For complementing this database and for converse constructions, the LADL tables (#REF) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions. In particular, (#REF) lists the converses of some 3 500 predicative nouns."
    },
    {
        "gold": {
            "text": [
                "The idea of using preferences among theories is new, hence it was described in more detail.",
                "`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #REF ; #TARGET_REF ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:The idea of using preferences among theories is new, hence it was described in more detail. `` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #REF ; #TARGET_REF ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .",
        "output": "introduction:The idea of using preferences among theories is new, hence it was described in more detail. `` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #REF ; #TARGET_REF<USE/> ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text ."
    },
    {
        "gold": {
            "text": [
                "To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #TARGET_REF ) .",
                "Then, we binarize the English parse trees using the head binarization approach (#REF) and use the resulting binary parse trees to build another s2t system."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #TARGET_REF ) . Then, we binarize the English parse trees using the head binarization approach (#REF) and use the resulting binary parse trees to build another s2t system.",
        "output": "experiments:To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #TARGET_REF<FUT/> ) . Then, we binarize the English parse trees using the head binarization approach (#REF) and use the resulting binary parse trees to build another s2t system."
    },
    {
        "gold": {
            "text": [
                "This situation leaves to the user the task of finding the pages relevant to the particular person he is interested in.",
                "The user might refine the original query with additional terms, but this risks excluding relevant documents in the process.",
                "In some cases, the existence of a predominant person (such as a celebrity or a historical figure) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name.",
                "The Web People Search task , as defined in the first WePS evaluation campaign ( #TARGET_REF ) , consists of grouping search results for a given name according to the different people that share it ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:This situation leaves to the user the task of finding the pages relevant to the particular person he is interested in. The user might refine the original query with additional terms, but this risks excluding relevant documents in the process. In some cases, the existence of a predominant person (such as a celebrity or a historical figure) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name. The Web People Search task , as defined in the first WePS evaluation campaign ( #TARGET_REF ) , consists of grouping search results for a given name according to the different people that share it .",
        "output": "introduction:This situation leaves to the user the task of finding the pages relevant to the particular person he is interested in. The user might refine the original query with additional terms, but this risks excluding relevant documents in the process. In some cases, the existence of a predominant person (such as a celebrity or a historical figure) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name. The Web People Search task , as defined in the first WePS evaluation campaign ( #TARGET_REF<BACK/> ) , consists of grouping search results for a given name according to the different people that share it ."
    },
    {
        "gold": {
            "text": [
                "Many ACE participants have also adopted a corpus-based approach to SC deter- mination that is investigated as part of the mention detection (MD) task (e.g., #REF).",
                "Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).",
                "Un- like them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowl- edge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC clas- sifier; instead, we use the BBN Entity Type Corpus (#REF), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and an- notated with their SCs.",
                "This provides us with a train- ing set that is approximately five times bigger than that of ACE.",
                "More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #TARGET_REF ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Many ACE participants have also adopted a corpus-based approach to SC deter- mination that is investigated as part of the mention detection (MD) task (e.g., #REF). Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). Un- like them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowl- edge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC clas- sifier; instead, we use the BBN Entity Type Corpus (#REF), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and an- notated with their SCs. This provides us with a train- ing set that is approximately five times bigger than that of ACE. More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #TARGET_REF ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.",
        "output": "introduction:Many ACE participants have also adopted a corpus-based approach to SC deter- mination that is investigated as part of the mention detection (MD) task (e.g., #REF). Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). Un- like them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowl- edge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC clas- sifier; instead, we use the BBN Entity Type Corpus (#REF), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and an- notated with their SCs. This provides us with a train- ing set that is approximately five times bigger than that of ACE. More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #TARGET_REF<USE/> ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms."
    },
    {
        "gold": {
            "text": [
                "With our typology of links, we aim to solve the framing problem as defined in Section 1.2.",
                "We want to model views as dynamic objects -the creation of context will be still arbitrary, but changes are very easily.",
                "We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:With our typology of links, we aim to solve the framing problem as defined in Section 1.2. We want to model views as dynamic objects -the creation of context will be still arbitrary, but changes are very easily. We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #TARGET_REF ) .",
        "output": "nan:With our typology of links, we aim to solve the framing problem as defined in Section 1.2. We want to model views as dynamic objects -the creation of context will be still arbitrary, but changes are very easily. We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "The Computer Vision community has also benefited greatly from efforts to unify the two modalities.",
                "To name a few examples , #REF and #REF show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #TARGET_REF show that verb clusters can be used to improve activity recognition in videos ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples , #REF and #REF show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #TARGET_REF show that verb clusters can be used to improve activity recognition in videos .",
        "output": "related work:The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples , #REF and #REF show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #TARGET_REF<BACK/> show that verb clusters can be used to improve activity recognition in videos ."
    },
    {
        "gold": {
            "text": [
                "For unsupervised tree structure induction, #REF adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.",
                "Our previous work (#REF) designed an EMbased method to construct unsupervised trees for tree-based translation models.",
                "This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.",
                "#TARGET_REF , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .",
                "The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (#REF).",
                "#REF employed a Bayesian method to learn discontinuous SCFG rules.",
                "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.",
                "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.",
                "#REF and #REF focused on joint parsing and alignment.",
                "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.",
                "adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:For unsupervised tree structure induction, #REF adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (#REF) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. #TARGET_REF , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (#REF). #REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #REF focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.",
        "output": "related work:For unsupervised tree structure induction, #REF adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (#REF) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. #TARGET_REF<USE/> , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus . The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (#REF). #REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #REF focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees."
    },
    {
        "gold": {
            "text": [
                "The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.",
                "To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #TARGET_REF ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) .",
                "We conducted experiments with gold features to assess the potential of these features, and with predicted fea- tures, obtained from training a simple maximum likelihood estimation classifier on this resource (#REF).",
                "19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input).",
                "The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values. To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #TARGET_REF ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) . We conducted experiments with gold features to assess the potential of these features, and with predicted fea- tures, obtained from training a simple maximum likelihood estimation classifier on this resource (#REF). 19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts.",
        "output": "related work:The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values. To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #TARGET_REF<FUT/> ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) . We conducted experiments with gold features to assess the potential of these features, and with predicted fea- tures, obtained from training a simple maximum likelihood estimation classifier on this resource (#REF). 19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts."
    },
    {
        "gold": {
            "text": [
                "Gradability is especially widespread in adjectives.",
                "A search of the British National Corpus (BNC), for example, shows at least seven of the ten most frequent adjectives (last, other, new, good, old, great, high, small, different, large) to be gradable.",
                "Children use vague adjectives among their first dozens of words ( #REF ) and understand some of their intricacies as early as their 24th month ( #TARGET_REF ) .",
                "These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are visible)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Gradability is especially widespread in adjectives. A search of the British National Corpus (BNC), for example, shows at least seven of the ten most frequent adjectives (last, other, new, good, old, great, high, small, different, large) to be gradable. Children use vague adjectives among their first dozens of words ( #REF ) and understand some of their intricacies as early as their 24th month ( #TARGET_REF ) . These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are visible).",
        "output": "nan:Gradability is especially widespread in adjectives. A search of the British National Corpus (BNC), for example, shows at least seven of the ten most frequent adjectives (last, other, new, good, old, great, high, small, different, large) to be gradable. Children use vague adjectives among their first dozens of words ( #REF ) and understand some of their intricacies as early as their 24th month ( #TARGET_REF<BACK/> ) . These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are visible)."
    },
    {
        "gold": {
            "text": [
                "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (#REF), \"domain closure assumption\" (ibid.),",
                "\"domain circumscription\" (cf.",
                "#REF), and their kin.",
                "Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #TARGET_REF , the `` diagnosis from first principles '' of #REF , `` explainability '' of #REF , and the subset principle of #REF .",
                "But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.",
                "These connections are being examined elsewhere (Zadrozny forthcoming)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (#REF), \"domain closure assumption\" (ibid.), \"domain circumscription\" (cf. #REF), and their kin. Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #TARGET_REF , the `` diagnosis from first principles '' of #REF , `` explainability '' of #REF , and the subset principle of #REF . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming).",
        "output": "introduction:Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (#REF), \"domain closure assumption\" (ibid.), \"domain circumscription\" (cf. #REF), and their kin. Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #TARGET_REF<USE/> , the `` diagnosis from first principles '' of #REF , `` explainability '' of #REF , and the subset principle of #REF . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming)."
    },
    {
        "gold": {
            "text": [
                "The metric we consider here is derived from an example-based machine translation.",
                "To retrieve translation examples for a test sentence , ( #TARGET_REF ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The metric we consider here is derived from an example-based machine translation. To retrieve translation examples for a test sentence , ( #TARGET_REF ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :",
        "output": "nan:The metric we consider here is derived from an example-based machine translation. To retrieve translation examples for a test sentence , ( #TARGET_REF<FUT/> ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :"
    },
    {
        "gold": {
            "text": [
                "Clinical question answering is an emerging area of research that has only recently begun to receive serious attention.",
                "As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated.",
                "In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems.",
                "For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (#REF).",
                "In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #TARGET_REFa ) for a brief overview ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Clinical question answering is an emerging area of research that has only recently begun to receive serious attention. As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated. In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (#REF). In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #TARGET_REFa ) for a brief overview .",
        "output": "related work:Clinical question answering is an emerging area of research that has only recently begun to receive serious attention. As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated. In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (#REF). In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #TARGET_REF<BACK/>a ) for a brief overview ."
    },
    {
        "gold": {
            "text": [
                "In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #TARGET_REF ) .",
                "For these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.",
                "Here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n-gram character contexts that appear in the training data.",
                "The character based model alone achieves a 94.5% exact match segmentation accuracy, considerably less accurate then the dictionary based model."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #TARGET_REF ) . For these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine. Here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n-gram character contexts that appear in the training data. The character based model alone achieves a 94.5% exact match segmentation accuracy, considerably less accurate then the dictionary based model.",
        "output": "nan:In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #TARGET_REF<USE/> ) . For these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine. Here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n-gram character contexts that appear in the training data. The character based model alone achieves a 94.5% exact match segmentation accuracy, considerably less accurate then the dictionary based model."
    },
    {
        "gold": {
            "text": [
                "There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus ( #TARGET_REF ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and #REF ) .",
                "The Brown corpus represents general English.",
                "It contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.",
                "The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure.",
                "Altogether there are about 500 documents in the Brown corpus, with an average length of 2,300 word tokens."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus ( #TARGET_REF ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and #REF ) . The Brown corpus represents general English. It contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech. The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure. Altogether there are about 500 documents in the Brown corpus, with an average length of 2,300 word tokens.",
        "output": "nan:There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus ( #TARGET_REF<FUT/> ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and #REF ) . The Brown corpus represents general English. It contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech. The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure. Altogether there are about 500 documents in the Brown corpus, with an average length of 2,300 word tokens."
    },
    {
        "gold": {
            "text": [
                "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.",
                "This includes work on question answering (#REF), sentiment analysis (#REF), MT reordering (#REF), and many other tasks.",
                "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #TARGET_REF ; #REF ; #REF ; #REF ) .",
                "But these accuracies are measured with respect to gold-standard out-of-domain parse trees.",
                "There are few tasks that actually depend on the complete parse tree.",
                "Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.",
                "While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.",
                "The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (#REF), sentiment analysis (#REF), MT reordering (#REF), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #TARGET_REF ; #REF ; #REF ; #REF ) . But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training. The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.",
        "output": "introduction:The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (#REF), sentiment analysis (#REF), MT reordering (#REF), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #TARGET_REF<BACK/> ; #REF ; #REF ; #REF ) . But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training. The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure."
    },
    {
        "gold": {
            "text": [
                "The problem of handling ill-formed input has been studied by #REF , #REF , #REF , #TARGET_REF , #REF , #REF , #REF , and #REF .",
                "A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.",
                "However, these methodologies have not used historical information at the dialogue level as described here.",
                "In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.",
                "The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.",
                "Thus, an error in this work has no pattern but occurs probabilistically.",
                "A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.",
                "The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors.",
                "However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.",
                "Another dialogue acquisition system has been developed by #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The problem of handling ill-formed input has been studied by #REF , #REF , #REF , #TARGET_REF , #REF , #REF , #REF , and #REF . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #REF.",
        "output": "nan:The problem of handling ill-formed input has been studied by #REF , #REF , #REF , #TARGET_REF<USE/> , #REF , #REF , #REF , and #REF . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #REF."
    },
    {
        "gold": {
            "text": [
                "We train the system by clustering the \"experiences\" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).",
                "We then use the program Snob ( #TARGET_REF ; #REF ) to cluster these experiences .",
                "Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.",
                "shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15",
                "These clusters were chosen because they illustrate clearly three situations of interest."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:We train the system by clustering the \"experiences\" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively). We then use the program Snob ( #TARGET_REF ; #REF ) to cluster these experiences . Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data. shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15 These clusters were chosen because they illustrate clearly three situations of interest.",
        "output": "nan:We train the system by clustering the \"experiences\" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively). We then use the program Snob ( #TARGET_REF<FUT/> ; #REF ) to cluster these experiences . Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data. shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15 These clusters were chosen because they illustrate clearly three situations of interest."
    },
    {
        "gold": {
            "text": [
                "NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #TARGET_REF ) : The selected expression should also be felicitous .",
                "Consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between \"observationally indifferent\" entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not?",
                "A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it.",
                "This is the strongest version of the sorites paradox (e.g., #REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #TARGET_REF ) : The selected expression should also be felicitous . Consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between \"observationally indifferent\" entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not? A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it. This is the strongest version of the sorites paradox (e.g., #REF).",
        "output": "nan:NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #TARGET_REF<BACK/> ) : The selected expression should also be felicitous . Consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between \"observationally indifferent\" entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not? A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it. This is the strongest version of the sorites paradox (e.g., #REF)."
    },
    {
        "gold": {
            "text": [
                "The proposed parser is related to the so-called Lemma Table deduction system (#REF) which allows the user to specify whether top-down sub-computations are to be tabled.",
                "In contrast to Johnson and DSrre's deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies.",
                "As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #TARGET_REF ) .",
                "Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub-computations are tabled/filtered.",
                "feature grammars on the basis of an example and introduce a dynamic bottom-up interpreter that can be used for goM-directed interpretation of magic-compiled typed feature grammars."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:The proposed parser is related to the so-called Lemma Table deduction system (#REF) which allows the user to specify whether top-down sub-computations are to be tabled. In contrast to Johnson and DSrre's deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies. As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #TARGET_REF ) . Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub-computations are tabled/filtered. feature grammars on the basis of an example and introduce a dynamic bottom-up interpreter that can be used for goM-directed interpretation of magic-compiled typed feature grammars.",
        "output": "introduction:The proposed parser is related to the so-called Lemma Table deduction system (#REF) which allows the user to specify whether top-down sub-computations are to be tabled. In contrast to Johnson and DSrre's deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies. As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #TARGET_REF<USE/> ) . Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub-computations are tabled/filtered. feature grammars on the basis of an example and introduce a dynamic bottom-up interpreter that can be used for goM-directed interpretation of magic-compiled typed feature grammars."
    },
    {
        "gold": {
            "text": [
                "SemCat (semantic category) of predicate, Sem-Cat of first word, SemCat of head word, SemCat of last word, SemCat of predicate + SemCat of first word, SemCat of predicate + SemCat of last word, predicate + SemCat of head word, SemCat of predicate + head word.",
                "The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:SemCat (semantic category) of predicate, Sem-Cat of first word, SemCat of head word, SemCat of last word, SemCat of predicate + SemCat of first word, SemCat of predicate + SemCat of last word, predicate + SemCat of head word, SemCat of predicate + head word. The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #TARGET_REF ) .",
        "output": "nan:SemCat (semantic category) of predicate, Sem-Cat of first word, SemCat of head word, SemCat of last word, SemCat of predicate + SemCat of first word, SemCat of predicate + SemCat of last word, predicate + SemCat of head word, SemCat of predicate + head word. The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "Most DOP models , such as in #REF , #REF , #REF , #TARGET_REF and Collins & #REF , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .",
                "We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by \"Likelihood-DOP\" the PCFG-reduction of #REF given in Section 2.2."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Most DOP models , such as in #REF , #REF , #REF , #TARGET_REF and Collins & #REF , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence . We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by \"Likelihood-DOP\" the PCFG-reduction of #REF given in Section 2.2.",
        "output": "nan:Most DOP models , such as in #REF , #REF , #REF , #TARGET_REF<BACK/> and Collins & #REF , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence . We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by \"Likelihood-DOP\" the PCFG-reduction of #REF given in Section 2.2."
    },
    {
        "gold": {
            "text": [
                "For unsupervised tree structure induction, #REF adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.",
                "Our previous work (#REF) designed an EMbased method to construct unsupervised trees for tree-based translation models.",
                "This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.",
                "Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.",
                "The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #TARGET_REF ) .",
                "#REF employed a Bayesian method to learn discontinuous SCFG rules.",
                "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.",
                "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.",
                "#REF and #REF focused on joint parsing and alignment.",
                "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.",
                "adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:For unsupervised tree structure induction, #REF adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (#REF) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #TARGET_REF ) . #REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #REF focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.",
        "output": "related work:For unsupervised tree structure induction, #REF adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (#REF) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #TARGET_REF<USE/> ) . #REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #REF focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees."
    },
    {
        "gold": {
            "text": [
                "Wikipedia (WIKI).",
                "We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #TARGET_REF ) to measure the relatedness between words in the dataset .",
                "Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (#REF).",
                "In this way we obtained 13760 word pairs."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:Wikipedia (WIKI). We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #TARGET_REF ) to measure the relatedness between words in the dataset . Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (#REF). In this way we obtained 13760 word pairs.",
        "output": "experiments:Wikipedia (WIKI). We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #TARGET_REF<FUT/> ) to measure the relatedness between words in the dataset . Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (#REF). In this way we obtained 13760 word pairs."
    },
    {
        "gold": {
            "text": [
                "For the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross-linguistically-plausible categories.",
                "To formalize the notion of what it means for a category to be more \"plausible\", we extend the category generator of our previous work, which we will call P CAT .",
                "We can define PCAT using a probabilistic grammar ( #TARGET_REF ) .",
                "The grammar may first generate a start or end category ( S , E ) with probability p se or a special tokendeletion category ( D ; explained in §5) with probability p del , or a standard CCG category C:"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "method:For the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross-linguistically-plausible categories. To formalize the notion of what it means for a category to be more \"plausible\", we extend the category generator of our previous work, which we will call P CAT . We can define PCAT using a probabilistic grammar ( #TARGET_REF ) . The grammar may first generate a start or end category ( S , E ) with probability p se or a special tokendeletion category ( D ; explained in §5) with probability p del , or a standard CCG category C:",
        "output": "method:For the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross-linguistically-plausible categories. To formalize the notion of what it means for a category to be more \"plausible\", we extend the category generator of our previous work, which we will call P CAT . We can define PCAT using a probabilistic grammar ( #TARGET_REF<BACK/> ) . The grammar may first generate a start or end category ( S , E ) with probability p se or a special tokendeletion category ( D ; explained in §5) with probability p del , or a standard CCG category C:"
    },
    {
        "gold": {
            "text": [
                "Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.",
                "In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) .",
                "The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) . The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.",
        "output": "related work:Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification (#REF; #REF; #REF ; #REF ; #TARGET_REF<USE/> ; #REF ; #REF ) . The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."
    },
    {
        "gold": {
            "text": [
                "• A deterministic algorithm for building labeled projective dependency graphs (#REF).",
                "• History-based feature models for predicting the next parser action (#REF).",
                "• Support vector machines for mapping histories to parser actions (#REF).",
                "â¢ Graph transformations for recovering nonprojective structures ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:• A deterministic algorithm for building labeled projective dependency graphs (#REF). • History-based feature models for predicting the next parser action (#REF). • Support vector machines for mapping histories to parser actions (#REF). â¢ Graph transformations for recovering nonprojective structures ( #TARGET_REF ) .",
        "output": "introduction:• A deterministic algorithm for building labeled projective dependency graphs (#REF). • History-based feature models for predicting the next parser action (#REF). • Support vector machines for mapping histories to parser actions (#REF). â¢ Graph transformations for recovering nonprojective structures ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "â¢ Learnability ( #REF ) â¢ Text generation ( #REF ; Milosavljevic , Tulloch , and #REF ) â¢ Speech generation ( #TARGET_REF ) â¢ Localization ( Sch Â¨ aler 1996 )"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:â¢ Learnability ( #REF ) â¢ Text generation ( #REF ; Milosavljevic , Tulloch , and #REF ) â¢ Speech generation ( #TARGET_REF ) â¢ Localization ( Sch Â¨ aler 1996 )",
        "output": "introduction:â¢ Learnability ( #REF ) â¢ Text generation ( #REF ; Milosavljevic , Tulloch , and #REF ) â¢ Speech generation ( #TARGET_REF<BACK/> ) â¢ Localization ( Sch Â¨ aler 1996 )"
    },
    {
        "gold": {
            "text": [
                "2We could just as easily use other symmetric \"association\" measures, such as 02 ( Gale & #REF ) or the Dice coefficient ( #TARGET_REF ) .",
                "co-occur is called a direct association.",
                "Now, suppose that uk and Uk+z often co-occur within their language.",
                "Then vk and uk+l will also co-occur more often than expected by chance.",
                "The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.",
                "Models of translational equivalence that are ignorant of indirect associations have \"a tendency ... to be confused by collocates\" (#REF).",
                "Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (#REFc).",
                "The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest.",
                "The competitive linking algorithm implements this heuristic:"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:2We could just as easily use other symmetric \"association\" measures, such as 02 ( Gale & #REF ) or the Dice coefficient ( #TARGET_REF ) . co-occur is called a direct association. Now, suppose that uk and Uk+z often co-occur within their language. Then vk and uk+l will also co-occur more often than expected by chance. The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk. Models of translational equivalence that are ignorant of indirect associations have \"a tendency ... to be confused by collocates\" (#REF). Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (#REFc). The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm implements this heuristic:",
        "output": "method:2We could just as easily use other symmetric \"association\" measures, such as 02 ( Gale & #REF ) or the Dice coefficient ( #TARGET_REF<USE/> ) . co-occur is called a direct association. Now, suppose that uk and Uk+z often co-occur within their language. Then vk and uk+l will also co-occur more often than expected by chance. The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk. Models of translational equivalence that are ignorant of indirect associations have \"a tendency ... to be confused by collocates\" (#REF). Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (#REFc). The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm implements this heuristic:"
    },
    {
        "gold": {
            "text": [
                "In the next section, we present a set of scoring functions that can be used in the inline reranker loss framework, resulting in a new augmented-loss for each one.",
                "Augmented-loss learning is then applied to target a downstream task using the loss functions to measure gains.",
                "We show empirical results for two extrinsic loss-functions (optimizing for the downstream task): machine translation and domain adaptation; and for one intrinsic loss-function: an arclength parsing score.",
                "For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:In the next section, we present a set of scoring functions that can be used in the inline reranker loss framework, resulting in a new augmented-loss for each one. Augmented-loss learning is then applied to target a downstream task using the loss functions to measure gains. We show empirical results for two extrinsic loss-functions (optimizing for the downstream task): machine translation and domain adaptation; and for one intrinsic loss-function: an arclength parsing score. For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #TARGET_REF ) .",
        "output": "experiments:In the next section, we present a set of scoring functions that can be used in the inline reranker loss framework, resulting in a new augmented-loss for each one. Augmented-loss learning is then applied to target a downstream task using the loss functions to measure gains. We show empirical results for two extrinsic loss-functions (optimizing for the downstream task): machine translation and domain adaptation; and for one intrinsic loss-function: an arclength parsing score. For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #TARGET_REF ; #REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #REF ) and the CATiB ( #REF ) .",
                "Recently, #REF analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.",
                "#REF reports experiments on Arabic parsing using his MaltParser (#REF), trained on the PADT.",
                "His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #TARGET_REF ; #REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #REF ) and the CATiB ( #REF ) . Recently, #REF analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. #REF reports experiments on Arabic parsing using his MaltParser (#REF), trained on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.",
        "output": "related work:As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and #REF ; #TARGET_REF<BACK/> ; #REF ) , the Prague Dependency Treebank ( PADT ) ( #REF ; #REF ) and the CATiB ( #REF ) . Recently, #REF analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. #REF reports experiments on Arabic parsing using his MaltParser (#REF), trained on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser."
    },
    {
        "gold": {
            "text": [
                "The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as #REF which produces weather reports from structured data or #REF which generates descriptions of objects from images.",
                "Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #REF ) ( #REF ) ( #REF ) or song lyrics ( #REF ) ( Ramakrishnan #TARGET_REF ) , where specified meter or rhyme schemes are enforced .",
                "In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as #REF which produces weather reports from structured data or #REF which generates descriptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #REF ) ( #REF ) ( #REF ) or song lyrics ( #REF ) ( Ramakrishnan #TARGET_REF ) , where specified meter or rhyme schemes are enforced . In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.",
        "output": "related work:The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as #REF which produces weather reports from structured data or #REF which generates descriptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #REF ) ( #REF ) ( #REF ) or song lyrics ( #REF ) ( Ramakrishnan #TARGET_REF<USE/> ) , where specified meter or rhyme schemes are enforced . In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric."
    },
    {
        "gold": {
            "text": [
                "We applied lexical-redundancy rules ( #TARGET_REF ) to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects.",
                "The resulting precision was very high (from 72.3% to 80.2%), and there was the expected drop in recall when prepositional details were included (from 54.7% to 29.3%)."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:We applied lexical-redundancy rules ( #TARGET_REF ) to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects. The resulting precision was very high (from 72.3% to 80.2%), and there was the expected drop in recall when prepositional details were included (from 54.7% to 29.3%).",
        "output": "introduction:We applied lexical-redundancy rules ( #TARGET_REF<FUT/> ) to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects. The resulting precision was very high (from 72.3% to 80.2%), and there was the expected drop in recall when prepositional details were included (from 54.7% to 29.3%)."
    },
    {
        "gold": {
            "text": [
                "• An easy approach is to normalize the options at each state to make the FST Markovian.",
                "Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation.",
                "Undesirable consequences of this fact have been termed \"label bias\" ( #TARGET_REF ) .",
                "Also, in the conditional case such per-state normalization is only correct if all states accept all input suffixes (since \"dead ends\" leak probability mass). 8",
                " A better-founded approach is global normalization, which simply divides each f (x, y) by"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:• An easy approach is to normalize the options at each state to make the FST Markovian. Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation. Undesirable consequences of this fact have been termed \"label bias\" ( #TARGET_REF ) . Also, in the conditional case such per-state normalization is only correct if all states accept all input suffixes (since \"dead ends\" leak probability mass). 8  A better-founded approach is global normalization, which simply divides each f (x, y) by",
        "output": "introduction:• An easy approach is to normalize the options at each state to make the FST Markovian. Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation. Undesirable consequences of this fact have been termed \"label bias\" ( #TARGET_REF<BACK/> ) . Also, in the conditional case such per-state normalization is only correct if all states accept all input suffixes (since \"dead ends\" leak probability mass). 8  A better-founded approach is global normalization, which simply divides each f (x, y) by"
    },
    {
        "gold": {
            "text": [
                "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF.",
                "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #TARGET_REF , assertional statements as in #REF , or semantic nets as in #REF .",
                "That is, the current system learns procedures rather than data structures.",
                "There is some literature on procedure acquisition such as the LISP synthesis work described in #REF and the PROLOG synthesis method of #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF. The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #TARGET_REF , assertional statements as in #REF , or semantic nets as in #REF . That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in #REF and the PROLOG synthesis method of #REF.",
        "output": "nan:The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF. The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #TARGET_REF<USE/> , assertional statements as in #REF , or semantic nets as in #REF . That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in #REF and the PROLOG synthesis method of #REF."
    },
    {
        "gold": {
            "text": [
                "As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.",
                "We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #TARGET_REF ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .",
                "Following #REF, we consider an anaphoric reference, NP i , correctly resolved if NP i and its closest antecedent are in the same coreference chain in the resulting partition.",
                "In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts. We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #TARGET_REF ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved . Following #REF, we consider an anaphoric reference, NP i , correctly resolved if NP i and its closest antecedent are in the same coreference chain in the resulting partition. In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder.",
        "output": "nan:As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts. We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #TARGET_REF<FUT/> ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved . Following #REF, we consider an anaphoric reference, NP i , correctly resolved if NP i and its closest antecedent are in the same coreference chain in the resulting partition. In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder."
    },
    {
        "gold": {
            "text": [
                "There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .",
                "Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #TARGET_REF ) .",
                "In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (#REF).",
                "Our group has developed a wide-coverage HPSG grammar for Japanese (#REF), which is used in a high-accuracy Japanese dependency analyzer (#REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts . Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #TARGET_REF ) . In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (#REF). Our group has developed a wide-coverage HPSG grammar for Japanese (#REF), which is used in a high-accuracy Japanese dependency analyzer (#REF).",
        "output": "introduction:There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts . Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #TARGET_REF<BACK/> ) . In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (#REF). Our group has developed a wide-coverage HPSG grammar for Japanese (#REF), which is used in a high-accuracy Japanese dependency analyzer (#REF)."
    },
    {
        "gold": {
            "text": [
                "), but at high ones its precision decreases almost dramatically.",
                "Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #TARGET_REF ) ) one can not really recommend this method ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:), but at high ones its precision decreases almost dramatically. Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #TARGET_REF ) ) one can not really recommend this method .",
        "output": "experiments:), but at high ones its precision decreases almost dramatically. Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #TARGET_REF<USE/> ) ) one can not really recommend this method ."
    },
    {
        "gold": {
            "text": [
                "We also compute GIST vectors ( #TARGET_REF ) for every image using LearGIST ( #REF ) .",
                "Unlike SURF descriptors, GIST produces a single vector representation for an image.",
                "The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall \"gist\" of the whole image.",
                "It is frequently used in tasks like scene identification, and #REF shows that distance in GIST space correlates well with semantic distance in WordNet.",
                "After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We also compute GIST vectors ( #TARGET_REF ) for every image using LearGIST ( #REF ) . Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall \"gist\" of the whole image. It is frequently used in tasks like scene identification, and #REF shows that distance in GIST space correlates well with semantic distance in WordNet. After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.",
        "output": "experiments:We also compute GIST vectors ( #TARGET_REF<FUT/> ) for every image using LearGIST ( #REF ) . Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall \"gist\" of the whole image. It is frequently used in tasks like scene identification, and #REF shows that distance in GIST space correlates well with semantic distance in WordNet. After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality."
    },
    {
        "gold": {
            "text": [
                "In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).",
                "Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #REF , 2009 ; #TARGET_REF ; #REF , 2006 ; #REF ; #REF ; #REFb ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #REF , 2009 ; #TARGET_REF ; #REF , 2006 ; #REF ; #REF ; #REFb ) .",
        "output": "introduction:In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #REF , 2009 ; #TARGET_REF<BACK/> ; #REF , 2006 ; #REF ; #REF ; #REFb ) ."
    },
    {
        "gold": {
            "text": [
                "We believe that the identification of OOV words should not be treated as a problem separate from word segmentation.",
                "We propose a unified approach that solves both problems simultaneously.",
                "A previous work along this line is #TARGET_REF , which is based on weighted finite-state transducers ( FSTs ) .",
                "Our approach is similarly motivated but is based on a different mechanism: linear mixture models.",
                "As we shall see, the models provide a more flexible framework for incorporating various kinds of lexical and statistical information.",
                "Many types of OOV words that are not covered in Sproat's system can be dealt with in our system.",
                "The linear models we used are originally derived from linear discriminant functions widely used for pattern classification (Duda, Hart, and #REF) and have been recently introduced into NLP tasks by #REF.",
                "Other frameworks of Chinese word segmentation, which are similar to the linear models, include maximum entropy models (#REF) and conditional random fields (Peng, Feng, and #REF).",
                "They also use a unified approach to word breaking and OOV identification."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:We believe that the identification of OOV words should not be treated as a problem separate from word segmentation. We propose a unified approach that solves both problems simultaneously. A previous work along this line is #TARGET_REF , which is based on weighted finite-state transducers ( FSTs ) . Our approach is similarly motivated but is based on a different mechanism: linear mixture models. As we shall see, the models provide a more flexible framework for incorporating various kinds of lexical and statistical information. Many types of OOV words that are not covered in Sproat's system can be dealt with in our system. The linear models we used are originally derived from linear discriminant functions widely used for pattern classification (Duda, Hart, and #REF) and have been recently introduced into NLP tasks by #REF. Other frameworks of Chinese word segmentation, which are similar to the linear models, include maximum entropy models (#REF) and conditional random fields (Peng, Feng, and #REF). They also use a unified approach to word breaking and OOV identification.",
        "output": "related work:We believe that the identification of OOV words should not be treated as a problem separate from word segmentation. We propose a unified approach that solves both problems simultaneously. A previous work along this line is #TARGET_REF<USE/> , which is based on weighted finite-state transducers ( FSTs ) . Our approach is similarly motivated but is based on a different mechanism: linear mixture models. As we shall see, the models provide a more flexible framework for incorporating various kinds of lexical and statistical information. Many types of OOV words that are not covered in Sproat's system can be dealt with in our system. The linear models we used are originally derived from linear discriminant functions widely used for pattern classification (Duda, Hart, and #REF) and have been recently introduced into NLP tasks by #REF. Other frameworks of Chinese word segmentation, which are similar to the linear models, include maximum entropy models (#REF) and conditional random fields (Peng, Feng, and #REF). They also use a unified approach to word breaking and OOV identification."
    },
    {
        "gold": {
            "text": [
                "Miyao, Ninomiya, and #REF and Nakanishi, Miyao, and #REF describe a methodology for acquiring an English HPSG from the Penn-II Treebank.",
                "Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #REF and #TARGET_REF ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.",
                "In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of �inverse schemata.�"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:Miyao, Ninomiya, and #REF and Nakanishi, Miyao, and #REF describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #REF and #TARGET_REF ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category. In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of �inverse schemata.�",
        "output": "related work:Miyao, Ninomiya, and #REF and Nakanishi, Miyao, and #REF describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #REF and #TARGET_REF<FUT/> ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category. In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of �inverse schemata.�"
    },
    {
        "gold": {
            "text": [
                "In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £).",
                "In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).",
                "Substitution replaces a substitution node with another initial tree (Figure 3).",
                "Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).",
                "FB-LTAG (#REF;#REF) is an extension of the LTAG formalism.",
                "In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.",
                "Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #TARGET_REF ) .",
                "The XTAG group (#REF) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.",
                "Development of a large-scale French grammar (Abeillé and #REF) has also started at the University of Pennsylvania and is expanded at University of Paris 7."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £). In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ). Substitution replaces a substitution node with another initial tree (Figure 3). Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4). FB-LTAG (#REF;#REF) is an extension of the LTAG formalism. In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #TARGET_REF ) . The XTAG group (#REF) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars. Development of a large-scale French grammar (Abeillé and #REF) has also started at the University of Pennsylvania and is expanded at University of Paris 7.",
        "output": "introduction:In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £). In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ). Substitution replaces a substitution node with another initial tree (Figure 3). Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4). FB-LTAG (#REF;#REF) is an extension of the LTAG formalism. In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #TARGET_REF<BACK/> ) . The XTAG group (#REF) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars. Development of a large-scale French grammar (Abeillé and #REF) has also started at the University of Pennsylvania and is expanded at University of Paris 7."
    },
    {
        "gold": {
            "text": [
                "Viewed in this way , gradable adjectives are an extreme example of the \"efficiency of language\" ( #TARGET_REF ) : Far from meaning something concrete like \"larger than 8 cm\" -- a concept that would have very limited applicability -- or even something more general like \"larger than the average N , \"a word like large is applicable across a wide range of different situations ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Viewed in this way , gradable adjectives are an extreme example of the \"efficiency of language\" ( #TARGET_REF ) : Far from meaning something concrete like \"larger than 8 cm\" -- a concept that would have very limited applicability -- or even something more general like \"larger than the average N , \"a word like large is applicable across a wide range of different situations .",
        "output": "introduction:Viewed in this way , gradable adjectives are an extreme example of the \"efficiency of language\" ( #TARGET_REF<USE/> ) : Far from meaning something concrete like \"larger than 8 cm\" -- a concept that would have very limited applicability -- or even something more general like \"larger than the average N , \"a word like large is applicable across a wide range of different situations ."
    },
    {
        "gold": {
            "text": [
                "7A11 our results are computed with the evalb program following the now-standard criteria in ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:7A11 our results are computed with the evalb program following the now-standard criteria in ( #TARGET_REF ) .",
        "output": "experiments:7A11 our results are computed with the evalb program following the now-standard criteria in ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in #REF, #REF, and #REF.",
                "The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.",
                "Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (#REF;Azzam, Humphreys, and #REF;#REF;#REF;#REF;#REF;Mitkov, Belguith, and #REF).",
                "Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( #REF ; #REF ; Ge , Hale , and #REF ; #REF ; the continuing interest in centering , used either in original or in revised form ( #REF ; #TARGET_REF ; #REF ; #REF ) ; and proposals related to the evaluation methodology in anaphora resolution ( #REFa , 2001b ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in #REF, #REF, and #REF. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (#REF;Azzam, Humphreys, and #REF;#REF;#REF;#REF;#REF;Mitkov, Belguith, and #REF). Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( #REF ; #REF ; Ge , Hale , and #REF ; #REF ; the continuing interest in centering , used either in original or in revised form ( #REF ; #TARGET_REF ; #REF ; #REF ) ; and proposals related to the evaluation methodology in anaphora resolution ( #REFa , 2001b ) .",
        "output": "nan:The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in #REF, #REF, and #REF. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (#REF;Azzam, Humphreys, and #REF;#REF;#REF;#REF;#REF;Mitkov, Belguith, and #REF). Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( #REF ; #REF ; Ge , Hale , and #REF ; #REF ; the continuing interest in centering , used either in original or in revised form ( #REF ; #TARGET_REF<BACK/> ; #REF ; #REF ) ; and proposals related to the evaluation methodology in anaphora resolution ( #REFa , 2001b ) ."
    },
    {
        "gold": {
            "text": [
                "#REF employed a Bayesian method to learn discontinuous SCFG rules.",
                "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.",
                "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.",
                "#REF and #REF focused on joint parsing and alignment.",
                "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.",
                "adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.",
                "#REF re-trained the linguistic parsers bilingually based on word alignment.",
                "#REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.",
                "Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.",
                "#TARGET_REF substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .",
                "#REF further labeled the SCFG rules with POS tags and unsupervised word classes.",
                "Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:#REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #REF focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #REF re-trained the linguistic parsers bilingually based on word alignment. #REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. #TARGET_REF substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories . #REF further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.",
        "output": "related work:#REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #REF focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #REF re-trained the linguistic parsers bilingually based on word alignment. #REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. #TARGET_REF<USE/> substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories . #REF further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules."
    },
    {
        "gold": {
            "text": [
                "The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (#REF).",
                "The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (#REF). The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #TARGET_REF ) .",
        "output": "experiments:The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (#REF). The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.",
                "In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., #REF, #REF).",
                "While these approaches have been reasonably successful (see #REF), #REF speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.",
                "In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.",
                "As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #REF ) , their semantic similarity as computed using WordNet ( e.g. , #REF ) or Wikipedia ( #REF ) , and the contextual role played by an NP ( see #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document. In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., #REF, #REF). While these approaches have been reasonably successful (see #REF), #REF speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #REF ) , their semantic similarity as computed using WordNet ( e.g. , #REF ) or Wikipedia ( #REF ) , and the contextual role played by an NP ( see #TARGET_REF ) .",
        "output": "introduction:In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document. In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., #REF, #REF). While these approaches have been reasonably successful (see #REF), #REF speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance. In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance. As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #REF ) , their semantic similarity as computed using WordNet ( e.g. , #REF ) or Wikipedia ( #REF ) , and the contextual role played by an NP ( see #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "We use the TRIPS dialogue parser (#REF) to parse the utterances.",
                "The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.",
                "The contextual interpreter then uses a reference resolution approach similar to #TARGET_REF , and an ontology mapping mechanism ( #REFa ) to produce a domain-specific semantic representation of the student 's output .",
                "Utterance content is represented as a set of extracted objects and relations between them.",
                "Negation is supported, together with a heuristic scoping algorithm.",
                "The interpreter also performs basic ellipsis resolution.",
                "For example, it can determine that in the answer to the question \"Which bulbs will be on and which bulbs will be off in this diagram?\",",
                "\"off\" can be taken to mean \"all bulbs in the di-agram will be off.\"",
                "The resulting output is then passed on to the domain reasoning and diagnosis components."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:We use the TRIPS dialogue parser (#REF) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to #TARGET_REF , and an ontology mapping mechanism ( #REFa ) to produce a domain-specific semantic representation of the student 's output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question \"Which bulbs will be on and which bulbs will be off in this diagram?\", \"off\" can be taken to mean \"all bulbs in the di-agram will be off.\" The resulting output is then passed on to the domain reasoning and diagnosis components.",
        "output": "experiments:We use the TRIPS dialogue parser (#REF) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to #TARGET_REF<USE/> , and an ontology mapping mechanism ( #REFa ) to produce a domain-specific semantic representation of the student 's output . Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question \"Which bulbs will be on and which bulbs will be off in this diagram?\", \"off\" can be taken to mean \"all bulbs in the di-agram will be off.\" The resulting output is then passed on to the domain reasoning and diagnosis components."
    },
    {
        "gold": {
            "text": [
                "This calculation is usually made computationally efficient using Dynamic Programming techniques.",
                "Therefore, Convolution Kernels alleviate the need of feature extraction (which usually requires domain knowledge, results in extraction of incomplete information and introduces noise in the data).",
                "Therefore, we use convolution kernels with a linear learning machine (Support Vector Machines) for our classification task.",
                "Now we present the \"discrete\" structures followed by the kernel we used.",
                "We use the structures previously used by #TARGET_REF , and propose one new structure .",
                "Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.",
                "All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT).",
                "For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (#REF).",
                "We use the Stanford parser (#REF) to get the basic PSTs and DTs.",
                "Following are the structures that we refer to in our experiments and results section: PET: This refers to the smallest common phrase structure tree that contains the two target entities.",
                "Dependency Words (DW) tree: This is the smallest common dependency tree that contains the two target entities."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:This calculation is usually made computationally efficient using Dynamic Programming techniques. Therefore, Convolution Kernels alleviate the need of feature extraction (which usually requires domain knowledge, results in extraction of incomplete information and introduces noise in the data). Therefore, we use convolution kernels with a linear learning machine (Support Vector Machines) for our classification task. Now we present the \"discrete\" structures followed by the kernel we used. We use the structures previously used by #TARGET_REF , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task. All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT). For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (#REF). We use the Stanford parser (#REF) to get the basic PSTs and DTs. Following are the structures that we refer to in our experiments and results section: PET: This refers to the smallest common phrase structure tree that contains the two target entities. Dependency Words (DW) tree: This is the smallest common dependency tree that contains the two target entities.",
        "output": "nan:This calculation is usually made computationally efficient using Dynamic Programming techniques. Therefore, Convolution Kernels alleviate the need of feature extraction (which usually requires domain knowledge, results in extraction of incomplete information and introduces noise in the data). Therefore, we use convolution kernels with a linear learning machine (Support Vector Machines) for our classification task. Now we present the \"discrete\" structures followed by the kernel we used. We use the structures previously used by #TARGET_REF<FUT/> , and propose one new structure . Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task. All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT). For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (#REF). We use the Stanford parser (#REF) to get the basic PSTs and DTs. Following are the structures that we refer to in our experiments and results section: PET: This refers to the smallest common phrase structure tree that contains the two target entities. Dependency Words (DW) tree: This is the smallest common dependency tree that contains the two target entities."
    },
    {
        "gold": {
            "text": [
                "A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names (#REF).",
                "According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( #TARGET_REF ) .",
                "As the amount of information in the WWW grows, more of these people are mentioned in different web pages.",
                "Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names (#REF). According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( #TARGET_REF ) . As the amount of information in the WWW grows, more of these people are mentioned in different web pages. Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned.",
        "output": "introduction:A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names (#REF). According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( #TARGET_REF<BACK/> ) . As the amount of information in the WWW grows, more of these people are mentioned in different web pages. Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned."
    },
    {
        "gold": {
            "text": [
                "Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #REF , #TARGET_REF , and #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #REF , #TARGET_REF , and #REF .",
        "output": "conclusion:Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #REF , #TARGET_REF<USE/> , and #REF ."
    },
    {
        "gold": {
            "text": [
                "Each set of translations is stored separately , and for each set the `` marker hypothesis '' ( #TARGET_REF ) is used to segment the phrasal lexicon into a `` marker lexicon . ''",
                "The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are \"marked\" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes.",
                "That is, a basic phrase-level segmentation of an input sentence can be achieved by exploiting a closed list of known marker words to signal the start and end of each segment."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:Each set of translations is stored separately , and for each set the `` marker hypothesis '' ( #TARGET_REF ) is used to segment the phrasal lexicon into a `` marker lexicon . '' The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are \"marked\" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes. That is, a basic phrase-level segmentation of an input sentence can be achieved by exploiting a closed list of known marker words to signal the start and end of each segment.",
        "output": "introduction:Each set of translations is stored separately , and for each set the `` marker hypothesis '' ( #TARGET_REF<FUT/> ) is used to segment the phrasal lexicon into a `` marker lexicon . '' The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are \"marked\" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes. That is, a basic phrase-level segmentation of an input sentence can be achieved by exploiting a closed list of known marker words to signal the start and end of each segment."
    },
    {
        "gold": {
            "text": [
                "This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ.",
                "We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.",
                "Bod (2001Bod ( , 2003.",
                "But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #REF and #REF , Bonnema et al. 's estimator performs worse and is comparable to #TARGET_REF ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ. We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001Bod ( , 2003. But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #REF and #REF , Bonnema et al. 's estimator performs worse and is comparable to #TARGET_REF .",
        "output": "introduction:This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ. We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001Bod ( , 2003. But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #REF and #REF , Bonnema et al. 's estimator performs worse and is comparable to #TARGET_REF<BACK/> ."
    },
    {
        "gold": {
            "text": [
                "Many approaches to cross-lingual IR have been published.",
                "One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #TARGET_REF ) .",
                "For most languages, there are no MT systems at all.",
                "Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Many approaches to cross-lingual IR have been published. One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #TARGET_REF ) . For most languages, there are no MT systems at all. Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived.",
        "output": "related work:Many approaches to cross-lingual IR have been published. One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #TARGET_REF<USE/> ) . For most languages, there are no MT systems at all. Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived."
    },
    {
        "gold": {
            "text": [
                "Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #TARGET_REF ] , which is still considered one of the best smoothing methods for n-gram language models .",
                "2. Sentence representation: Each sentence was represented as the collection of unigrams, bigrams and trigrams it contained.",
                "A coordinate was reserved for each such n-gram which appeared in the data, whether real or sampled.",
                "The value of the n'th coordinate in the vector representation of Interestingly, in practice both methods result in near identical rejection probabilities, within a precision of 0.0001.",
                "This indicates that satisfying the constraint (18) for the new feature is more important, in terms of perplexity, than preserving the constraints of the previous features, insofar as those get violated."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #TARGET_REF ] , which is still considered one of the best smoothing methods for n-gram language models . 2. Sentence representation: Each sentence was represented as the collection of unigrams, bigrams and trigrams it contained. A coordinate was reserved for each such n-gram which appeared in the data, whether real or sampled. The value of the n'th coordinate in the vector representation of Interestingly, in practice both methods result in near identical rejection probabilities, within a precision of 0.0001. This indicates that satisfying the constraint (18) for the new feature is more important, in terms of perplexity, than preserving the constraints of the previous features, insofar as those get violated.",
        "output": "experiments:Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #TARGET_REF<FUT/> ] , which is still considered one of the best smoothing methods for n-gram language models . 2. Sentence representation: Each sentence was represented as the collection of unigrams, bigrams and trigrams it contained. A coordinate was reserved for each such n-gram which appeared in the data, whether real or sampled. The value of the n'th coordinate in the vector representation of Interestingly, in practice both methods result in near identical rejection probabilities, within a precision of 0.0001. This indicates that satisfying the constraint (18) for the new feature is more important, in terms of perplexity, than preserving the constraints of the previous features, insofar as those get violated."
    },
    {
        "gold": {
            "text": [
                "While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.",
                "Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #TARGET_REF ; #REF ) and the .",
                "lexical rules (DLRs; #REF).",
                "5"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup. Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #TARGET_REF ; #REF ) and the . lexical rules (DLRs; #REF). 5",
        "output": "introduction:While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup. Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #TARGET_REF<BACK/> ; #REF ) and the . lexical rules (DLRs; #REF). 5"
    },
    {
        "gold": {
            "text": [
                "The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #TARGET_REF ) and case-based reasoning ( #REF ) .",
                "Such technologies require significant human input, and are difficult to create and maintain (#REF).",
                "In contrast, the techniques examined in this article are corpus-based and data-driven.",
                "The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #TARGET_REF ) and case-based reasoning ( #REF ) . Such technologies require significant human input, and are difficult to create and maintain (#REF). In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus.",
        "output": "nan:The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #TARGET_REF<USE/> ) and case-based reasoning ( #REF ) . Such technologies require significant human input, and are difficult to create and maintain (#REF). In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus."
    },
    {
        "gold": {
            "text": [
                "Because of the latent alignment variables z, the log-likelihood function for the HMM model is not concave, and the model is fit using the Expectation Maximization (EM) algorithm (Dempster, Laird, and #REF).",
                "EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) ( #TARGET_REF ) :"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:Because of the latent alignment variables z, the log-likelihood function for the HMM model is not concave, and the model is fit using the Expectation Maximization (EM) algorithm (Dempster, Laird, and #REF). EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) ( #TARGET_REF ) :",
        "output": "introduction:Because of the latent alignment variables z, the log-likelihood function for the HMM model is not concave, and the model is fit using the Expectation Maximization (EM) algorithm (Dempster, Laird, and #REF). EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) ( #TARGET_REF<FUT/> ) :"
    },
    {
        "gold": {
            "text": [
                "This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.",
                "Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.",
                "As a generalization , #TARGET_REF notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .",
                "#REF report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.",
                "Precision was quite high (95%), but recall was low (84%).",
                "This has an effect on both the precision and recall scores of our system against COMLEX.",
                "In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.",
                "We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as \"correct\" or \"incorrect.\""
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization , #TARGET_REF notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . #REF report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX. Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX. In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26. We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as \"correct\" or \"incorrect.\"",
        "output": "nan:This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization , #TARGET_REF<BACK/> notes that lexicons such as COMLEX tend to demonstrate high precision but low recall . #REF report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX. Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX. In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26. We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as \"correct\" or \"incorrect.\""
    },
    {
        "gold": {
            "text": [
                "For compound splitting, we follow #REF, using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.",
                "Other approaches use less deep linguistic resources ( e.g. , POS-tags #REF ) or are ( almost ) knowledge-free ( e.g. , #TARGET_REF ) .",
                "Compound merging is less well studied.",
                "#REF used a simple, list-based merging approach, merging all consecutive words included in a merging list.",
                "This approach resulted in too many compounds.",
                "We follow #REF, for compound merging.",
                "We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:For compound splitting, we follow #REF, using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources ( e.g. , POS-tags #REF ) or are ( almost ) knowledge-free ( e.g. , #TARGET_REF ) . Compound merging is less well studied. #REF used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow #REF, for compound merging. We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets.",
        "output": "related work:For compound splitting, we follow #REF, using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources ( e.g. , POS-tags #REF ) or are ( almost ) knowledge-free ( e.g. , #TARGET_REF<USE/> ) . Compound merging is less well studied. #REF used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow #REF, for compound merging. We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets."
    },
    {
        "gold": {
            "text": [
                "Encoding a finite-state automaton as definite relations is rather straightforward.",
                "In fact, one can view the representations as notational variants of one another.",
                "Each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause.",
                "Using an accumulator passing technique ( #TARGET_REF ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .",
                "Because of the word class specialization step discussed in Section 3.3, the execution avoids trying out many lexical rule applications that are guaranteed to fail."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:Encoding a finite-state automaton as definite relations is rather straightforward. In fact, one can view the representations as notational variants of one another. Each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause. Using an accumulator passing technique ( #TARGET_REF ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules . Because of the word class specialization step discussed in Section 3.3, the execution avoids trying out many lexical rule applications that are guaranteed to fail.",
        "output": "introduction:Encoding a finite-state automaton as definite relations is rather straightforward. In fact, one can view the representations as notational variants of one another. Each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause. Using an accumulator passing technique ( #TARGET_REF<FUT/> ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules . Because of the word class specialization step discussed in Section 3.3, the execution avoids trying out many lexical rule applications that are guaranteed to fail."
    },
    {
        "gold": {
            "text": [
                "where f and e (e ) are source and target sentences, respectively.",
                "h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .",
                "Some methods are based on likelihood ( #REF ; #REF ) , error rate ( #REF ; #REF ; #REF ; #TARGET_REF ) , margin ( #REF ; #REF ) and ranking ( #REF ) , and among which minimum error rate training ( MERT ) ( #REF ) is the most popular one .",
                "All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.",
                "We call them a global training method.",
                "One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.",
                "However, due to the diversity and uneven distribution of source sentences (#REF), there are some shortcomings in this pipeline."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( #REF ; #REF ) , error rate ( #REF ; #REF ; #REF ; #TARGET_REF ) , margin ( #REF ; #REF ) and ranking ( #REF ) , and among which minimum error rate training ( MERT ) ( #REF ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences (#REF), there are some shortcomings in this pipeline.",
        "output": "introduction:where f and e (e ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood ( #REF ; #REF ) , error rate ( #REF ; #REF ; #REF ; #TARGET_REF<BACK/> ) , margin ( #REF ; #REF ) and ranking ( #REF ) , and among which minimum error rate training ( MERT ) ( #REF ) is the most popular one . All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences (#REF), there are some shortcomings in this pipeline."
    },
    {
        "gold": {
            "text": [
                "â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and #REF ; Jijkoun and de #REF ) , or the corpus itself was significantly smaller than ours ( #REF ; #TARGET_REF ) .",
                "The representativeness of the sample size was not discussed in any of these studies."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and #REF ; Jijkoun and de #REF ) , or the corpus itself was significantly smaller than ours ( #REF ; #TARGET_REF ) . The representativeness of the sample size was not discussed in any of these studies.",
        "output": "method:â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and #REF ; Jijkoun and de #REF ) , or the corpus itself was significantly smaller than ours ( #REF ; #TARGET_REF<USE/> ) . The representativeness of the sample size was not discussed in any of these studies."
    },
    {
        "gold": {
            "text": [
                "The tutorial policy makes a high-level decision as to which strategy to use (for example, \"acknowledge the correct part and give a high specificity hint\") based on the answer analysis and dialogue context.",
                "At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #TARGET_REF ) .",
                "Since the system accepts unrestricted input, interpretation errors are unavoidable.",
                "Our recovery policy is modeled on the TargetedHelp (#REF) policy used in task-oriented dialogue.",
                "If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I'm sorry, I'm having a problem understanding.",
                "I don't know the word power.\"",
                "The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The tutorial policy makes a high-level decision as to which strategy to use (for example, \"acknowledge the correct part and give a high specificity hint\") based on the answer analysis and dialogue context. At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #TARGET_REF ) . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp (#REF) policy used in task-oriented dialogue. If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I'm sorry, I'm having a problem understanding. I don't know the word power.\" The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.",
        "output": "experiments:The tutorial policy makes a high-level decision as to which strategy to use (for example, \"acknowledge the correct part and give a high specificity hint\") based on the answer analysis and dialogue context. At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #TARGET_REF<FUT/> ) . Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp (#REF) policy used in task-oriented dialogue. If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I'm sorry, I'm having a problem understanding. I don't know the word power.\" The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers."
    },
    {
        "gold": {
            "text": [
                "Let T i = x i •f •y i .",
                "Then t i is the total semiring weight w 0n of paths in T i from initial state 0 to final state n (assumed WLOG to be unique and unweighted).",
                "It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #TARGET_REF ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version .",
                "If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (#REFb).",
                "For a general graph T i , Tarjan (1981b) shows how to partition into \"hard\" subgraphs that localize the cyclicity or irreducibility, then run the O(n 3 ) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.",
                "The overhead of partitioning and recombining is essentially only O(m)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Let T i = x i •f •y i . Then t i is the total semiring weight w 0n of paths in T i from initial state 0 to final state n (assumed WLOG to be unique and unweighted). It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #TARGET_REF ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version . If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (#REFb). For a general graph T i , Tarjan (1981b) shows how to partition into \"hard\" subgraphs that localize the cyclicity or irreducibility, then run the O(n 3 ) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results. The overhead of partitioning and recombining is essentially only O(m).",
        "output": "nan:Let T i = x i •f •y i . Then t i is the total semiring weight w 0n of paths in T i from initial state 0 to final state n (assumed WLOG to be unique and unweighted). It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #TARGET_REF<BACK/> ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version . If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (#REFb). For a general graph T i , Tarjan (1981b) shows how to partition into \"hard\" subgraphs that localize the cyclicity or irreducibility, then run the O(n 3 ) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results. The overhead of partitioning and recombining is essentially only O(m)."
    },
    {
        "gold": {
            "text": [
                "Another common approach to lexical rules is to encode them as unary phrase structure rules.",
                "This approach is taken , for example , in LKB ( #TARGET_REF ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #REF , 31 ) .",
                "A similar method is included in PATR-II (#REF) and can be used to encode lexical rules as binary relations in the CUF system (#REF; D6rre and #REFb) or the TFS system (#REF;#REF).",
                "The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken , for example , in LKB ( #TARGET_REF ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #REF , 31 ) . A similar method is included in PATR-II (#REF) and can be used to encode lexical rules as binary relations in the CUF system (#REF; D6rre and #REFb) or the TFS system (#REF;#REF). The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.",
        "output": "related work:Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken , for example , in LKB ( #TARGET_REF<USE/> ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #REF , 31 ) . A similar method is included in PATR-II (#REF) and can be used to encode lexical rules as binary relations in the CUF system (#REF; D6rre and #REFb) or the TFS system (#REF;#REF). The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."
    },
    {
        "gold": {
            "text": [
                "The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0",
                "dataset only has mention annotations.",
                "Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #TARGET_REF ) with gold constituency parsing information and gold named entity information .",
                "The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.",
                "We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec.",
                "3.1.1.",
                "The nonoverlapping mention head assumption in Sec.",
                "3.1.1",
                "can be verified empirically on both ACE-2004 and OntoNotes-5.0",
                "datasets.",
                "Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (#REF), Berkeley system (#REF) and HOTCoref system (Björkelund and #REF).",
                "Developed Systems Our developed system is built on the work by #REF, using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0 dataset only has mention annotations. Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #TARGET_REF ) with gold constituency parsing information and gold named entity information . The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec. 3.1.1. The nonoverlapping mention head assumption in Sec. 3.1.1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (#REF), Berkeley system (#REF) and HOTCoref system (Björkelund and #REF). Developed Systems Our developed system is built on the work by #REF, using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 .",
        "output": "experiments:The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0 dataset only has mention annotations. Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #TARGET_REF<FUT/> ) with gold constituency parsing information and gold named entity information . The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads. We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec. 3.1.1. The nonoverlapping mention head assumption in Sec. 3.1.1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (#REF), Berkeley system (#REF) and HOTCoref system (Björkelund and #REF). Developed Systems Our developed system is built on the work by #REF, using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 ."
    },
    {
        "gold": {
            "text": [
                "The names given to the components vary; they have been called \"strategic\" and \"tactical\" components (e.g., #REF;#REF;#REF) 1, \"planning\" and\"realization\" (e.g., #REF;#REFa), or simply \"what to say\" versus \"how to say it\" (e.g., #REF;#REF).",
                "The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.",
                "Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.",
                "For example, DIOGENES (#REF), EPICURE (#REF), SPOKESMAN (#REF), Sibun's work on local organization of text (#REF), and COMET (#REF) all are organized this way.",
                "McDonald has even argued for extending the model to a large number of components ( #REF ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #TARGET_REF ; #REF ; #REF ) .",
                "Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (#REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:The names given to the components vary; they have been called \"strategic\" and \"tactical\" components (e.g., #REF;#REF;#REF) 1, \"planning\" and\"realization\" (e.g., #REF;#REFa), or simply \"what to say\" versus \"how to say it\" (e.g., #REF;#REF). The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed. Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design. For example, DIOGENES (#REF), EPICURE (#REF), SPOKESMAN (#REF), Sibun's work on local organization of text (#REF), and COMET (#REF) all are organized this way. McDonald has even argued for extending the model to a large number of components ( #REF ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #TARGET_REF ; #REF ; #REF ) . Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (#REF).",
        "output": "nan:The names given to the components vary; they have been called \"strategic\" and \"tactical\" components (e.g., #REF;#REF;#REF) 1, \"planning\" and\"realization\" (e.g., #REF;#REFa), or simply \"what to say\" versus \"how to say it\" (e.g., #REF;#REF). The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed. Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design. For example, DIOGENES (#REF), EPICURE (#REF), SPOKESMAN (#REF), Sibun's work on local organization of text (#REF), and COMET (#REF) all are organized this way. McDonald has even argued for extending the model to a large number of components ( #REF ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #TARGET_REF<BACK/> ; #REF ; #REF ) . Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (#REF)."
    },
    {
        "gold": {
            "text": [
                "Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #REF ) and the coreference resolution system is similar to the one described in ( #TARGET_REF ) .",
                "Both systems are built around from the maximum-entropy technique (#REF).",
                "We formulate the mention detection task as a sequence classification problem.",
                "While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.",
                "The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.",
                "We begin with a segmentation of the written text before starting the classification.",
                "This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens).",
                "The resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions).",
                "Additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #REF ) and the coreference resolution system is similar to the one described in ( #TARGET_REF ) . Both systems are built around from the maximum-entropy technique (#REF). We formulate the mention detection task as a sequence classification problem. While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language. The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes. We begin with a segmentation of the written text before starting the classification. This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens). The resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions). Additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness.",
        "output": "introduction:Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #REF ) and the coreference resolution system is similar to the one described in ( #TARGET_REF<USE/> ) . Both systems are built around from the maximum-entropy technique (#REF). We formulate the mention detection task as a sequence classification problem. While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language. The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes. We begin with a segmentation of the written text before starting the classification. This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens). The resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions). Additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness."
    },
    {
        "gold": {
            "text": [
                "Our knowledge extractors rely extensively on MetaMap ( #TARGET_REF ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .",
                "Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and #REF), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS).",
                "An additional feature we take advantage of (when present) is explicit section markers present in some abstracts.",
                "These so-called structured abstracts were recommended by the Ad Hoc Working Group for Critical Appraisal of the Medical #REF to help humans assess the reliability and content of a publication and to facilitate the indexing and retrieval processes.",
                "These abstracts loosely adhere to the introduction, methods, results, and conclusions format common in scientific writing, and delineate a study using explicitly marked sections with variations of the above headings.",
                "Although many core clinical journals require structured abstracts, there is a great deal of variation in the actual headings.",
                "Even when present, the headings are not organized in a manner focused on patient care.",
                "In addition, abstracts of much high-quality work remain unstructured."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Our knowledge extractors rely extensively on MetaMap ( #TARGET_REF ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus . Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and #REF), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS). An additional feature we take advantage of (when present) is explicit section markers present in some abstracts. These so-called structured abstracts were recommended by the Ad Hoc Working Group for Critical Appraisal of the Medical #REF to help humans assess the reliability and content of a publication and to facilitate the indexing and retrieval processes. These abstracts loosely adhere to the introduction, methods, results, and conclusions format common in scientific writing, and delineate a study using explicitly marked sections with variations of the above headings. Although many core clinical journals require structured abstracts, there is a great deal of variation in the actual headings. Even when present, the headings are not organized in a manner focused on patient care. In addition, abstracts of much high-quality work remain unstructured.",
        "output": "nan:Our knowledge extractors rely extensively on MetaMap ( #TARGET_REF<FUT/> ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus . Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and #REF), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS). An additional feature we take advantage of (when present) is explicit section markers present in some abstracts. These so-called structured abstracts were recommended by the Ad Hoc Working Group for Critical Appraisal of the Medical #REF to help humans assess the reliability and content of a publication and to facilitate the indexing and retrieval processes. These abstracts loosely adhere to the introduction, methods, results, and conclusions format common in scientific writing, and delineate a study using explicitly marked sections with variations of the above headings. Although many core clinical journals require structured abstracts, there is a great deal of variation in the actual headings. Even when present, the headings are not organized in a manner focused on patient care. In addition, abstracts of much high-quality work remain unstructured."
    },
    {
        "gold": {
            "text": [
                "Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (#REF).",
                "In other methods, lexical resources are specifically tailored to meet the requirements of the domain (#REF) or the system (#REF).",
                "Such systems extract information from some types of syntactic units ( clauses in ( #REF ; #TARGET_REF ; #REF ) ; noun phrases in ( #REF ; #REF ) ) .",
                "Lists of semantic relations are designed to capture salient domain information."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (#REF). In other methods, lexical resources are specifically tailored to meet the requirements of the domain (#REF) or the system (#REF). Such systems extract information from some types of syntactic units ( clauses in ( #REF ; #TARGET_REF ; #REF ) ; noun phrases in ( #REF ; #REF ) ) . Lists of semantic relations are designed to capture salient domain information.",
        "output": "related work:Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (#REF). In other methods, lexical resources are specifically tailored to meet the requirements of the domain (#REF) or the system (#REF). Such systems extract information from some types of syntactic units ( clauses in ( #REF ; #TARGET_REF<BACK/> ; #REF ) ; noun phrases in ( #REF ; #REF ) ) . Lists of semantic relations are designed to capture salient domain information."
    },
    {
        "gold": {
            "text": [
                "Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors).",
                "However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results.",
                "As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors). However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results. As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #REF ; #TARGET_REF ) .",
        "output": "experiments:Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors). However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results. As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #REF ; #TARGET_REF<USE/> ) ."
    },
    {
        "gold": {
            "text": [
                "We use support vector machines to predict the next parser action from a feature vector representing the history.",
                "More specifically , we use LIBSVM ( #TARGET_REF ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification .",
                "Symbolic features are converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components. 4",
                "or some languages, we divide the training data into smaller sets, based on some feature s (normally the CPOS or POS of the next input token), which may reduce training times without a significant loss in accuracy (#REF).",
                "To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:We use support vector machines to predict the next parser action from a feature vector representing the history. More specifically , we use LIBSVM ( #TARGET_REF ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification . Symbolic features are converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components. 4 or some languages, we divide the training data into smaller sets, based on some feature s (normally the CPOS or POS of the next input token), which may reduce training times without a significant loss in accuracy (#REF). To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t.",
        "output": "method:We use support vector machines to predict the next parser action from a feature vector representing the history. More specifically , we use LIBSVM ( #TARGET_REF<FUT/> ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification . Symbolic features are converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components. 4 or some languages, we divide the training data into smaller sets, based on some feature s (normally the CPOS or POS of the next input token), which may reduce training times without a significant loss in accuracy (#REF). To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t."
    },
    {
        "gold": {
            "text": [
                "We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.",
                "This domain is well-suited for exploring the posed research questions for several reasons.",
                "First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and #REF).",
                "Second , software for utilizing this ontology already exists : MetaMap ( #TARGET_REF ) identifies concepts in free text , and SemRep ( #REF ) extracts relations between the concepts .",
                "Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.",
                "#REF version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.",
                "The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.",
                "Third, the paradigm of evidence-based medicine (#REF) provides a task-based model of the clinical information-seeking process."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings. This domain is well-suited for exploring the posed research questions for several reasons. First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and #REF). Second , software for utilizing this ontology already exists : MetaMap ( #TARGET_REF ) identifies concepts in free text , and SemRep ( #REF ) extracts relations between the concepts . Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon. #REF version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies. The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third, the paradigm of evidence-based medicine (#REF) provides a task-based model of the clinical information-seeking process.",
        "output": "introduction:We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings. This domain is well-suited for exploring the posed research questions for several reasons. First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and #REF). Second , software for utilizing this ontology already exists : MetaMap ( #TARGET_REF<BACK/> ) identifies concepts in free text , and SemRep ( #REF ) extracts relations between the concepts . Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon. #REF version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies. The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third, the paradigm of evidence-based medicine (#REF) provides a task-based model of the clinical information-seeking process."
    },
    {
        "gold": {
            "text": [
                "All logical notions that we are going to consider, such as theory or model, will be finitary.",
                "For example, a model would typically contain fewer than a hundred elements of different logical sorts.",
                "Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.",
                "The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.",
                "This Principle of Finitism is also assumed by #REF , #TARGET_REF , #REF , and implicitly or explicitly by almost all researchers in computational linguistics .",
                "As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.",
                "#REF)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred elements of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by #REF , #TARGET_REF , #REF , and implicitly or explicitly by almost all researchers in computational linguistics . As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. #REF).",
        "output": "introduction:All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred elements of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by #REF , #TARGET_REF<USE/> , #REF , and implicitly or explicitly by almost all researchers in computational linguistics . As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. #REF)."
    },
    {
        "gold": {
            "text": [
                "At present, we have available at MIT two systems, VOYAGER and ATIS, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.",
                "In both of these systems, TINA provides the interface between the recognizer and the application back-end.",
                "In this section, I will describe our current interfaces between TINA and the recognizer and our future plans in this area.",
                "In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.",
                "This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.",
                "The recognizer for these systems is the SUMMIT system ( #TARGET_REF ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .",
                "The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.",
                "The search algorithm is the standard Viterbi search (#REF), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:At present, we have available at MIT two systems, VOYAGER and ATIS, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input. In both of these systems, TINA provides the interface between the recognizer and the application back-end. In this section, I will describe our current interfaces between TINA and the recognizer and our future plans in this area. In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation. This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail. The recognizer for these systems is the SUMMIT system ( #TARGET_REF ) , which uses a segmental-based framework and includes an auditory model in the front-end processing . The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules. The search algorithm is the standard Viterbi search (#REF), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence.",
        "output": "nan:At present, we have available at MIT two systems, VOYAGER and ATIS, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input. In both of these systems, TINA provides the interface between the recognizer and the application back-end. In this section, I will describe our current interfaces between TINA and the recognizer and our future plans in this area. In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation. This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail. The recognizer for these systems is the SUMMIT system ( #TARGET_REF<FUT/> ) , which uses a segmental-based framework and includes an auditory model in the front-end processing . The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules. The search algorithm is the standard Viterbi search (#REF), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence."
    },
    {
        "gold": {
            "text": [
                "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).",
                "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .",
                "Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.",
                "Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.",
                "John Mayor), nominal (the president) or pronominal (she, it).",
                "An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #TARGET_REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.",
        "output": "introduction:In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #TARGET_REF<BACK/> ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity."
    },
    {
        "gold": {
            "text": [
                "Several works have proposed discriminative techniques to train log-linear model for SMT.",
                "(#REF;#REF) used maximum likelihood estimation to learn weights for MT.",
                "(#REF;#REF;#REF; #REF) employed an evaluation metric as a loss function and directly optimized it.",
                "( #REF ; #REF ; #TARGET_REF ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Several works have proposed discriminative techniques to train log-linear model for SMT. (#REF;#REF) used maximum likelihood estimation to learn weights for MT. (#REF;#REF;#REF; #REF) employed an evaluation metric as a loss function and directly optimized it. ( #REF ; #REF ; #TARGET_REF ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .",
        "output": "related work:Several works have proposed discriminative techniques to train log-linear model for SMT. (#REF;#REF) used maximum likelihood estimation to learn weights for MT. (#REF;#REF;#REF; #REF) employed an evaluation metric as a loss function and directly optimized it. ( #REF ; #REF ; #TARGET_REF<USE/> ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions ."
    },
    {
        "gold": {
            "text": [
                "An implementation of the transition-based dependency parsing framework ( #TARGET_REF ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #REF with a beam size of 8 .",
                "Beams with varying sizes can be used to produce k-best lists.",
                "The features used by all models are: the part-of- speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available); dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available).",
                "All feature conjunctions are included."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:An implementation of the transition-based dependency parsing framework ( #TARGET_REF ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #REF with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists. The features used by all models are: the part-of- speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available); dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available). All feature conjunctions are included.",
        "output": "experiments:An implementation of the transition-based dependency parsing framework ( #TARGET_REF<FUT/> ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #REF with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists. The features used by all models are: the part-of- speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available); dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available). All feature conjunctions are included."
    },
    {
        "gold": {
            "text": [
                "NLP is experiencing an explosion in the quantity of electronic text available.",
                "Some of this new data will be manually annotated.",
                "For example , 10 million words of the American National Corpus ( #REF ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #TARGET_REF ) , currently used for training POS taggers .",
                "This will require more efficient learning algorithms and implementations."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:NLP is experiencing an explosion in the quantity of electronic text available. Some of this new data will be manually annotated. For example , 10 million words of the American National Corpus ( #REF ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #TARGET_REF ) , currently used for training POS taggers . This will require more efficient learning algorithms and implementations.",
        "output": "introduction:NLP is experiencing an explosion in the quantity of electronic text available. Some of this new data will be manually annotated. For example , 10 million words of the American National Corpus ( #REF ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #TARGET_REF<BACK/> ) , currently used for training POS taggers . This will require more efficient learning algorithms and implementations."
    },
    {
        "gold": {
            "text": [
                "Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.",
                "One solution to this problem is to add more complexity to the model to better reflect the translation process.",
                "This is the approach taken by IBM Models 4 + ( #REFb ; #TARGET_REF ) , and more recently by the LEAF model ( #REF ) .",
                "Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.",
                "Instead, we propose to use a learning framework called Posterior Regularization (Graça, Ganchev, and #REF) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors.",
                "The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).",
                "Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior.",
                "On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments. One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( #REFb ; #TARGET_REF ) , and more recently by the LEAF model ( #REF ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Graça, Ganchev, and #REF) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model). Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior. On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable.",
        "output": "nan:Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments. One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( #REFb ; #TARGET_REF<USE/> ) , and more recently by the LEAF model ( #REF ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Graça, Ganchev, and #REF) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model). Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior. On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable."
    },
    {
        "gold": {
            "text": [
                "Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques.",
                "Traditionally, these techniques have combined extensive syntactic, semantic, and discourse knowledge.",
                "The acquisition of such knowledge is time-consuming, difficult, and error-prone.",
                "Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf.",
                "(#REF), (#REF) (#REF)).",
                "For example, CogNIAC (#REF), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.",
                "For this research , we used a coreference resolution system ( ( #TARGET_REF ) ) that implements different sets of heuristics corresponding to various forms of coreference .",
                "This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g.",
                "subjects of communication verbs are more likely to refer to the last person mentioned in the text).",
                "These constraints are implemented as a set of heuristics ordered by their priority.",
                "Moreover, the COCKTAIL framework uniformly addresses the problem of interaction between different forms of coreference, thus making the extension to multilingual coreference very natural."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques. Traditionally, these techniques have combined extensive syntactic, semantic, and discourse knowledge. The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf. (#REF), (#REF) (#REF)). For example, CogNIAC (#REF), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research , we used a coreference resolution system ( ( #TARGET_REF ) ) that implements different sets of heuristics corresponding to various forms of coreference . This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text). These constraints are implemented as a set of heuristics ordered by their priority. Moreover, the COCKTAIL framework uniformly addresses the problem of interaction between different forms of coreference, thus making the extension to multilingual coreference very natural.",
        "output": "nan:Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques. Traditionally, these techniques have combined extensive syntactic, semantic, and discourse knowledge. The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf. (#REF), (#REF) (#REF)). For example, CogNIAC (#REF), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference. For this research , we used a coreference resolution system ( ( #TARGET_REF<FUT/> ) ) that implements different sets of heuristics corresponding to various forms of coreference . This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text). These constraints are implemented as a set of heuristics ordered by their priority. Moreover, the COCKTAIL framework uniformly addresses the problem of interaction between different forms of coreference, thus making the extension to multilingual coreference very natural."
    },
    {
        "gold": {
            "text": [
                "Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #TARGET_REF ; #REF ; #REF ; #REFa ; #REFb ; #REF ) .",
                "Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the \"meaning of words is entirely given by other words\" (#REFb)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #TARGET_REF ; #REF ; #REF ; #REFa ; #REFb ; #REF ) . Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the \"meaning of words is entirely given by other words\" (#REFb).",
        "output": "introduction:Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #REF ; #REFb ; #TARGET_REF<BACK/> ; #REF ; #REF ; #REFa ; #REFb ; #REF ) . Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the \"meaning of words is entirely given by other words\" (#REFb)."
    },
    {
        "gold": {
            "text": [
                "Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval.",
                "Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #TARGET_REF with WoRDNET relations ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval. Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #TARGET_REF with WoRDNET relations ) .",
        "output": "introduction:Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval. Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #TARGET_REF<USE/> with WoRDNET relations ) ."
    },
    {
        "gold": {
            "text": [
                "We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #TARGET_REF ; #REF ; #REF ; #REF ) for Bangla morphologically complex words .",
                "Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming).",
                "The subjects were asked to make a lexical decision whether the given target is a valid word in that language.",
                "The same target word is again probed but with a different audio or visual probe called the control word.",
                "The control shows no relationship with the target.",
                "For example, baYaska (aged) and baYasa (age) is a prime-target pair, for which the corresponding control-target pair could be naYana (eye) and baYasa (age)."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #TARGET_REF ; #REF ; #REF ; #REF ) for Bangla morphologically complex words . Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming). The subjects were asked to make a lexical decision whether the given target is a valid word in that language. The same target word is again probed but with a different audio or visual probe called the control word. The control shows no relationship with the target. For example, baYaska (aged) and baYasa (age) is a prime-target pair, for which the corresponding control-target pair could be naYana (eye) and baYasa (age).",
        "output": "method:We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #TARGET_REF<FUT/> ; #REF ; #REF ; #REF ) for Bangla morphologically complex words . Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming). The subjects were asked to make a lexical decision whether the given target is a valid word in that language. The same target word is again probed but with a different audio or visual probe called the control word. The control shows no relationship with the target. For example, baYaska (aged) and baYasa (age) is a prime-target pair, for which the corresponding control-target pair could be naYana (eye) and baYasa (age)."
    },
    {
        "gold": {
            "text": [
                "In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £).",
                "In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).",
                "Substitution replaces a substitution node with another initial tree (Figure 3).",
                "Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).",
                "FB-LTAG (#REF;#REF) is an extension of the LTAG formalism.",
                "In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.",
                "Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research #REF).",
                "The XTAG group ( #TARGET_REF ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .",
                "Development of a large-scale French grammar (Abeillé and #REF) has also started at the University of Pennsylvania and is expanded at University of Paris 7."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £). In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ). Substitution replaces a substitution node with another initial tree (Figure 3). Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4). FB-LTAG (#REF;#REF) is an extension of the LTAG formalism. In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research #REF). The XTAG group ( #TARGET_REF ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars . Development of a large-scale French grammar (Abeillé and #REF) has also started at the University of Pennsylvania and is expanded at University of Paris 7.",
        "output": "introduction:In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £). In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ). Substitution replaces a substitution node with another initial tree (Figure 3). Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4). FB-LTAG (#REF;#REF) is an extension of the LTAG formalism. In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node. Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research #REF). The XTAG group ( #TARGET_REF<BACK/> ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars . Development of a large-scale French grammar (Abeillé and #REF) has also started at the University of Pennsylvania and is expanded at University of Paris 7."
    },
    {
        "gold": {
            "text": [
                "In our experiment , we annotated a high number of pairs similar in size to the test sets by #REF and #TARGET_REF .",
                "We used the revised experimental setup (#REF), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.",
                "We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.",
                "Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:In our experiment , we annotated a high number of pairs similar in size to the test sets by #REF and #TARGET_REF . We used the revised experimental setup (#REF), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs. We annotated semantic relatedness instead of similarity and included also non noun-noun pairs. Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure.",
        "output": "related work:In our experiment , we annotated a high number of pairs similar in size to the test sets by #REF and #TARGET_REF<USE/> . We used the revised experimental setup (#REF), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs. We annotated semantic relatedness instead of similarity and included also non noun-noun pairs. Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure."
    },
    {
        "gold": {
            "text": [
                "where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #TARGET_REF ) .",
        "output": "nan:where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( #REF ) and Cross-document Coreference ( CDC ) ( #TARGET_REF ) .",
                "Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.",
                "It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task -Web People Search -on its own (#REF;#REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( #REF ) and Cross-document Coreference ( CDC ) ( #TARGET_REF ) . Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature. It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task -Web People Search -on its own (#REF;#REF).",
        "output": "related work:The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( #REF ) and Cross-document Coreference ( CDC ) ( #TARGET_REF<BACK/> ) . Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature. It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task -Web People Search -on its own (#REF;#REF)."
    },
    {
        "gold": {
            "text": [
                "It is interesting to compare this analysis with that described in Dalrymple , Shieber , and #REF and #TARGET_REF , 1991 ) .",
                "Recall that in their treatment, quantified noun phrases are treated in two stages: firstly, what they call a \"free variable\" of type e is introduced in the NP position, with an associated \"quantifier assumption,\" which is added as a kind of premise.",
                "At a later stage the quantifier assumption is \"discharged,\" capturing all occurrences of the free variable.",
                "Thus their analysis of something like every manager disappeared would proceed as follows:"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:It is interesting to compare this analysis with that described in Dalrymple , Shieber , and #REF and #TARGET_REF , 1991 ) . Recall that in their treatment, quantified noun phrases are treated in two stages: firstly, what they call a \"free variable\" of type e is introduced in the NP position, with an associated \"quantifier assumption,\" which is added as a kind of premise. At a later stage the quantifier assumption is \"discharged,\" capturing all occurrences of the free variable. Thus their analysis of something like every manager disappeared would proceed as follows:",
        "output": "nan:It is interesting to compare this analysis with that described in Dalrymple , Shieber , and #REF and #TARGET_REF<USE/> , 1991 ) . Recall that in their treatment, quantified noun phrases are treated in two stages: firstly, what they call a \"free variable\" of type e is introduced in the NP position, with an associated \"quantifier assumption,\" which is added as a kind of premise. At a later stage the quantifier assumption is \"discharged,\" capturing all occurrences of the free variable. Thus their analysis of something like every manager disappeared would proceed as follows:"
    },
    {
        "gold": {
            "text": [
                "4To prove ( 1 ) â ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( #TARGET_REF ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp .",
                "A full proof is straightforward, as are proofs of (3)_(2), (2)_(1)."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:4To prove ( 1 ) â ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( #TARGET_REF ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp . A full proof is straightforward, as are proofs of (3)_(2), (2)_(1).",
        "output": "introduction:4To prove ( 1 ) â ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( #TARGET_REF<FUT/> ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp . A full proof is straightforward, as are proofs of (3)_(2), (2)_(1)."
    },
    {
        "gold": {
            "text": [
                "â¢ language learning ( #REF ; #REF ; Morgan , Meier , and #REF ) â¢ monolingual grammar induction ( #TARGET_REF ) â¢ grammar optimization ( #REF ) â¢ insights into universal grammar ( #REF ) â¢ machine translation ( #REF , 1997 ; #REF ; Gough , Way , and #REF )"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:â¢ language learning ( #REF ; #REF ; Morgan , Meier , and #REF ) â¢ monolingual grammar induction ( #TARGET_REF ) â¢ grammar optimization ( #REF ) â¢ insights into universal grammar ( #REF ) â¢ machine translation ( #REF , 1997 ; #REF ; Gough , Way , and #REF )",
        "output": "introduction:â¢ language learning ( #REF ; #REF ; Morgan , Meier , and #REF ) â¢ monolingual grammar induction ( #TARGET_REF<BACK/> ) â¢ grammar optimization ( #REF ) â¢ insights into universal grammar ( #REF ) â¢ machine translation ( #REF , 1997 ; #REF ; Gough , Way , and #REF )"
    },
    {
        "gold": {
            "text": [
                "At this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others.",
                "We are going to make such a comparison with the theories proposed by J. #TARGET_REF , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. #REF , who are more interested in addressing psychological and cognitive aspects of discourse coherence .",
                "The quoted works seem to be good representatives for each of the directions; they also point to related literature."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:At this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others. We are going to make such a comparison with the theories proposed by J. #TARGET_REF , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. #REF , who are more interested in addressing psychological and cognitive aspects of discourse coherence . The quoted works seem to be good representatives for each of the directions; they also point to related literature.",
        "output": "introduction:At this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others. We are going to make such a comparison with the theories proposed by J. #TARGET_REF<USE/> , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. #REF , who are more interested in addressing psychological and cognitive aspects of discourse coherence . The quoted works seem to be good representatives for each of the directions; they also point to related literature."
    },
    {
        "gold": {
            "text": [
                "Coreference resolution has been extensively studied, with several state-of-the-art approaches addressing this task (#REF;#REF;Björkelund and #REF;#REF).",
                "Many of the early rule-based systems like #REF and #REF gained considerable popularity.",
                "The early designs were easy to understand and the rules were designed manually.",
                "Machine learning approaches were introduced in many works (#REF;#REF;#REF;#REF).",
                "The introduction of ILP methods has influenced the coreference area too #REF).",
                "In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #TARGET_REF in our experiments ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:Coreference resolution has been extensively studied, with several state-of-the-art approaches addressing this task (#REF;#REF;Björkelund and #REF;#REF). Many of the early rule-based systems like #REF and #REF gained considerable popularity. The early designs were easy to understand and the rules were designed manually. Machine learning approaches were introduced in many works (#REF;#REF;#REF;#REF). The introduction of ILP methods has influenced the coreference area too #REF). In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #TARGET_REF in our experiments .",
        "output": "related work:Coreference resolution has been extensively studied, with several state-of-the-art approaches addressing this task (#REF;#REF;Björkelund and #REF;#REF). Many of the early rule-based systems like #REF and #REF gained considerable popularity. The early designs were easy to understand and the rules were designed manually. Machine learning approaches were introduced in many works (#REF;#REF;#REF;#REF). The introduction of ILP methods has influenced the coreference area too #REF). In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #TARGET_REF<FUT/> in our experiments ."
    },
    {
        "gold": {
            "text": [
                "A number of applications have relied on distributional analysis ( #TARGET_REF ) in order to build classes of semantically related terms .",
                "This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (#REF, for example), does not specify the relationship itself.",
                "Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:A number of applications have relied on distributional analysis ( #TARGET_REF ) in order to build classes of semantically related terms . This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (#REF, for example), does not specify the relationship itself. Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated.",
        "output": "related work:A number of applications have relied on distributional analysis ( #TARGET_REF<BACK/> ) in order to build classes of semantically related terms . This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (#REF, for example), does not specify the relationship itself. Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated."
    },
    {
        "gold": {
            "text": [
                "Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.",
                "The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (#REF).",
                "#REF employed a Bayesian method to learn discontinuous SCFG rules.",
                "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.",
                "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.",
                "#TARGET_REF and #REF focused on joint parsing and alignment .",
                "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.",
                "adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.",
                "#REF re-trained the linguistic parsers bilingually based on word alignment.",
                "#REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.",
                "Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (#REF). #REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #TARGET_REF and #REF focused on joint parsing and alignment . They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #REF re-trained the linguistic parsers bilingually based on word alignment. #REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.",
        "output": "related work:Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (#REF). #REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #TARGET_REF<USE/> and #REF focused on joint parsing and alignment . They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #REF re-trained the linguistic parsers bilingually based on word alignment. #REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models."
    },
    {
        "gold": {
            "text": [
                "In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .",
                "We run TreeTagger (#REF) for tokenization, and used the Giza++ (#REF) to align the tokenized corpora at the word level.",
                "Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #TARGET_REF ) .",
                "Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.",
                "In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.",
                "The resulting 1 http://www.statmt.org/wmt10/",
                "phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 . We run TreeTagger (#REF) for tokenization, and used the Giza++ (#REF) to align the tokenized corpora at the word level. Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #TARGET_REF ) . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1 http://www.statmt.org/wmt10/ phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.",
        "output": "nan:In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 . We run TreeTagger (#REF) for tokenization, and used the Giza++ (#REF) to align the tokenized corpora at the word level. Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #TARGET_REF<FUT/> ) . Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1 http://www.statmt.org/wmt10/ phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase."
    },
    {
        "gold": {
            "text": [
                "Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.",
                "#TARGET_REF obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #REF examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .",
                "#REF and #REF study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (#REF) and find correlations between the various modalities both within and across speakers.",
                "Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (#REF;#REF;#REF;#REF).",
                "Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.",
                "In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. #TARGET_REF obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #REF examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues . #REF and #REF study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (#REF) and find correlations between the various modalities both within and across speakers. Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (#REF;#REF;#REF;#REF). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.",
        "output": "introduction:Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. #TARGET_REF<BACK/> obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #REF examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues . #REF and #REF study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (#REF) and find correlations between the various modalities both within and across speakers. Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (#REF;#REF;#REF;#REF). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features."
    },
    {
        "gold": {
            "text": [
                "There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and #REF ; #TARGET_REF ; #REF ; Malik , Subramaniam , and #REF ) .",
                "eResponder, the system developed by Carmel, Shtalhaim, and #REF, retrieves a list of request-response pairs and presents a ranked list of responses to the user.",
                "If the user is unsatisfied with this list, an operator is asked to generate a new response.",
                "The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses.",
                "However, there is no attempt to automatically generate a single response.",
                "#REF compared the performance of document retrieval and document prediction for generating help-desk responses.",
                "Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs.",
                "Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes.",
                "The generated response is the answer that is closest to the centroid of the cluster."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and #REF ; #TARGET_REF ; #REF ; Malik , Subramaniam , and #REF ) . eResponder, the system developed by Carmel, Shtalhaim, and #REF, retrieves a list of request-response pairs and presents a ranked list of responses to the user. If the user is unsatisfied with this list, an operator is asked to generate a new response. The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses. However, there is no attempt to automatically generate a single response. #REF compared the performance of document retrieval and document prediction for generating help-desk responses. Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs. Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes. The generated response is the answer that is closest to the centroid of the cluster.",
        "output": "nan:There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and #REF ; #TARGET_REF<USE/> ; #REF ; Malik , Subramaniam , and #REF ) . eResponder, the system developed by Carmel, Shtalhaim, and #REF, retrieves a list of request-response pairs and presents a ranked list of responses to the user. If the user is unsatisfied with this list, an operator is asked to generate a new response. The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses. However, there is no attempt to automatically generate a single response. #REF compared the performance of document retrieval and document prediction for generating help-desk responses. Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs. Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes. The generated response is the answer that is closest to the centroid of the cluster."
    },
    {
        "gold": {
            "text": [
                "These edges are then used as weak supervision when training a generative or discriminative dependency parser.",
                "In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side.",
                "By changing the threshold in MBR decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges.",
                "We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and #REF) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish.",
                "Following Ganchev, Gillenwater, and #REF, we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).",
                "results are based on a corpus of movie subtitles ( #REF ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( #TARGET_REF ) .",
                "We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:These edges are then used as weak supervision when training a generative or discriminative dependency parser. In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side. By changing the threshold in MBR decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges. We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and #REF) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish. Following Ganchev, Gillenwater, and #REF, we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es). results are based on a corpus of movie subtitles ( #REF ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( #TARGET_REF ) . We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.",
        "output": "nan:These edges are then used as weak supervision when training a generative or discriminative dependency parser. In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side. By changing the threshold in MBR decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges. We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and #REF) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish. Following Ganchev, Gillenwater, and #REF, we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es). results are based on a corpus of movie subtitles ( #REF ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( #TARGET_REF<FUT/> ) . We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM."
    },
    {
        "gold": {
            "text": [
                "Many different features have been used to represent documents where an ambiguous name is mentioned.",
                "The most basic is a Bag of Words (BoW) representation of the document text.",
                "Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #TARGET_REF ; #REF ) .",
                "Nevertheless, the full document text is present in most systems, sometimes as the only feature (#REF) and sometimes in combination with otherssee for instance (#REF;#REF)-.",
                "Other representations use the link structure (#REF) or generate graph representations of the extracted features (#REF).",
                "Some researchers (#REF;#REF) have explored the use of Wikipedia information to improve the disambiguation process.",
                "Wikipedia provides candidate entities that are linked to specific mentions in a text.",
                "The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.",
                "These approaches are yet to be applied to the specific task of grouping search results."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #TARGET_REF ; #REF ) . Nevertheless, the full document text is present in most systems, sometimes as the only feature (#REF) and sometimes in combination with otherssee for instance (#REF;#REF)-. Other representations use the link structure (#REF) or generate graph representations of the extracted features (#REF). Some researchers (#REF;#REF) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results.",
        "output": "related work:Many different features have been used to represent documents where an ambiguous name is mentioned. The most basic is a Bag of Words (BoW) representation of the document text. Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #TARGET_REF<BACK/> ; #REF ) . Nevertheless, the full document text is present in most systems, sometimes as the only feature (#REF) and sometimes in combination with otherssee for instance (#REF;#REF)-. Other representations use the link structure (#REF) or generate graph representations of the extracted features (#REF). Some researchers (#REF;#REF) have explored the use of Wikipedia information to improve the disambiguation process. Wikipedia provides candidate entities that are linked to specific mentions in a text. The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way. These approaches are yet to be applied to the specific task of grouping search results."
    },
    {
        "gold": {
            "text": [
                "A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #REF , #REF , #REF , #REF , #TARGET_REF , #REF , and #REF ) .",
                "Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.",
                "While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.",
                "A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #REF , #REF , #REF , #REF , #TARGET_REF , #REF , and #REF ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF.",
        "output": "nan:A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #REF , #REF , #REF , #REF , #TARGET_REF<USE/> , #REF , and #REF ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF."
    },
    {
        "gold": {
            "text": [
                "As stated herein, we studied two document-based methods: Document Retrieval and Document Prediction.",
                "(Doc-Ret).",
                "This method follows a traditional Information Retrieval paradigm ( #TARGET_REF ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .",
                "In our case, the query is a new request e-mail to be addressed by the system, and we have considered three views of the documents in the corpus:"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:As stated herein, we studied two document-based methods: Document Retrieval and Document Prediction. (Doc-Ret). This method follows a traditional Information Retrieval paradigm ( #TARGET_REF ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query . In our case, the query is a new request e-mail to be addressed by the system, and we have considered three views of the documents in the corpus:",
        "output": "method:As stated herein, we studied two document-based methods: Document Retrieval and Document Prediction. (Doc-Ret). This method follows a traditional Information Retrieval paradigm ( #TARGET_REF<FUT/> ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query . In our case, the query is a new request e-mail to be addressed by the system, and we have considered three views of the documents in the corpus:"
    },
    {
        "gold": {
            "text": [
                "Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #REF.",
                "Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (#REF;#REF).",
                "It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #TARGET_REF ) .",
                "These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (#REF).",
                "However, prior work using mLDA is limited to two modalities at a time.",
                "In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "method:Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #REF. Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (#REF;#REF). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #TARGET_REF ) . These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (#REF). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities.",
        "output": "method:Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #REF. Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (#REF;#REF). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #TARGET_REF<BACK/> ) . These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (#REF). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities."
    },
    {
        "gold": {
            "text": [
                "This required mapping is a significant problem for generaliza- tion.",
                "We view this issue as a different sort of prob- lem entirely, one of word-formation (rather than inflection).",
                "We apply a �split in preprocessing and resynthesize in postprocessing� approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al.",
                "The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.",
                "There has been other work on solving inflection.",
                "#REF introduced factored SMT.",
                "We use more complex context features.",
                "#REF tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to in- flected forms.",
                "#REF improved on this by marking prepositions with the case they mark (one of the most important markups in our system).",
                "Both efforts were ineffective on large data sets.",
                "#TARGET_REF used unification in an SMT system to model some of the agreement phenomena that we model.",
                "Our CRF framework allows us to use more complex con- text features."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:This required mapping is a significant problem for generaliza- tion. We view this issue as a different sort of prob- lem entirely, one of word-formation (rather than inflection). We apply a �split in preprocessing and resynthesize in postprocessing� approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al. The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. #REF introduced factored SMT. We use more complex context features. #REF tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to in- flected forms. #REF improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. #TARGET_REF used unification in an SMT system to model some of the agreement phenomena that we model. Our CRF framework allows us to use more complex con- text features.",
        "output": "related work:This required mapping is a significant problem for generaliza- tion. We view this issue as a different sort of prob- lem entirely, one of word-formation (rather than inflection). We apply a �split in preprocessing and resynthesize in postprocessing� approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al. The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. #REF introduced factored SMT. We use more complex context features. #REF tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to in- flected forms. #REF improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. #TARGET_REF<USE/> used unification in an SMT system to model some of the agreement phenomena that we model. Our CRF framework allows us to use more complex con- text features."
    },
    {
        "gold": {
            "text": [
                "Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.",
                "We go beyond previous work, however, and explore additional lexical and inflectional features.",
                "Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and #REF;Nivre, Boguslavsky, and #REF;).",
                "Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.",
                "Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #TARGET_REF ) ( Section 6 ) .",
                "#REF have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.",
                "These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing. We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and #REF;Nivre, Boguslavsky, and #REF;). Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #TARGET_REF ) ( Section 6 ) . #REF have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor. These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.",
        "output": "related work:Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing. We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and #REF;Nivre, Boguslavsky, and #REF;). Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #TARGET_REF<FUT/> ) ( Section 6 ) . #REF have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor. These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs."
    },
    {
        "gold": {
            "text": [
                "Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #REF ; #REF ; #REF ; #TARGET_REF ) , which was difficult both to represent and to process , and which required considerable human input .",
                "However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.",
                "A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ("
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #REF ; #REF ; #REF ; #TARGET_REF ) , which was difficult both to represent and to process , and which required considerable human input . However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (",
        "output": "nan:Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #REF ; #REF ; #REF ; #TARGET_REF<BACK/> ) , which was difficult both to represent and to process , and which required considerable human input . However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ("
    },
    {
        "gold": {
            "text": [
                "Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained.",
                "#TARGET_REF has made the first attempt working on the single semantic role level to make further improvement .",
                "However, the impact of this idea is limited due to that the amount of the research target, ARG2, is few in PropBank.",
                "What if we could extend the idea of hierarchical architecture to the single semantic role level?",
                "Would that help the improvement of SRC?"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained. #TARGET_REF has made the first attempt working on the single semantic role level to make further improvement . However, the impact of this idea is limited due to that the amount of the research target, ARG2, is few in PropBank. What if we could extend the idea of hierarchical architecture to the single semantic role level? Would that help the improvement of SRC?",
        "output": "conclusion:Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained. #TARGET_REF<USE/> has made the first attempt working on the single semantic role level to make further improvement . However, the impact of this idea is limited due to that the amount of the research target, ARG2, is few in PropBank. What if we could extend the idea of hierarchical architecture to the single semantic role level? Would that help the improvement of SRC?"
    },
    {
        "gold": {
            "text": [
                "The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #TARGET_REF ; #REFa ) .",
                "We tested the covariation approach with a complex grammar implementing an HPSG analysis covering the so-called aux-flip phenomenon, and partial-VP topicalization in the three clause types of German (Hinrichs, Meurers, and #REF).",
                "This test grammar includes eight lexical rules; some serve syntactic purposes, like the Partial-VP Topicalization Lexical Rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form.",
                "Our compiler distinguished seven word classes.",
                "Some nouns and most verbal lexical entries fed lexical rules, and a single base lexical entry resulted in up to 12 derivations."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #TARGET_REF ; #REFa ) . We tested the covariation approach with a complex grammar implementing an HPSG analysis covering the so-called aux-flip phenomenon, and partial-VP topicalization in the three clause types of German (Hinrichs, Meurers, and #REF). This test grammar includes eight lexical rules; some serve syntactic purposes, like the Partial-VP Topicalization Lexical Rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form. Our compiler distinguished seven word classes. Some nouns and most verbal lexical entries fed lexical rules, and a single base lexical entry resulted in up to 12 derivations.",
        "output": "nan:The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #TARGET_REF<FUT/> ; #REFa ) . We tested the covariation approach with a complex grammar implementing an HPSG analysis covering the so-called aux-flip phenomenon, and partial-VP topicalization in the three clause types of German (Hinrichs, Meurers, and #REF). This test grammar includes eight lexical rules; some serve syntactic purposes, like the Partial-VP Topicalization Lexical Rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form. Our compiler distinguished seven word classes. Some nouns and most verbal lexical entries fed lexical rules, and a single base lexical entry resulted in up to 12 derivations."
    },
    {
        "gold": {
            "text": [
                "Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.",
                "Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #REF ) , neural networks ( #TARGET_REF ) , and maximum-entropy modeling ( #REF ) .",
                "Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign.",
                "Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.",
                "This implies an investment in the data annotation phase."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #REF ) , neural networks ( #TARGET_REF ) , and maximum-entropy modeling ( #REF ) . Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign. Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training. This implies an investment in the data annotation phase.",
        "output": "nan:Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #REF ) , neural networks ( #TARGET_REF<BACK/> ) , and maximum-entropy modeling ( #REF ) . Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign. Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training. This implies an investment in the data annotation phase."
    },
    {
        "gold": {
            "text": [
                "In this paper, we have introduced the novel tasks of social event detection and classification.",
                "We show that data sampling techniques play a crucial role for the task of relation detection.",
                "Through oversampling we achieve an increase in F1-measure of 22.2% absolute over a baseline system.",
                "Our experiments show that as a result of how language expresses the relevant information, dependency-based structures are best suited for encoding this information.",
                "Furthermore, because of the complexity of the task, a combination of phrase based structures and dependency-based structures perform the best.",
                "This revalidates the observation of #TARGET_REF that phrase structure representations and dependency representations add complimentary value to the learning task .",
                "We also introduced a new sequence structure (SqGRW) which plays a role in achieving the best accuracy for both, social event detection and social event classification tasks."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:In this paper, we have introduced the novel tasks of social event detection and classification. We show that data sampling techniques play a crucial role for the task of relation detection. Through oversampling we achieve an increase in F1-measure of 22.2% absolute over a baseline system. Our experiments show that as a result of how language expresses the relevant information, dependency-based structures are best suited for encoding this information. Furthermore, because of the complexity of the task, a combination of phrase based structures and dependency-based structures perform the best. This revalidates the observation of #TARGET_REF that phrase structure representations and dependency representations add complimentary value to the learning task . We also introduced a new sequence structure (SqGRW) which plays a role in achieving the best accuracy for both, social event detection and social event classification tasks.",
        "output": "conclusion:In this paper, we have introduced the novel tasks of social event detection and classification. We show that data sampling techniques play a crucial role for the task of relation detection. Through oversampling we achieve an increase in F1-measure of 22.2% absolute over a baseline system. Our experiments show that as a result of how language expresses the relevant information, dependency-based structures are best suited for encoding this information. Furthermore, because of the complexity of the task, a combination of phrase based structures and dependency-based structures perform the best. This revalidates the observation of #TARGET_REF<USE/> that phrase structure representations and dependency representations add complimentary value to the learning task . We also introduced a new sequence structure (SqGRW) which plays a role in achieving the best accuracy for both, social event detection and social event classification tasks."
    },
    {
        "gold": {
            "text": [
                "We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.",
                "PropBank 1.0 includes the annotations for files chtb_001.fid",
                "to chtb_931.fid,",
                "or the first 250K words of the Chinese TreeBank 5.1.",
                "For the experiments, the data of PropBank is divided into three parts.",
                "648 files (from chtb_081 to chtb_899.fid)",
                "are used as the training set.",
                "The development set includes 40 files, from chtb_041.fid to chtb_080.fid.",
                "The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931.",
                "We use the same data setting with #TARGET_REF , however a bit different from #REF ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments. PropBank 1.0 includes the annotations for files chtb_001.fid to chtb_931.fid, or the first 250K words of the Chinese TreeBank 5.1. For the experiments, the data of PropBank is divided into three parts. 648 files (from chtb_081 to chtb_899.fid) are used as the training set. The development set includes 40 files, from chtb_041.fid to chtb_080.fid. The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931. We use the same data setting with #TARGET_REF , however a bit different from #REF .",
        "output": "experiments:We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments. PropBank 1.0 includes the annotations for files chtb_001.fid to chtb_931.fid, or the first 250K words of the Chinese TreeBank 5.1. For the experiments, the data of PropBank is divided into three parts. 648 files (from chtb_081 to chtb_899.fid) are used as the training set. The development set includes 40 files, from chtb_041.fid to chtb_080.fid. The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931. We use the same data setting with #TARGET_REF<FUT/> , however a bit different from #REF ."
    },
    {
        "gold": {
            "text": [
                "Shallow parsing is studied as an alternative to full-sentence parsing.",
                "Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (#REF;#REF;#REF).",
                "A lot of recent work on shallow parsing has been influenced by Abney's work (#REF), who has suggested to \"chunk\" sentences to base level phrases.",
                "For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September .\"",
                "would be chunked as follows (Tjong Kim #REF): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.",
                "Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #TARGET_REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (#REF;#REF;#REF). A lot of recent work on shallow parsing has been influenced by Abney's work (#REF), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September .\" would be chunked as follows (Tjong Kim #REF): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #TARGET_REF ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) .",
        "output": "introduction:Shallow parsing is studied as an alternative to full-sentence parsing. Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (#REF;#REF;#REF). A lot of recent work on shallow parsing has been influenced by Abney's work (#REF), who has suggested to \"chunk\" sentences to base level phrases. For example, the sentence \"He reckons the current account deficit will narrow to only $ 1.8 billion in September .\" would be chunked as follows (Tjong Kim #REF): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information. Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #REF ; #REFa ; #REFb ; #TARGET_REF<BACK/> ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; Tjong Kim #REF ) ."
    },
    {
        "gold": {
            "text": [
                "The system was trained on the Penn Treebank ( #REF ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #REF , #TARGET_REF , and #REF , and became a common testbed ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The system was trained on the Penn Treebank ( #REF ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #REF , #TARGET_REF , and #REF , and became a common testbed .",
        "output": "nan:The system was trained on the Penn Treebank ( #REF ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #REF , #TARGET_REF<USE/> , and #REF , and became a common testbed ."
    },
    {
        "gold": {
            "text": [
                "Corpus frequency : ( #TARGET_REF ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency .",
                "His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms.",
                "Our corpus frequency variable specifies the frequency of each unknown word in a 2.6 million word corpus of business news closed captions."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:Corpus frequency : ( #TARGET_REF ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency . His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms. Our corpus frequency variable specifies the frequency of each unknown word in a 2.6 million word corpus of business news closed captions.",
        "output": "experiments:Corpus frequency : ( #TARGET_REF<FUT/> ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency . His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms. Our corpus frequency variable specifies the frequency of each unknown word in a 2.6 million word corpus of business news closed captions."
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:#TARGET_REF",
        "output": "related work:#TARGET_REF<BACK/>"
    },
    {
        "gold": {
            "text": [
                "Several works have proposed discriminative tech- niques to train log-linear model for SMT.",
                "(#REF; #REF) used maximum likelihood estimation to learn weights for MT.",
                "( #REF ; #TARGET_REF ; #REF ; #REF ) employed an evaluation metric as a loss function and directly optimized it .",
                "(#REF; #REF; #REF) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Several works have proposed discriminative tech- niques to train log-linear model for SMT. (#REF; #REF) used maximum likelihood estimation to learn weights for MT. ( #REF ; #TARGET_REF ; #REF ; #REF ) employed an evaluation metric as a loss function and directly optimized it . (#REF; #REF; #REF) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.",
        "output": "related work:Several works have proposed discriminative tech- niques to train log-linear model for SMT. (#REF; #REF) used maximum likelihood estimation to learn weights for MT. ( #REF ; #TARGET_REF<USE/> ; #REF ; #REF ) employed an evaluation metric as a loss function and directly optimized it . (#REF; #REF; #REF) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions."
    },
    {
        "gold": {
            "text": [
                "As already mentioned, all words in DanPASS are phonetically and prosodically annotated.",
                "In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both.",
                "For this study, we added semantic labels -including dialogue acts -and gesture annotation.",
                "Both kinds of annotation were carried out using ANVIL ( #TARGET_REF ) .",
                "To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources.",
                "This subset comprises the categories Accept, Decline, RepeatRephrase and Answer.",
                "Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant.",
                "Finally, the two turn management categories Turn-Take and TurnElicit were also coded."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:As already mentioned, all words in DanPASS are phonetically and prosodically annotated. In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both. For this study, we added semantic labels -including dialogue acts -and gesture annotation. Both kinds of annotation were carried out using ANVIL ( #TARGET_REF ) . To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources. This subset comprises the categories Accept, Decline, RepeatRephrase and Answer. Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant. Finally, the two turn management categories Turn-Take and TurnElicit were also coded.",
        "output": "introduction:As already mentioned, all words in DanPASS are phonetically and prosodically annotated. In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both. For this study, we added semantic labels -including dialogue acts -and gesture annotation. Both kinds of annotation were carried out using ANVIL ( #TARGET_REF<FUT/> ) . To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources. This subset comprises the categories Accept, Decline, RepeatRephrase and Answer. Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant. Finally, the two turn management categories Turn-Take and TurnElicit were also coded."
    },
    {
        "gold": {
            "text": [
                "Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.",
                "In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.",
                "Some examples include text categorization ( #TARGET_REF ) , base noun phrase chunking ( #REF ) , part-of-speech tagging ( Engelson #REF ) , spelling confusion set disambiguation ( #REF ) , and word sense disambiguation ( #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing. In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications. Some examples include text categorization ( #TARGET_REF ) , base noun phrase chunking ( #REF ) , part-of-speech tagging ( Engelson #REF ) , spelling confusion set disambiguation ( #REF ) , and word sense disambiguation ( #REF ) .",
        "output": "related work:Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing. In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications. Some examples include text categorization ( #TARGET_REF<BACK/> ) , base noun phrase chunking ( #REF ) , part-of-speech tagging ( Engelson #REF ) , spelling confusion set disambiguation ( #REF ) , and word sense disambiguation ( #REF ) ."
    },
    {
        "gold": {
            "text": [
                "Thus, it is important that we make a best effort to find a parse.",
                "To accomplish this, we implemented a parsing backoff strategy.",
                "The parser first tries to find a valid parse that has either s dcl or np at its root.",
                "If that fails, then it searches for a parse with any root.",
                "If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without).",
                "This is similar to the \"deletion\" strategy employed by #TARGET_REF , but we do it directly in the grammar .",
                "We add unary rules of the form D →u for every potential supertag u in the tree.",
                "Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t→ D , v and t→ v, D .",
                "Recall that in §3.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary.",
                "Additionally, since u will still remain as the preterminal, it will be the category examined as the context by adjacent constituents."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Thus, it is important that we make a best effort to find a parse. To accomplish this, we implemented a parsing backoff strategy. The parser first tries to find a valid parse that has either s dcl or np at its root. If that fails, then it searches for a parse with any root. If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the \"deletion\" strategy employed by #TARGET_REF , but we do it directly in the grammar . We add unary rules of the form D →u for every potential supertag u in the tree. Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t→ D , v and t→ v, D . Recall that in §3.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary. Additionally, since u will still remain as the preterminal, it will be the category examined as the context by adjacent constituents.",
        "output": "experiments:Thus, it is important that we make a best effort to find a parse. To accomplish this, we implemented a parsing backoff strategy. The parser first tries to find a valid parse that has either s dcl or np at its root. If that fails, then it searches for a parse with any root. If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the \"deletion\" strategy employed by #TARGET_REF<USE/> , but we do it directly in the grammar . We add unary rules of the form D →u for every potential supertag u in the tree. Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t→ D , v and t→ v, D . Recall that in §3.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary. Additionally, since u will still remain as the preterminal, it will be the category examined as the context by adjacent constituents."
    },
    {
        "gold": {
            "text": [
                "The expectation parser uses an ATN-like representation for its grammar ( #TARGET_REF ) .",
                "Its strategy is top-down.",
                "The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conjunctions (#REF).",
                "An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses.",
                "Sentences have the same \"meaning\" if they \"result in identical tasks being performed.",
                "The various sentence structures that"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The expectation parser uses an ATN-like representation for its grammar ( #TARGET_REF ) . Its strategy is top-down. The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conjunctions (#REF). An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses. Sentences have the same \"meaning\" if they \"result in identical tasks being performed. The various sentence structures that",
        "output": "experiments:The expectation parser uses an ATN-like representation for its grammar ( #TARGET_REF<FUT/> ) . Its strategy is top-down. The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conjunctions (#REF). An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses. Sentences have the same \"meaning\" if they \"result in identical tasks being performed. The various sentence structures that"
    },
    {
        "gold": {
            "text": [
                "In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #REF : 56 ) unless the web is used as a corpus ( #TARGET_REF ) .",
                "Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (#REF).",
                "This topic generated intense interest at workshops held at the University of Heidelberg (#REF), University of Bologna (#REF), University of Birmingham (#REF) and now in Trento in #REF.",
                "In addition, the advantages of using linguistically annotated data over raw data are well documented (#REF;#REF).",
                "As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.",
                "Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #REF : 56 ) unless the web is used as a corpus ( #TARGET_REF ) . Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (#REF). This topic generated intense interest at workshops held at the University of Heidelberg (#REF), University of Bologna (#REF), University of Birmingham (#REF) and now in Trento in #REF. In addition, the advantages of using linguistically annotated data over raw data are well documented (#REF;#REF). As the size of a corpus increases, a near linear increase in computing power is required to annotate the text. Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.",
        "output": "introduction:In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #REF : 56 ) unless the web is used as a corpus ( #TARGET_REF<BACK/> ) . Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (#REF). This topic generated intense interest at workshops held at the University of Heidelberg (#REF), University of Bologna (#REF), University of Birmingham (#REF) and now in Trento in #REF. In addition, the advantages of using linguistically annotated data over raw data are well documented (#REF;#REF). As the size of a corpus increases, a near linear increase in computing power is required to annotate the text. Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus."
    },
    {
        "gold": {
            "text": [
                "This section has given an overview of the approach to history-based expectation processing.",
                "The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented.",
                "The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve.",
                "The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.",
                "An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (#REF, Biermann and#REF).",
                "The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.",
                "The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, #REF).",
                "[ The current system should be distinguished from an earlier voice system ( VNLC , #TARGET_REF ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:This section has given an overview of the approach to history-based expectation processing. The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented. The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (#REF, Biermann and#REF). The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, #REF). [ The current system should be distinguished from an earlier voice system ( VNLC , #TARGET_REF ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]",
        "output": "experiments:This section has given an overview of the approach to history-based expectation processing. The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented. The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (#REF, Biermann and#REF). The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, #REF). [ The current system should be distinguished from an earlier voice system ( VNLC , #TARGET_REF<USE/> ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]"
    },
    {
        "gold": {
            "text": [
                "2The WePS-1 corpus includes data from the Web03 testbed ( #TARGET_REF ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:2The WePS-1 corpus includes data from the Web03 testbed ( #TARGET_REF ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable .",
        "output": "experiments:2The WePS-1 corpus includes data from the Web03 testbed ( #TARGET_REF<FUT/> ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable ."
    },
    {
        "gold": {
            "text": [
                "The idea of parallelized formalisms is widely used one, and one which has been applied in many different ways.",
                "The use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.",
                "For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeillé et al.,1990), (Harbusch & #REF), or for relating a syntactic TAG and semantic one for the same language (Shieber & #REF).",
                "S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & #REF) to characterize correspondences between tree adjoining languages.",
                "Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.",
                "It allows the construction of a non-TAL ( #REF ) , ( #TARGET_REF ) .",
                "As a result, #REF propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.",
                "In this case only TAL can be formed in each component.",
                "This isomorphism requirement is formally attractive, but for practical applications somewhat too strict."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:The idea of parallelized formalisms is widely used one, and one which has been applied in many different ways. The use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures. For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeillé et al.,1990), (Harbusch & #REF), or for relating a syntactic TAG and semantic one for the same language (Shieber & #REF). S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & #REF) to characterize correspondences between tree adjoining languages. Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( #REF ) , ( #TARGET_REF ) . As a result, #REF propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component. This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.",
        "output": "nan:The idea of parallelized formalisms is widely used one, and one which has been applied in many different ways. The use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures. For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeillé et al.,1990), (Harbusch & #REF), or for relating a syntactic TAG and semantic one for the same language (Shieber & #REF). S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & #REF) to characterize correspondences between tree adjoining languages. Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages. It allows the construction of a non-TAL ( #REF ) , ( #TARGET_REF<BACK/> ) . As a result, #REF propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG. In this case only TAL can be formed in each component. This isomorphism requirement is formally attractive, but for practical applications somewhat too strict."
    },
    {
        "gold": {
            "text": [
                "Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.",
                "For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #TARGET_REF ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #REF ) .",
                "Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.",
                "To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.",
                "In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.",
                "The construction and annotation of the paraphrases reflects the paraphrase typology."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #TARGET_REF ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #REF ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics. The construction and annotation of the paraphrases reflects the paraphrase typology.",
        "output": "nan:Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #TARGET_REF<USE/> ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #REF ) . Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics. The construction and annotation of the paraphrases reflects the paraphrase typology."
    },
    {
        "gold": {
            "text": [
                "We use an in-house developed hierarchical phrase-based translation ( #TARGET_REF ) as our baseline system , and we denote it as In-Hiero .",
                "To obtain satisfactory baseline performance, we tune In-Hiero system for 5 times using MERT, and then se-    (#REF).",
                "Both of these systems are with default setting.",
                "All three systems are trained by MERT with 100 best candidates."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We use an in-house developed hierarchical phrase-based translation ( #TARGET_REF ) as our baseline system , and we denote it as In-Hiero . To obtain satisfactory baseline performance, we tune In-Hiero system for 5 times using MERT, and then se-    (#REF). Both of these systems are with default setting. All three systems are trained by MERT with 100 best candidates.",
        "output": "experiments:We use an in-house developed hierarchical phrase-based translation ( #TARGET_REF<FUT/> ) as our baseline system , and we denote it as In-Hiero . To obtain satisfactory baseline performance, we tune In-Hiero system for 5 times using MERT, and then se-    (#REF). Both of these systems are with default setting. All three systems are trained by MERT with 100 best candidates."
    },
    {
        "gold": {
            "text": [
                "9.3 Multidimensionality 9.3.1 Combinations of Adjectives.",
                "When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #TARGET_REF ) .",
                "Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective.",
                "The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., #REF) was used as the sole criterion: Formally, an object r ∈ C has a Pareto-optimal combination of Values V iff there is no other x ∈ C such that 1. ∃V i ∈ V : V i (x) > V i (r) and 2. ¬∃V j ∈ V : V j (x) < V j (r)"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:9.3 Multidimensionality 9.3.1 Combinations of Adjectives. When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #TARGET_REF ) . Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., #REF) was used as the sole criterion: Formally, an object r ∈ C has a Pareto-optimal combination of Values V iff there is no other x ∈ C such that 1. ∃V i ∈ V : V i (x) > V i (r) and 2. ¬∃V j ∈ V : V j (x) < V j (r)",
        "output": "experiments:9.3 Multidimensionality 9.3.1 Combinations of Adjectives. When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #TARGET_REF<BACK/> ) . Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective. The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., #REF) was used as the sole criterion: Formally, an object r ∈ C has a Pareto-optimal combination of Values V iff there is no other x ∈ C such that 1. ∃V i ∈ V : V i (x) > V i (r) and 2. ¬∃V j ∈ V : V j (x) < V j (r)"
    },
    {
        "gold": {
            "text": [
                "Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.",
                "State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.",
                "The best performance on the WSJ corpus was achieved by a combination of the SATZ system (#REF) with the Alembic system (#REF): a 0.5% error rate.",
                "The best performance on the Brown corpus , a 0.2 % error rate , was reported by #TARGET_REF , who trained a decision tree classifier on a 25-million-word corpus .",
                "In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in #REF.",
                "We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system (#REF) with the Alembic system (#REF): a 0.5% error rate. The best performance on the Brown corpus , a 0.2 % error rate , was reported by #TARGET_REF , who trained a decision tree classifier on a 25-million-word corpus . In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in #REF. We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.",
        "output": "nan:Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system (#REF) with the Alembic system (#REF): a 0.5% error rate. The best performance on the Brown corpus , a 0.2 % error rate , was reported by #TARGET_REF<USE/> , who trained a decision tree classifier on a 25-million-word corpus . In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in #REF. We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus."
    },
    {
        "gold": {
            "text": [
                "The PIR-NREF database is a comprehensive database for sequence searching and protein identification.",
                "It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB.",
                "Three UniRef tables UniRef100 , #REF and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and #REF and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #TARGET_REF such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence .",
                "NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE.",
                "GenPept entries are those translated from the GenBanknucleotide sequence database.",
                "RefSeq is a comprehensive, integrated, non-redundant set of sequences, including genomic DNA, transcript (RNA), and protein products, for major research organisms.",
                "Entrez GENE provides a unified query environment for genes defined by sequence and/or in NCBI's Map Viewer."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The PIR-NREF database is a comprehensive database for sequence searching and protein identification. It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB. Three UniRef tables UniRef100 , #REF and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and #REF and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #TARGET_REF such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence . NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE. GenPept entries are those translated from the GenBanknucleotide sequence database. RefSeq is a comprehensive, integrated, non-redundant set of sequences, including genomic DNA, transcript (RNA), and protein products, for major research organisms. Entrez GENE provides a unified query environment for genes defined by sequence and/or in NCBI's Map Viewer.",
        "output": "nan:The PIR-NREF database is a comprehensive database for sequence searching and protein identification. It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB. Three UniRef tables UniRef100 , #REF and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and #REF and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #TARGET_REF<FUT/> such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence . NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE. GenPept entries are those translated from the GenBanknucleotide sequence database. RefSeq is a comprehensive, integrated, non-redundant set of sequences, including genomic DNA, transcript (RNA), and protein products, for major research organisms. Entrez GENE provides a unified query environment for genes defined by sequence and/or in NCBI's Map Viewer."
    },
    {
        "gold": {
            "text": [
                "• The authors are listed alphabetically.",
                "SFB 340, Kleine Wilhelmstr.",
                "113, D-72074 Tiibingen, Germany.",
                "email: {dm,minnen}@sfs.nphil.uni-tuebingen.de",
                "URL: http://www.sfs.nphil.uni-tuebingen.de/sfb",
                "/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (#REF) that also use lexical rules such as the Complement Extraction Lexical Rule (#REF) or the Complement Extraction Lexical Rule ( #TARGET_REF ) to operate on those raised elements.",
                "Also an analysis treating adjunct extraction via lexical rules (#REF) results in an infinite lexicon."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:• The authors are listed alphabetically. SFB 340, Kleine Wilhelmstr. 113, D-72074 Tiibingen, Germany. email: {dm,minnen}@sfs.nphil.uni-tuebingen.de URL: http://www.sfs.nphil.uni-tuebingen.de/sfb /b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (#REF) that also use lexical rules such as the Complement Extraction Lexical Rule (#REF) or the Complement Extraction Lexical Rule ( #TARGET_REF ) to operate on those raised elements. Also an analysis treating adjunct extraction via lexical rules (#REF) results in an infinite lexicon.",
        "output": "introduction:• The authors are listed alphabetically. SFB 340, Kleine Wilhelmstr. 113, D-72074 Tiibingen, Germany. email: {dm,minnen}@sfs.nphil.uni-tuebingen.de URL: http://www.sfs.nphil.uni-tuebingen.de/sfb /b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (#REF) that also use lexical rules such as the Complement Extraction Lexical Rule (#REF) or the Complement Extraction Lexical Rule ( #TARGET_REF<BACK/> ) to operate on those raised elements. Also an analysis treating adjunct extraction via lexical rules (#REF) results in an infinite lexicon."
    },
    {
        "gold": {
            "text": [
                "Our method resorts to some translation examples , which is similar as example-based translation or translation memory ( #TARGET_REF ; #REF ; #REF ) .",
                "Instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Our method resorts to some translation examples , which is similar as example-based translation or translation memory ( #TARGET_REF ; #REF ; #REF ) . Instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights.",
        "output": "related work:Our method resorts to some translation examples , which is similar as example-based translation or translation memory ( #TARGET_REF<USE/> ; #REF ; #REF ) . Instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights."
    },
    {
        "gold": {
            "text": [
                "The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish.",
                "It consists of 1600 pairs derived from the RTE3 development and test sets (800+800).",
                "Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #TARGET_REF ) .",
                "The method relies on translation-validation cycles, defined as separate jobs routed to MTurk's workforce.",
                "Translation jobs return one Spanish version for each hypothesis.",
                "Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference.",
                "At each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job.",
                "Although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired CLTE corpus."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish. It consists of 1600 pairs derived from the RTE3 development and test sets (800+800). Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #TARGET_REF ) . The method relies on translation-validation cycles, defined as separate jobs routed to MTurk's workforce. Translation jobs return one Spanish version for each hypothesis. Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference. At each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job. Although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired CLTE corpus.",
        "output": "experiments:The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish. It consists of 1600 pairs derived from the RTE3 development and test sets (800+800). Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #TARGET_REF<FUT/> ) . The method relies on translation-validation cycles, defined as separate jobs routed to MTurk's workforce. Translation jobs return one Spanish version for each hypothesis. Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference. At each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job. Although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired CLTE corpus."
    },
    {
        "gold": {
            "text": [
                "We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.",
                "We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve (#REF).",
                "Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work. We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve (#REF). Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #TARGET_REF ) .",
        "output": "nan:We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work. We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve (#REF). Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "We assume here that a translation of the surface forms of sentences into a logical formalism is possible.",
                "Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.",
                "So we will use a very simple formalism, like the one above, resembling the standard first order language.",
                "But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of #TARGET_REF , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora .",
                "The logical notation of #REF is more sophisticated, and may be considered another possibility.",
                "Jackendoff's (1983) formalism is richer and resembles more closely an English grammar.",
                "Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.\"",
                "Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.",
                "It will also be a model for our simplified logical notation (cf.",
                "Section 5)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:We assume here that a translation of the surface forms of sentences into a logical formalism is possible. Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of #TARGET_REF , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The logical notation of #REF is more sophisticated, and may be considered another possibility. Jackendoff's (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.\" Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics. It will also be a model for our simplified logical notation (cf. Section 5).",
        "output": "introduction:We assume here that a translation of the surface forms of sentences into a logical formalism is possible. Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of #TARGET_REF<USE/> , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora . The logical notation of #REF is more sophisticated, and may be considered another possibility. Jackendoff's (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.\" Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics. It will also be a model for our simplified logical notation (cf. Section 5)."
    },
    {
        "gold": {
            "text": [
                "The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.).",
                "Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K�bler, McDonald, and #TARGET_REF describe a �typical� MaltParser model configuration of attributes and features.13",
                "Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags.",
                "For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].",
                "We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument)."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.). Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K�bler, McDonald, and #TARGET_REF describe a �typical� MaltParser model configuration of attributes and features.13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags. For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0]. We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument).",
        "output": "related work:The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.). Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K�bler, McDonald, and #TARGET_REF<FUT/> describe a �typical� MaltParser model configuration of attributes and features.13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags. For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0]. We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument)."
    },
    {
        "gold": {
            "text": [
                "While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.",
                "Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; #REF; #REF) and the description-level lexical rules ( DLRs ; #TARGET_REF )"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup. Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; #REF; #REF) and the description-level lexical rules ( DLRs ; #TARGET_REF )",
        "output": "introduction:While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup. Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; #REF; #REF) and the description-level lexical rules ( DLRs ; #TARGET_REF<BACK/> )"
    },
    {
        "gold": {
            "text": [
                "Another application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data.",
                "Consider, for example, the case of questions.",
                "#TARGET_REF observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .",
                "Table 2   We consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer.",
                "The question should be posed so that the resulting answer produces a partially labeled dependency tree.",
                "Root-F1 scores from Table 2 suggest that one simple question is \"what is the main verb of this sentence?\"",
                "for sentences that are questions.",
                "In most cases this task is straight-forward and will result in a single dependency, that from the root to the main verb of the sentence.",
                "We feel this is a realistic partial labeled training setting where it would be possible to quickly collect a significant amount of data."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Another application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data. Consider, for example, the case of questions. #TARGET_REF observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank . Table 2   We consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer. The question should be posed so that the resulting answer produces a partially labeled dependency tree. Root-F1 scores from Table 2 suggest that one simple question is \"what is the main verb of this sentence?\" for sentences that are questions. In most cases this task is straight-forward and will result in a single dependency, that from the root to the main verb of the sentence. We feel this is a realistic partial labeled training setting where it would be possible to quickly collect a significant amount of data.",
        "output": "experiments:Another application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data. Consider, for example, the case of questions. #TARGET_REF<USE/> observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank . Table 2   We consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer. The question should be posed so that the resulting answer produces a partially labeled dependency tree. Root-F1 scores from Table 2 suggest that one simple question is \"what is the main verb of this sentence?\" for sentences that are questions. In most cases this task is straight-forward and will result in a single dependency, that from the root to the main verb of the sentence. We feel this is a realistic partial labeled training setting where it would be possible to quickly collect a significant amount of data."
    },
    {
        "gold": {
            "text": [
                "The RenTAL system is implemented in LiLFeS ( #TARGET_REF ) 2 .",
                "LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (#REF;.",
                "We applied our system to the XTAG English grammar (The XTAG Research #REF) 3 , which is a large-scale FB-LTAG grammar for English.",
                "The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (#REF) 6 (the average length is 6.32 words).",
                "This result empirically attested the strong equivalence of our algorithm."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The RenTAL system is implemented in LiLFeS ( #TARGET_REF ) 2 . LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (#REF;. We applied our system to the XTAG English grammar (The XTAG Research #REF) 3 , which is a large-scale FB-LTAG grammar for English. The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (#REF) 6 (the average length is 6.32 words). This result empirically attested the strong equivalence of our algorithm.",
        "output": "experiments:The RenTAL system is implemented in LiLFeS ( #TARGET_REF<FUT/> ) 2 . LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (#REF;. We applied our system to the XTAG English grammar (The XTAG Research #REF) 3 , which is a large-scale FB-LTAG grammar for English. The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (#REF) 6 (the average length is 6.32 words). This result empirically attested the strong equivalence of our algorithm."
    },
    {
        "gold": {
            "text": [
                "The terminology used in the literature varies.",
                "Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.",
                "To avoid confusion, we will only use the terminology introduced in the text.",
                "4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #TARGET_REF ; #REF ) .",
                "5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).",
                "6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.",
                "The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.",
                "In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The terminology used in the literature varies. Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #TARGET_REF ; #REF ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input. In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.",
        "output": "introduction:The terminology used in the literature varies. Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #TARGET_REF<BACK/> ; #REF ) . 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input. In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement."
    },
    {
        "gold": {
            "text": [
                "For unsupervised tree structure induction, #REF adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.",
                "Our previous work ( #TARGET_REF ) designed an EMbased method to construct unsupervised trees for tree-based translation models .",
                "This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.",
                "Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.",
                "The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (#REF).",
                "#REF employed a Bayesian method to learn discontinuous SCFG rules.",
                "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.",
                "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.",
                "#REF and #REF focused on joint parsing and alignment.",
                "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.",
                "adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:For unsupervised tree structure induction, #REF adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work ( #TARGET_REF ) designed an EMbased method to construct unsupervised trees for tree-based translation models . This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (#REF). #REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #REF focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.",
        "output": "related work:For unsupervised tree structure induction, #REF adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work ( #TARGET_REF<USE/> ) designed an EMbased method to construct unsupervised trees for tree-based translation models . This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (#REF). #REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #REF focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees."
    },
    {
        "gold": {
            "text": [
                "The denominator of equation ( 1) is the total probability of all accepting paths in x i • f • y i .",
                "But while computing this, we will also compute the numerator.",
                "The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.",
                "We will enforce an invariant: the weight of any pathset Π must be ( π∈Π P (π), π∈Π P (π) val(π)) ∈ R ≥0 × V , from which (1) is trivial to compute.",
                "#TARGET_REF give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .",
                "Multiplication and addition are replaced by binary operations ⊗ and ⊕ on K. Thus ⊗ is used to combine arc weights into a path weight and ⊕ is used to combine the weights of alternative paths.",
                "To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = ∞ i=0 k i .",
                "The usual finite-state algorithms work if (K, ⊕, ⊗, * ) has the structure of a closed semiring. 15"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The denominator of equation ( 1) is the total probability of all accepting paths in x i • f • y i . But while computing this, we will also compute the numerator. The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability. We will enforce an invariant: the weight of any pathset Π must be ( π∈Π P (π), π∈Π P (π) val(π)) ∈ R ≥0 × V , from which (1) is trivial to compute. #TARGET_REF give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations ⊗ and ⊕ on K. Thus ⊗ is used to combine arc weights into a path weight and ⊕ is used to combine the weights of alternative paths. To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = ∞ i=0 k i . The usual finite-state algorithms work if (K, ⊕, ⊗, * ) has the structure of a closed semiring. 15",
        "output": "nan:The denominator of equation ( 1) is the total probability of all accepting paths in x i • f • y i . But while computing this, we will also compute the numerator. The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability. We will enforce an invariant: the weight of any pathset Π must be ( π∈Π P (π), π∈Π P (π) val(π)) ∈ R ≥0 × V , from which (1) is trivial to compute. #TARGET_REF<FUT/> give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) . Multiplication and addition are replaced by binary operations ⊗ and ⊕ on K. Thus ⊗ is used to combine arc weights into a path weight and ⊕ is used to combine the weights of alternative paths. To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = ∞ i=0 k i . The usual finite-state algorithms work if (K, ⊕, ⊗, * ) has the structure of a closed semiring. 15"
    },
    {
        "gold": {
            "text": [
                "In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (#REF: 56) unless the web is used as a corpus (#REF).",
                "Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #TARGET_REF ) .",
                "This topic generated intense interest at workshops held at the University of Heidelberg (#REF), University of Bologna (#REF), University of Birmingham (#REF) and now in Trento in #REF.",
                "In addition, the advantages of using linguistically annotated data over raw data are well documented (#REF;#REF).",
                "As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.",
                "Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (#REF: 56) unless the web is used as a corpus (#REF). Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #TARGET_REF ) . This topic generated intense interest at workshops held at the University of Heidelberg (#REF), University of Bologna (#REF), University of Birmingham (#REF) and now in Trento in #REF. In addition, the advantages of using linguistically annotated data over raw data are well documented (#REF;#REF). As the size of a corpus increases, a near linear increase in computing power is required to annotate the text. Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.",
        "output": "introduction:In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (#REF: 56) unless the web is used as a corpus (#REF). Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #TARGET_REF<BACK/> ) . This topic generated intense interest at workshops held at the University of Heidelberg (#REF), University of Bologna (#REF), University of Birmingham (#REF) and now in Trento in #REF. In addition, the advantages of using linguistically annotated data over raw data are well documented (#REF;#REF). As the size of a corpus increases, a near linear increase in computing power is required to annotate the text. Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus."
    },
    {
        "gold": {
            "text": [
                "The four verbs which are misclassified as Object Equi and which do not have T5 codes anywhere in their entries are elect, love, represent and require.",
                "None of these verbs take sentential complements and therefore they appear to be counterexamples to our Object Raising rule.",
                "In addition , #TARGET_REF note that our Object Raising rule would assign mean to this category incorrectly .",
                "Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e.",
                "\"intend\"), however, when it is used in this sense it must be treated as an Object Equi verb."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The four verbs which are misclassified as Object Equi and which do not have T5 codes anywhere in their entries are elect, love, represent and require. None of these verbs take sentential complements and therefore they appear to be counterexamples to our Object Raising rule. In addition , #TARGET_REF note that our Object Raising rule would assign mean to this category incorrectly . Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e. \"intend\"), however, when it is used in this sense it must be treated as an Object Equi verb.",
        "output": "nan:The four verbs which are misclassified as Object Equi and which do not have T5 codes anywhere in their entries are elect, love, represent and require. None of these verbs take sentential complements and therefore they appear to be counterexamples to our Object Raising rule. In addition , #TARGET_REF<USE/> note that our Object Raising rule would assign mean to this category incorrectly . Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e. \"intend\"), however, when it is used in this sense it must be treated as an Object Equi verb."
    },
    {
        "gold": {
            "text": [
                "The list of semantic relations with which we work is based on extensive literature study ( #TARGET_REFa ) .",
                "Three lists of relations for three syntactic levels -inter-clause, intra-clause (case) and nounmodifier relations -were next combined based on syntactic and semantic phenomena.",
                "The resulting list is the one used in the experiments we present in this paper.",
                "The relations are grouped by general similarity into 6 relation classes (H denotes the head of a base NP, M denotes the modifier).",
                "There is no consensus in the literature on a list of semantic relations that would work in all situations.",
                "This is, no doubt, because a general list of relations such as the one we use would not be appropriate for the semantic analysis of texts in a specific domain, such as for example medical texts.",
                "All the relations in the list we use were necessary, and sufficient, for the analysis of the input text."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The list of semantic relations with which we work is based on extensive literature study ( #TARGET_REFa ) . Three lists of relations for three syntactic levels -inter-clause, intra-clause (case) and nounmodifier relations -were next combined based on syntactic and semantic phenomena. The resulting list is the one used in the experiments we present in this paper. The relations are grouped by general similarity into 6 relation classes (H denotes the head of a base NP, M denotes the modifier). There is no consensus in the literature on a list of semantic relations that would work in all situations. This is, no doubt, because a general list of relations such as the one we use would not be appropriate for the semantic analysis of texts in a specific domain, such as for example medical texts. All the relations in the list we use were necessary, and sufficient, for the analysis of the input text.",
        "output": "experiments:The list of semantic relations with which we work is based on extensive literature study ( #TARGET_REF<FUT/>a ) . Three lists of relations for three syntactic levels -inter-clause, intra-clause (case) and nounmodifier relations -were next combined based on syntactic and semantic phenomena. The resulting list is the one used in the experiments we present in this paper. The relations are grouped by general similarity into 6 relation classes (H denotes the head of a base NP, M denotes the modifier). There is no consensus in the literature on a list of semantic relations that would work in all situations. This is, no doubt, because a general list of relations such as the one we use would not be appropriate for the semantic analysis of texts in a specific domain, such as for example medical texts. All the relations in the list we use were necessary, and sufficient, for the analysis of the input text."
    },
    {
        "gold": {
            "text": [
                "Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.",
                "#REF obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while #REF examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.",
                "#REF and #REF study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #TARGET_REF ) and find correlations between the various modalities both within and across speakers .",
                "Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (#REF;#REF;#REF;#REF).",
                "Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.",
                "In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. #REF obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while #REF examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. #REF and #REF study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #TARGET_REF ) and find correlations between the various modalities both within and across speakers . Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (#REF;#REF;#REF;#REF). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.",
        "output": "introduction:Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. #REF obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while #REF examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues. #REF and #REF study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #TARGET_REF<BACK/> ) and find correlations between the various modalities both within and across speakers . Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (#REF;#REF;#REF;#REF). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features."
    },
    {
        "gold": {
            "text": [
                "Other studies which view lR as a query generation process include #TARGET_REF ; #REF ; #REF ; Miller et al , 1999 .",
                "Our work has focused on cross-lingual retrieval."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Other studies which view lR as a query generation process include #TARGET_REF ; #REF ; #REF ; Miller et al , 1999 . Our work has focused on cross-lingual retrieval.",
        "output": "related work:Other studies which view lR as a query generation process include #TARGET_REF<USE/> ; #REF ; #REF ; Miller et al , 1999 . Our work has focused on cross-lingual retrieval."
    },
    {
        "gold": {
            "text": [
                "The focus of our work is on the general applicability of the different response automa- tion methods, rather than on comparing the performance of particular implementa- tion techniques.",
                "Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research.",
                "Specifically , we used Decision Graphs ( #REF ) for Doc-Pred , and SVMs ( #TARGET_REF ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .",
                "Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.",
                "These methodological variations are summarized in Table 2."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:The focus of our work is on the general applicability of the different response automa- tion methods, rather than on comparing the performance of particular implementa- tion techniques. Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research. Specifically , we used Decision Graphs ( #REF ) for Doc-Pred , and SVMs ( #TARGET_REF ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) . Because this variation was uniformly implemented for both approaches, it does not affect their relative performance. These methodological variations are summarized in Table 2.",
        "output": "method:The focus of our work is on the general applicability of the different response automa- tion methods, rather than on comparing the performance of particular implementa- tion techniques. Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research. Specifically , we used Decision Graphs ( #REF ) for Doc-Pred , and SVMs ( #TARGET_REF<FUT/> ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) . Because this variation was uniformly implemented for both approaches, it does not affect their relative performance. These methodological variations are summarized in Table 2."
    },
    {
        "gold": {
            "text": [
                "One approach to partial parsing was presented by #TARGET_REF , who extended a shallow-parsing technique to partial parsing ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:One approach to partial parsing was presented by #TARGET_REF , who extended a shallow-parsing technique to partial parsing .",
        "output": "introduction:One approach to partial parsing was presented by #TARGET_REF<BACK/> , who extended a shallow-parsing technique to partial parsing ."
    },
    {
        "gold": {
            "text": [
                "Our baseline coreference system uses the C4.5 decision tree learner (#REF) to acquire a classifier on the training texts for determining whether two NPs are coreferent.",
                "Following previous work (e.g., #REF and #REF), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\u000e, NPi+2, � � �, NPj.",
                "Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #TARGET_REF and #REF, as described below."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Our baseline coreference system uses the C4.5 decision tree learner (#REF) to acquire a classifier on the training texts for determining whether two NPs are coreferent. Following previous work (e.g., #REF and #REF), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\u000e, NPi+2, � � �, NPj. Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #TARGET_REF and #REF, as described below.",
        "output": "nan:Our baseline coreference system uses the C4.5 decision tree learner (#REF) to acquire a classifier on the training texts for determining whether two NPs are coreferent. Following previous work (e.g., #REF and #REF), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\u000e, NPi+2, � � �, NPj. Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #TARGET_REF<USE/> and #REF, as described below."
    },
    {
        "gold": {
            "text": [
                "This section describes our joint coreference resolution and mention head detection framework.",
                "Our work is inspired by the latent left-linking model in #TARGET_REF and the ILP formulation from #REF .",
                "The joint learning and inference model takes as input mention head candidates (Sec.",
                "3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.",
                "This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier.",
                "The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones.",
                "By learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from #REF.",
                "This joint framework aims to improve performance on both mention head detection and on coreference."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:This section describes our joint coreference resolution and mention head detection framework. Our work is inspired by the latent left-linking model in #TARGET_REF and the ILP formulation from #REF . The joint learning and inference model takes as input mention head candidates (Sec. 3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions. This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier. The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones. By learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from #REF. This joint framework aims to improve performance on both mention head detection and on coreference.",
        "output": "nan:This section describes our joint coreference resolution and mention head detection framework. Our work is inspired by the latent left-linking model in #TARGET_REF<FUT/> and the ILP formulation from #REF . The joint learning and inference model takes as input mention head candidates (Sec. 3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions. This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier. The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones. By learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from #REF. This joint framework aims to improve performance on both mention head detection and on coreference."
    },
    {
        "gold": {
            "text": [
                "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).",
                "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .",
                "Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.",
                "Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.",
                "John Mayor), nominal (the president) or pronominal (she, it).",
                "An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.",
        "output": "introduction:In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ; #TARGET_REF<BACK/> ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks . Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity."
    },
    {
        "gold": {
            "text": [
                "The problem of handling ill-formed input has been studied by #TARGET_REF , #REF , #REF , #REF , #REF , #REF , #REF , and #REF .",
                "A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.",
                "However, these methodologies have not used historical information at the dialogue level as described here.",
                "In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.",
                "The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.",
                "Thus, an error in this work has no pattern but occurs probabilistically.",
                "A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.",
                "The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors.",
                "However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.",
                "Another dialogue acquisition system has been developed by #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The problem of handling ill-formed input has been studied by #TARGET_REF , #REF , #REF , #REF , #REF , #REF , #REF , and #REF . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #REF.",
        "output": "nan:The problem of handling ill-formed input has been studied by #TARGET_REF<USE/> , #REF , #REF , #REF , #REF , #REF , #REF , and #REF . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #REF where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by #REF."
    },
    {
        "gold": {
            "text": [
                "In all settings, we fix all Dirichlet priors at 0.1, use a learning rate 0.7, and use minibatch sizes of 1024 documents.",
                "We do not optimize these hyperparameters or vary them over time.",
                "The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #TARGET_REF ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:In all settings, we fix all Dirichlet priors at 0.1, use a learning rate 0.7, and use minibatch sizes of 1024 documents. We do not optimize these hyperparameters or vary them over time. The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #TARGET_REF .",
        "output": "experiments:In all settings, we fix all Dirichlet priors at 0.1, use a learning rate 0.7, and use minibatch sizes of 1024 documents. We do not optimize these hyperparameters or vary them over time. The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #TARGET_REF<FUT/> ."
    },
    {
        "gold": {
            "text": [
                "Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., #REF), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., #REF).",
                "#REF helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (#REF) in the prediction of association norms.",
                "#TARGET_REF furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .",
                "In a similar vein, #REF showed that a different feature-topic model improved predictions on a fill-in-the-blank task.",
                "#REF take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., #REF), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., #REF). #REF helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (#REF) in the prediction of association norms. #TARGET_REF furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks . In a similar vein, #REF showed that a different feature-topic model improved predictions on a fill-in-the-blank task. #REF take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.",
        "output": "related work:Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., #REF), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., #REF). #REF helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (#REF) in the prediction of association norms. #TARGET_REF<BACK/> furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks . In a similar vein, #REF showed that a different feature-topic model improved predictions on a fill-in-the-blank task. #REF take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity."
    },
    {
        "gold": {
            "text": [
                "Table 1 shows the Pearson's product correlation between each topical feature and candidate's power.",
                "We obtain a highly significant (p = 0.002) negative correlation between topic shift tendency of a candidate (PI) and his/her power.",
                "In other words, the variation in the topic shifting tendencies is significantly correlated with the candidates' recent poll standings.",
                "Candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often.",
                "This is in line with our previous findings from ( #TARGET_REF ) that candidates with higher power attempt to shift topics less often than others when responding to moderators .",
                "It is also in line with the findings by Prabhakaran et al. (2013a) that candidates with higher power tend not to interrupt others.",
                "On the other hand, we did not obtain any significant correlation for the features proposed by #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:Table 1 shows the Pearson's product correlation between each topical feature and candidate's power. We obtain a highly significant (p = 0.002) negative correlation between topic shift tendency of a candidate (PI) and his/her power. In other words, the variation in the topic shifting tendencies is significantly correlated with the candidates' recent poll standings. Candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often. This is in line with our previous findings from ( #TARGET_REF ) that candidates with higher power attempt to shift topics less often than others when responding to moderators . It is also in line with the findings by Prabhakaran et al. (2013a) that candidates with higher power tend not to interrupt others. On the other hand, we did not obtain any significant correlation for the features proposed by #REF.",
        "output": "method:Table 1 shows the Pearson's product correlation between each topical feature and candidate's power. We obtain a highly significant (p = 0.002) negative correlation between topic shift tendency of a candidate (PI) and his/her power. In other words, the variation in the topic shifting tendencies is significantly correlated with the candidates' recent poll standings. Candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often. This is in line with our previous findings from ( #TARGET_REF<USE/> ) that candidates with higher power attempt to shift topics less often than others when responding to moderators . It is also in line with the findings by Prabhakaran et al. (2013a) that candidates with higher power tend not to interrupt others. On the other hand, we did not obtain any significant correlation for the features proposed by #REF."
    },
    {
        "gold": {
            "text": [
                "Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.",
                "We optimize the dual objective using the gradient based methods shown in Algorithm 1.",
                "Here η is an optimization precision, α is a step size chosen with the strong Wolfe's rule (#REF).",
                "Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #TARGET_REF ) ; for equality constraints with slack , we use conjugate gradient ( #REF ) , noting that when A = 0 , the objective is not differentiable .",
                "In practice this only happens at the start of optimization and we use a sub-gradient for the first direction."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ. We optimize the dual objective using the gradient based methods shown in Algorithm 1. Here η is an optimization precision, α is a step size chosen with the strong Wolfe's rule (#REF). Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #TARGET_REF ) ; for equality constraints with slack , we use conjugate gradient ( #REF ) , noting that when A = 0 , the objective is not differentiable . In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.",
        "output": "nan:Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ. We optimize the dual objective using the gradient based methods shown in Algorithm 1. Here η is an optimization precision, α is a step size chosen with the strong Wolfe's rule (#REF). Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #TARGET_REF<FUT/> ) ; for equality constraints with slack , we use conjugate gradient ( #REF ) , noting that when A = 0 , the objective is not differentiable . In practice this only happens at the start of optimization and we use a sub-gradient for the first direction."
    },
    {
        "gold": {
            "text": [
                "As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses.",
                "An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response.",
                "This task can be cast as extractive multi-document summarization.",
                "Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable.",
                "In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #TARGET_REF ; Barzilay , Elhadad , and #REF ; #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "method:As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses. An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response. This task can be cast as extractive multi-document summarization. Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable. In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #TARGET_REF ; Barzilay , Elhadad , and #REF ; #REF ) .",
        "output": "method:As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses. An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response. This task can be cast as extractive multi-document summarization. Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable. In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #TARGET_REF<BACK/> ; Barzilay , Elhadad , and #REF ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.",
                "The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (#REF: 0.28% vs. 0.20% error rate).",
                "On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #TARGET_REF ( 0.44 % vs. 0.5 % error rate ) .",
                "Although these error rates seem to be very small, they are quite significant.",
                "Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000).",
                "This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level.",
                "On the other hand, one error in 200 periods means that there is one error in every two documents in the Brown corpus and one error in every four documents in the WSJ corpus."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections. The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (#REF: 0.28% vs. 0.20% error rate). On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #TARGET_REF ( 0.44 % vs. 0.5 % error rate ) . Although these error rates seem to be very small, they are quite significant. Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000). This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level. On the other hand, one error in 200 periods means that there is one error in every two documents in the Brown corpus and one error in every four documents in the WSJ corpus.",
        "output": "conclusion:Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections. The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (#REF: 0.28% vs. 0.20% error rate). On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #TARGET_REF<USE/> ( 0.44 % vs. 0.5 % error rate ) . Although these error rates seem to be very small, they are quite significant. Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000). This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level. On the other hand, one error in 200 periods means that there is one error in every two documents in the Brown corpus and one error in every four documents in the WSJ corpus."
    },
    {
        "gold": {
            "text": [
                "All results are reported mainly in terms of labeled attachment accuracy score (the parent word and the type of dependency relation to it, abbreviated as LAS), which is also used for greedy (hill-climbing) decisions for feature combination.",
                "Unlabeled attachment accuracy score (UAS) and label accuracy (dependency relation regardless of parent, LS) are also given.",
                "For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #TARGET_REF .",
                "We denote p < 0.05 and p < 0.01 with + and ++ , respectively."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:All results are reported mainly in terms of labeled attachment accuracy score (the parent word and the type of dependency relation to it, abbreviated as LAS), which is also used for greedy (hill-climbing) decisions for feature combination. Unlabeled attachment accuracy score (UAS) and label accuracy (dependency relation regardless of parent, LS) are also given. For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #TARGET_REF . We denote p < 0.05 and p < 0.01 with + and ++ , respectively.",
        "output": "related work:All results are reported mainly in terms of labeled attachment accuracy score (the parent word and the type of dependency relation to it, abbreviated as LAS), which is also used for greedy (hill-climbing) decisions for feature combination. Unlabeled attachment accuracy score (UAS) and label accuracy (dependency relation regardless of parent, LS) are also given. For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #TARGET_REF<FUT/> . We denote p < 0.05 and p < 0.01 with + and ++ , respectively."
    },
    {
        "gold": {
            "text": [
                "An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.",
                "before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.",
                "If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.",
                "Typical examples are Bulgarian ( #REF ; #TARGET_REF ) , Chinese ( #REF ) , Danish ( #REF ) , and Swedish ( #REF ) .",
                "Japanese (#REF), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6. Typical examples are Bulgarian ( #REF ; #TARGET_REF ) , Chinese ( #REF ) , Danish ( #REF ) , and Swedish ( #REF ) . Japanese (#REF), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.",
        "output": "experiments:An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6. Typical examples are Bulgarian ( #REF ; #TARGET_REF<BACK/> ) , Chinese ( #REF ) , Danish ( #REF ) , and Swedish ( #REF ) . Japanese (#REF), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances."
    },
    {
        "gold": {
            "text": [
                "6The analysis is reminiscent of the treatment of coordination in the Collins parser ( #TARGET_REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:6The analysis is reminiscent of the treatment of coordination in the Collins parser ( #TARGET_REF ) .",
        "output": "experiments:6The analysis is reminiscent of the treatment of coordination in the Collins parser ( #TARGET_REF<USE/> ) ."
    },
    {
        "gold": {
            "text": [
                "We currently have two application domains that can carry on a spoken dialog with a user.",
                "One, the VOYAGER domain (#REF), answers questions about places of interest in an urban area, in our case, the vicinity of MIT and Harvard University.",
                "The second one, ATIS ( #TARGET_REF et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.",
                "Work continues on improving all aspects of these domains. Our current research is directed at a number of different remaining issues."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:We currently have two application domains that can carry on a spoken dialog with a user. One, the VOYAGER domain (#REF), answers questions about places of interest in an urban area, in our case, the vicinity of MIT and Harvard University. The second one, ATIS ( #TARGET_REF et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights. Work continues on improving all aspects of these domains. Our current research is directed at a number of different remaining issues.",
        "output": "nan:We currently have two application domains that can carry on a spoken dialog with a user. One, the VOYAGER domain (#REF), answers questions about places of interest in an urban area, in our case, the vicinity of MIT and Harvard University. The second one, ATIS ( #TARGET_REF<FUT/> et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights. Work continues on improving all aspects of these domains. Our current research is directed at a number of different remaining issues."
    },
    {
        "gold": {
            "text": [
                "Co-occurrence With the exception of (#REFb), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts Kumano & #REF;#REFa;#REF).",
                "A bitext comprises a pair of texts in two languages, where each text is a translation of the other.",
                "Word co-occurrence can be defined in various ways.",
                "The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & #REF ; #TARGET_REFa ) .",
                "Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.",
                "The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (#REF;Resnik & #REF), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (#REF;."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Co-occurrence With the exception of (#REFb), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts Kumano & #REF;#REFa;#REF). A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence can be defined in various ways. The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & #REF ; #TARGET_REFa ) . Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti. The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (#REF;Resnik & #REF), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (#REF;.",
        "output": "introduction:Co-occurrence With the exception of (#REFb), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts Kumano & #REF;#REFa;#REF). A bitext comprises a pair of texts in two languages, where each text is a translation of the other. Word co-occurrence can be defined in various ways. The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & #REF ; #TARGET_REF<BACK/>a ) . Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti. The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (#REF;Resnik & #REF), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (#REF;."
    },
    {
        "gold": {
            "text": [
                "Rankboost, like other machine learning programs of the boosting family, can handle a very large number of features.",
                "Therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let Rank-Boost choose the relevant ones.",
                "In total, we used 3,291 features in training the SPR.",
                "Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #TARGET_REF .",
                "The motivation for the features was to capture declaratively decisions made by the randomized SPG.",
                "We avoided features specific to particular text plans by discarding those that occurred fewer than 10 times."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Rankboost, like other machine learning programs of the boosting family, can handle a very large number of features. Therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let Rank-Boost choose the relevant ones. In total, we used 3,291 features in training the SPR. Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #TARGET_REF . The motivation for the features was to capture declaratively decisions made by the randomized SPG. We avoided features specific to particular text plans by discarding those that occurred fewer than 10 times.",
        "output": "nan:Rankboost, like other machine learning programs of the boosting family, can handle a very large number of features. Therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let Rank-Boost choose the relevant ones. In total, we used 3,291 features in training the SPR. Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #TARGET_REF<USE/> . The motivation for the features was to capture declaratively decisions made by the randomized SPG. We avoided features specific to particular text plans by discarding those that occurred fewer than 10 times."
    },
    {
        "gold": {
            "text": [
                "To address this inconsistency in the correspondence between inflectional features and morphemes, and inspired by Smrž (2007), we distinguish between two types of inflectional features: formbased (a.k.a.",
                "surface, or illusory) features and functional features. 6",
                "ost available Arabic NLP tools and resources model morphology using formbased (\"surface\") inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (#REF), the Buckwalter morphological analyzer (#REF), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (#REF;Habash, Rambow, and #REF).",
                "The Elixir-FM analyzer (Smrž 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.",
                "In this article , we use an in-house system which provides functional gender , number , and rationality features ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:To address this inconsistency in the correspondence between inflectional features and morphemes, and inspired by Smrž (2007), we distinguish between two types of inflectional features: formbased (a.k.a. surface, or illusory) features and functional features. 6 ost available Arabic NLP tools and resources model morphology using formbased (\"surface\") inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (#REF), the Buckwalter morphological analyzer (#REF), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (#REF;Habash, Rambow, and #REF). The Elixir-FM analyzer (Smrž 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. In this article , we use an in-house system which provides functional gender , number , and rationality features ( #TARGET_REF ) .",
        "output": "experiments:To address this inconsistency in the correspondence between inflectional features and morphemes, and inspired by Smrž (2007), we distinguish between two types of inflectional features: formbased (a.k.a. surface, or illusory) features and functional features. 6 ost available Arabic NLP tools and resources model morphology using formbased (\"surface\") inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (#REF), the Buckwalter morphological analyzer (#REF), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (#REF;Habash, Rambow, and #REF). The Elixir-FM analyzer (Smrž 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. In this article , we use an in-house system which provides functional gender , number , and rationality features ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "There has been some controversy , at least for simple stemmers ( #REF ; #REF ) , about the effectiveness of morphological analysis for document retrieval ( #TARGET_REF ; #REF ; #REF ) .",
                "The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.",
                "Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (#REF;#REF;#REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "conclusion:There has been some controversy , at least for simple stemmers ( #REF ; #REF ) , about the effectiveness of morphological analysis for document retrieval ( #TARGET_REF ; #REF ; #REF ) . The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary. Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (#REF;#REF;#REF).",
        "output": "conclusion:There has been some controversy , at least for simple stemmers ( #REF ; #REF ) , about the effectiveness of morphological analysis for document retrieval ( #TARGET_REF<BACK/> ; #REF ; #REF ) . The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary. Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (#REF;#REF;#REF)."
    },
    {
        "gold": {
            "text": [
                "On the other hand, other work has been carried out in order to acquire collocations.",
                "Most of these endeavours have focused on purely statistical acquisition techniques (#REF), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #TARGET_REF ) or, more frequently, on a combination of the two (#REF; Kilgarri\u001b and #REF, for example).",
                "It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs.",
                "The original acquisition methodology we present in the next section will allow us to overcome this limitation."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:On the other hand, other work has been carried out in order to acquire collocations. Most of these endeavours have focused on purely statistical acquisition techniques (#REF), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #TARGET_REF ) or, more frequently, on a combination of the two (#REF; Kilgarri\u001b and #REF, for example). It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs. The original acquisition methodology we present in the next section will allow us to overcome this limitation.",
        "output": "related work:On the other hand, other work has been carried out in order to acquire collocations. Most of these endeavours have focused on purely statistical acquisition techniques (#REF), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #TARGET_REF<USE/> ) or, more frequently, on a combination of the two (#REF; Kilgarri\u001b and #REF, for example). It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs. The original acquisition methodology we present in the next section will allow us to overcome this limitation."
    },
    {
        "gold": {
            "text": [
                "The ACE-2004 dataset contains 443 documents.",
                "We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #TARGET_REF ; #REF ) .",
                "The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (#REF, contains 3,145 annotated documents.",
                "These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.",
                "We report results on the test documents for both datasets.",
                ") indicate evaluations on (mentions, mention heads) respectively.",
                "For gold mentions and mention heads, they yield the same performance for coreference.",
                "Our proposed H-Joint-M system achieves the highest performance.",
                "Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #TARGET_REF ; #REF ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (#REF, contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. ) indicate evaluations on (mentions, mention heads) respectively. For gold mentions and mention heads, they yield the same performance for coreference. Our proposed H-Joint-M system achieves the highest performance. Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3.",
        "output": "experiments:The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #TARGET_REF<FUT/> ; #REF ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (#REF, contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. ) indicate evaluations on (mentions, mention heads) respectively. For gold mentions and mention heads, they yield the same performance for coreference. Our proposed H-Joint-M system achieves the highest performance. Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3."
    },
    {
        "gold": {
            "text": [
                "A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.",
                "#REF considers the second verb V2 as an aspectual complex comparable to the auxiliaries.",
                "#TARGET_REF argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .",
                "#REF tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\".",
                "Similar findings have been proposed by #REF that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.",
                "#REF tried to represent Bangla CVs in terms of HPSG formalism.",
                "She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.",
                "Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. #REF considers the second verb V2 as an aspectual complex comparable to the auxiliaries. #TARGET_REF argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure . #REF tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by #REF that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. #REF tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible. Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.",
        "output": "related work:A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. #REF considers the second verb V2 as an aspectual complex comparable to the auxiliaries. #TARGET_REF<BACK/> argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure . #REF tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by #REF that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. #REF tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible. Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models."
    },
    {
        "gold": {
            "text": [
                "A recent study by  also investigates the task of training parsers to improve MT reordering.",
                "In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.",
                "The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.",
                "The method is called targeted self-training as it is similar in vein to self-training ( #TARGET_REF ) , with the exception that the new parse data is targeted to produce accurate word reorderings .",
                "Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.",
                "This allows us to give guarantees of convergence.",
                "Furthermore, we also evaluate the method on alternate extrinsic loss functions.",
                "#REF presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system.",
                "Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.",
                "In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:A recent study by  also investigates the task of training parsers to improve MT reordering. In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training ( #TARGET_REF ) , with the exception that the new parse data is targeted to produce accurate word reorderings . Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions. #REF presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system. Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.",
        "output": "related work:A recent study by  also investigates the task of training parsers to improve MT reordering. In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training ( #TARGET_REF<USE/> ) , with the exception that the new parse data is targeted to produce accurate word reorderings . Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions. #REF presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system. Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score."
    },
    {
        "gold": {
            "text": [
                "The results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C.",
                "Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries.",
                "The one-sided t-test ( #TARGET_REF ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C. Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries. The one-sided t-test ( #TARGET_REF ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .",
        "output": "nan:The results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C. Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries. The one-sided t-test ( #TARGET_REF<FUT/> ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant ."
    },
    {
        "gold": {
            "text": [
                "• The authors are listed alphabetically.",
                "SFB 340, Kleine Wilhelmstr.",
                "113, D-72074 Tiibingen, Germany.",
                "email: {dm,minnen}@sfs.nphil.uni-tuebingen.de",
                "URL: http://www.sfs.nphil.uni-tuebingen.de/sfb",
                "/b4home.html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #TARGET_REF ) that also use lexical rules such as the Complement Extraction Lexical Rule (#REF) or the Complement Cliticization Lexical Rule (#REF) to operate on those raised elements.",
                "Also an analysis treating adjunct extraction via lexical rules (#REF) results in an infinite lexicon."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:• The authors are listed alphabetically. SFB 340, Kleine Wilhelmstr. 113, D-72074 Tiibingen, Germany. email: {dm,minnen}@sfs.nphil.uni-tuebingen.de URL: http://www.sfs.nphil.uni-tuebingen.de/sfb /b4home.html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #TARGET_REF ) that also use lexical rules such as the Complement Extraction Lexical Rule (#REF) or the Complement Cliticization Lexical Rule (#REF) to operate on those raised elements. Also an analysis treating adjunct extraction via lexical rules (#REF) results in an infinite lexicon.",
        "output": "introduction:• The authors are listed alphabetically. SFB 340, Kleine Wilhelmstr. 113, D-72074 Tiibingen, Germany. email: {dm,minnen}@sfs.nphil.uni-tuebingen.de URL: http://www.sfs.nphil.uni-tuebingen.de/sfb /b4home.html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #TARGET_REF<BACK/> ) that also use lexical rules such as the Complement Extraction Lexical Rule (#REF) or the Complement Cliticization Lexical Rule (#REF) to operate on those raised elements. Also an analysis treating adjunct extraction via lexical rules (#REF) results in an infinite lexicon."
    },
    {
        "gold": {
            "text": [
                "Several works have proposed discriminative tech- niques to train log-linear model for SMT.",
                "(#REF; #REF) used maximum likelihood estimation to learn weights for MT.",
                "( #REF ; #REF ; #TARGET_REF ; #REF ) employed an evaluation metric as a loss function and directly optimized it .",
                "(#REF; #REF; #REF) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Several works have proposed discriminative tech- niques to train log-linear model for SMT. (#REF; #REF) used maximum likelihood estimation to learn weights for MT. ( #REF ; #REF ; #TARGET_REF ; #REF ) employed an evaluation metric as a loss function and directly optimized it . (#REF; #REF; #REF) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.",
        "output": "related work:Several works have proposed discriminative tech- niques to train log-linear model for SMT. (#REF; #REF) used maximum likelihood estimation to learn weights for MT. ( #REF ; #REF ; #TARGET_REF<USE/> ; #REF ) employed an evaluation metric as a loss function and directly optimized it . (#REF; #REF; #REF) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions."
    },
    {
        "gold": {
            "text": [
                "In addition to evaluating the quality of our extracted semantic forms, we also examined the rate at which they are induced.",
                "This can be expressed as a measure of the coverage of the induced lexicon on new data.",
                "Following Hockenmaier , Bierner , and #REF , #TARGET_REF , and Miyao , Ninomiya , and #REF , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .",
                "We then compare this to a test lexicon from Section 23.",
                "Table 27 shows the results of the evaluation of the coverage of an induced lexicon for verbs only.",
                "There is a corresponding semantic form in the reference lexicon for 89.89% of the verbs in Section 23. 10.11% of the entries in the test lexicon did not appear in the reference lexicon.",
                "Within this group, we can distinguish between known words, which have an entry in the reference lexicon, and unknown words, which do not exist at all in the reference lexicon.",
                "In the same way we make the distinction  between known frames and unknown frames.",
                "There are, therefore, four different cases in which an entry may not appear in the reference lexicon."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:In addition to evaluating the quality of our extracted semantic forms, we also examined the rate at which they are induced. This can be expressed as a measure of the coverage of the induced lexicon on new data. Following Hockenmaier , Bierner , and #REF , #TARGET_REF , and Miyao , Ninomiya , and #REF , we extract a reference lexicon from Sections 02 -- 21 of the WSJ . We then compare this to a test lexicon from Section 23. Table 27 shows the results of the evaluation of the coverage of an induced lexicon for verbs only. There is a corresponding semantic form in the reference lexicon for 89.89% of the verbs in Section 23. 10.11% of the entries in the test lexicon did not appear in the reference lexicon. Within this group, we can distinguish between known words, which have an entry in the reference lexicon, and unknown words, which do not exist at all in the reference lexicon. In the same way we make the distinction  between known frames and unknown frames. There are, therefore, four different cases in which an entry may not appear in the reference lexicon.",
        "output": "nan:In addition to evaluating the quality of our extracted semantic forms, we also examined the rate at which they are induced. This can be expressed as a measure of the coverage of the induced lexicon on new data. Following Hockenmaier , Bierner , and #REF , #TARGET_REF<FUT/> , and Miyao , Ninomiya , and #REF , we extract a reference lexicon from Sections 02 -- 21 of the WSJ . We then compare this to a test lexicon from Section 23. Table 27 shows the results of the evaluation of the coverage of an induced lexicon for verbs only. There is a corresponding semantic form in the reference lexicon for 89.89% of the verbs in Section 23. 10.11% of the entries in the test lexicon did not appear in the reference lexicon. Within this group, we can distinguish between known words, which have an entry in the reference lexicon, and unknown words, which do not exist at all in the reference lexicon. In the same way we make the distinction  between known frames and unknown frames. There are, therefore, four different cases in which an entry may not appear in the reference lexicon."
    },
    {
        "gold": {
            "text": [
                "Using this tool, linguists can either perform syntactic searches via parse trees on a pre-analysed web collection of around three million sentences from the Internet Archive (www.archive.org)",
                "or build their own collections from AltaVista search engine results.",
                "The second method pushes the new collection onto a queue for the LSE annotator to analyse.",
                "A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server.",
                "The Gsearch system ( #TARGET_REF ) also selects sentences by syntactic criteria from large on-line text collections .",
                "Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up.",
                "In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora.",
                "A word sketch is an automatic one-page corpus-derived summary of a word's grammatical and collocational behaviour.",
                "Word Sketches were first used to prepare the Macmillan English Dictionary for Advanced Learners (2002, edited by Michael Rundell)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Using this tool, linguists can either perform syntactic searches via parse trees on a pre-analysed web collection of around three million sentences from the Internet Archive (www.archive.org) or build their own collections from AltaVista search engine results. The second method pushes the new collection onto a queue for the LSE annotator to analyse. A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server. The Gsearch system ( #TARGET_REF ) also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up. In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora. A word sketch is an automatic one-page corpus-derived summary of a word's grammatical and collocational behaviour. Word Sketches were first used to prepare the Macmillan English Dictionary for Advanced Learners (2002, edited by Michael Rundell).",
        "output": "related work:Using this tool, linguists can either perform syntactic searches via parse trees on a pre-analysed web collection of around three million sentences from the Internet Archive (www.archive.org) or build their own collections from AltaVista search engine results. The second method pushes the new collection onto a queue for the LSE annotator to analyse. A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server. The Gsearch system ( #TARGET_REF<BACK/> ) also selects sentences by syntactic criteria from large on-line text collections . Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up. In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora. A word sketch is an automatic one-page corpus-derived summary of a word's grammatical and collocational behaviour. Word Sketches were first used to prepare the Macmillan English Dictionary for Advanced Learners (2002, edited by Michael Rundell)."
    },
    {
        "gold": {
            "text": [
                "The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #TARGET_REF , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) .",
                "In the CLE-QLF approach, as ra-tionally reconstructed by #REF and #REF, the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.",
                "Just as here, these QLFs represent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #TARGET_REF , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) . In the CLE-QLF approach, as ra-tionally reconstructed by #REF and #REF, the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules. Just as here, these QLFs represent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.",
        "output": "nan:The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #TARGET_REF<USE/> , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) . In the CLE-QLF approach, as ra-tionally reconstructed by #REF and #REF, the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules. Just as here, these QLFs represent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context."
    },
    {
        "gold": {
            "text": [
                "To model d ( FWi â 1 , S â T ) , d ( FWi +1 , S â T ) , i.e. whether Li , S â T and Ri , S â T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #TARGET_REF .",
                "Taking d(F W i−1,S→T ) as a case in point, this model takes the form"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:To model d ( FWi â 1 , S â T ) , d ( FWi +1 , S â T ) , i.e. whether Li , S â T and Ri , S â T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #TARGET_REF . Taking d(F W i−1,S→T ) as a case in point, this model takes the form",
        "output": "nan:To model d ( FWi â 1 , S â T ) , d ( FWi +1 , S â T ) , i.e. whether Li , S â T and Ri , S â T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #TARGET_REF<FUT/> . Taking d(F W i−1,S→T ) as a case in point, this model takes the form"
    },
    {
        "gold": {
            "text": [
                "More recently , #TARGET_REF have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and Carl ( 2003 , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .",
                "This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from _source, target_ phrasal segments, and from there they suggest that �it is a reasonably short step to enabling an automated solution via the recombination element of EBMT systems such as those described in [#REF].�"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:More recently , #TARGET_REF have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and Carl ( 2003 , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment . This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from _source, target_ phrasal segments, and from there they suggest that �it is a reasonably short step to enabling an automated solution via the recombination element of EBMT systems such as those described in [#REF].�",
        "output": "introduction:More recently , #TARGET_REF<BACK/> have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and Carl ( 2003 , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment . This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from _source, target_ phrasal segments, and from there they suggest that �it is a reasonably short step to enabling an automated solution via the recombination element of EBMT systems such as those described in [#REF].�"
    },
    {
        "gold": {
            "text": [
                "Our previous work (#REF) designed an EMbased method to construct unsupervised trees for tree-based translation models.",
                "This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.",
                "Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.",
                "The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (#REF).",
                "#REF employed a Bayesian method to learn discontinuous SCFG rules.",
                "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.",
                "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.",
                "#REF and #TARGET_REF focused on joint parsing and alignment .",
                "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.",
                "adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.",
                "#REF re-trained the linguistic parsers bilingually based on word alignment."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Our previous work (#REF) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (#REF). #REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #TARGET_REF focused on joint parsing and alignment . They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #REF re-trained the linguistic parsers bilingually based on word alignment.",
        "output": "related work:Our previous work (#REF) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (#REF). #REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #TARGET_REF<USE/> focused on joint parsing and alignment . They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #REF re-trained the linguistic parsers bilingually based on word alignment."
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .",
                "A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:#TARGET_REF demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation . A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules.",
        "output": "nan:#TARGET_REF<FUT/> demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation . A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules."
    },
    {
        "gold": {
            "text": [
                "Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.",
                "Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (#REF), minimum description length principal (#REF), and the χ 2 statistic.",
                "( #TARGET_REF ) has found strong correlations between DF , IG and the X2 statistic for a term .",
                "On the other hand, (#REF) reports the χ 2 to produce best performance.",
                "In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (#REF).",
                "TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (#REF).",
                "Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (#REF), minimum description length principal (#REF), and the χ 2 statistic. ( #TARGET_REF ) has found strong correlations between DF , IG and the X2 statistic for a term . On the other hand, (#REF) reports the χ 2 to produce best performance. In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (#REF). TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (#REF). Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.",
        "output": "experiments:Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (#REF), minimum description length principal (#REF), and the χ 2 statistic. ( #TARGET_REF<BACK/> ) has found strong correlations between DF , IG and the X2 statistic for a term . On the other hand, (#REF) reports the χ 2 to produce best performance. In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (#REF). TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (#REF). Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents."
    },
    {
        "gold": {
            "text": [
                "The results for Arabic (Hajič et al., 2004;Smrž et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2).",
                "By contrast , Turkish ( #REF ; #TARGET_REF ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .",
                "It is noteworthy that Arabic and Turkish, being \"typological outliers\", show patterns that are different both from each other and from most of the other languages."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:The results for Arabic (Hajič et al., 2004;Smrž et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2). By contrast , Turkish ( #REF ; #TARGET_REF ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) . It is noteworthy that Arabic and Turkish, being \"typological outliers\", show patterns that are different both from each other and from most of the other languages.",
        "output": "experiments:The results for Arabic (Hajič et al., 2004;Smrž et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2). By contrast , Turkish ( #REF ; #TARGET_REF<USE/> ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) . It is noteworthy that Arabic and Turkish, being \"typological outliers\", show patterns that are different both from each other and from most of the other languages."
    },
    {
        "gold": {
            "text": [
                "results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition.",
                "In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions.",
                "This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations.",
                "The result holds for both the MaltParser ( #TARGET_REF ) and the Easy-First Parser ( #REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition. In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions. This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations. The result holds for both the MaltParser ( #TARGET_REF ) and the Easy-First Parser ( #REF ) .",
        "output": "introduction:results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition. In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions. This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations. The result holds for both the MaltParser ( #TARGET_REF<FUT/> ) and the Easy-First Parser ( #REF ) ."
    },
    {
        "gold": {
            "text": [
                "Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.",
                "For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( #TARGET_REF ) .",
        "output": "nan:Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & #REF ) , ( #REF ) , ( #REF ) , ( #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #TARGET_REF .",
                "This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume.",
                "Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution rules: for example, those that can only arise via a violation of scoping or binding constraints.",
                "The role of resolution rules (for perfectly good presentational reasons) is completely ignored by their treatment.",
                "However, it is really the case that in giving the semantics of a QLF, one is interested only in the set of RQLFs that are obtainable from it under closure of the resolution rules.",
                "Ideally, therefore, we would like a formal reconstruction of resolution rules as well.",
                "This is so, not just for reasons of formal hygiene in trying to make logical sense out of underspecified representations, but also because resolution rules and the knowledge they express are an important object of study in their own right."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #TARGET_REF . This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume. Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution rules: for example, those that can only arise via a violation of scoping or binding constraints. The role of resolution rules (for perfectly good presentational reasons) is completely ignored by their treatment. However, it is really the case that in giving the semantics of a QLF, one is interested only in the set of RQLFs that are obtainable from it under closure of the resolution rules. Ideally, therefore, we would like a formal reconstruction of resolution rules as well. This is so, not just for reasons of formal hygiene in trying to make logical sense out of underspecified representations, but also because resolution rules and the knowledge they express are an important object of study in their own right.",
        "output": "nan:A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #TARGET_REF<USE/> . This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume. Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution rules: for example, those that can only arise via a violation of scoping or binding constraints. The role of resolution rules (for perfectly good presentational reasons) is completely ignored by their treatment. However, it is really the case that in giving the semantics of a QLF, one is interested only in the set of RQLFs that are obtainable from it under closure of the resolution rules. Ideally, therefore, we would like a formal reconstruction of resolution rules as well. This is so, not just for reasons of formal hygiene in trying to make logical sense out of underspecified representations, but also because resolution rules and the knowledge they express are an important object of study in their own right."
    },
    {
        "gold": {
            "text": [
                "5 Significant bigrams are obtained using the n-gram statistics package NSP ( #TARGET_REF ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:5 Significant bigrams are obtained using the n-gram statistics package NSP ( #TARGET_REF ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) .",
        "output": "method:5 Significant bigrams are obtained using the n-gram statistics package NSP ( #TARGET_REF<FUT/> ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) ."
    },
    {
        "gold": {
            "text": [
                "The task of mention detection is closely related to Named Entity Recognition (NER).",
                "#REF thoroughly study phrase identification in sentences and propose three different general approaches.",
                "They aim to learn several different local classifiers and combine them to optimally satisfy some global constraints.",
                "#REF propose to select certain rules based on a given corpus, to identify base noun phrases.",
                "However, the phrases detected are not necessarily mentions that we need to discover.",
                "#TARGET_REF present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .",
                "NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g.",
                "Hidden Markov Model (#REF) or Conditional Random Fields (#REF).",
                "The typical BIO representation was introduced in #REF; OC representations were introduced in #REF, while #REF further study nested named entity recognition, which employs a tree structure as a representation of identifying named entities within other named entities."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:The task of mention detection is closely related to Named Entity Recognition (NER). #REF thoroughly study phrase identification in sentences and propose three different general approaches. They aim to learn several different local classifiers and combine them to optimally satisfy some global constraints. #REF propose to select certain rules based on a given corpus, to identify base noun phrases. However, the phrases detected are not necessarily mentions that we need to discover. #TARGET_REF present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge . NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g. Hidden Markov Model (#REF) or Conditional Random Fields (#REF). The typical BIO representation was introduced in #REF; OC representations were introduced in #REF, while #REF further study nested named entity recognition, which employs a tree structure as a representation of identifying named entities within other named entities.",
        "output": "related work:The task of mention detection is closely related to Named Entity Recognition (NER). #REF thoroughly study phrase identification in sentences and propose three different general approaches. They aim to learn several different local classifiers and combine them to optimally satisfy some global constraints. #REF propose to select certain rules based on a given corpus, to identify base noun phrases. However, the phrases detected are not necessarily mentions that we need to discover. #TARGET_REF<BACK/> present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge . NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g. Hidden Markov Model (#REF) or Conditional Random Fields (#REF). The typical BIO representation was introduced in #REF; OC representations were introduced in #REF, while #REF further study nested named entity recognition, which employs a tree structure as a representation of identifying named entities within other named entities."
    },
    {
        "gold": {
            "text": [
                "It is worth emphasizing that we do not necessarily expect the dependency representations produced by the training method to be traditional dependency structures for the two languages.",
                "Instead, the aim is to produce bilingual (i.e., synchronized, see below) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus.",
                "For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching.",
                "This contrasts with one of the traditional approaches ( e.g. , #TARGET_REF ; #REF ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:It is worth emphasizing that we do not necessarily expect the dependency representations produced by the training method to be traditional dependency structures for the two languages. Instead, the aim is to produce bilingual (i.e., synchronized, see below) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus. For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching. This contrasts with one of the traditional approaches ( e.g. , #TARGET_REF ; #REF ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .",
        "output": "method:It is worth emphasizing that we do not necessarily expect the dependency representations produced by the training method to be traditional dependency structures for the two languages. Instead, the aim is to produce bilingual (i.e., synchronized, see below) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus. For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching. This contrasts with one of the traditional approaches ( e.g. , #TARGET_REF<USE/> ; #REF ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language ."
    },
    {
        "gold": {
            "text": [
                "The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #TARGET_REF ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #REF ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and #REF ] ) , both outperforming it : ( d ) Kulick , Gabbard , and #REF 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #TARGET_REF ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #REF ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and #REF ] ) , both outperforming it : ( d ) Kulick , Gabbard , and #REF 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .",
        "output": "experiments:The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #TARGET_REF<FUT/> ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #REF ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and #REF ] ) , both outperforming it : ( d ) Kulick , Gabbard , and #REF 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 ."
    },
    {
        "gold": {
            "text": [
                "Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., #REF).",
                "The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #TARGET_REF ; #REF ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .",
                "Our own proposal will abstract away from the effects of linguistic context.",
                "We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.",
                "This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., #REF). The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #TARGET_REF ; #REF ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large . Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words. This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of",
        "output": "introduction:Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., #REF). The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #TARGET_REF<BACK/> ; #REF ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large . Our own proposal will abstract away from the effects of linguistic context. We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words. This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of"
    },
    {
        "gold": {
            "text": [
                "At the same time , we believe our method has advantages over the approach developed initially at IBM ( #TARGET_REF ; #REF ) for training translation systems automatically .",
                "One advantage is that our method attempts to model the natural decomposition of sentences into phrases.",
                "Another is that the compilation of this decomposition into lexically anchored finite-state head transducers produces implementations that are much more efficient than those for the IBM model.",
                "In particular, our search algorithm finds optimal transductions of test sentences in less than \"real time\" on a 300MHz processor, that is, the time to translate an utterance is less than the time taken to speak it, an important consideration for our speech translation application."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:At the same time , we believe our method has advantages over the approach developed initially at IBM ( #TARGET_REF ; #REF ) for training translation systems automatically . One advantage is that our method attempts to model the natural decomposition of sentences into phrases. Another is that the compilation of this decomposition into lexically anchored finite-state head transducers produces implementations that are much more efficient than those for the IBM model. In particular, our search algorithm finds optimal transductions of test sentences in less than \"real time\" on a 300MHz processor, that is, the time to translate an utterance is less than the time taken to speak it, an important consideration for our speech translation application.",
        "output": "conclusion:At the same time , we believe our method has advantages over the approach developed initially at IBM ( #TARGET_REF<USE/> ; #REF ) for training translation systems automatically . One advantage is that our method attempts to model the natural decomposition of sentences into phrases. Another is that the compilation of this decomposition into lexically anchored finite-state head transducers produces implementations that are much more efficient than those for the IBM model. In particular, our search algorithm finds optimal transductions of test sentences in less than \"real time\" on a 300MHz processor, that is, the time to translate an utterance is less than the time taken to speak it, an important consideration for our speech translation application."
    },
    {
        "gold": {
            "text": [
                "Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #REF ; #REF ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #TARGET_REF ) to compare it to other shallow parsers .",
                "Table 1 shows that it ranks among the top shallow parsers evaluated there 1 ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #REF ; #REF ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #TARGET_REF ) to compare it to other shallow parsers . Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .",
        "output": "experiments:Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #REF ; #REF ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #TARGET_REF<FUT/> ) to compare it to other shallow parsers . Table 1 shows that it ranks among the top shallow parsers evaluated there 1 ."
    },
    {
        "gold": {
            "text": [
                "Semantic similarity is typically defined via the lexical relations of synonymy (automobile -car) and hypernymy (vehicle -car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be-tween two words (#REF). 3",
                "Dissimilar words can be semantically related, e.g.",
                "via functional relationships (night -dark) or when they are antonyms (high -low).",
                "Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Semantic similarity is typically defined via the lexical relations of synonymy (automobile -car) and hypernymy (vehicle -car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be-tween two words (#REF). 3 Dissimilar words can be semantically related, e.g. via functional relationships (night -dark) or when they are antonyms (high -low). Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #TARGET_REF ) .",
        "output": "introduction:Semantic similarity is typically defined via the lexical relations of synonymy (automobile -car) and hypernymy (vehicle -car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be-tween two words (#REF). 3 Dissimilar words can be semantically related, e.g. via functional relationships (night -dark) or when they are antonyms (high -low). Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as #REF which produces weather reports from structured data or #REF which generates descriptions of objects from images.",
                "Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #REF ) ( #REF ) ( #REF ) or song lyrics ( #TARGET_REF ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .",
                "In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as #REF which produces weather reports from structured data or #REF which generates descriptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #REF ) ( #REF ) ( #REF ) or song lyrics ( #TARGET_REF ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced . In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.",
        "output": "related work:The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as #REF which produces weather reports from structured data or #REF which generates descriptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #REF ) ( #REF ) ( #REF ) or song lyrics ( #TARGET_REF<USE/> ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced . In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric."
    },
    {
        "gold": {
            "text": [
                "In applying sample selection to training a PP-attachment model, we have observed that all effective evaluation functions make use of the model's current hypothesis in estimating the training utility of the candidates.",
                "Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor.",
                "In this section, we investigate whether these observations hold true for training statistical parsing models as well.",
                "Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and #REF ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #REF ; #TARGET_REF ) , and Collins 's Model 2 parser ( 1997 ) .",
                "Although both models are lexicalized, statistical parsers, their learning algorithms are different.",
                "The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data.",
                "In contrast, PLTIG's expectation-maximization-based induction algorithm is partially supervised; the model's parameters are estimated indirectly from the training data."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:In applying sample selection to training a PP-attachment model, we have observed that all effective evaluation functions make use of the model's current hypothesis in estimating the training utility of the candidates. Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor. In this section, we investigate whether these observations hold true for training statistical parsing models as well. Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and #REF ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #REF ; #TARGET_REF ) , and Collins 's Model 2 parser ( 1997 ) . Although both models are lexicalized, statistical parsers, their learning algorithms are different. The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data. In contrast, PLTIG's expectation-maximization-based induction algorithm is partially supervised; the model's parameters are estimated indirectly from the training data.",
        "output": "nan:In applying sample selection to training a PP-attachment model, we have observed that all effective evaluation functions make use of the model's current hypothesis in estimating the training utility of the candidates. Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor. In this section, we investigate whether these observations hold true for training statistical parsing models as well. Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and #REF ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #REF ; #TARGET_REF<FUT/> ) , and Collins 's Model 2 parser ( 1997 ) . Although both models are lexicalized, statistical parsers, their learning algorithms are different. The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data. In contrast, PLTIG's expectation-maximization-based induction algorithm is partially supervised; the model's parameters are estimated indirectly from the training data."
    },
    {
        "gold": {
            "text": [
                "A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.",
                "#TARGET_REF considers the second verb V2 as an aspectual complex comparable to the auxiliaries .",
                "#REF argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.",
                "#REF tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\".",
                "Similar findings have been proposed by #REF that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.",
                "#REF tried to represent Bangla CVs in terms of HPSG formalism.",
                "She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.",
                "Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. #TARGET_REF considers the second verb V2 as an aspectual complex comparable to the auxiliaries . #REF argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. #REF tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by #REF that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. #REF tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible. Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.",
        "output": "related work:A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. #TARGET_REF<BACK/> considers the second verb V2 as an aspectual complex comparable to the auxiliaries . #REF argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. #REF tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by #REF that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints. #REF tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible. Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models."
    },
    {
        "gold": {
            "text": [
                "Thus far, we have added all semantic filters by hand, and they are implemented in a hard-fail mode, i.e., if the semantic restrictions fail, the node dies.",
                "This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.",
                "In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.",
                "Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.",
                "This approach resembles the work by #TARGET_REF and #REF on selectional restrictions .",
                "The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.",
                "There is obviously a great deal more work to be done in this important area."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Thus far, we have added all semantic filters by hand, and they are implemented in a hard-fail mode, i.e., if the semantic restrictions fail, the node dies. This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains. In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. This approach resembles the work by #TARGET_REF and #REF on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences. There is obviously a great deal more work to be done in this important area.",
        "output": "nan:Thus far, we have added all semantic filters by hand, and they are implemented in a hard-fail mode, i.e., if the semantic restrictions fail, the node dies. This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains. In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. This approach resembles the work by #TARGET_REF<USE/> and #REF on selectional restrictions . The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences. There is obviously a great deal more work to be done in this important area."
    },
    {
        "gold": {
            "text": [
                "The EM algorithm ( #TARGET_REF ) can maximize these functions .",
                "Roughly, the E step guesses hidden information: if (x i , y i ) was generated from the current f θ , which FST paths stand a chance of having been the path used?",
                "(Guessing the path also guesses the exact input and output.)",
                "The M step updates θ to make those paths more likely.",
                "EM alternates these steps and converges to a local optimum.",
                "The M step's form depends on the parameterization and the E step serves the M step's needs."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The EM algorithm ( #TARGET_REF ) can maximize these functions . Roughly, the E step guesses hidden information: if (x i , y i ) was generated from the current f θ , which FST paths stand a chance of having been the path used? (Guessing the path also guesses the exact input and output.) The M step updates θ to make those paths more likely. EM alternates these steps and converges to a local optimum. The M step's form depends on the parameterization and the E step serves the M step's needs.",
        "output": "nan:The EM algorithm ( #TARGET_REF<FUT/> ) can maximize these functions . Roughly, the E step guesses hidden information: if (x i , y i ) was generated from the current f θ , which FST paths stand a chance of having been the path used? (Guessing the path also guesses the exact input and output.) The M step updates θ to make those paths more likely. EM alternates these steps and converges to a local optimum. The M step's form depends on the parameterization and the E step serves the M step's needs."
    },
    {
        "gold": {
            "text": [
                "The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1�5) for statistical machine translation and the concept of �word-by- word� alignment, the correspondence between words in source and target languages.",
                "Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment.",
                "Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and #REF ] and rules [ #TARGET_REF ; #REF ] ) as well as for MT system combination (Matusov, Ueffing, and #REF).",
                "But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (#REF; #REF; Ganchev, Gillenwater, and #REF); discovery of paraphrases (#REF); and joint unsupervised POS and parser induction across languages (#REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1�5) for statistical machine translation and the concept of �word-by- word� alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and #REF ] and rules [ #TARGET_REF ; #REF ] ) as well as for MT system combination (Matusov, Ueffing, and #REF). But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (#REF; #REF; Ganchev, Gillenwater, and #REF); discovery of paraphrases (#REF); and joint unsupervised POS and parser induction across languages (#REF).",
        "output": "introduction:The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1�5) for statistical machine translation and the concept of �word-by- word� alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and #REF, are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and #REF ] and rules [ #TARGET_REF<BACK/> ; #REF ] ) as well as for MT system combination (Matusov, Ueffing, and #REF). But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (#REF; #REF; Ganchev, Gillenwater, and #REF); discovery of paraphrases (#REF); and joint unsupervised POS and parser induction across languages (#REF)."
    },
    {
        "gold": {
            "text": [
                "The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and #REF ) and tagging ( #TARGET_REF ) .",
                "It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (#REF).",
                "To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n.",
                "We can compute the per-utterance posterior DA probabilities by summing:"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and #REF ) and tagging ( #TARGET_REF ) . It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (#REF). To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n. We can compute the per-utterance posterior DA probabilities by summing:",
        "output": "nan:The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and #REF ) and tagging ( #TARGET_REF<USE/> ) . It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (#REF). To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n. We can compute the per-utterance posterior DA probabilities by summing:"
    },
    {
        "gold": {
            "text": [
                "We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users' requests. 7",
                "A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request.",
                "During the For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person (#REF), but the simple binary bag-of-lemmas representation yielded similar results.",
                "7 We employed the LIBSVM package ( #TARGET_REF ) .",
                "prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3.",
                "We then apply the following steps."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users' requests. 7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request. During the For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person (#REF), but the simple binary bag-of-lemmas representation yielded similar results. 7 We employed the LIBSVM package ( #TARGET_REF ) . prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3. We then apply the following steps.",
        "output": "method:We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users' requests. 7 A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request. During the For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person (#REF), but the simple binary bag-of-lemmas representation yielded similar results. 7 We employed the LIBSVM package ( #TARGET_REF<FUT/> ) . prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3. We then apply the following steps."
    },
    {
        "gold": {
            "text": [
                "9.4.1 A New Perspective on Salience.",
                "#TARGET_REF have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available .",
                "In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse.",
                "Now suppose we let GRE treat salience just like other gradable Attributes.",
                "Suppose there are ten mice, five of which are black, whose degrees of salience are 1, 1, 3, 4, and 5 (the last one being most salient), while the other objects in the domain (cats, white mice) all have a higher salience.",
                "Then our algorithm might generate this list of properties: L = mouse, black, salience > 4 .",
                "This is a distinguishing description for the black mouse whose salience is 5: the most salient black mouse.",
                "The simpler description the black mouse can be derived by stipulating that the property of being most salient can be left implicit in English."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:9.4.1 A New Perspective on Salience. #TARGET_REF have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse. Now suppose we let GRE treat salience just like other gradable Attributes. Suppose there are ten mice, five of which are black, whose degrees of salience are 1, 1, 3, 4, and 5 (the last one being most salient), while the other objects in the domain (cats, white mice) all have a higher salience. Then our algorithm might generate this list of properties: L = mouse, black, salience > 4 . This is a distinguishing description for the black mouse whose salience is 5: the most salient black mouse. The simpler description the black mouse can be derived by stipulating that the property of being most salient can be left implicit in English.",
        "output": "experiments:9.4.1 A New Perspective on Salience. #TARGET_REF<BACK/> have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available . In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse. Now suppose we let GRE treat salience just like other gradable Attributes. Suppose there are ten mice, five of which are black, whose degrees of salience are 1, 1, 3, 4, and 5 (the last one being most salient), while the other objects in the domain (cats, white mice) all have a higher salience. Then our algorithm might generate this list of properties: L = mouse, black, salience > 4 . This is a distinguishing description for the black mouse whose salience is 5: the most salient black mouse. The simpler description the black mouse can be derived by stipulating that the property of being most salient can be left implicit in English."
    },
    {
        "gold": {
            "text": [
                "An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #TARGET_REF ) .",
                "This technique provides two important advantages.",
                "First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics.",
                "This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training.",
                "Second, using continuous distributions allows us to leverage a variety of tools (e.g., LDA) that have been shown to be successful in other fields, such as speech recognition (#REF)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #TARGET_REF ) . This technique provides two important advantages. First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics. This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training. Second, using continuous distributions allows us to leverage a variety of tools (e.g., LDA) that have been shown to be successful in other fields, such as speech recognition (#REF).",
        "output": "conclusion:An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #TARGET_REF<USE/> ) . This technique provides two important advantages. First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics. This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training. Second, using continuous distributions allows us to leverage a variety of tools (e.g., LDA) that have been shown to be successful in other fields, such as speech recognition (#REF)."
    },
    {
        "gold": {
            "text": [
                "They are widely used in MT as a way to figure out how to translate input in one language into output in another language (#REF).",
                "There are several methods to build phrase tables.",
                "The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.",
                "In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .",
                "We run TreeTagger ( #REF ) for tokenization , and used the Giza + + ( #TARGET_REF ) to align the tokenized corpora at the word level .",
                "Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (#REF).",
                "Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.",
                "In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:They are widely used in MT as a way to figure out how to translate input in one language into output in another language (#REF). There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 . We run TreeTagger ( #REF ) for tokenization , and used the Giza + + ( #TARGET_REF ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (#REF). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.",
        "output": "nan:They are widely used in MT as a way to figure out how to translate input in one language into output in another language (#REF). There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 . We run TreeTagger ( #REF ) for tokenization , and used the Giza + + ( #TARGET_REF<FUT/> ) to align the tokenized corpora at the word level . Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (#REF). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5."
    },
    {
        "gold": {
            "text": [
                "There has been some controversy, at least for simple stemmers (#REF;#REF), about the effectiveness of morphological analysis for document retrieval (#REF;#REF;#REF).",
                "The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.",
                "Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "conclusion:There has been some controversy, at least for simple stemmers (#REF;#REF), about the effectiveness of morphological analysis for document retrieval (#REF;#REF;#REF). The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary. Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( #REF ; #TARGET_REF ; #REF ) .",
        "output": "conclusion:There has been some controversy, at least for simple stemmers (#REF;#REF), about the effectiveness of morphological analysis for document retrieval (#REF;#REF;#REF). The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary. Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( #REF ; #TARGET_REF<BACK/> ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.",
                "PropBank 1.0 includes the annotations for files chtb_001.fid",
                "to chtb_931.fid,",
                "or the first 250K words of the Chinese TreeBank 5.1.",
                "For the experiments, the data of PropBank is divided into three parts.",
                "648 files (from chtb_081 to chtb_899.fid)",
                "are used as the training set.",
                "The development set includes 40 files, from chtb_041.fid to chtb_080.fid.",
                "The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931.",
                "We use the same data setting with #REF , however a bit different from #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments. PropBank 1.0 includes the annotations for files chtb_001.fid to chtb_931.fid, or the first 250K words of the Chinese TreeBank 5.1. For the experiments, the data of PropBank is divided into three parts. 648 files (from chtb_081 to chtb_899.fid) are used as the training set. The development set includes 40 files, from chtb_041.fid to chtb_080.fid. The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931. We use the same data setting with #REF , however a bit different from #TARGET_REF .",
        "output": "experiments:We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments. PropBank 1.0 includes the annotations for files chtb_001.fid to chtb_931.fid, or the first 250K words of the Chinese TreeBank 5.1. For the experiments, the data of PropBank is divided into three parts. 648 files (from chtb_081 to chtb_899.fid) are used as the training set. The development set includes 40 files, from chtb_041.fid to chtb_080.fid. The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931. We use the same data setting with #REF , however a bit different from #TARGET_REF<USE/> ."
    },
    {
        "gold": {
            "text": [
                "The ACE-2004 dataset contains 443 documents.",
                "We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #REF ; #TARGET_REF ) .",
                "The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (#REF, contains 3,145 annotated documents.",
                "These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.",
                "We report results on the test documents for both datasets.",
                ") indicate evaluations on (mentions, mention heads) respectively.",
                "For gold mentions and mention heads, they yield the same performance for coreference.",
                "Our proposed H-Joint-M system achieves the highest performance.",
                "Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #REF ; #TARGET_REF ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (#REF, contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. ) indicate evaluations on (mentions, mention heads) respectively. For gold mentions and mention heads, they yield the same performance for coreference. Our proposed H-Joint-M system achieves the highest performance. Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3.",
        "output": "experiments:The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #REF ; #TARGET_REF<FUT/> ) . The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (#REF, contains 3,145 annotated documents. These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. ) indicate evaluations on (mentions, mention heads) respectively. For gold mentions and mention heads, they yield the same performance for coreference. Our proposed H-Joint-M system achieves the highest performance. Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3."
    },
    {
        "gold": {
            "text": [
                "A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.",
                "#REF considers the second verb V2 as an aspectual complex comparable to the auxiliaries.",
                "#REF argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.",
                "#REF tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\".",
                "Similar findings have been proposed by #TARGET_REF that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .",
                "#REF tried to represent Bangla CVs in terms of HPSG formalism.",
                "She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.",
                "Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. #REF considers the second verb V2 as an aspectual complex comparable to the auxiliaries. #REF argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. #REF tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by #TARGET_REF that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints . #REF tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible. Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.",
        "output": "related work:A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus. #REF considers the second verb V2 as an aspectual complex comparable to the auxiliaries. #REF argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure. #REF tried to construct a semantic analysis based on \"prepared\" and \"unprepared mind\". Similar findings have been proposed by #TARGET_REF<BACK/> that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints . #REF tried to represent Bangla CVs in terms of HPSG formalism. She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible. Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models."
    },
    {
        "gold": {
            "text": [
                "However, it is worth noting that their corpus is significantly smaller than ours (805 question-answer pairs), their questions seem to be much simpler and shorter than those in our corpus, and the replies shorter and more homogeneous.",
                "Malik, Subramaniam, and #REF developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer.",
                "This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences.",
                "In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates.",
                "#TARGET_REF investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .",
                "Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed.",
                "The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches.",
                "However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:However, it is worth noting that their corpus is significantly smaller than ours (805 question-answer pairs), their questions seem to be much simpler and shorter than those in our corpus, and the replies shorter and more homogeneous. Malik, Subramaniam, and #REF developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer. This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences. In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates. #TARGET_REF investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering . Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed. The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches. However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts).",
        "output": "nan:However, it is worth noting that their corpus is significantly smaller than ours (805 question-answer pairs), their questions seem to be much simpler and shorter than those in our corpus, and the replies shorter and more homogeneous. Malik, Subramaniam, and #REF developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer. This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences. In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates. #TARGET_REF<USE/> investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering . Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed. The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches. However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts)."
    },
    {
        "gold": {
            "text": [
                "We run GIZA++ (#REF) on the training corpus in both directions (#REF) to obtain the word alignment for each sentence pair.",
                "We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (#REF) with modified Kneser-Ney smoothing (#REF).",
                "In our experiments the translation performances are measured by case-insensitive BLEU4 metric (#REF) and we use mteval-v13a.pl",
                "as the evaluation tool.",
                "The significance testing is performed by paired bootstrap re-sampling ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We run GIZA++ (#REF) on the training corpus in both directions (#REF) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (#REF) with modified Kneser-Ney smoothing (#REF). In our experiments the translation performances are measured by case-insensitive BLEU4 metric (#REF) and we use mteval-v13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling ( #TARGET_REF ) .",
        "output": "experiments:We run GIZA++ (#REF) on the training corpus in both directions (#REF) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (#REF) with modified Kneser-Ney smoothing (#REF). In our experiments the translation performances are measured by case-insensitive BLEU4 metric (#REF) and we use mteval-v13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "When analysing texts, it is essential to see how elements of meaning are interconnected.",
                "This is an old idea.",
                "The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1 .",
                "He was a grammarian who analysed Sanskrit ( #TARGET_REF ) .",
                "The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; #REF; Fill- more, 1968).",
                "Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (#REF; #REF; #REF; #REF; #REF; #REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:When analysing texts, it is essential to see how elements of meaning are interconnected. This is an old idea. The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1 . He was a grammarian who analysed Sanskrit ( #TARGET_REF ) . The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; #REF; Fill- more, 1968). Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (#REF; #REF; #REF; #REF; #REF; #REF).",
        "output": "introduction:When analysing texts, it is essential to see how elements of meaning are interconnected. This is an old idea. The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1 . He was a grammarian who analysed Sanskrit ( #TARGET_REF<BACK/> ) . The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; #REF; Fill- more, 1968). Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (#REF; #REF; #REF; #REF; #REF; #REF)."
    },
    {
        "gold": {
            "text": [
                "Although there are other discussions of the paragraph as a central element of discourse ( e.g. #REF , #REF , #REF , #TARGET_REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .",
                "Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.",
                "Our interest, however, lies precisely in that area."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Although there are other discussions of the paragraph as a central element of discourse ( e.g. #REF , #REF , #REF , #TARGET_REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area.",
        "output": "introduction:Although there are other discussions of the paragraph as a central element of discourse ( e.g. #REF , #REF , #REF , #TARGET_REF<USE/> ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area."
    },
    {
        "gold": {
            "text": [
                "For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( #TARGET_REF ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 .",
                "We used the same training / devtest split as in Zitouni, Sorensen, and #REF; and we further split the devtest into two equal parts: a development (dev) set and a blind test set.",
                "For all experiments, unless specified otherwise, we used the dev set. 10 We kept the test unseen (\"blind\") during training and model development.",
                "Statistics about this split (after conversion to the CATiB dependency format) are given in Table 1."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( #TARGET_REF ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 . We used the same training / devtest split as in Zitouni, Sorensen, and #REF; and we further split the devtest into two equal parts: a development (dev) set and a blind test set. For all experiments, unless specified otherwise, we used the dev set. 10 We kept the test unseen (\"blind\") during training and model development. Statistics about this split (after conversion to the CATiB dependency format) are given in Table 1.",
        "output": "related work:For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( #TARGET_REF<FUT/> ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 . We used the same training / devtest split as in Zitouni, Sorensen, and #REF; and we further split the devtest into two equal parts: a development (dev) set and a blind test set. For all experiments, unless specified otherwise, we used the dev set. 10 We kept the test unseen (\"blind\") during training and model development. Statistics about this split (after conversion to the CATiB dependency format) are given in Table 1."
    },
    {
        "gold": {
            "text": [
                "There has been some controversy, at least for simple stemmers (#REF;#REF), about the effectiveness of morphological analysis for document retrieval (#REF;#REF;#REF).",
                "The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.",
                "Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "conclusion:There has been some controversy, at least for simple stemmers (#REF;#REF), about the effectiveness of morphological analysis for document retrieval (#REF;#REF;#REF). The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary. Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( #REF ; #REF ; #TARGET_REF ) .",
        "output": "conclusion:There has been some controversy, at least for simple stemmers (#REF;#REF), about the effectiveness of morphological analysis for document retrieval (#REF;#REF;#REF). The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary. Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( #REF ; #REF ; #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "The situation is worse if there are English words (e.g., adjectives) separating his and brother.",
                "This required mapping is a significant problem for generalization.",
                "We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection).",
                "We apply a \"split in preprocessing and resynthesize in postprocessing\" approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et.",
                "al.",
                "The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008), which deals with verbal morphology and attached pronouns.",
                "There has been other work on solving inflection.",
                "#REF introduced factored SMT.",
                "We use more complex context features.",
                "#REF tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.",
                "#TARGET_REF improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .",
                "Both efforts were ineffective on large data sets.",
                "#REF used unification in an SMT system to model some of the agreement phenomena that we model.",
                "Our CRF framework allows us to use more complex context features."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:The situation is worse if there are English words (e.g., adjectives) separating his and brother. This required mapping is a significant problem for generalization. We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection). We apply a \"split in preprocessing and resynthesize in postprocessing\" approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al. The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008), which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. #REF introduced factored SMT. We use more complex context features. #REF tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. #TARGET_REF improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) . Both efforts were ineffective on large data sets. #REF used unification in an SMT system to model some of the agreement phenomena that we model. Our CRF framework allows us to use more complex context features.",
        "output": "related work:The situation is worse if there are English words (e.g., adjectives) separating his and brother. This required mapping is a significant problem for generalization. We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection). We apply a \"split in preprocessing and resynthesize in postprocessing\" approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al. The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008), which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. #REF introduced factored SMT. We use more complex context features. #REF tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. #TARGET_REF<USE/> improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) . Both efforts were ineffective on large data sets. #REF used unification in an SMT system to model some of the agreement phenomena that we model. Our CRF framework allows us to use more complex context features."
    },
    {
        "gold": {
            "text": [
                "In the first experiment , we use an induction algorithm ( #TARGET_REFa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .",
                "The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data.",
                "In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing.",
                "Here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units.",
                "For example, the sentence Several fund managers expect a rough market this morning before prices stabilize.",
                "would be labeled as \"((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)\"",
                "Our algorithm is similar to the approach taken by #REF for inducing PCFG parsers."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:In the first experiment , we use an induction algorithm ( #TARGET_REFa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs . The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data. In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing. Here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units. For example, the sentence Several fund managers expect a rough market this morning before prices stabilize. would be labeled as \"((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)\" Our algorithm is similar to the approach taken by #REF for inducing PCFG parsers.",
        "output": "nan:In the first experiment , we use an induction algorithm ( #TARGET_REF<FUT/>a ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs . The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data. In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing. Here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units. For example, the sentence Several fund managers expect a rough market this morning before prices stabilize. would be labeled as \"((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)\" Our algorithm is similar to the approach taken by #REF for inducing PCFG parsers."
    },
    {
        "gold": {
            "text": [
                "There is a well recognised problem with providing computational support for machine readable dictionaries, in particular where issues of access are concerned.",
                "On the one hand, dictionaries exhibit far too much structure for conventional techniques for managing 'flat' text to apply to them.",
                "On the other hand, the equally large amounts of free text in dictionary entries, as well as the implicitly marked relationships commonly used to encode linguistic information, makes a dictionary difficult to represent as a structured database of a standard, eg.",
                "relational, type.",
                "In addition, in order to link the machine readable version of LDOCE to our development environment, and eventually to our natural language processing systems, we need to provide fast access from Lisp to data held in secondary storage.",
                "Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #TARGET_REF ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:There is a well recognised problem with providing computational support for machine readable dictionaries, in particular where issues of access are concerned. On the one hand, dictionaries exhibit far too much structure for conventional techniques for managing 'flat' text to apply to them. On the other hand, the equally large amounts of free text in dictionary entries, as well as the implicitly marked relationships commonly used to encode linguistic information, makes a dictionary difficult to represent as a structured database of a standard, eg. relational, type. In addition, in order to link the machine readable version of LDOCE to our development environment, and eventually to our natural language processing systems, we need to provide fast access from Lisp to data held in secondary storage. Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #TARGET_REF ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape .",
        "output": "nan:There is a well recognised problem with providing computational support for machine readable dictionaries, in particular where issues of access are concerned. On the one hand, dictionaries exhibit far too much structure for conventional techniques for managing 'flat' text to apply to them. On the other hand, the equally large amounts of free text in dictionary entries, as well as the implicitly marked relationships commonly used to encode linguistic information, makes a dictionary difficult to represent as a structured database of a standard, eg. relational, type. In addition, in order to link the machine readable version of LDOCE to our development environment, and eventually to our natural language processing systems, we need to provide fast access from Lisp to data held in secondary storage. Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #TARGET_REF<BACK/> ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape ."
    },
    {
        "gold": {
            "text": [
                "Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #REF ; #TARGET_REF ; #REF ) .",
                "#REF;#REF;#REF).",
                "But we won't pursue this topic further here."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #REF ; #TARGET_REF ; #REF ) . #REF;#REF;#REF). But we won't pursue this topic further here.",
        "output": "introduction:Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #REF ; #TARGET_REF<USE/> ; #REF ) . #REF;#REF;#REF). But we won't pursue this topic further here."
    },
    {
        "gold": {
            "text": [
                "• A deterministic algorithm for building labeled projective dependency graphs (#REF).",
                "• History-based feature models for predicting the next parser action (#REF).",
                "Support vector machines for mapping histories to parser actions ( #TARGET_REF ) .",
                "• Graph transformations for recovering nonprojective structures ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:• A deterministic algorithm for building labeled projective dependency graphs (#REF). • History-based feature models for predicting the next parser action (#REF). Support vector machines for mapping histories to parser actions ( #TARGET_REF ) . • Graph transformations for recovering nonprojective structures .",
        "output": "introduction:• A deterministic algorithm for building labeled projective dependency graphs (#REF). • History-based feature models for predicting the next parser action (#REF). Support vector machines for mapping histories to parser actions ( #TARGET_REF<FUT/> ) . • Graph transformations for recovering nonprojective structures ."
    },
    {
        "gold": {
            "text": [
                "The psycholinguistic studies of #REF , #REF , #REF , #REF , #REF , and #TARGET_REF , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .",
                "For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.",
                "#REF claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.",
                "Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The psycholinguistic studies of #REF , #REF , #REF , #REF , #REF , and #TARGET_REF , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency . For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings. #REF claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it. Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.",
        "output": "introduction:The psycholinguistic studies of #REF , #REF , #REF , #REF , #REF , and #TARGET_REF<BACK/> , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency . For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings. #REF claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it. Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly."
    },
    {
        "gold": {
            "text": [
                "A recent study by #TARGET_REF also investigates the task of training parsers to improve MT reordering .",
                "In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.",
                "The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.",
                "The method is called targeted self-training as it is similar in vein to self-training (#REF), with the exception that the new parse data is targeted to produce accurate word reorderings.",
                "Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.",
                "This allows us to give guarantees of convergence.",
                "Furthermore, we also evaluate the method on alternate extrinsic loss functions.",
                "#REF presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system.",
                "Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.",
                "In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:A recent study by #TARGET_REF also investigates the task of training parsers to improve MT reordering . In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training (#REF), with the exception that the new parse data is targeted to produce accurate word reorderings. Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions. #REF presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system. Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.",
        "output": "related work:A recent study by #TARGET_REF<USE/> also investigates the task of training parsers to improve MT reordering . In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training (#REF), with the exception that the new parse data is targeted to produce accurate word reorderings. Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions. #REF presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system. Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score."
    },
    {
        "gold": {
            "text": [
                "Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #TARGET_REF .",
                "Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (#REF;#REF).",
                "It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (#REF).",
                "These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (#REF).",
                "However, prior work using mLDA is limited to two modalities at a time.",
                "In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #TARGET_REF . Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (#REF;#REF). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (#REF). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (#REF). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities.",
        "output": "method:Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #TARGET_REF<FUT/> . Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (#REF;#REF). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (#REF). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (#REF). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities."
    },
    {
        "gold": {
            "text": [
                "All current approaches to monolingual TE , either syntactically oriented ( #REF ) , or applying logical inference ( #REF ) , or adopting transformation-based techniques ( #TARGET_REF ; #REF ) , incorporate different types of lexical knowledge to support textual inference .",
                "Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.",
                "WordNet, the most widely used resource in TE, provides all the three types of information.",
                "Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.",
                "Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.",
                "Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:All current approaches to monolingual TE , either syntactically oriented ( #REF ) , or applying logical inference ( #REF ) , or adopting transformation-based techniques ( #TARGET_REF ; #REF ) , incorporate different types of lexical knowledge to support textual inference . Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores. WordNet, the most widely used resource in TE, provides all the three types of information. Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable. Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text. Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis.",
        "output": "introduction:All current approaches to monolingual TE , either syntactically oriented ( #REF ) , or applying logical inference ( #REF ) , or adopting transformation-based techniques ( #TARGET_REF<BACK/> ; #REF ) , incorporate different types of lexical knowledge to support textual inference . Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores. WordNet, the most widely used resource in TE, provides all the three types of information. Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable. Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text. Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis."
    },
    {
        "gold": {
            "text": [
                "The language chosen for semantic representation is a flat semantics along the line of ( #TARGET_REF ; #REF ; #REF ) .",
                "However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.",
                "Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .",
                ", x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The language chosen for semantic representation is a flat semantics along the line of ( #TARGET_REF ; #REF ; #REF ) . However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships. Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . . , x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.",
        "output": "nan:The language chosen for semantic representation is a flat semantics along the line of ( #TARGET_REF<USE/> ; #REF ; #REF ) . However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships. Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . . , x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing."
    },
    {
        "gold": {
            "text": [
                "To date, four distinct domain-specific versions of TINA have been implemented.",
                "The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (#REF).",
                "The second version ( RM ) concerns the Resource Management task ( #TARGET_REF ) that has been popular within the DARPA community in recent years .",
                "The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (#REF).",
                "The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.",
                "A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:To date, four distinct domain-specific versions of TINA have been implemented. The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (#REF). The second version ( RM ) concerns the Resource Management task ( #TARGET_REF ) that has been popular within the DARPA community in recent years . The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (#REF). The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region. A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community.",
        "output": "nan:To date, four distinct domain-specific versions of TINA have been implemented. The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (#REF). The second version ( RM ) concerns the Resource Management task ( #TARGET_REF<FUT/> ) that has been popular within the DARPA community in recent years . The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (#REF). The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region. A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community."
    },
    {
        "gold": {
            "text": [
                "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.",
                "This includes work on question answering (#REF), sentiment analysis (#REF), MT reordering (#REF), and many other tasks.",
                "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #REF ; #TARGET_REF ; #REF ; #REF ) .",
                "But these accuracies are measured with respect to gold-standard out-of-domain parse trees.",
                "There are few tasks that actually depend on the complete parse tree.",
                "Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.",
                "While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.",
                "The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (#REF), sentiment analysis (#REF), MT reordering (#REF), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #REF ; #TARGET_REF ; #REF ; #REF ) . But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training. The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.",
        "output": "introduction:The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (#REF), sentiment analysis (#REF), MT reordering (#REF), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #REF ; #TARGET_REF<BACK/> ; #REF ; #REF ) . But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training. The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure."
    },
    {
        "gold": {
            "text": [
                "Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task.",
                "Judgments for such relations strongly depend on experience and cultural background of the test subjects.",
                "While most people may agree 10 Note that Resnik used the averaged correlation coefficient.",
                "We computed the summarized correlation coefficient using a Fisher Z-value transformation.",
                "that (car -vehicle) are highly related, a strong connection between (parts -speech) may only be established by a certain group.",
                "Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.",
                "Therefore , inter-subject correlation is lower than the results obtained by #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task. Judgments for such relations strongly depend on experience and cultural background of the test subjects. While most people may agree 10 Note that Resnik used the averaged correlation coefficient. We computed the summarized correlation coefficient using a Fisher Z-value transformation. that (car -vehicle) are highly related, a strong connection between (parts -speech) may only be established by a certain group. Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set. Therefore , inter-subject correlation is lower than the results obtained by #TARGET_REF .",
        "output": "experiments:Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task. Judgments for such relations strongly depend on experience and cultural background of the test subjects. While most people may agree 10 Note that Resnik used the averaged correlation coefficient. We computed the summarized correlation coefficient using a Fisher Z-value transformation. that (car -vehicle) are highly related, a strong connection between (parts -speech) may only be established by a certain group. Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set. Therefore , inter-subject correlation is lower than the results obtained by #TARGET_REF<USE/> ."
    },
    {
        "gold": {
            "text": [
                "In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #TARGET_REF ) and automatically trained transducers ( #REF ) with a larger range of positions ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #TARGET_REF ) and automatically trained transducers ( #REF ) with a larger range of positions .",
        "output": "nan:In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #TARGET_REF<FUT/> ) and automatically trained transducers ( #REF ) with a larger range of positions ."
    },
    {
        "gold": {
            "text": [
                "Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.",
                "For example, #REF consider the case in which acquiring additional human-annotated training data is not possible.",
                "They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.",
                "This approach assumes that there are enough existing labeled data to train the individual parsers.",
                "Another technique for making better use of unlabeled data is cotraining (#REF), in which two sufficiently different learners help each other learn by labeling training data for one another.",
                "The work of #REF and  suggests that co-training can be helpful for statistical parsing.",
                "#REF have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.",
                "Similar approaches are being explored for parsing ( Steedman , #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing. For example, #REF consider the case in which acquiring additional human-annotated training data is not possible. They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers. This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining (#REF), in which two sufficiently different learners help each other learn by labeling training data for one another. The work of #REF and  suggests that co-training can be helpful for statistical parsing. #REF have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing ( Steedman , #TARGET_REF ; #REF ) .",
        "output": "related work:Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing. For example, #REF consider the case in which acquiring additional human-annotated training data is not possible. They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers. This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining (#REF), in which two sufficiently different learners help each other learn by labeling training data for one another. The work of #REF and  suggests that co-training can be helpful for statistical parsing. #REF have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing ( Steedman , #TARGET_REF<BACK/> ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.",
                "We go beyond previous work, however, and explore additional lexical and inflectional features.",
                "Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and #REF ; Nivre , Boguslavsky , and #REF ; #TARGET_REF ) .",
                "Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.",
                "Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (#REF) (Section 6).",
                "#REF have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.",
                "These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing. We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and #REF ; Nivre , Boguslavsky , and #REF ; #TARGET_REF ) . Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (#REF) (Section 6). #REF have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor. These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.",
        "output": "related work:Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing. We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and #REF ; Nivre , Boguslavsky , and #REF ; #TARGET_REF<USE/> ) . Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (#REF) (Section 6). #REF have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor. These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs."
    },
    {
        "gold": {
            "text": [
                "So far we discussed optimal (gold) conditions.",
                "But in prac- tice, POS tags are annotated by automatic taggers, so parsers get predicted POS tags as input, as opposed to gold (human-annotated) tags.14",
                "The more informative the tag set, the less accurate the tag prediction might be, so the effect on overall parsing quality is unclear.",
                "Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit ( #TARGET_REF ; Habash, Rambow, and #REF)15 (see Table 2, columns 5�7).",
                "It turned out that BW, the best gold performer but with lowest POS pre- diction accuracy (81.8%), suffered the biggest drop (11.4%) and was the worst performer with predicted tags.",
                "The simplest tag set, CATIB6, and its extension, CATIBEX, benefited from the highest POS prediction accuracy (97.7%), and their performance suffered the least.",
                "CATIBEX was the best performer with predicted POS tags.",
                "Performance drop and POS prediction accuracy are given in columns 8 and 9."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:So far we discussed optimal (gold) conditions. But in prac- tice, POS tags are annotated by automatic taggers, so parsers get predicted POS tags as input, as opposed to gold (human-annotated) tags.14 The more informative the tag set, the less accurate the tag prediction might be, so the effect on overall parsing quality is unclear. Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit ( #TARGET_REF ; Habash, Rambow, and #REF)15 (see Table 2, columns 5�7). It turned out that BW, the best gold performer but with lowest POS pre- diction accuracy (81.8%), suffered the biggest drop (11.4%) and was the worst performer with predicted tags. The simplest tag set, CATIB6, and its extension, CATIBEX, benefited from the highest POS prediction accuracy (97.7%), and their performance suffered the least. CATIBEX was the best performer with predicted POS tags. Performance drop and POS prediction accuracy are given in columns 8 and 9.",
        "output": "related work:So far we discussed optimal (gold) conditions. But in prac- tice, POS tags are annotated by automatic taggers, so parsers get predicted POS tags as input, as opposed to gold (human-annotated) tags.14 The more informative the tag set, the less accurate the tag prediction might be, so the effect on overall parsing quality is unclear. Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit ( #TARGET_REF<FUT/> ; Habash, Rambow, and #REF)15 (see Table 2, columns 5�7). It turned out that BW, the best gold performer but with lowest POS pre- diction accuracy (81.8%), suffered the biggest drop (11.4%) and was the worst performer with predicted tags. The simplest tag set, CATIB6, and its extension, CATIBEX, benefited from the highest POS prediction accuracy (97.7%), and their performance suffered the least. CATIBEX was the best performer with predicted POS tags. Performance drop and POS prediction accuracy are given in columns 8 and 9."
    },
    {
        "gold": {
            "text": [
                "In Arabic, however, the use of stems will not yield satisfactory categorization.",
                "This is mainly due to the fact that Arabic is a non-concatenative language ( #TARGET_REF ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .",
                "The infix form (or stem) needs further to be processed in order to obtain the root.",
                "This processing is not straightforward: it necessitates expert knowledge in Arabic language word morphology (#REF).",
                "As an example, two close roots (i.e., roots made of the same letters), but semantically different, can yield the same infix form thus creating ambiguity."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:In Arabic, however, the use of stems will not yield satisfactory categorization. This is mainly due to the fact that Arabic is a non-concatenative language ( #TARGET_REF ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root . The infix form (or stem) needs further to be processed in order to obtain the root. This processing is not straightforward: it necessitates expert knowledge in Arabic language word morphology (#REF). As an example, two close roots (i.e., roots made of the same letters), but semantically different, can yield the same infix form thus creating ambiguity.",
        "output": "nan:In Arabic, however, the use of stems will not yield satisfactory categorization. This is mainly due to the fact that Arabic is a non-concatenative language ( #TARGET_REF<BACK/> ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root . The infix form (or stem) needs further to be processed in order to obtain the root. This processing is not straightforward: it necessitates expert knowledge in Arabic language word morphology (#REF). As an example, two close roots (i.e., roots made of the same letters), but semantically different, can yield the same infix form thus creating ambiguity."
    },
    {
        "gold": {
            "text": [
                "The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in Alshawi (1990, 1992), and implemented in SRI's Core Language Engine (CLE).",
                "In the CLE-QLF approach, as rationally reconstructed by #TARGET_REF and #REF , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.",
                "Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in Alshawi (1990, 1992), and implemented in SRI's Core Language Engine (CLE). In the CLE-QLF approach, as rationally reconstructed by #TARGET_REF and #REF , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules. Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.",
        "output": "nan:The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in Alshawi (1990, 1992), and implemented in SRI's Core Language Engine (CLE). In the CLE-QLF approach, as rationally reconstructed by #TARGET_REF<USE/> and #REF , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules. Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context."
    },
    {
        "gold": {
            "text": [
                "can be verified empirically on both ACE-2004 and OntoNotes-5.0",
                "datasets.",
                "Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (#REF), Berkeley system (#REF) and HOTCoref system (Björkelund and #REF).",
                "Developed Systems Our developed system is built on the work by #TARGET_REF , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .",
                "When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.",
                "The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.",
                "We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10",
                "We use Gurobi v5.0.1 as our ILP solver.",
                "3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (#REF), Berkeley system (#REF) and HOTCoref system (Björkelund and #REF). Developed Systems Our developed system is built on the work by #TARGET_REF , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10 We use Gurobi v5.0.1 as our ILP solver. 3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.",
        "output": "experiments:can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (#REF), Berkeley system (#REF) and HOTCoref system (Björkelund and #REF). Developed Systems Our developed system is built on the work by #TARGET_REF<FUT/> , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10 We use Gurobi v5.0.1 as our ILP solver. 3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset."
    },
    {
        "gold": {
            "text": [
                "In current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.",
                "Most approaches rely on VerbNet ( #REF ) and FrameNet ( #REF ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( #REF ; #REF ) and also ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:In current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments. Most approaches rely on VerbNet ( #REF ) and FrameNet ( #REF ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( #REF ; #REF ) and also ( #REF ; #REF ; #TARGET_REF ) .",
        "output": "related work:In current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments. Most approaches rely on VerbNet ( #REF ) and FrameNet ( #REF ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( #REF ; #REF ) and also ( #REF ; #REF ; #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "Previous semantic role classifiers always did the classification problem in one-step.",
                "However, in this paper, we did SRC in two steps.",
                "The architectures of hierarchical semantic role classifiers can 2 Extra features e.g.",
                "predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g.",
                "phrase type, are limited.",
                "be found in figure 2 , which is similar with that in #TARGET_REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Previous semantic role classifiers always did the classification problem in one-step. However, in this paper, we did SRC in two steps. The architectures of hierarchical semantic role classifiers can 2 Extra features e.g. predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g. phrase type, are limited. be found in figure 2 , which is similar with that in #TARGET_REF .",
        "output": "nan:Previous semantic role classifiers always did the classification problem in one-step. However, in this paper, we did SRC in two steps. The architectures of hierarchical semantic role classifiers can 2 Extra features e.g. predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g. phrase type, are limited. be found in figure 2 , which is similar with that in #TARGET_REF<USE/> ."
    },
    {
        "gold": {
            "text": [
                "These roles are similar to DCG (#REF) rules; they can include logical variables and these variables can be bound when these rules are applied.",
                "It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.",
                "The priorities are used for disambiguating interpretation in the incremental understanding method ( #TARGET_REFb ) .",
                "When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state.",
                "The command is one of the following:"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:These roles are similar to DCG (#REF) rules; they can include logical variables and these variables can be bound when these rules are applied. It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper. The priorities are used for disambiguating interpretation in the incremental understanding method ( #TARGET_REFb ) . When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state. The command is one of the following:",
        "output": "experiments:These roles are similar to DCG (#REF) rules; they can include logical variables and these variables can be bound when these rules are applied. It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper. The priorities are used for disambiguating interpretation in the incremental understanding method ( #TARGET_REF<FUT/>b ) . When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state. The command is one of the following:"
    },
    {
        "gold": {
            "text": [
                "Much of the earlier work in anaphora resolution heavily exploited domain and lin- guistic knowledge (#REF; #REF; #REF; #REF), which was difficult both to represent and to process, and which required considerable human input.",
                "However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.",
                "A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( #REF , 1991 ; #REF ; #REF ; #TARGET_REF ; Williams , Harvey , and #REF ; #REF ; #REF , 1998b ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Much of the earlier work in anaphora resolution heavily exploited domain and lin- guistic knowledge (#REF; #REF; #REF; #REF), which was difficult both to represent and to process, and which required considerable human input. However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( #REF , 1991 ; #REF ; #REF ; #TARGET_REF ; Williams , Harvey , and #REF ; #REF ; #REF , 1998b ) .",
        "output": "nan:Much of the earlier work in anaphora resolution heavily exploited domain and lin- guistic knowledge (#REF; #REF; #REF; #REF), which was difficult both to represent and to process, and which required considerable human input. However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( #REF , 1991 ; #REF ; #REF ; #TARGET_REF<BACK/> ; Williams , Harvey , and #REF ; #REF ; #REF , 1998b ) ."
    },
    {
        "gold": {
            "text": [
                "The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.",
                "The method is called targeted self-training as it is similar in vein to self-training (#REF), with the exception that the new parse data is targeted to produce accurate word reorderings.",
                "Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.",
                "This allows us to give guarantees of convergence.",
                "Furthermore, we also evaluate the method on alternate extrinsic loss functions.",
                "#TARGET_REF presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .",
                "Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.",
                "In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.",
                "Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training (#REF), with the exception that the new parse data is targeted to produce accurate word reorderings. Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions. #TARGET_REF presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system . Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score. Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives.",
        "output": "related work:The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training (#REF), with the exception that the new parse data is targeted to produce accurate word reorderings. Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions. #TARGET_REF<USE/> presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system . Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score. Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives."
    },
    {
        "gold": {
            "text": [
                "Miyao, Ninomiya, and #REF and Nakanishi, Miyao, and #REF describe a methodology for acquiring an English HPSG from the Penn-II Treebank.",
                "Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #TARGET_REF and #REF ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.",
                "In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of �inverse schemata.�"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:Miyao, Ninomiya, and #REF and Nakanishi, Miyao, and #REF describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #TARGET_REF and #REF ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category. In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of �inverse schemata.�",
        "output": "related work:Miyao, Ninomiya, and #REF and Nakanishi, Miyao, and #REF describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #TARGET_REF<FUT/> and #REF ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category. In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of �inverse schemata.�"
    },
    {
        "gold": {
            "text": [
                "Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.",
                "This is so since the lexical rule in Figure 2 \"(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.\"",
                "(Pollard and Sag [1994, 314], following Flickinger [1987]).",
                "This idea of preserving properties can be considered an instance of the well-known frame problem in AI (#REF), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule.",
                "Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output. This is so since the lexical rule in Figure 2 \"(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.\" (Pollard and Sag [1994, 314], following Flickinger [1987]). This idea of preserving properties can be considered an instance of the well-known frame problem in AI (#REF), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule. Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #TARGET_REF ) .",
        "output": "introduction:Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output. This is so since the lexical rule in Figure 2 \"(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.\" (Pollard and Sag [1994, 314], following Flickinger [1987]). This idea of preserving properties can be considered an instance of the well-known frame problem in AI (#REF), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule. Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.",
                "This is noticeable for German ( #TARGET_REF ) and Portuguese ( #REF ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der #REF ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .",
                "On the other hand, all three languages behave like high-accuracy languages with respect to attachment score.",
                "A very similar pattern is found for Spanish (Civit Torruella and Martí Antonín, 2002), although this cannot be explained by a high proportion of non-projective structures.",
                "One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German ( #TARGET_REF ) and Portuguese ( #REF ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der #REF ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively . On the other hand, all three languages behave like high-accuracy languages with respect to attachment score. A very similar pattern is found for Spanish (Civit Torruella and Martí Antonín, 2002), although this cannot be explained by a high proportion of non-projective structures. One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features.",
        "output": "experiments:A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German ( #TARGET_REF<USE/> ) and Portuguese ( #REF ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der #REF ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively . On the other hand, all three languages behave like high-accuracy languages with respect to attachment score. A very similar pattern is found for Spanish (Civit Torruella and Martí Antonín, 2002), although this cannot be explained by a high proportion of non-projective structures. One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features."
    },
    {
        "gold": {
            "text": [
                "To construct this test set, we have focused our attention on ten domain-speci\u001cc terms: commande (command), con\u001cguration, \u001cchier (\u001cle), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), syst�me (system), utilisateur (user ).",
                "The terms have been identified as the most specific to our corpus by a program developed by #TARGET_REF and called TER1vloSTAT .",
                "The ten most speci\u001cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (#REF).",
                "Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.",
                "(They were removed from the example set.)"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:To construct this test set, we have focused our attention on ten domain-speci\u001cc terms: commande (command), con\u001cguration, \u001cchier (\u001cle), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), syst�me (system), utilisateur (user ). The terms have been identified as the most specific to our corpus by a program developed by #TARGET_REF and called TER1vloSTAT . The ten most speci\u001cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (#REF). Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step. (They were removed from the example set.)",
        "output": "nan:To construct this test set, we have focused our attention on ten domain-speci\u001cc terms: commande (command), con\u001cguration, \u001cchier (\u001cle), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), syst�me (system), utilisateur (user ). The terms have been identified as the most specific to our corpus by a program developed by #TARGET_REF<FUT/> and called TER1vloSTAT . The ten most speci\u001cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (#REF). Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step. (They were removed from the example set.)"
    },
    {
        "gold": {
            "text": [
                "#REF helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (#REF) in the prediction of association norms.",
                "#REF furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.",
                "In a similar vein , #TARGET_REF showed that a different feature-topic model improved predictions on a fill-in-the-blank task .",
                "#REF take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.",
                "#REF introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:#REF helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (#REF) in the prediction of association norms. #REF furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein , #TARGET_REF showed that a different feature-topic model improved predictions on a fill-in-the-blank task . #REF take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. #REF introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.",
        "output": "related work:#REF helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (#REF) in the prediction of association norms. #REF furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein , #TARGET_REF<BACK/> showed that a different feature-topic model improved predictions on a fill-in-the-blank task . #REF take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. #REF introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity."
    },
    {
        "gold": {
            "text": [
                "The nonoverlapping mention head assumption in Sec.",
                "3.1.1",
                "can be verified empirically on both ACE-2004 and OntoNotes-5.0",
                "datasets.",
                "Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( #REF ) , Berkeley system ( #TARGET_REF ) and HOTCoref system ( Bj Â¨ orkelund and #REF ) .",
                "Developed Systems Our developed system is built on the work by #REF, using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 .",
                "When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.",
                "The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.",
                "We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10",
                "We use Gurobi v5.0.1 as our ILP solver."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:The nonoverlapping mention head assumption in Sec. 3.1.1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( #REF ) , Berkeley system ( #TARGET_REF ) and HOTCoref system ( Bj Â¨ orkelund and #REF ) . Developed Systems Our developed system is built on the work by #REF, using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10 We use Gurobi v5.0.1 as our ILP solver.",
        "output": "experiments:The nonoverlapping mention head assumption in Sec. 3.1.1 can be verified empirically on both ACE-2004 and OntoNotes-5.0 datasets. Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( #REF ) , Berkeley system ( #TARGET_REF<USE/> ) and HOTCoref system ( Bj Â¨ orkelund and #REF ) . Developed Systems Our developed system is built on the work by #REF, using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 . When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted. The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD. We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10 We use Gurobi v5.0.1 as our ILP solver."
    },
    {
        "gold": {
            "text": [
                "The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.",
                "In syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages.",
                "One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more difficult to analyze.",
                "Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance.",
                "In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.",
                "We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.",
                "This heuristic is called soft union ( #TARGET_REF ) .",
                "Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.",
                "The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization.",
                "This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time.",
                "Applying the symmetrization to the model with symmetry constraints does not affect performance."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The alignment produced has high recall relative to the intersection and only slightly lower recall than the union. In syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages. One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more difficult to analyze. Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance. In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model. We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union ( #TARGET_REF ) . Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus. The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization. This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time. Applying the symmetrization to the model with symmetry constraints does not affect performance.",
        "output": "nan:The alignment produced has high recall relative to the intersection and only slightly lower recall than the union. In syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages. One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more difficult to analyze. Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance. In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model. We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union ( #TARGET_REF<FUT/> ) . Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus. The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization. This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time. Applying the symmetrization to the model with symmetry constraints does not affect performance."
    },
    {
        "gold": {
            "text": [
                "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #TARGET_REF ; #REF ) .",
                "However, most existing systems use pre-authored tutor responses for addressing student errors.",
                "The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.",
                "The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.",
                "It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #TARGET_REF ; #REF ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state. It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.",
        "output": "introduction:Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #TARGET_REF<BACK/> ; #REF ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state. It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations."
    },
    {
        "gold": {
            "text": [
                "#REF employed a Bayesian method to learn discontinuous SCFG rules.",
                "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.",
                "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.",
                "#REF and #REF focused on joint parsing and alignment.",
                "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.",
                "adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.",
                "#REF re-trained the linguistic parsers bilingually based on word alignment.",
                "#TARGET_REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .",
                "Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.",
                "#REF substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.",
                "#REF further labeled the SCFG rules with POS tags and unsupervised word classes.",
                "Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:#REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #REF focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #REF re-trained the linguistic parsers bilingually based on word alignment. #TARGET_REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. #REF substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. #REF further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.",
        "output": "related work:#REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #REF focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #REF re-trained the linguistic parsers bilingually based on word alignment. #TARGET_REF<USE/> utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation . Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. #REF substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. #REF further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules."
    },
    {
        "gold": {
            "text": [
                "The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.",
                "ASARES is presented in detail in ( #TARGET_REF ) .",
                "We simply give a short account of its basic principles herein."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool. ASARES is presented in detail in ( #TARGET_REF ) . We simply give a short account of its basic principles herein.",
        "output": "method:The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool. ASARES is presented in detail in ( #TARGET_REF<FUT/> ) . We simply give a short account of its basic principles herein."
    },
    {
        "gold": {
            "text": [
                "An example of psycholinguistically oriented research work can be found in #REF.",
                "These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.",
                "Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.",
                "Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #REF and #TARGET_REF ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:An example of psycholinguistically oriented research work can be found in #REF. These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information. Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #REF and #TARGET_REF .",
        "output": "introduction:An example of psycholinguistically oriented research work can be found in #REF. These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information. Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #REF and #TARGET_REF<BACK/> ."
    },
    {
        "gold": {
            "text": [
                "Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors).",
                "However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results.",
                "As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors). However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results. As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #TARGET_REF ; #REF ) .",
        "output": "experiments:Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors). However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results. As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #TARGET_REF<USE/> ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "An implementation of the transition-based dependency parsing frame- work ( #TARGET_REF ) using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in #REF with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.",
                "The features used by all models are: the part-of- speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word iden- tity of the syntactic head of the top word on the stack (if available); dependency arc label iden- tities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available).",
                "All feature conjunc- tions are included."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:An implementation of the transition-based dependency parsing frame- work ( #TARGET_REF ) using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in #REF with a beam size of 8. Beams with varying sizes can be used to produce k-best lists. The features used by all models are: the part-of- speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word iden- tity of the syntactic head of the top word on the stack (if available); dependency arc label iden- tities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available). All feature conjunc- tions are included.",
        "output": "experiments:An implementation of the transition-based dependency parsing frame- work ( #TARGET_REF<FUT/> ) using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in #REF with a beam size of 8. Beams with varying sizes can be used to produce k-best lists. The features used by all models are: the part-of- speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word iden- tity of the syntactic head of the top word on the stack (if available); dependency arc label iden- tities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available). All feature conjunc- tions are included."
    },
    {
        "gold": {
            "text": [
                "Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g.",
                "restricted access or document size).",
                "Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.",
                "This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.",
                "The use of the web as a corpus for teaching and research on language has been proposed a number of times (#REF;#REF;#REF;#REF#REFb and received a special issue of the journal Computational Linguistics (#REF).",
                "Studies have used several different methods to mine web data.",
                "#REF extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.",
                "#REF built a corpus by iteratively searching Google for a small set of seed terms.",
                "Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( #REF ) , KWiCFinder ( #REFa ) and the Linguist 's Search Engine ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times (#REF;#REF;#REF;#REF#REFb and received a special issue of the journal Computational Linguistics (#REF). Studies have used several different methods to mine web data. #REF extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. #REF built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( #REF ) , KWiCFinder ( #REFa ) and the Linguist 's Search Engine ( #REF ; #TARGET_REF ) .",
        "output": "related work:Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g. restricted access or document size). Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times (#REF;#REF;#REF;#REF#REFb and received a special issue of the journal Computational Linguistics (#REF). Studies have used several different methods to mine web data. #REF extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. #REF built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( #REF ) , KWiCFinder ( #REFa ) and the Linguist 's Search Engine ( #REF ; #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.",
                "#REF reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%.",
                "Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.",
                "For many applications, this is the desired behavior.",
                "The most detailed evaluation of link tokens to date was performed by ( #TARGET_REF ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .",
                "These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.",
                "We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.",
                "The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.",
                "Therefore, we ignored English words that were linked to nothing."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports. #REF reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%. Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by ( #TARGET_REF ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models. The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words. Therefore, we ignored English words that were linked to nothing.",
        "output": "nan:The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports. #REF reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%. Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence. For many applications, this is the desired behavior. The most detailed evaluation of link tokens to date was performed by ( #TARGET_REF<USE/> ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards . These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set. We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models. The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words. Therefore, we ignored English words that were linked to nothing."
    },
    {
        "gold": {
            "text": [
                "The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( #REF ) and the BitPar parser , which is a state-of-the-art parser of German ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( #REF ) and the BitPar parser , which is a state-of-the-art parser of German ( #TARGET_REF ) .",
        "output": "introduction:The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( #REF ) and the BitPar parser , which is a state-of-the-art parser of German ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "â¢ language learning ( #REF ; #REF ; Morgan , Meier , and #REF ) â¢ monolingual grammar induction ( #REF ) â¢ grammar optimization ( #REF ) â¢ insights into universal grammar ( #REF ) â¢ machine translation ( #REF , 1997 ; #TARGET_REF ; Gough , Way , and #REF )"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:â¢ language learning ( #REF ; #REF ; Morgan , Meier , and #REF ) â¢ monolingual grammar induction ( #REF ) â¢ grammar optimization ( #REF ) â¢ insights into universal grammar ( #REF ) â¢ machine translation ( #REF , 1997 ; #TARGET_REF ; Gough , Way , and #REF )",
        "output": "introduction:â¢ language learning ( #REF ; #REF ; Morgan , Meier , and #REF ) â¢ monolingual grammar induction ( #REF ) â¢ grammar optimization ( #REF ) â¢ insights into universal grammar ( #REF ) â¢ machine translation ( #REF , 1997 ; #TARGET_REF<BACK/> ; Gough , Way , and #REF )"
    },
    {
        "gold": {
            "text": [
                "A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , and #TARGET_REF ) .",
                "Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.",
                "While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.",
                "A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , and #TARGET_REF ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF.",
        "output": "nan:A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , and #TARGET_REF<USE/> ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF."
    },
    {
        "gold": {
            "text": [
                "In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #TARGET_REF ) .",
                "We also make use of the Brown corpus, and the Question Treebank (QTB) (#REF"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #TARGET_REF ) . We also make use of the Brown corpus, and the Question Treebank (QTB) (#REF",
        "output": "experiments:In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #TARGET_REF<FUT/> ) . We also make use of the Brown corpus, and the Question Treebank (QTB) (#REF"
    },
    {
        "gold": {
            "text": [
                "When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.",
                "In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.",
                "Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #REF ; #TARGET_REF ; #REF ) .7"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string. In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #REF ; #TARGET_REF ; #REF ) .7",
        "output": "introduction:When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string. In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #REF ; #TARGET_REF<BACK/> ; #REF ) .7"
    },
    {
        "gold": {
            "text": [
                "All the work mentioned so far uses statistical models of various kinds.",
                "As we have shown here, such models offer some fundamental advantages, such as modularity and composability (e.g., of discourse grammars with DA models) and the ability to deal with noisy input (e.g., from a speech recognizer) in a principled way.",
                "However, many other classifier architectures are applicable to the tasks discussed, in particular to DA classification.",
                "A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and #REF is transformation-based learning ( #TARGET_REF ) .",
                "Finally it should be noted that there are other tasks with a mathematical structure similar to that of DA tagging, such as shallow parsing for natural language processing (#REF) and DNA classification tasks (Ohler, Harbeck, and #REF), from which further techniques could be borrowed."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:All the work mentioned so far uses statistical models of various kinds. As we have shown here, such models offer some fundamental advantages, such as modularity and composability (e.g., of discourse grammars with DA models) and the ability to deal with noisy input (e.g., from a speech recognizer) in a principled way. However, many other classifier architectures are applicable to the tasks discussed, in particular to DA classification. A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and #REF is transformation-based learning ( #TARGET_REF ) . Finally it should be noted that there are other tasks with a mathematical structure similar to that of DA tagging, such as shallow parsing for natural language processing (#REF) and DNA classification tasks (Ohler, Harbeck, and #REF), from which further techniques could be borrowed.",
        "output": "nan:All the work mentioned so far uses statistical models of various kinds. As we have shown here, such models offer some fundamental advantages, such as modularity and composability (e.g., of discourse grammars with DA models) and the ability to deal with noisy input (e.g., from a speech recognizer) in a principled way. However, many other classifier architectures are applicable to the tasks discussed, in particular to DA classification. A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and #REF is transformation-based learning ( #TARGET_REF<USE/> ) . Finally it should be noted that there are other tasks with a mathematical structure similar to that of DA tagging, such as shallow parsing for natural language processing (#REF) and DNA classification tasks (Ohler, Harbeck, and #REF), from which further techniques could be borrowed."
    },
    {
        "gold": {
            "text": [
                "criteria and data used in our experiments are based on the work of #TARGET_REF ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:criteria and data used in our experiments are based on the work of #TARGET_REF .",
        "output": "experiments:criteria and data used in our experiments are based on the work of #TARGET_REF<FUT/> ."
    },
    {
        "gold": {
            "text": [
                "What we said above has also disregarded elements of the \"global\" (i.e., not immediately available) context.",
                "For some adjectives , including the ones that #TARGET_REF called evaluative ( as opposed to dimensional ) , this is clearly inadequate .",
                "He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms.",
                "For example (after #REF),"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:What we said above has also disregarded elements of the \"global\" (i.e., not immediately available) context. For some adjectives , including the ones that #TARGET_REF called evaluative ( as opposed to dimensional ) , this is clearly inadequate . He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms. For example (after #REF),",
        "output": "introduction:What we said above has also disregarded elements of the \"global\" (i.e., not immediately available) context. For some adjectives , including the ones that #TARGET_REF<BACK/> called evaluative ( as opposed to dimensional ) , this is clearly inadequate . He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms. For example (after #REF),"
    },
    {
        "gold": {
            "text": [
                "Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.",
                "As a generalization, #REF notes that lexicons such as COMLEX tend to demonstrate high precision but low recall.",
                "#TARGET_REF report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .",
                "Precision was quite high (95%), but recall was low (84%).",
                "This has an effect on both the precision and recall scores of our system against COMLEX.",
                "In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.",
                "We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as \"correct\" or \"incorrect.\"",
                "Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization, #REF notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. #TARGET_REF report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX. In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26. We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as \"correct\" or \"incorrect.\" Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames.",
        "output": "nan:Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization, #REF notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. #TARGET_REF<USE/> report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX . Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX. In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26. We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as \"correct\" or \"incorrect.\" Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames."
    },
    {
        "gold": {
            "text": [
                "18 In this article , we use a newer version of the corpus by #TARGET_REF than the one we used in Marton , Habash , and #REF ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:18 In this article , we use a newer version of the corpus by #TARGET_REF than the one we used in Marton , Habash , and #REF .",
        "output": "related work:18 In this article , we use a newer version of the corpus by #TARGET_REF<FUT/> than the one we used in Marton , Habash , and #REF ."
    },
    {
        "gold": {
            "text": [
                "Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #TARGET_REF ) .",
                "Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #TARGET_REF ) . Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.",
        "output": "nan:Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #TARGET_REF<BACK/> ) . Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies."
    },
    {
        "gold": {
            "text": [
                "The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) .",
                "The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.",
                "The best current model (#REF) has only 6% less precision error and only 11% less recall error than the lexicalized model.",
                "The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.",
                "It is also achieved without any explicit notion of lexical head."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF ) . The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. The best current model (#REF) has only 6% less precision error and only 11% less recall error than the lexicalized model. The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words. It is also achieved without any explicit notion of lexical head.",
        "output": "experiments:The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( #REF ; #REF ; #REF ; #REF ; #TARGET_REF<USE/> ) . The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. The best current model (#REF) has only 6% less precision error and only 11% less recall error than the lexicalized model. The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words. It is also achieved without any explicit notion of lexical head."
    },
    {
        "gold": {
            "text": [
                "Most of the head mentions proposed by the algorithms described in Sec. 3 are positive examples.",
                "We ensure a balanced training of the mention head detection model by adding sub-sampled invalid mention head candidates as negative examples.",
                "Specifically, after mention head candidate generation (described in Sec.",
                "3), we train on a set of candidates with precision larger than 50%.",
                "We then use Illinois Chunker ( #REF ) 6 to extract more noun phrases from the text and employ Collins head rules ( #TARGET_REF ) to identify their heads .",
                "When these extracted heads do not overlap with gold mention heads, we treat them as negative examples."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Most of the head mentions proposed by the algorithms described in Sec. 3 are positive examples. We ensure a balanced training of the mention head detection model by adding sub-sampled invalid mention head candidates as negative examples. Specifically, after mention head candidate generation (described in Sec. 3), we train on a set of candidates with precision larger than 50%. We then use Illinois Chunker ( #REF ) 6 to extract more noun phrases from the text and employ Collins head rules ( #TARGET_REF ) to identify their heads . When these extracted heads do not overlap with gold mention heads, we treat them as negative examples.",
        "output": "nan:Most of the head mentions proposed by the algorithms described in Sec. 3 are positive examples. We ensure a balanced training of the mention head detection model by adding sub-sampled invalid mention head candidates as negative examples. Specifically, after mention head candidate generation (described in Sec. 3), we train on a set of candidates with precision larger than 50%. We then use Illinois Chunker ( #REF ) 6 to extract more noun phrases from the text and employ Collins head rules ( #TARGET_REF<FUT/> ) to identify their heads . When these extracted heads do not overlap with gold mention heads, we treat them as negative examples."
    },
    {
        "gold": {
            "text": [
                "Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.",
                "For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #TARGET_REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #TARGET_REF ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) .",
        "output": "nan:Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs. For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( #REF ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #TARGET_REF<BACK/> ) , ( #REF ) , ( #REF ) , ( Al-Adhaileh & #REF ) ."
    },
    {
        "gold": {
            "text": [
                "The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.",
                "This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.",
                "#REF reported a correlation of r=.9026. 10",
                "#TARGET_REF reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .",
                "#REF did not report inter-subject correlation for their larger dataset.",
                "#REF reported a correlation of r=.69.",
                "Test subjects were trained students of computational linguistics, and word pairs were selected analytically."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05. This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset. #REF reported a correlation of r=.9026. 10 #TARGET_REF reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . #REF did not report inter-subject correlation for their larger dataset. #REF reported a correlation of r=.69. Test subjects were trained students of computational linguistics, and word pairs were selected analytically.",
        "output": "experiments:The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05. This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset. #REF reported a correlation of r=.9026. 10 #TARGET_REF<USE/> reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness . #REF did not report inter-subject correlation for their larger dataset. #REF reported a correlation of r=.69. Test subjects were trained students of computational linguistics, and word pairs were selected analytically."
    },
    {
        "gold": {
            "text": [
                "Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set.",
                "We use the same splits as #TARGET_REF .",
                "Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\X)/X rather than introducing special conjunction rules.",
                "In order to increase the amount of raw data available to the sampler, we supplemented the English data with raw, unannotated newswire sentences from the NYT Giga- word 5 corpus (#REF) and supplemented Italian with the out-of-domain WaCky corpus (#REF).",
                "For English and Italian, this allowed us to use 100k raw tokens for training (Chinese uses 62k).",
                "For Chinese and Italian, for training efficiency, we used only raw sentences that were 50 words or fewer (note that we did not drop tag dictionary set or test set sentences)."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as #TARGET_REF . Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\X)/X rather than introducing special conjunction rules. In order to increase the amount of raw data available to the sampler, we supplemented the English data with raw, unannotated newswire sentences from the NYT Giga- word 5 corpus (#REF) and supplemented Italian with the out-of-domain WaCky corpus (#REF). For English and Italian, this allowed us to use 100k raw tokens for training (Chinese uses 62k). For Chinese and Italian, for training efficiency, we used only raw sentences that were 50 words or fewer (note that we did not drop tag dictionary set or test set sentences).",
        "output": "experiments:Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as #TARGET_REF<FUT/> . Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\X)/X rather than introducing special conjunction rules. In order to increase the amount of raw data available to the sampler, we supplemented the English data with raw, unannotated newswire sentences from the NYT Giga- word 5 corpus (#REF) and supplemented Italian with the out-of-domain WaCky corpus (#REF). For English and Italian, this allowed us to use 100k raw tokens for training (Chinese uses 62k). For Chinese and Italian, for training efficiency, we used only raw sentences that were 50 words or fewer (note that we did not drop tag dictionary set or test set sentences)."
    },
    {
        "gold": {
            "text": [
                "All logical notions that we are going to consider, such as theory or model, will be finitary.",
                "For example, a model would typically contain fewer than a hundred elements of different logical sorts.",
                "Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.",
                "The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.",
                "This Principle of Finitism is also assumed by #REF, Jackendoff (1983, #REF, and implicitly or explicitly by almost all researchers in computational linguistics.",
                "As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #TARGET_REF ) .",
                "#REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred elements of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by #REF, Jackendoff (1983, #REF, and implicitly or explicitly by almost all researchers in computational linguistics. As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #TARGET_REF ) . #REF).",
        "output": "introduction:All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred elements of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by #REF, Jackendoff (1983, #REF, and implicitly or explicitly by almost all researchers in computational linguistics. As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #TARGET_REF<BACK/> ) . #REF)."
    },
    {
        "gold": {
            "text": [
                "A common way to combine different models consists of selecting the model that is most confident regarding its decision ( #TARGET_REF ) .",
                "However, in our case, the individual confidence (applicability) measures employed by our response-generation methods are not comparable (e.g., the retrieval score in Doc-Ret is different in nature from the prediction probability in Doc-Pred).",
                "Hence, prior to selecting the most confident method, we need to find a way to compare the different measures of confidence.",
                "Because the performances of the different methods are comparable, we do this by establishing a link between confidence and performance.",
                "In other words, our meta-level process learns to predict the performance of the different methods from their confidence levels on the basis of previous experience.",
                "These predictions enable our system to recommend a particular method for handling a new (unseen) request (Marom, Zukerman, and #REF)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:A common way to combine different models consists of selecting the model that is most confident regarding its decision ( #TARGET_REF ) . However, in our case, the individual confidence (applicability) measures employed by our response-generation methods are not comparable (e.g., the retrieval score in Doc-Ret is different in nature from the prediction probability in Doc-Pred). Hence, prior to selecting the most confident method, we need to find a way to compare the different measures of confidence. Because the performances of the different methods are comparable, we do this by establishing a link between confidence and performance. In other words, our meta-level process learns to predict the performance of the different methods from their confidence levels on the basis of previous experience. These predictions enable our system to recommend a particular method for handling a new (unseen) request (Marom, Zukerman, and #REF).",
        "output": "nan:A common way to combine different models consists of selecting the model that is most confident regarding its decision ( #TARGET_REF<USE/> ) . However, in our case, the individual confidence (applicability) measures employed by our response-generation methods are not comparable (e.g., the retrieval score in Doc-Ret is different in nature from the prediction probability in Doc-Pred). Hence, prior to selecting the most confident method, we need to find a way to compare the different measures of confidence. Because the performances of the different methods are comparable, we do this by establishing a link between confidence and performance. In other words, our meta-level process learns to predict the performance of the different methods from their confidence levels on the basis of previous experience. These predictions enable our system to recommend a particular method for handling a new (unseen) request (Marom, Zukerman, and #REF)."
    },
    {
        "gold": {
            "text": [
                "This string representation of paths is used to capture both the subject consistency and the object consistency.",
                "Since they are non-numerical features, and the variability of their values can be extremely large, so we applied an instance-based classification model (e.g., k-nearest neighbor) to determine alignments between verb terms.",
                "We measure the distance between two path features by their minimal string edit distance, and then simply use the Euclidean distance to measure the closeness between any two verbs.",
                "Again this model is trained from our development data described in #REF.",
                "Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2. Note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.",
                "This alignment is obtained by following the same set of rules learned from the development dataset as in ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:This string representation of paths is used to capture both the subject consistency and the object consistency. Since they are non-numerical features, and the variability of their values can be extremely large, so we applied an instance-based classification model (e.g., k-nearest neighbor) to determine alignments between verb terms. We measure the distance between two path features by their minimal string edit distance, and then simply use the Euclidean distance to measure the closeness between any two verbs. Again this model is trained from our development data described in #REF. Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2. Note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act. This alignment is obtained by following the same set of rules learned from the development dataset as in ( #TARGET_REF ) .",
        "output": "method:This string representation of paths is used to capture both the subject consistency and the object consistency. Since they are non-numerical features, and the variability of their values can be extremely large, so we applied an instance-based classification model (e.g., k-nearest neighbor) to determine alignments between verb terms. We measure the distance between two path features by their minimal string edit distance, and then simply use the Euclidean distance to measure the closeness between any two verbs. Again this model is trained from our development data described in #REF. Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2. Note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act. This alignment is obtained by following the same set of rules learned from the development dataset as in ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "In principle, we could have used a classification method to predict clusters from the values of the confidence measures for unseen cases.",
                "We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs ( #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:In principle, we could have used a classification method to predict clusters from the values of the confidence measures for unseen cases. We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs ( #TARGET_REF ) .",
        "output": "nan:In principle, we could have used a classification method to predict clusters from the values of the confidence measures for unseen cases. We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs ( #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "A number of speech understanding systems have been developed during the past fifteen years ( #TARGET_REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , and #REF ) .",
                "Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.",
                "While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.",
                "A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:A number of speech understanding systems have been developed during the past fifteen years ( #TARGET_REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , and #REF ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF.",
        "output": "nan:A number of speech understanding systems have been developed during the past fifteen years ( #TARGET_REF<USE/> , #REF , #REF , #REF , #REF , #REF , #REF , #REF , #REF , and #REF ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF."
    },
    {
        "gold": {
            "text": [
                "The second key task of our approach is the disambiguation of capitalized words that follow a potential sentence boundary punctuation sign.",
                "Apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the SBD task, as we showed in Section 3. We tackle capitalized words in a similar fashion as we tackled the abbreviations: through a document-centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.",
                "This is implemented as a cascade of simple strategies , which were briefly described in #TARGET_REF ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The second key task of our approach is the disambiguation of capitalized words that follow a potential sentence boundary punctuation sign. Apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the SBD task, as we showed in Section 3. We tackle capitalized words in a similar fashion as we tackled the abbreviations: through a document-centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document. This is implemented as a cascade of simple strategies , which were briefly described in #TARGET_REF .",
        "output": "nan:The second key task of our approach is the disambiguation of capitalized words that follow a potential sentence boundary punctuation sign. Apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the SBD task, as we showed in Section 3. We tackle capitalized words in a similar fashion as we tackled the abbreviations: through a document-centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document. This is implemented as a cascade of simple strategies , which were briefly described in #TARGET_REF<FUT/> ."
    },
    {
        "gold": {
            "text": [
                "The language grounding problem has come in many different flavors with just as many different approaches.",
                "Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (#REF).",
                "Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (#REF) or robot commands (#REF;#REF).",
                "Some efforts have tackled tasks such as automatic image caption generation ( #REFa ; #REF ) , text illustration ( #REF ) , or automatic location identification of Twitter users ( #REF ; #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (#REF). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (#REF) or robot commands (#REF;#REF). Some efforts have tackled tasks such as automatic image caption generation ( #REFa ; #REF ) , text illustration ( #REF ) , or automatic location identification of Twitter users ( #REF ; #TARGET_REF ; #REF ) .",
        "output": "introduction:The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (#REF). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (#REF) or robot commands (#REF;#REF). Some efforts have tackled tasks such as automatic image caption generation ( #REFa ; #REF ) , text illustration ( #REF ) , or automatic location identification of Twitter users ( #REF ; #TARGET_REF<BACK/> ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "Another common approach to lexical rules is to encode them as unary phrase structure rules.",
                "This approach is taken, for example, in LKB (#REF) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (#REF, 31).",
                "A similar method is included in PATR-II ( #REF ) and can be used to encode lexical rules as binary relations in the CUF system ( #REF ; #REFb ) or the TFS system ( #REF ; #TARGET_REF ) .",
                "The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken, for example, in LKB (#REF) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (#REF, 31). A similar method is included in PATR-II ( #REF ) and can be used to encode lexical rules as binary relations in the CUF system ( #REF ; #REFb ) or the TFS system ( #REF ; #TARGET_REF ) . The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.",
        "output": "related work:Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken, for example, in LKB (#REF) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (#REF, 31). A similar method is included in PATR-II ( #REF ) and can be used to encode lexical rules as binary relations in the CUF system ( #REF ; #REFb ) or the TFS system ( #REF ; #TARGET_REF<USE/> ) . The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."
    },
    {
        "gold": {
            "text": [
                "Additionally, we also evaluate using the Association Norms data set described in Section 3. Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.",
                "Following #TARGET_REF , we measure association norm prediction as an average of percentile ranks .",
                "For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words.",
                "We then compute the percentile ranks of similarity for each word pair, e.g., \"cat\" is more similar to \"dog\" than 97.3% of the rest of the vocabulary.",
                "We report the weighted mean percentile ranks for all cue-association pairs, i.e., if a cue-association is given more than once, it is counted more than once."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:Additionally, we also evaluate using the Association Norms data set described in Section 3. Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality. Following #TARGET_REF , we measure association norm prediction as an average of percentile ranks . For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words. We then compute the percentile ranks of similarity for each word pair, e.g., \"cat\" is more similar to \"dog\" than 97.3% of the rest of the vocabulary. We report the weighted mean percentile ranks for all cue-association pairs, i.e., if a cue-association is given more than once, it is counted more than once.",
        "output": "experiments:Additionally, we also evaluate using the Association Norms data set described in Section 3. Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality. Following #TARGET_REF<FUT/> , we measure association norm prediction as an average of percentile ranks . For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words. We then compute the percentile ranks of similarity for each word pair, e.g., \"cat\" is more similar to \"dog\" than 97.3% of the rest of the vocabulary. We report the weighted mean percentile ranks for all cue-association pairs, i.e., if a cue-association is given more than once, it is counted more than once."
    },
    {
        "gold": {
            "text": [
                "Concept mapping has been used at least in education for over thirty years, in particular at the Cornell University, where Piaget's ideas gave the roots to the assimilation theory by David Ausubel.",
                "Very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.",
                "Every arc always has a definite direction , i.e. arcs are arrows ( #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Concept mapping has been used at least in education for over thirty years, in particular at the Cornell University, where Piaget's ideas gave the roots to the assimilation theory by David Ausubel. Very briefly, concept maps show the relationships between concepts labelling both nodes and arcs. Every arc always has a definite direction , i.e. arcs are arrows ( #TARGET_REF ) .",
        "output": "related work:Concept mapping has been used at least in education for over thirty years, in particular at the Cornell University, where Piaget's ideas gave the roots to the assimilation theory by David Ausubel. Very briefly, concept maps show the relationships between concepts labelling both nodes and arcs. Every arc always has a definite direction , i.e. arcs are arrows ( #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12.",
                "But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.",
                "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.",
                "This is roughly an 11 % relative reduction in error rate over #TARGET_REF and Bods PCFG-reduction reported in Table 1 .",
                "Compared to the reranking technique in #REF, who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction.",
                "While SL-DOP and LS-DOP have been compared before in"
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12. But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11 % relative reduction in error rate over #TARGET_REF and Bods PCFG-reduction reported in Table 1 . Compared to the reranking technique in #REF, who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. While SL-DOP and LS-DOP have been compared before in",
        "output": "conclusion:Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12. But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11 % relative reduction in error rate over #TARGET_REF<USE/> and Bods PCFG-reduction reported in Table 1 . Compared to the reranking technique in #REF, who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. While SL-DOP and LS-DOP have been compared before in"
    },
    {
        "gold": {
            "text": [
                "We use the TRIPS dialogue parser ( #TARGET_REF ) to parse the utterances .",
                "The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.",
                "The contextual interpreter then uses a reference resolution approach similar to #REF, and an ontology mapping mechanism (#REFa) to produce a domain-specific semantic representation of the student's output.",
                "Utterance content is represented as a set of extracted objects and relations between them.",
                "Negation is supported, together with a heuristic scoping algorithm.",
                "The interpreter also performs basic ellipsis resolution.",
                "For example, it can determine that in the answer to the question \"Which bulbs will be on and which bulbs will be off in this diagram?\",",
                "\"off\" can be taken to mean \"all bulbs in the di-agram will be off.\"",
                "The resulting output is then passed on to the domain reasoning and diagnosis components."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We use the TRIPS dialogue parser ( #TARGET_REF ) to parse the utterances . The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to #REF, and an ontology mapping mechanism (#REFa) to produce a domain-specific semantic representation of the student's output. Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question \"Which bulbs will be on and which bulbs will be off in this diagram?\", \"off\" can be taken to mean \"all bulbs in the di-agram will be off.\" The resulting output is then passed on to the domain reasoning and diagnosis components.",
        "output": "experiments:We use the TRIPS dialogue parser ( #TARGET_REF<FUT/> ) to parse the utterances . The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to #REF, and an ontology mapping mechanism (#REFa) to produce a domain-specific semantic representation of the student's output. Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question \"Which bulbs will be on and which bulbs will be off in this diagram?\", \"off\" can be taken to mean \"all bulbs in the di-agram will be off.\" The resulting output is then passed on to the domain reasoning and diagnosis components."
    },
    {
        "gold": {
            "text": [
                "The language grounding problem has come in many different flavors with just as many different approaches.",
                "Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (#REF).",
                "Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (#REF) or robot commands (#REF;#REF).",
                "Some efforts have tackled tasks such as automatic image caption generation ( #TARGET_REFa ; #REF ) , text illustration ( #REF ) , or automatic location identification of Twitter users ( #REF ; #REF ; #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (#REF). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (#REF) or robot commands (#REF;#REF). Some efforts have tackled tasks such as automatic image caption generation ( #TARGET_REFa ; #REF ) , text illustration ( #REF ) , or automatic location identification of Twitter users ( #REF ; #REF ; #REF ) .",
        "output": "introduction:The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (#REF). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (#REF) or robot commands (#REF;#REF). Some efforts have tackled tasks such as automatic image caption generation ( #TARGET_REF<BACK/>a ; #REF ) , text illustration ( #REF ) , or automatic location identification of Twitter users ( #REF ; #REF ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF.",
                "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF , assertional statements as in #TARGET_REF , or semantic nets as in #REF .",
                "That is, the current system learns procedures rather than data structures.",
                "There is some literature on procedure acquisition such as the LISP synthesis work described in #REF and the PROLOG synthesis method of #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF. The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF , assertional statements as in #TARGET_REF , or semantic nets as in #REF . That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in #REF and the PROLOG synthesis method of #REF.",
        "output": "nan:The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF. The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF , assertional statements as in #TARGET_REF<USE/> , or semantic nets as in #REF . That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in #REF and the PROLOG synthesis method of #REF."
    },
    {
        "gold": {
            "text": [
                "However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.",
                "Nevertheless, Juola (1998, page 23) observes that \"a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its case-marking), can capture this sort of result quite handily.\"",
                "Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated.",
                "Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #TARGET_REF to permit a limited form of insertion in the translation process .",
                "As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon.",
                "These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless, Juola (1998, page 23) observes that \"a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its case-marking), can capture this sort of result quite handily.\" Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #TARGET_REF to permit a limited form of insertion in the translation process . As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon. These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input.",
        "output": "introduction:However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word. Nevertheless, Juola (1998, page 23) observes that \"a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its case-marking), can capture this sort of result quite handily.\" Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated. Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #TARGET_REF<FUT/> to permit a limited form of insertion in the translation process . As a byproduct of the chosen methodology, we also derive a standard \"word-level\" translation lexicon. These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input."
    },
    {
        "gold": {
            "text": [
                "A series of systems in Cambridge are implemented in Lisp running under UnixTM.",
                "They all make use of an efficient dictionary access system which services requests for s-expression entries made by client pro- grams.",
                "A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved.",
                "As #TARGET_REF points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.",
                "The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.",
                "For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system.",
                "To the Lisp programmer, the creation of a dictionary process and subsequent requests for information from the dictionary appear simply as Lisp function calls."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:A series of systems in Cambridge are implemented in Lisp running under UnixTM. They all make use of an efficient dictionary access system which services requests for s-expression entries made by client pro- grams. A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved. As #TARGET_REF points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate. The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation. For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system. To the Lisp programmer, the creation of a dictionary process and subsequent requests for information from the dictionary appear simply as Lisp function calls.",
        "output": "nan:A series of systems in Cambridge are implemented in Lisp running under UnixTM. They all make use of an efficient dictionary access system which services requests for s-expression entries made by client pro- grams. A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved. As #TARGET_REF<BACK/> points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate. The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation. For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system. To the Lisp programmer, the creation of a dictionary process and subsequent requests for information from the dictionary appear simply as Lisp function calls."
    },
    {
        "gold": {
            "text": [
                "The idea of using preferences among theories is new, hence it was described in more detail.",
                "`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #TARGET_REF ; #REF ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:The idea of using preferences among theories is new, hence it was described in more detail. `` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #TARGET_REF ; #REF ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .",
        "output": "introduction:The idea of using preferences among theories is new, hence it was described in more detail. `` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #TARGET_REF<USE/> ; #REF ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text ."
    },
    {
        "gold": {
            "text": [
                "We work with a semi-technical text on meteorological phenomena ( #TARGET_REF ) , meant for primary school students .",
                "The text gradually introduces concepts related to precipitation, and explains them.",
                "Its nature makes it appropriate for the semantic analysis task in an incremental approach.",
                "The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We work with a semi-technical text on meteorological phenomena ( #TARGET_REF ) , meant for primary school students . The text gradually introduces concepts related to precipitation, and explains them. Its nature makes it appropriate for the semantic analysis task in an incremental approach. The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text.",
        "output": "experiments:We work with a semi-technical text on meteorological phenomena ( #TARGET_REF<FUT/> ) , meant for primary school students . The text gradually introduces concepts related to precipitation, and explains them. Its nature makes it appropriate for the semantic analysis task in an incremental approach. The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text."
    },
    {
        "gold": {
            "text": [
                "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in #REF, #REF, and #REF.",
                "The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.",
                "Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (#REF;Azzam, Humphreys, and #REF;#REF;#REF;#REF;#REF;Mitkov, Belguith, and #REF).",
                "Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( #REF ; #REF ; Ge , Hale , and #REF ; #REF ; the continuing interest in centering , used either in original or in revised form ( #REF ; #REF ; #REF ; #TARGET_REF ) ; and proposals related to the evaluation methodology in anaphora resolution ( #REFa , 2001b ) .",
                "For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in #REF, #REF, and #REF. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (#REF;Azzam, Humphreys, and #REF;#REF;#REF;#REF;#REF;Mitkov, Belguith, and #REF). Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( #REF ; #REF ; Ge , Hale , and #REF ; #REF ; the continuing interest in centering , used either in original or in revised form ( #REF ; #REF ; #REF ; #TARGET_REF ) ; and proposals related to the evaluation methodology in anaphora resolution ( #REFa , 2001b ) . For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).",
        "output": "nan:The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in #REF, #REF, and #REF. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (#REF;Azzam, Humphreys, and #REF;#REF;#REF;#REF;#REF;Mitkov, Belguith, and #REF). Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( #REF ; #REF ; Ge , Hale , and #REF ; #REF ; the continuing interest in centering , used either in original or in revised form ( #REF ; #REF ; #REF ; #TARGET_REF<BACK/> ) ; and proposals related to the evaluation methodology in anaphora resolution ( #REFa , 2001b ) . For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming)."
    },
    {
        "gold": {
            "text": [
                "There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and #REF ; #REF ; #TARGET_REF ; Malik , Subramaniam , and #REF ) .",
                "eResponder, the system developed by Carmel, Shtalhaim, and #REF, retrieves a list of request-response pairs and presents a ranked list of responses to the user.",
                "If the user is unsatisfied with this list, an operator is asked to generate a new response.",
                "The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses.",
                "However, there is no attempt to automatically generate a single response.",
                "#REF compared the performance of document retrieval and document prediction for generating help-desk responses.",
                "Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs.",
                "Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes.",
                "The generated response is the answer that is closest to the centroid of the cluster."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and #REF ; #REF ; #TARGET_REF ; Malik , Subramaniam , and #REF ) . eResponder, the system developed by Carmel, Shtalhaim, and #REF, retrieves a list of request-response pairs and presents a ranked list of responses to the user. If the user is unsatisfied with this list, an operator is asked to generate a new response. The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses. However, there is no attempt to automatically generate a single response. #REF compared the performance of document retrieval and document prediction for generating help-desk responses. Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs. Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes. The generated response is the answer that is closest to the centroid of the cluster.",
        "output": "nan:There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and #REF ; #REF ; #TARGET_REF<USE/> ; Malik , Subramaniam , and #REF ) . eResponder, the system developed by Carmel, Shtalhaim, and #REF, retrieves a list of request-response pairs and presents a ranked list of responses to the user. If the user is unsatisfied with this list, an operator is asked to generate a new response. The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses. However, there is no attempt to automatically generate a single response. #REF compared the performance of document retrieval and document prediction for generating help-desk responses. Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs. Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes. The generated response is the answer that is closest to the centroid of the cluster."
    },
    {
        "gold": {
            "text": [
                "However, an analysis of the errors indicated that the character based model is more effective at segmenting words that do not appear in the training data.",
                "We seeked to exploit this ability to generalize to improve the dictionary based model.",
                "As in ( #TARGET_REF ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .",
                "In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus.",
                "From this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:However, an analysis of the errors indicated that the character based model is more effective at segmenting words that do not appear in the training data. We seeked to exploit this ability to generalize to improve the dictionary based model. As in ( #TARGET_REF ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems . In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus. From this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus.",
        "output": "nan:However, an analysis of the errors indicated that the character based model is more effective at segmenting words that do not appear in the training data. We seeked to exploit this ability to generalize to improve the dictionary based model. As in ( #TARGET_REF<FUT/> ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems . In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus. From this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus."
    },
    {
        "gold": {
            "text": [
                "When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.",
                "For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e.",
                "literary criticism -unlike in the previous times (#REF).",
                "Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (#REF).",
                "For example , a ` web page ' is more similar to an infinite canvas than a written page ( #TARGET_REF ) .",
                "Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.",
                "From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962).",
                "From a more pessimistic one, an author may feel to have lost power in this openness.",
                "Henceforth the collaborative traits of blogs and wikis (#REF) emphasize annotation, comment, and strong editing.",
                "They give more power to readers, eventually filling the gap -the so-called active readers become authors as well."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose. For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e. literary criticism -unlike in the previous times (#REF). Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (#REF). For example , a ` web page ' is more similar to an infinite canvas than a written page ( #TARGET_REF ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis (#REF) emphasize annotation, comment, and strong editing. They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.",
        "output": "introduction:When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose. For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e. literary criticism -unlike in the previous times (#REF). Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (#REF). For example , a ` web page ' is more similar to an infinite canvas than a written page ( #TARGET_REF<BACK/> ) . Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness. Henceforth the collaborative traits of blogs and wikis (#REF) emphasize annotation, comment, and strong editing. They give more power to readers, eventually filling the gap -the so-called active readers become authors as well."
    },
    {
        "gold": {
            "text": [
                "The system was trained on the Penn Treebank ( #REF ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #TARGET_REF , #REF , and #REF , and became a common testbed ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The system was trained on the Penn Treebank ( #REF ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #TARGET_REF , #REF , and #REF , and became a common testbed .",
        "output": "nan:The system was trained on the Penn Treebank ( #REF ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #TARGET_REF<USE/> , #REF , and #REF , and became a common testbed ."
    },
    {
        "gold": {
            "text": [
                "The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by #REF and extended to labeled dependency parsing by #TARGET_REF ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by #REF and extended to labeled dependency parsing by #TARGET_REF .",
        "output": "method:The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by #REF and extended to labeled dependency parsing by #TARGET_REF<FUT/> ."
    },
    {
        "gold": {
            "text": [
                "Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.",
                "In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.",
                "Some examples include text categorization ( #REF ) , base noun phrase chunking ( #TARGET_REF ) , part-of-speech tagging ( Engelson #REF ) , spelling confusion set disambiguation ( #REF ) , and word sense disambiguation ( #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing. In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications. Some examples include text categorization ( #REF ) , base noun phrase chunking ( #TARGET_REF ) , part-of-speech tagging ( Engelson #REF ) , spelling confusion set disambiguation ( #REF ) , and word sense disambiguation ( #REF ) .",
        "output": "related work:Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing. In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications. Some examples include text categorization ( #REF ) , base noun phrase chunking ( #TARGET_REF<BACK/> ) , part-of-speech tagging ( Engelson #REF ) , spelling confusion set disambiguation ( #REF ) , and word sense disambiguation ( #REF ) ."
    },
    {
        "gold": {
            "text": [
                "These results make it clear that doing under-sampling greatly improves the performance of the classifier, despite the fact that we are using less training data (fewer negative examples).",
                "This is as expected because we are evaluating on F1-measure and the classifier is optimizing for accuracy.",
                "absolute.",
                "As in the baseline system, a combination of structures performs best.",
                "As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall.",
                "Here , the PET and GR kernel perform similar : this is different from the results of ( #TARGET_REF ) where GR performed much worse than PET for ACE data .",
                "This exemplifies the difference in the nature of our event annotations from that of ACE relations.",
                "Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger.",
                "This means that implicit feature space is much sparser and thus not the best representation.",
                "4 presents results for using the oversampling method with transformation that produces synthetic positive examples by using a transformation on dependency trees such that the new synthetic examples are \"close\" to the original examples.",
                "This method achieves a gain 16.78% over the baseline system."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:These results make it clear that doing under-sampling greatly improves the performance of the classifier, despite the fact that we are using less training data (fewer negative examples). This is as expected because we are evaluating on F1-measure and the classifier is optimizing for accuracy. absolute. As in the baseline system, a combination of structures performs best. As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. Here , the PET and GR kernel perform similar : this is different from the results of ( #TARGET_REF ) where GR performed much worse than PET for ACE data . This exemplifies the difference in the nature of our event annotations from that of ACE relations. Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger. This means that implicit feature space is much sparser and thus not the best representation. 4 presents results for using the oversampling method with transformation that produces synthetic positive examples by using a transformation on dependency trees such that the new synthetic examples are \"close\" to the original examples. This method achieves a gain 16.78% over the baseline system.",
        "output": "experiments:These results make it clear that doing under-sampling greatly improves the performance of the classifier, despite the fact that we are using less training data (fewer negative examples). This is as expected because we are evaluating on F1-measure and the classifier is optimizing for accuracy. absolute. As in the baseline system, a combination of structures performs best. As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. Here , the PET and GR kernel perform similar : this is different from the results of ( #TARGET_REF<USE/> ) where GR performed much worse than PET for ACE data . This exemplifies the difference in the nature of our event annotations from that of ACE relations. Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger. This means that implicit feature space is much sparser and thus not the best representation. 4 presents results for using the oversampling method with transformation that produces synthetic positive examples by using a transformation on dependency trees such that the new synthetic examples are \"close\" to the original examples. This method achieves a gain 16.78% over the baseline system."
    },
    {
        "gold": {
            "text": [
                "The system uses a knowledge base implemented in the KM representation language ( #REF ; #TARGET_REF ) to represent the state of the world .",
                "At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The system uses a knowledge base implemented in the KM representation language ( #REF ; #TARGET_REF ) to represent the state of the world . At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits.",
        "output": "experiments:The system uses a knowledge base implemented in the KM representation language ( #REF ; #TARGET_REF<FUT/> ) to represent the state of the world . At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits."
    },
    {
        "gold": {
            "text": [
                "Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #TARGET_REF ; #REFb ; #REF ; #REF ; #REF ; #REFa ; #REFb ; #REF ) .",
                "Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the \"meaning of words is entirely given by other words\" (#REFb)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #TARGET_REF ; #REFb ; #REF ; #REF ; #REF ; #REFa ; #REFb ; #REF ) . Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the \"meaning of words is entirely given by other words\" (#REFb).",
        "output": "introduction:Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #REF ; #TARGET_REF<BACK/> ; #REFb ; #REF ; #REF ; #REF ; #REFa ; #REFb ; #REF ) . Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the \"meaning of words is entirely given by other words\" (#REFb)."
    },
    {
        "gold": {
            "text": [
                "A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #TARGET_REF , #REF , #REF , #REF , #REF , #REF , #REF , and #REF ) .",
                "Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.",
                "While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.",
                "A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #TARGET_REF , #REF , #REF , #REF , #REF , #REF , #REF , and #REF ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF.",
        "output": "nan:A number of speech understanding systems have been developed during the past fifteen years ( #REF , #REF , #TARGET_REF<USE/> , #REF , #REF , #REF , #REF , #REF , #REF , and #REF ) . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #REF."
    },
    {
        "gold": {
            "text": [
                "The shallow parser used is the SNoW-based CSCL parser ( #TARGET_REF ; #REF ) .",
                "SNoW (#REF;#REF) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.",
                "It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.",
                "Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.",
                "However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.",
                "Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).",
                "The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The shallow parser used is the SNoW-based CSCL parser ( #TARGET_REF ; #REF ) . SNoW (#REF;#REF) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example. It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes. However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference. Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.). The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.",
        "output": "experiments:The shallow parser used is the SNoW-based CSCL parser ( #TARGET_REF<FUT/> ; #REF ) . SNoW (#REF;#REF) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example. It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes. However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference. Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.). The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes."
    },
    {
        "gold": {
            "text": [
                "The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #TARGET_REF ) .",
                "We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively.",
                "For a token t i , the backward token n-gram feature will contains the previous n − 1 tokens in the history (t i−n+1 , . . .",
                "t i−1 ) and the forward token n-gram feature will contains the next n − 1 tokens (t i+1 , . . .",
                "t i+n−1 )."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #TARGET_REF ) . We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively. For a token t i , the backward token n-gram feature will contains the previous n − 1 tokens in the history (t i−n+1 , . . . t i−1 ) and the forward token n-gram feature will contains the next n − 1 tokens (t i+1 , . . . t i+n−1 ).",
        "output": "nan:The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #TARGET_REF<BACK/> ) . We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively. For a token t i , the backward token n-gram feature will contains the previous n − 1 tokens in the history (t i−n+1 , . . . t i−1 ) and the forward token n-gram feature will contains the next n − 1 tokens (t i+1 , . . . t i+n−1 )."
    },
    {
        "gold": {
            "text": [
                "Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.",
                "One solution to this problem is to add more complexity to the model to better reflect the translation process.",
                "This is the approach taken by IBM Models 4 + ( #REFb ; #REF ) , and more recently by the LEAF model ( #TARGET_REF ) .",
                "Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.",
                "Instead, we propose to use a learning framework called Posterior Regularization (Graça, Ganchev, and #REF) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors.",
                "The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).",
                "Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior.",
                "On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments. One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( #REFb ; #REF ) , and more recently by the LEAF model ( #TARGET_REF ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Graça, Ganchev, and #REF) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model). Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior. On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable.",
        "output": "nan:Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments. One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4 + ( #REFb ; #REF ) , and more recently by the LEAF model ( #TARGET_REF<USE/> ) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Graça, Ganchev, and #REF) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model). Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior. On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable."
    },
    {
        "gold": {
            "text": [
                "The idea behind the Doc-Pred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected.",
                "In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #TARGET_REF ; #REF ) .",
                "We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2).",
                "The input to Snob is a set of binary vectors, one vector per response document.",
                "The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency).",
                "The predictive model is a Decision Graph (#REF), which, like Snob, is based on the MML principle."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:The idea behind the Doc-Pred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #TARGET_REF ; #REF ) . We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency). The predictive model is a Decision Graph (#REF), which, like Snob, is based on the MML principle.",
        "output": "method:The idea behind the Doc-Pred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected. In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #TARGET_REF<FUT/> ; #REF ) . We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency). The predictive model is a Decision Graph (#REF), which, like Snob, is based on the MML principle."
    },
    {
        "gold": {
            "text": [
                "Much work has been done on the use of morphological features for parsing of morphologically rich languages.",
                "#TARGET_REF report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .",
                "This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ≈3000+).",
                "They also report that the use of gender, number, and person features did not yield any improvements.",
                "The results for Czech are the opposite of our results for Arabic, as we will see.",
                "This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajič and Vidová-Hladká 1998) compared with Arabic (≈14.0%,",
                "see Table 3).",
                "Similarly, #REF report that the use of a subset of Spanish morphological features (number for adjectives, determiners, nouns, pronouns, and verbs; and mode for verbs) outperforms other combinations.",
                "Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.",
                "We also find that the number feature helps for Arabic."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Much work has been done on the use of morphological features for parsing of morphologically rich languages. #TARGET_REF report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) . This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ≈3000+). They also report that the use of gender, number, and person features did not yield any improvements. The results for Czech are the opposite of our results for Arabic, as we will see. This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajič and Vidová-Hladká 1998) compared with Arabic (≈14.0%, see Table 3). Similarly, #REF report that the use of a subset of Spanish morphological features (number for adjectives, determiners, nouns, pronouns, and verbs; and mode for verbs) outperforms other combinations. Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic.",
        "output": "related work:Much work has been done on the use of morphological features for parsing of morphologically rich languages. #TARGET_REF<BACK/> report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) . This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ≈3000+). They also report that the use of gender, number, and person features did not yield any improvements. The results for Czech are the opposite of our results for Arabic, as we will see. This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajič and Vidová-Hladká 1998) compared with Arabic (≈14.0%, see Table 3). Similarly, #REF report that the use of a subset of Spanish morphological features (number for adjectives, determiners, nouns, pronouns, and verbs; and mode for verbs) outperforms other combinations. Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic."
    },
    {
        "gold": {
            "text": [
                "For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.",
                "#REF claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.",
                "Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.",
                "Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #TARGET_REF ( henceforth G&G ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings. #REF claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it. Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly. Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #TARGET_REF ( henceforth G&G ) .",
        "output": "introduction:For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings. #REF claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it. Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly. Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #TARGET_REF<USE/> ( henceforth G&G ) ."
    },
    {
        "gold": {
            "text": [
                "For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #REF , 2008 ; KÃ¼bler , McDonald , and #TARGET_REF ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term \"dev set\" to denote a non-blind test set, used for model development (feature selection and feature engineering).",
                "We do not perform further weight optimization (which, if done, is done on a separate \"tuning set\").",
                "to predict the next state in the parse derivation.",
                "All experiments were done using the Nivre \"eager\" algorithm.",
                "11 There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent).",
                "There are default MaltParser features (in the machine learning sense), 12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #REF , 2008 ; KÃ¼bler , McDonald , and #TARGET_REF ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term \"dev set\" to denote a non-blind test set, used for model development (feature selection and feature engineering). We do not perform further weight optimization (which, if done, is done on a separate \"tuning set\"). to predict the next state in the parse derivation. All experiments were done using the Nivre \"eager\" algorithm. 11 There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent). There are default MaltParser features (in the machine learning sense), 12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers.",
        "output": "related work:For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #REF , 2008 ; KÃ¼bler , McDonald , and #TARGET_REF<FUT/> ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term \"dev set\" to denote a non-blind test set, used for model development (feature selection and feature engineering). We do not perform further weight optimization (which, if done, is done on a separate \"tuning set\"). to predict the next state in the parse derivation. All experiments were done using the Nivre \"eager\" algorithm. 11 There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent). There are default MaltParser features (in the machine learning sense), 12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers."
    },
    {
        "gold": {
            "text": [
                "#REF attempts to improve on the approach of #REF by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.",
                "He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.",
                "The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following #REF.",
                "Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.",
                "#TARGET_REF predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and #REF ) and ANLT ( #REF ) dictionaries and adding around 30 frames found by manual inspection .",
                "The frames incorporate control information and details of specific prepositions."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:#REF attempts to improve on the approach of #REF by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur. He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following #REF. Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. #TARGET_REF predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and #REF ) and ANLT ( #REF ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions.",
        "output": "related work:#REF attempts to improve on the approach of #REF by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur. He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following #REF. Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. #TARGET_REF<BACK/> predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and #REF ) and ANLT ( #REF ) dictionaries and adding around 30 frames found by manual inspection . The frames incorporate control information and details of specific prepositions."
    },
    {
        "gold": {
            "text": [
                "Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (#REF;#REF;#REF;#REF).",
                "Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.",
                "In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.",
                "Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.",
                "We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.",
                "The results , which partly confirm those obtained on a smaller dataset in #TARGET_REF , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions .",
                "The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (#REF;#REF;#REF;#REF). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features. Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information. We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions. The results , which partly confirm those obtained on a smaller dataset in #TARGET_REF , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions . The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.",
        "output": "introduction:Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (#REF;#REF;#REF;#REF). Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures. In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features. Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information. We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions. The results , which partly confirm those obtained on a smaller dataset in #TARGET_REF<USE/> , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions . The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category."
    },
    {
        "gold": {
            "text": [
                "In this paper, we show that the strongly equivalent grammars enable the sharing of \"parsing techniques\", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.",
                "We applied our system to the XTAG English grammar ( The XTAG Research #TARGET_REF ) 3 , which is a large-scale FB-LTAG grammar for English .",
                "A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .",
                "This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.",
                "We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:In this paper, we show that the strongly equivalent grammars enable the sharing of \"parsing techniques\", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities. We applied our system to the XTAG English grammar ( The XTAG Research #TARGET_REF ) 3 , which is a large-scale FB-LTAG grammar for English . A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser . This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.",
        "output": "experiments:In this paper, we show that the strongly equivalent grammars enable the sharing of \"parsing techniques\", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities. We applied our system to the XTAG English grammar ( The XTAG Research #TARGET_REF<FUT/> ) 3 , which is a large-scale FB-LTAG grammar for English . A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser . This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing."
    },
    {
        "gold": {
            "text": [
                "We separate the EDR task into two parts: a mention detection step, which identifies and classifies all the mentions in a text -and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.",
                "In its entirety, the EDR task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non-named mentions (nominal and pronominal) and the requirement of grouping mentions into entities.",
                "This is particularly true for Arabic where nominals and pronouns are also attached to the word they modify.",
                "In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words).",
                "In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.",
                "It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:We separate the EDR task into two parts: a mention detection step, which identifies and classifies all the mentions in a text -and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object. In its entirety, the EDR task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non-named mentions (nominal and pronominal) and the requirement of grouping mentions into entities. This is particularly true for Arabic where nominals and pronouns are also attached to the word they modify. In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words). In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word. It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #REF ; #TARGET_REF ) .",
        "output": "introduction:We separate the EDR task into two parts: a mention detection step, which identifies and classifies all the mentions in a text -and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object. In its entirety, the EDR task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non-named mentions (nominal and pronominal) and the requirement of grouping mentions into entities. This is particularly true for Arabic where nominals and pronouns are also attached to the word they modify. In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words). In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word. It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #REF ; #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) ( #TARGET_REF ) , the Buckwalter morphological analyzer (#REF), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (#REF;  Habash, Rambow, and #REF).",
                "The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.",
                "In this article, we use an in-house system which provides functional gender, number, and rationality features (#REF).",
                "See Section 5.2 for more details."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) ( #TARGET_REF ) , the Buckwalter morphological analyzer (#REF), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (#REF;  Habash, Rambow, and #REF). The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. In this article, we use an in-house system which provides functional gender, number, and rationality features (#REF). See Section 5.2 for more details.",
        "output": "experiments:Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) ( #TARGET_REF<USE/> ) , the Buckwalter morphological analyzer (#REF), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (#REF;  Habash, Rambow, and #REF). The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. In this article, we use an in-house system which provides functional gender, number, and rationality features (#REF). See Section 5.2 for more details."
    },
    {
        "gold": {
            "text": [
                "We implemented a set of filters for word pairs.",
                "One group of filters removed unwanted word pairs.",
                "Word pairs are filtered if they contain at least one word that a) has less than three letters b) contains only uppercase letters (mostly acronyms) or c) can be found in a stoplist.",
                "Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set.",
                "We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on.",
                "Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #TARGET_REF ) , the German equivalent to WordNet , as a sense inventory for each word .",
                "It is the most complete resource of this type for German."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We implemented a set of filters for word pairs. One group of filters removed unwanted word pairs. Word pairs are filtered if they contain at least one word that a) has less than three letters b) contains only uppercase letters (mostly acronyms) or c) can be found in a stoplist. Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set. We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on. Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #TARGET_REF ) , the German equivalent to WordNet , as a sense inventory for each word . It is the most complete resource of this type for German.",
        "output": "experiments:We implemented a set of filters for word pairs. One group of filters removed unwanted word pairs. Word pairs are filtered if they contain at least one word that a) has less than three letters b) contains only uppercase letters (mostly acronyms) or c) can be found in a stoplist. Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set. We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on. Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #TARGET_REF<FUT/> ) , the German equivalent to WordNet , as a sense inventory for each word . It is the most complete resource of this type for German."
    },
    {
        "gold": {
            "text": [
                "The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.",
                "Labeled attachment score is used as our base model loss function.",
                "In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i.",
                "The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).",
                "The score is normalized by the summed arc lengths for the sentence.",
                "The labeled version of this score requires that the labels of the arc are also correct.",
                "Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (#REF) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction ( #TARGET_REF ) and textual entailment ( #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well. Labeled attachment score is used as our base model loss function. In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i. The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise). The score is normalized by the summed arc lengths for the sentence. The labeled version of this score requires that the labels of the arc are also correct. Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (#REF) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction ( #TARGET_REF ) and textual entailment ( #REF ) .",
        "output": "experiments:The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well. Labeled attachment score is used as our base model loss function. In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i. The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise). The score is normalized by the summed arc lengths for the sentence. The labeled version of this score requires that the labels of the arc are also correct. Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (#REF) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction ( #TARGET_REF<BACK/> ) and textual entailment ( #REF ) ."
    },
    {
        "gold": {
            "text": [
                "The work that is most similar to ours is that of #TARGET_REF , who introduced the Constraint Driven Learning algorithm ( CODL ) .",
                "Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets).",
                "For each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints.",
                "These induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items.",
                "The augmented-loss algorithm can be viewed as an online version of this algorithm which performs model updates based on the augmented-loss functions directly (rather than adding a set of examples to the training set).",
                "Unlike the CODL approach, we do not perform complete optimization on each iteration over the unlabeled dataset; rather, we incorporate the updates in our online learning algorithm.",
                "As mentioned earlier, CODL is one example of learning algorithms that use weak supervision, others include #REF and #REF.",
                "Again, these works are typically interested in using the extrinsic metric -or, in general, extrinsic information -to optimize the intrinsic metric in the absence of any labeled intrinsic data.",
                "Our goal is to optimize both simultaneously."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:The work that is most similar to ours is that of #TARGET_REF , who introduced the Constraint Driven Learning algorithm ( CODL ) . Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets). For each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints. These induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items. The augmented-loss algorithm can be viewed as an online version of this algorithm which performs model updates based on the augmented-loss functions directly (rather than adding a set of examples to the training set). Unlike the CODL approach, we do not perform complete optimization on each iteration over the unlabeled dataset; rather, we incorporate the updates in our online learning algorithm. As mentioned earlier, CODL is one example of learning algorithms that use weak supervision, others include #REF and #REF. Again, these works are typically interested in using the extrinsic metric -or, in general, extrinsic information -to optimize the intrinsic metric in the absence of any labeled intrinsic data. Our goal is to optimize both simultaneously.",
        "output": "related work:The work that is most similar to ours is that of #TARGET_REF<USE/> , who introduced the Constraint Driven Learning algorithm ( CODL ) . Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets). For each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints. These induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items. The augmented-loss algorithm can be viewed as an online version of this algorithm which performs model updates based on the augmented-loss functions directly (rather than adding a set of examples to the training set). Unlike the CODL approach, we do not perform complete optimization on each iteration over the unlabeled dataset; rather, we incorporate the updates in our online learning algorithm. As mentioned earlier, CODL is one example of learning algorithms that use weak supervision, others include #REF and #REF. Again, these works are typically interested in using the extrinsic metric -or, in general, extrinsic information -to optimize the intrinsic metric in the absence of any labeled intrinsic data. Our goal is to optimize both simultaneously."
    },
    {
        "gold": {
            "text": [
                "Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #TARGET_REF ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #TARGET_REF .",
        "output": "nan:Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #TARGET_REF<FUT/> ."
    },
    {
        "gold": {
            "text": [
                "The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #TARGET_REF ; #REF ) .",
                "An example of using the Python tagger interface is shown in Figure 1."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #TARGET_REF ; #REF ) . An example of using the Python tagger interface is shown in Figure 1.",
        "output": "nan:The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #TARGET_REF<BACK/> ; #REF ) . An example of using the Python tagger interface is shown in Figure 1."
    },
    {
        "gold": {
            "text": [
                "To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #TARGET_REF ) .",
                "In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments.",
                "The corresponding performances in (#REF) are 75.6% and 50%, respectively.",
                "Thus, the overall performance (including cross validation and evaluation set experiments) in this work is comparable to that in (#REF).",
                "This offers some indication that the performance of NB algorithm in classifying Arabic documents is not sensitive to the Arabic root extraction algorithm.",
                "Future work will be directed at experimenting with other root extraction algorithms.",
                "Further improvement of NB's performance may be effected by using unlabeled documents; e.g., EM has been used successfully for this purpose in (Nigam et al.,200), where EM has increased the classification accuracy by 30% for classifying English documents."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #TARGET_REF ) . In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments. The corresponding performances in (#REF) are 75.6% and 50%, respectively. Thus, the overall performance (including cross validation and evaluation set experiments) in this work is comparable to that in (#REF). This offers some indication that the performance of NB algorithm in classifying Arabic documents is not sensitive to the Arabic root extraction algorithm. Future work will be directed at experimenting with other root extraction algorithms. Further improvement of NB's performance may be effected by using unlabeled documents; e.g., EM has been used successfully for this purpose in (Nigam et al.,200), where EM has increased the classification accuracy by 30% for classifying English documents.",
        "output": "conclusion:To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #TARGET_REF<USE/> ) . In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments. The corresponding performances in (#REF) are 75.6% and 50%, respectively. Thus, the overall performance (including cross validation and evaluation set experiments) in this work is comparable to that in (#REF). This offers some indication that the performance of NB algorithm in classifying Arabic documents is not sensitive to the Arabic root extraction algorithm. Future work will be directed at experimenting with other root extraction algorithms. Further improvement of NB's performance may be effected by using unlabeled documents; e.g., EM has been used successfully for this purpose in (Nigam et al.,200), where EM has increased the classification accuracy by 30% for classifying English documents."
    },
    {
        "gold": {
            "text": [
                "Our baseline coreference system uses the C4 .5 decision tree learner ( #TARGET_REF ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .",
                "Following previous work (e.g., #REF and #REF), we generate training instances as follows: a positive instance is created for each anaphoric NP, NP j , and its closest antecedent, NP i ; and a negative instance is created for NP j paired with each of the intervening NPs, NP i+1 , NP i+2 , . .",
                "., NP j−1 .",
                "Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as #REF and #REF, as described below."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Our baseline coreference system uses the C4 .5 decision tree learner ( #TARGET_REF ) to acquire a classifier on the training texts for determining whether two NPs are coreferent . Following previous work (e.g., #REF and #REF), we generate training instances as follows: a positive instance is created for each anaphoric NP, NP j , and its closest antecedent, NP i ; and a negative instance is created for NP j paired with each of the intervening NPs, NP i+1 , NP i+2 , . . ., NP j−1 . Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as #REF and #REF, as described below.",
        "output": "nan:Our baseline coreference system uses the C4 .5 decision tree learner ( #TARGET_REF<FUT/> ) to acquire a classifier on the training texts for determining whether two NPs are coreferent . Following previous work (e.g., #REF and #REF), we generate training instances as follows: a positive instance is created for each anaphoric NP, NP j , and its closest antecedent, NP i ; and a negative instance is created for NP j paired with each of the intervening NPs, NP i+1 , NP i+2 , . . ., NP j−1 . Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as #REF and #REF, as described below."
    },
    {
        "gold": {
            "text": [
                "Not much information has been published on abbreviation identification.",
                "One of the better-known approaches is described in #TARGET_REF , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .",
                "This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.",
                "The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.",
                "#REF recently described a hybrid method for finding abbreviations and their definitions.",
                "This method first applies an \"abbreviation recognizer\" that generates a set of \"candidate abbreviations\" for a document.",
                "Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).",
                "The abbreviation recognizer for these purposes is allowed to overgenerate significantly.",
                "There is no harm (apart from the performance issues) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Not much information has been published on abbreviation identification. One of the better-known approaches is described in #TARGET_REF , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing . This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level. The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents. #REF recently described a hybrid method for finding abbreviations and their definitions. This method first applies an \"abbreviation recognizer\" that generates a set of \"candidate abbreviations\" for a document. Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK). The abbreviation recognizer for these purposes is allowed to overgenerate significantly. There is no harm (apart from the performance issues) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.",
        "output": "nan:Not much information has been published on abbreviation identification. One of the better-known approaches is described in #TARGET_REF<BACK/> , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing . This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level. The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents. #REF recently described a hybrid method for finding abbreviations and their definitions. This method first applies an \"abbreviation recognizer\" that generates a set of \"candidate abbreviations\" for a document. Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK). The abbreviation recognizer for these purposes is allowed to overgenerate significantly. There is no harm (apart from the performance issues) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained."
    },
    {
        "gold": {
            "text": [
                "The results of our second set of experiments (with RCTs only) are shown in Tables 2(a) and 2(b). Table 2(a) reports the multi-way classification error rate; once again, applying the Markov assumption to model discourse transitions improves performance, and using LDA further reduces error rate.",
                "Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table).",
                "The table also presents the closest comparable experimental results reported by #TARGET_REF .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 .",
                "This collection has significantly more training examples than our corpus of 27,075 abstracts, which could be a source of performance differences.",
                "Furthermore, details regarding their procedure for mapping structured abstract headings to one of the four general labels was not discussed in their paper.",
                "Nevertheless, our HMM- based approach is at least competitive with SVMs, perhaps better in some cases."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:The results of our second set of experiments (with RCTs only) are shown in Tables 2(a) and 2(b). Table 2(a) reports the multi-way classification error rate; once again, applying the Markov assumption to model discourse transitions improves performance, and using LDA further reduces error rate. Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table). The table also presents the closest comparable experimental results reported by #TARGET_REF .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 . This collection has significantly more training examples than our corpus of 27,075 abstracts, which could be a source of performance differences. Furthermore, details regarding their procedure for mapping structured abstract headings to one of the four general labels was not discussed in their paper. Nevertheless, our HMM- based approach is at least competitive with SVMs, perhaps better in some cases.",
        "output": "experiments:The results of our second set of experiments (with RCTs only) are shown in Tables 2(a) and 2(b). Table 2(a) reports the multi-way classification error rate; once again, applying the Markov assumption to model discourse transitions improves performance, and using LDA further reduces error rate. Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table). The table also presents the closest comparable experimental results reported by #TARGET_REF<USE/> .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 . This collection has significantly more training examples than our corpus of 27,075 abstracts, which could be a source of performance differences. Furthermore, details regarding their procedure for mapping structured abstract headings to one of the four general labels was not discussed in their paper. Nevertheless, our HMM- based approach is at least competitive with SVMs, perhaps better in some cases."
    },
    {
        "gold": {
            "text": [
                "Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.",
                "The direction of the slash operator gives the behavior of the function.",
                "A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject.",
                "We follow #TARGET_REF in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .",
                "We further add rules for combining with punctuation to the left and right and allow for the merge rule X → X X of #REF."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1. The direction of the slash operator gives the behavior of the function. A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject. We follow #TARGET_REF in allowing a small set of generic , linguistically-plausible unary and binary grammar rules . We further add rules for combining with punctuation to the left and right and allow for the merge rule X → X X of #REF.",
        "output": "nan:Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1. The direction of the slash operator gives the behavior of the function. A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject. We follow #TARGET_REF<FUT/> in allowing a small set of generic , linguistically-plausible unary and binary grammar rules . We further add rules for combining with punctuation to the left and right and allow for the merge rule X → X X of #REF."
    },
    {
        "gold": {
            "text": [
                "#REF predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and #REF) and ANLT (#REF) dictionaries and adding around 30 frames found by manual inspection.",
                "The frames incorporate control information and details of specific prepositions.",
                "#REF refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.",
                "Recent work by #TARGET_REF on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection .",
                "#REF use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.",
                "The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.",
                "They perform a mapping between their frames and those of the OALD, resulting in 15 frame types."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:#REF predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and #REF) and ANLT (#REF) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions. #REF refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by #TARGET_REF on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . #REF use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns. The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate. They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.",
        "output": "related work:#REF predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and #REF) and ANLT (#REF) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions. #REF refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by #TARGET_REF<BACK/> on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . #REF use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns. The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate. They perform a mapping between their frames and those of the OALD, resulting in 15 frame types."
    },
    {
        "gold": {
            "text": [
                "In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #TARGET_REF ) 5 .",
                "This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.",
                "Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #TARGET_REF ) 5 . This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models. Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models.",
        "output": "nan:In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #TARGET_REF<USE/> ) 5 . This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models. Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models."
    },
    {
        "gold": {
            "text": [
                "To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by #REF and used by #TARGET_REF that samples entire parse trees .",
                "For a sentence w, the strategy is to use the Inside algorithm (#REF) to inductively compute, for each potential non-terminal position spanning words w i through w j−1 and category t, going \"up\" the tree, the probability of generating w i , . . .",
                ", w j−1 via any arrangement of productions that is rooted by y ij = t."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by #REF and used by #TARGET_REF that samples entire parse trees . For a sentence w, the strategy is to use the Inside algorithm (#REF) to inductively compute, for each potential non-terminal position spanning words w i through w j−1 and category t, going \"up\" the tree, the probability of generating w i , . . . , w j−1 via any arrangement of productions that is rooted by y ij = t.",
        "output": "nan:To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by #REF and used by #TARGET_REF<FUT/> that samples entire parse trees . For a sentence w, the strategy is to use the Inside algorithm (#REF) to inductively compute, for each potential non-terminal position spanning words w i through w j−1 and category t, going \"up\" the tree, the probability of generating w i , . . . , w j−1 via any arrangement of productions that is rooted by y ij = t."
    },
    {
        "gold": {
            "text": [
                "We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any ( #TARGET_REF ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any ( #TARGET_REF ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity.",
        "output": "nan:We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any ( #TARGET_REF<BACK/> ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity."
    },
    {
        "gold": {
            "text": [
                "The language chosen for semantic representation is a flat semantics along the line of ( #REF ; #TARGET_REF ; #REF ) .",
                "However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.",
                "Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .",
                ", x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The language chosen for semantic representation is a flat semantics along the line of ( #REF ; #TARGET_REF ; #REF ) . However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships. Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . . , x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.",
        "output": "nan:The language chosen for semantic representation is a flat semantics along the line of ( #REF ; #TARGET_REF<USE/> ; #REF ) . However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships. Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . . , x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing."
    },
    {
        "gold": {
            "text": [
                "The ACE-2004 dataset contains 443 documents.",
                "We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (#REF;#REF).",
                "The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #TARGET_REF ) , contains 3,145 annotated documents .",
                "These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.",
                "We report results on the test documents for both datasets.",
                ") indicate evaluations on (mentions, mention heads) respectively.",
                "For gold mentions and mention heads, they yield the same performance for coreference.",
                "Our proposed H-Joint-M system achieves the highest performance.",
                "Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (#REF;#REF). The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #TARGET_REF ) , contains 3,145 annotated documents . These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. ) indicate evaluations on (mentions, mention heads) respectively. For gold mentions and mention heads, they yield the same performance for coreference. Our proposed H-Joint-M system achieves the highest performance. Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3.",
        "output": "experiments:The ACE-2004 dataset contains 443 documents. We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (#REF;#REF). The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #TARGET_REF<FUT/> ) , contains 3,145 annotated documents . These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs. We report results on the test documents for both datasets. ) indicate evaluations on (mentions, mention heads) respectively. For gold mentions and mention heads, they yield the same performance for coreference. Our proposed H-Joint-M system achieves the highest performance. Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3."
    },
    {
        "gold": {
            "text": [
                "#REF was the first scholar who stressed the impact of the digital revolution to the medium of writing.",
                "Terms as �chapter�, �page� or �foot- note� simply become meaningless in the new texts, or they highly change their meaning.",
                "When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.",
                "For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( #TARGET_REF ) .",
                "Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (#REF).",
                "For example, a �web page� is more similar to an infinite canvas than a written page (#REF).",
                "Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.",
                "From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an �opera aperta� (open work), as Eco would define it (1962).",
                "From a more pessimistic one, an author may feel to have lost power in this openness."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:#REF was the first scholar who stressed the impact of the digital revolution to the medium of writing. Terms as �chapter�, �page� or �foot- note� simply become meaningless in the new texts, or they highly change their meaning. When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose. For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( #TARGET_REF ) . Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (#REF). For example, a �web page� is more similar to an infinite canvas than a written page (#REF). Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an �opera aperta� (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness.",
        "output": "introduction:#REF was the first scholar who stressed the impact of the digital revolution to the medium of writing. Terms as �chapter�, �page� or �foot- note� simply become meaningless in the new texts, or they highly change their meaning. When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose. For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( #TARGET_REF<BACK/> ) . Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (#REF). For example, a �web page� is more similar to an infinite canvas than a written page (#REF). Moreover, what seems to be lost is the relations, like the texture underpinning the text itself. From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an �opera aperta� (open work), as Eco would define it (1962). From a more pessimistic one, an author may feel to have lost power in this openness."
    },
    {
        "gold": {
            "text": [
                "The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) .",
                "The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.",
                "The best current model (#REF) has only 6% less precision error and only 11% less recall error than the lexicalized model.",
                "The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.",
                "It is also achieved without any explicit notion of lexical head."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( #REF ; #REF ; #TARGET_REF ; #REF ; #REF ) . The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. The best current model (#REF) has only 6% less precision error and only 11% less recall error than the lexicalized model. The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words. It is also achieved without any explicit notion of lexical head.",
        "output": "experiments:The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( #REF ; #REF ; #TARGET_REF<USE/> ; #REF ; #REF ) . The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. The best current model (#REF) has only 6% less precision error and only 11% less recall error than the lexicalized model. The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words. It is also achieved without any explicit notion of lexical head."
    },
    {
        "gold": {
            "text": [
                "The shallow parser used is the SNoW-based CSCL parser (#REF;#REF).",
                "SNoW ( #TARGET_REF ; #REF ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .",
                "It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.",
                "Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.",
                "However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.",
                "Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).",
                "The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The shallow parser used is the SNoW-based CSCL parser (#REF;#REF). SNoW ( #TARGET_REF ; #REF ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example . It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes. However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference. Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.). The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.",
        "output": "experiments:The shallow parser used is the SNoW-based CSCL parser (#REF;#REF). SNoW ( #TARGET_REF<FUT/> ; #REF ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example . It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes. However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference. Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.). The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes."
    },
    {
        "gold": {
            "text": [
                "A formula for the test set perplexity ( #TARGET_REF ) is :13"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:A formula for the test set perplexity ( #TARGET_REF ) is :13",
        "output": "nan:A formula for the test set perplexity ( #TARGET_REF<BACK/> ) is :13"
    },
    {
        "gold": {
            "text": [
                "Research that is more similar in goal to that outlined in this paper is Vosse ( #TARGET_REF ) .",
                "Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names.",
                "Capitalization is his sole means of identifying names.",
                "However, capitalization information is not available in closed captions.",
                "Hence, his system would be ineffective on the closed caption domain with which we are working.",
                "(#REF) uses expectations generated by scripts to anMyze unknown words.",
                "The drawback of his system is that it lacks portability since it incorporates scripts that make use of world knowledge of the situation being described; in this case, naval ship-to-shore messages."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Research that is more similar in goal to that outlined in this paper is Vosse ( #TARGET_REF ) . Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names. Capitalization is his sole means of identifying names. However, capitalization information is not available in closed captions. Hence, his system would be ineffective on the closed caption domain with which we are working. (#REF) uses expectations generated by scripts to anMyze unknown words. The drawback of his system is that it lacks portability since it incorporates scripts that make use of world knowledge of the situation being described; in this case, naval ship-to-shore messages.",
        "output": "nan:Research that is more similar in goal to that outlined in this paper is Vosse ( #TARGET_REF<USE/> ) . Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names. Capitalization is his sole means of identifying names. However, capitalization information is not available in closed captions. Hence, his system would be ineffective on the closed caption domain with which we are working. (#REF) uses expectations generated by scripts to anMyze unknown words. The drawback of his system is that it lacks portability since it incorporates scripts that make use of world knowledge of the situation being described; in this case, naval ship-to-shore messages."
    },
    {
        "gold": {
            "text": [
                "We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.",
                "We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #TARGET_REF ) .",
                "Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (#REF)."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work. We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #TARGET_REF ) . Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (#REF).",
        "output": "nan:We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work. We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #TARGET_REF<FUT/> ) . Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (#REF)."
    },
    {
        "gold": {
            "text": [
                "In our example, b is the only object that has a Pareto-optimal combination of Values, predicting correctly that b can be called the tall fat giraffe.",
                "It seems likely, however, that people use doubly graded descriptions more liberally.",
                "For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe.",
                "Many alternative strategies are possible.",
                "The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #REF ; cfXXX #TARGET_REF ; #REF , for other plans ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:In our example, b is the only object that has a Pareto-optimal combination of Values, predicting correctly that b can be called the tall fat giraffe. It seems likely, however, that people use doubly graded descriptions more liberally. For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe. Many alternative strategies are possible. The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #REF ; cfXXX #TARGET_REF ; #REF , for other plans ) .",
        "output": "experiments:In our example, b is the only object that has a Pareto-optimal combination of Values, predicting correctly that b can be called the tall fat giraffe. It seems likely, however, that people use doubly graded descriptions more liberally. For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe. Many alternative strategies are possible. The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #REF ; cfXXX #TARGET_REF<BACK/> ; #REF , for other plans ) ."
    },
    {
        "gold": {
            "text": [
                "Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all.",
                "This viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.",
                "Now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence.",
                "We can then later (Section 5.2) define p-models--a category of models corresponding to paragraphs--as models of coherent theories that satisfy all metalevel conditions.",
                "The partial theories pick up from the referential level the most obvious or the most important information about a formula.",
                "This immediate information may be insufficient to decide the truth of certain predicates.",
                "It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #TARGET_REFb ) .",
                "#REFb)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all. This viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates. Now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence. We can then later (Section 5.2) define p-models--a category of models corresponding to paragraphs--as models of coherent theories that satisfy all metalevel conditions. The partial theories pick up from the referential level the most obvious or the most important information about a formula. This immediate information may be insufficient to decide the truth of certain predicates. It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #TARGET_REFb ) . #REFb).",
        "output": "introduction:Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all. This viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates. Now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence. We can then later (Section 5.2) define p-models--a category of models corresponding to paragraphs--as models of coherent theories that satisfy all metalevel conditions. The partial theories pick up from the referential level the most obvious or the most important information about a formula. This immediate information may be insufficient to decide the truth of certain predicates. It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #TARGET_REF<USE/>b ) . #REFb)."
    },
    {
        "gold": {
            "text": [
                "We want to investigate the usefulness of stem n- gram features in the mention detection system.",
                "As stated before , the experiments are run in the ACE '04 framework ( #TARGET_REF ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) .",
                "Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.",
                "The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class.",
                "Hence, to build our mention detection system we spent a lot of effort in improving the rst step: detecting the mention boundary and their main type.",
                "In this paper, we report the results in terms of precision, recall, and F-measure3."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We want to investigate the usefulness of stem n- gram features in the mention detection system. As stated before , the experiments are run in the ACE '04 framework ( #TARGET_REF ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) . Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system. The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class. Hence, to build our mention detection system we spent a lot of effort in improving the rst step: detecting the mention boundary and their main type. In this paper, we report the results in terms of precision, recall, and F-measure3.",
        "output": "experiments:We want to investigate the usefulness of stem n- gram features in the mention detection system. As stated before , the experiments are run in the ACE '04 framework ( #TARGET_REF<FUT/> ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) . Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system. The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class. Hence, to build our mention detection system we spent a lot of effort in improving the rst step: detecting the mention boundary and their main type. In this paper, we report the results in terms of precision, recall, and F-measure3."
    },
    {
        "gold": {
            "text": [
                "1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.",
                "This step significantly reduces the computational burden of the algorithm.",
                "It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #REF ; #TARGET_REF ; #REF ) .",
                "To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "method:1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #REF ; #TARGET_REF ; #REF ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.",
        "output": "method:1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #REF ; #TARGET_REF<BACK/> ; #REF ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly."
    },
    {
        "gold": {
            "text": [
                "A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.",
                "This is noticeable for German ( #REF ) and Portuguese ( #TARGET_REF ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der #REF ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .",
                "On the other hand, all three languages behave like high-accuracy languages with respect to attachment score.",
                "A very similar pattern is found for Spanish (Civit Torruella and Martí Antonín, 2002), although this cannot be explained by a high proportion of non-projective structures.",
                "One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German ( #REF ) and Portuguese ( #TARGET_REF ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der #REF ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively . On the other hand, all three languages behave like high-accuracy languages with respect to attachment score. A very similar pattern is found for Spanish (Civit Torruella and Martí Antonín, 2002), although this cannot be explained by a high proportion of non-projective structures. One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features.",
        "output": "experiments:A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German ( #REF ) and Portuguese ( #TARGET_REF<USE/> ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der #REF ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively . On the other hand, all three languages behave like high-accuracy languages with respect to attachment score. A very similar pattern is found for Spanish (Civit Torruella and Martí Antonín, 2002), although this cannot be explained by a high proportion of non-projective structures. One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features."
    },
    {
        "gold": {
            "text": [
                "Our translation model consists of the hidden parameters A+ and A-, and likelihood ratios L(u, v).",
                "The two hidden parameters are the probabilities of the model generating true and false positives in the data.",
                "L(u,v) represents the likelihood that u and v can be mutual translations.",
                "For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #TARGET_REF ) 2 .",
                "When the L(u, v) are re-estimated, the model's hidden parameters come into play."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:Our translation model consists of the hidden parameters A+ and A-, and likelihood ratios L(u, v). The two hidden parameters are the probabilities of the model generating true and false positives in the data. L(u,v) represents the likelihood that u and v can be mutual translations. For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #TARGET_REF ) 2 . When the L(u, v) are re-estimated, the model's hidden parameters come into play.",
        "output": "method:Our translation model consists of the hidden parameters A+ and A-, and likelihood ratios L(u, v). The two hidden parameters are the probabilities of the model generating true and false positives in the data. L(u,v) represents the likelihood that u and v can be mutual translations. For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #TARGET_REF<FUT/> ) 2 . When the L(u, v) are re-estimated, the model's hidden parameters come into play."
    },
    {
        "gold": {
            "text": [
                "The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in #REF, #REF, and #REF.",
                "The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.",
                "Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( #REF ; Azzam , Humphreys , and #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; Mitkov , Belguith , and #REF ) .",
                "Other milestones of recent research include the deployment of probabilistic and machine learning techniques (#REF;#REF;Ge, Hale, and #REF;#REF; the continuing interest in centering, used either in original or in revised form (Abra~os and #REF;#REF;#REF;#REF); and proposals related to the evaluation methodology in anaphora resolution (#REFa(#REFb.",
                "For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in #REF, #REF, and #REF. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( #REF ; Azzam , Humphreys , and #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; Mitkov , Belguith , and #REF ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (#REF;#REF;Ge, Hale, and #REF;#REF; the continuing interest in centering, used either in original or in revised form (Abra~os and #REF;#REF;#REF;#REF); and proposals related to the evaluation methodology in anaphora resolution (#REFa(#REFb. For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).",
        "output": "nan:The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in #REF, #REF, and #REF. The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( #REF ; Azzam , Humphreys , and #REF ; #TARGET_REF<BACK/> ; #REF ; #REF ; #REF ; Mitkov , Belguith , and #REF ) . Other milestones of recent research include the deployment of probabilistic and machine learning techniques (#REF;#REF;Ge, Hale, and #REF;#REF; the continuing interest in centering, used either in original or in revised form (Abra~os and #REF;#REF;#REF;#REF); and proposals related to the evaluation methodology in anaphora resolution (#REFa(#REFb. For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming)."
    },
    {
        "gold": {
            "text": [
                "In this work we use a method for automatically inducing a finite set of features for representing the derivation history.",
                "The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (#REF;#REF).",
                "The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (#REF).",
                "Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #TARGET_REF ) .",
                "The difference from previous approaches is in the nature of the input to the log-linear model.",
                "We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process.",
                "These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.",
                "In neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:In this work we use a method for automatically inducing a finite set of features for representing the derivation history. The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (#REF;#REF). The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (#REF). Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #TARGET_REF ) . The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process. These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities. In neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features.",
        "output": "nan:In this work we use a method for automatically inducing a finite set of features for representing the derivation history. The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (#REF;#REF). The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (#REF). Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #TARGET_REF<USE/> ) . The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process. These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities. In neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features."
    },
    {
        "gold": {
            "text": [
                "The system utilizes several large size biological databases including three NCBI databases ( #REF , #REF , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #TARGET_REF , and",
                "Additionally, several model organism databases or nomenclature databases were used.",
                "Correspondences among records from these databases are identified using the rich cross-reference information provided by the iProClass database of PIR [14].",
                "The following provides a brief description of each of the database."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The system utilizes several large size biological databases including three NCBI databases ( #REF , #REF , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #TARGET_REF , and Additionally, several model organism databases or nomenclature databases were used. Correspondences among records from these databases are identified using the rich cross-reference information provided by the iProClass database of PIR [14]. The following provides a brief description of each of the database.",
        "output": "nan:The system utilizes several large size biological databases including three NCBI databases ( #REF , #REF , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #TARGET_REF<FUT/> , and Additionally, several model organism databases or nomenclature databases were used. Correspondences among records from these databases are identified using the rich cross-reference information provided by the iProClass database of PIR [14]. The following provides a brief description of each of the database."
    },
    {
        "gold": {
            "text": [
                "Most DOP models , such as in #REF , #REF , #TARGET_REF , Sima'an ( 2000 ) and Collins & #REF , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e.",
                "most probable) tree as a candidate for the best tree of a sentence.",
                "We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by \"Likelihood-DOP\" the PCFG-reduction of #REF given in Section 2.2."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Most DOP models , such as in #REF , #REF , #TARGET_REF , Sima'an ( 2000 ) and Collins & #REF , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable) tree as a candidate for the best tree of a sentence. We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by \"Likelihood-DOP\" the PCFG-reduction of #REF given in Section 2.2.",
        "output": "nan:Most DOP models , such as in #REF , #REF , #TARGET_REF<BACK/> , Sima'an ( 2000 ) and Collins & #REF , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable) tree as a candidate for the best tree of a sentence. We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by \"Likelihood-DOP\" the PCFG-reduction of #REF given in Section 2.2."
    },
    {
        "gold": {
            "text": [
                "1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.",
                "This step significantly reduces the computational burden of the algorithm.",
                "It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #TARGET_REF ; #REF ; #REF ) .",
                "To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #TARGET_REF ; #REF ; #REF ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.",
        "output": "method:1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #TARGET_REF<USE/> ; #REF ; #REF ) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly."
    },
    {
        "gold": {
            "text": [
                "In our case, the clustering is performed by the program Snob, which implements mixture model- ing combined with model selection based on the Minimum Message Length (MML) criterion (#REF; #REF).",
                "We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2).",
                "The input to Snob is a set of binary vectors, one vector per response document.",
                "The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency).4",
                "The predictive model is a Decision Graph ( #TARGET_REF ) , which , like Snob , is based on the MML principle .",
                "The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature.",
                "The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct.",
                "This probability is our indicator of whether the Doc-Pred method can address a new request."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:In our case, the clustering is performed by the program Snob, which implements mixture model- ing combined with model selection based on the Minimum Message Length (MML) criterion (#REF; #REF). We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency).4 The predictive model is a Decision Graph ( #TARGET_REF ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct. This probability is our indicator of whether the Doc-Pred method can address a new request.",
        "output": "method:In our case, the clustering is performed by the program Snob, which implements mixture model- ing combined with model selection based on the Minimum Message Length (MML) criterion (#REF; #REF). We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2). The input to Snob is a set of binary vectors, one vector per response document. The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency).4 The predictive model is a Decision Graph ( #TARGET_REF<FUT/> ) , which , like Snob , is based on the MML principle . The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature. The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct. This probability is our indicator of whether the Doc-Pred method can address a new request."
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF 's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) .",
                "They found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents.",
                "While CCM is designed to learn which tag pairs make for likely contexts, without regard for the constituents themselves, our model attempts to learn the relationships between context categories and the types of the constituents, allowing us to take advantage of the natural a priori knowledge about which contexts fit with which constituent labels."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:#TARGET_REF 's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) . They found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents. While CCM is designed to learn which tag pairs make for likely contexts, without regard for the constituents themselves, our model attempts to learn the relationships between context categories and the types of the constituents, allowing us to take advantage of the natural a priori knowledge about which contexts fit with which constituent labels.",
        "output": "related work:#TARGET_REF<BACK/> 's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) . They found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents. While CCM is designed to learn which tag pairs make for likely contexts, without regard for the constituents themselves, our model attempts to learn the relationships between context categories and the types of the constituents, allowing us to take advantage of the natural a priori knowledge about which contexts fit with which constituent labels."
    },
    {
        "gold": {
            "text": [
                "#REF achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.",
                "Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions.",
                "Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3.",
                "#TARGET_REF evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .",
                "However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument�adjunct distinctions posited by their sys- tem.",
                "The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German.",
                "She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (#REF).",
                "We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:#REF achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each. Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions. Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. #TARGET_REF evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument�adjunct distinctions posited by their sys- tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German. She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (#REF). We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.",
        "output": "nan:#REF achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each. Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions. Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. #TARGET_REF<USE/> evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % . However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument�adjunct distinctions posited by their sys- tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German. She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (#REF). We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3."
    },
    {
        "gold": {
            "text": [
                "• A deterministic algorithm for building labeled projective dependency graphs (#REF).",
                "â¢ History-based feature models for predicting the next parser action ( #TARGET_REF ) .",
                "• Support vector machines for mapping histories to parser actions (#REF).",
                "• Graph transformations for recovering nonprojective structures ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:• A deterministic algorithm for building labeled projective dependency graphs (#REF). â¢ History-based feature models for predicting the next parser action ( #TARGET_REF ) . • Support vector machines for mapping histories to parser actions (#REF). • Graph transformations for recovering nonprojective structures .",
        "output": "introduction:• A deterministic algorithm for building labeled projective dependency graphs (#REF). â¢ History-based feature models for predicting the next parser action ( #TARGET_REF<FUT/> ) . • Support vector machines for mapping histories to parser actions (#REF). • Graph transformations for recovering nonprojective structures ."
    },
    {
        "gold": {
            "text": [
                "In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.",
                "Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #TARGET_REF ) .",
                "Figure 1 shows the model.",
                "History snapshots of the timeline may be considered as permanent views, i.e. views with a timestamp.",
                "Consequently, except in the case of sandboxes, every change in the document cannot be erased.",
                "This model will have a strong impact on the role of links and on the underpinning structure of Novelle itself."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself. Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #TARGET_REF ) . Figure 1 shows the model. History snapshots of the timeline may be considered as permanent views, i.e. views with a timestamp. Consequently, except in the case of sandboxes, every change in the document cannot be erased. This model will have a strong impact on the role of links and on the underpinning structure of Novelle itself.",
        "output": "introduction:In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself. Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #TARGET_REF<BACK/> ) . Figure 1 shows the model. History snapshots of the timeline may be considered as permanent views, i.e. views with a timestamp. Consequently, except in the case of sandboxes, every change in the document cannot be erased. This model will have a strong impact on the role of links and on the underpinning structure of Novelle itself."
    },
    {
        "gold": {
            "text": [
                "The description of the EAGLE workbench for linguistic engineering ( #TARGET_REF ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .",
                "This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions.",
                "It is quite similar to our method for capitalized-word disambiguation.",
                "The description of the EAGLE case normalization module provided by Baldwin et al. is, however, very brief and provides no performance evaluation or other details."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The description of the EAGLE workbench for linguistic engineering ( #TARGET_REF ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document . This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions. It is quite similar to our method for capitalized-word disambiguation. The description of the EAGLE case normalization module provided by Baldwin et al. is, however, very brief and provides no performance evaluation or other details.",
        "output": "nan:The description of the EAGLE workbench for linguistic engineering ( #TARGET_REF<USE/> ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document . This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions. It is quite similar to our method for capitalized-word disambiguation. The description of the EAGLE case normalization module provided by Baldwin et al. is, however, very brief and provides no performance evaluation or other details."
    },
    {
        "gold": {
            "text": [
                "First, we compute a simple Bag of Visual Words (BoVW) model for our images using SURF keypoints (#REF).",
                "SURF is a method for selecting points-of-interest within an image.",
                "It is faster and more forgiving than the commonly known SIFT algorithm.",
                "We compute SURF keypoints for every image in our data set using Sim-pleCV 3 and randomly sample 1% of the keypoints.",
                "The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering ( #TARGET_REF ) , and images are then quantized over the 5,000 codewords .",
                "All images for a given word are summed together to provide an average representation for the word.",
                "We refer to this representation as the SURF modality."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:First, we compute a simple Bag of Visual Words (BoVW) model for our images using SURF keypoints (#REF). SURF is a method for selecting points-of-interest within an image. It is faster and more forgiving than the commonly known SIFT algorithm. We compute SURF keypoints for every image in our data set using Sim-pleCV 3 and randomly sample 1% of the keypoints. The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering ( #TARGET_REF ) , and images are then quantized over the 5,000 codewords . All images for a given word are summed together to provide an average representation for the word. We refer to this representation as the SURF modality.",
        "output": "experiments:First, we compute a simple Bag of Visual Words (BoVW) model for our images using SURF keypoints (#REF). SURF is a method for selecting points-of-interest within an image. It is faster and more forgiving than the commonly known SIFT algorithm. We compute SURF keypoints for every image in our data set using Sim-pleCV 3 and randomly sample 1% of the keypoints. The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering ( #TARGET_REF<FUT/> ) , and images are then quantized over the 5,000 codewords . All images for a given word are summed together to provide an average representation for the word. We refer to this representation as the SURF modality."
    },
    {
        "gold": {
            "text": [
                "Generally speaking , we find that the personal public diary metaphor behind blogs ( #TARGET_REF ) may bring to an unsatisfactory representation of the context .",
                "The only way to retrieve information is through a search engine or a calendar, i.e. the date of the 'post' -a lexia in the jargon of bloggers."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Generally speaking , we find that the personal public diary metaphor behind blogs ( #TARGET_REF ) may bring to an unsatisfactory representation of the context . The only way to retrieve information is through a search engine or a calendar, i.e. the date of the 'post' -a lexia in the jargon of bloggers.",
        "output": "related work:Generally speaking , we find that the personal public diary metaphor behind blogs ( #TARGET_REF<BACK/> ) may bring to an unsatisfactory representation of the context . The only way to retrieve information is through a search engine or a calendar, i.e. the date of the 'post' -a lexia in the jargon of bloggers."
    },
    {
        "gold": {
            "text": [
                "â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #TARGET_REF ; #REF ) ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #TARGET_REF ; #REF ) .",
        "output": "method:â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #TARGET_REF<USE/> ; #REF ) ."
    },
    {
        "gold": {
            "text": [
                "This paper presents WIT 1, which is a toolkit IWIT is an acronym of Workable spoken dialogue lnter-for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.",
                "WIT features an incremental understanding method ( #TARGET_REFb ) that makes it possible to build a robust and real-time system .",
                "In addition, WIT compiles domain-dependent system specifications into internal knowledge sources so that building systems is easier.",
                "Although WIT requires more domaindependent specifications than finite-state-modelbased toolkits, WIT-based systems are capable of taking full advantage of language processing technology.",
                "WIT has been implemented and used to build several spoken dialogue systems."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:This paper presents WIT 1, which is a toolkit IWIT is an acronym of Workable spoken dialogue lnter-for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output. WIT features an incremental understanding method ( #TARGET_REFb ) that makes it possible to build a robust and real-time system . In addition, WIT compiles domain-dependent system specifications into internal knowledge sources so that building systems is easier. Although WIT requires more domaindependent specifications than finite-state-modelbased toolkits, WIT-based systems are capable of taking full advantage of language processing technology. WIT has been implemented and used to build several spoken dialogue systems.",
        "output": "introduction:This paper presents WIT 1, which is a toolkit IWIT is an acronym of Workable spoken dialogue lnter-for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output. WIT features an incremental understanding method ( #TARGET_REF<FUT/>b ) that makes it possible to build a robust and real-time system . In addition, WIT compiles domain-dependent system specifications into internal knowledge sources so that building systems is easier. Although WIT requires more domaindependent specifications than finite-state-modelbased toolkits, WIT-based systems are capable of taking full advantage of language processing technology. WIT has been implemented and used to build several spoken dialogue systems."
    },
    {
        "gold": {
            "text": [
                "Some generalizations of our method are fairly straightforward.",
                "For example , consider a relational description ( cfXXX , #TARGET_REF ) involving a gradable adjective , as in the dog in the large shed .",
                "CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm (#REF, Section 8.6.2):",
                "Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size (say, size 5); if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed.",
                "Note that this is felicitous even if the shed is not the largest one in the domain, as is true for d 2 in the following situation (contains-a=b means that a is contained by b): In other words, the dog in the large shed denotes 'the dog such that there is no other shed that is equally large or larger and that contains a dog'.",
                "Note that it would be odd, in the above-sketched situation, to say the dog in the largest shed."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:Some generalizations of our method are fairly straightforward. For example , consider a relational description ( cfXXX , #TARGET_REF ) involving a gradable adjective , as in the dog in the large shed . CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm (#REF, Section 8.6.2): Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size (say, size 5); if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed. Note that this is felicitous even if the shed is not the largest one in the domain, as is true for d 2 in the following situation (contains-a=b means that a is contained by b): In other words, the dog in the large shed denotes 'the dog such that there is no other shed that is equally large or larger and that contains a dog'. Note that it would be odd, in the above-sketched situation, to say the dog in the largest shed.",
        "output": "experiments:Some generalizations of our method are fairly straightforward. For example , consider a relational description ( cfXXX , #TARGET_REF<BACK/> ) involving a gradable adjective , as in the dog in the large shed . CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm (#REF, Section 8.6.2): Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size (say, size 5); if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed. Note that this is felicitous even if the shed is not the largest one in the domain, as is true for d 2 in the following situation (contains-a=b means that a is contained by b): In other words, the dog in the large shed denotes 'the dog such that there is no other shed that is equally large or larger and that contains a dog'. Note that it would be odd, in the above-sketched situation, to say the dog in the largest shed."
    },
    {
        "gold": {
            "text": [
                "For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #TARGET_REF ) .",
                "Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #TARGET_REF ) . Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.",
        "output": "experiments:For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #TARGET_REF<USE/> ) . Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario."
    },
    {
        "gold": {
            "text": [
                "In order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non- textual modalities.",
                "We use the same method as #TARGET_REF for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair .",
                "Words without grounded features are all given the same placeholder feature, also resulting in a word- feature pair.5",
                "That is, for the feature norm modal- ity, we generate (word, feature norm) pairs; for the SURF modality, we generate (word, codeword) pairs, etc.",
                "The resulting stochastically generated corpus is used in its corresponding experiments."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:In order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non- textual modalities. We use the same method as #TARGET_REF for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair . Words without grounded features are all given the same placeholder feature, also resulting in a word- feature pair.5 That is, for the feature norm modal- ity, we generate (word, feature norm) pairs; for the SURF modality, we generate (word, codeword) pairs, etc. The resulting stochastically generated corpus is used in its corresponding experiments.",
        "output": "experiments:In order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non- textual modalities. We use the same method as #TARGET_REF<FUT/> for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair . Words without grounded features are all given the same placeholder feature, also resulting in a word- feature pair.5 That is, for the feature norm modal- ity, we generate (word, feature norm) pairs; for the SURF modality, we generate (word, codeword) pairs, etc. The resulting stochastically generated corpus is used in its corresponding experiments."
    },
    {
        "gold": {
            "text": [
                "aSee (#REF) for a discussion of the appropriateness of T~-£: for HPSG and a comparison with other feature logic approaches designed for HPSG.",
                "append ([~,[~,[~).",
                "#REFb) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.",
                "4 Because of space limitations we have to refrain from an example.",
                "The ConTroll grammar development system as described in ( #TARGET_REFb ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:aSee (#REF) for a discussion of the appropriateness of T~-£: for HPSG and a comparison with other feature logic approaches designed for HPSG. append ([~,[~,[~). #REFb) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints. 4 Because of space limitations we have to refrain from an example. The ConTroll grammar development system as described in ( #TARGET_REFb ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .",
        "output": "nan:aSee (#REF) for a discussion of the appropriateness of T~-£: for HPSG and a comparison with other feature logic approaches designed for HPSG. append ([~,[~,[~). #REFb) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints. 4 Because of space limitations we have to refrain from an example. The ConTroll grammar development system as described in ( #TARGET_REF<BACK/>b ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars ."
    },
    {
        "gold": {
            "text": [
                "Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.",
                "So we will use a very simple formalism, like the one above, resembling the standard first order language.",
                "But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of #REF, which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora.",
                "The logical notation of #TARGET_REF is more sophisticated , and may be considered another possibility .",
                "Jackendoff's (1983) formalism is richer and resembles more closely an English grammar.",
                "Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.\"",
                "Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.",
                "It will also be a model for our simplified logical notation (cf.",
                "Section 5).",
                "We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of #REF, which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of #TARGET_REF is more sophisticated , and may be considered another possibility . Jackendoff's (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.\" Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics. It will also be a model for our simplified logical notation (cf. Section 5). We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.",
        "output": "introduction:Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of #REF, which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of #TARGET_REF<USE/> is more sophisticated , and may be considered another possibility . Jackendoff's (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.\" Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics. It will also be a model for our simplified logical notation (cf. Section 5). We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences."
    },
    {
        "gold": {
            "text": [
                "Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.",
                "We optimize the dual objective using the gradient based methods shown in Algorithm 1.",
                "Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( #TARGET_REF ) .",
                "Here, β∇(λ) represents an ascent direction chosen as follows: For inequality constraints, it is the projected gradient (#REF); for equality constraints with slack, we use conjugate gradient (#REF), noting that when λ = 0, the objective is not differentiable.",
                "In practice this only happens at the start of optimization and we use a sub-gradient for the first direction."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ. We optimize the dual objective using the gradient based methods shown in Algorithm 1. Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( #TARGET_REF ) . Here, β∇(λ) represents an ascent direction chosen as follows: For inequality constraints, it is the projected gradient (#REF); for equality constraints with slack, we use conjugate gradient (#REF), noting that when λ = 0, the objective is not differentiable. In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.",
        "output": "nan:Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ. We optimize the dual objective using the gradient based methods shown in Algorithm 1. Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( #TARGET_REF<FUT/> ) . Here, β∇(λ) represents an ascent direction chosen as follows: For inequality constraints, it is the projected gradient (#REF); for equality constraints with slack, we use conjugate gradient (#REF), noting that when λ = 0, the objective is not differentiable. In practice this only happens at the start of optimization and we use a sub-gradient for the first direction."
    },
    {
        "gold": {
            "text": [
                "The main benefits of this acquisition technique lie in the inferred patterns.",
                "Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #TARGET_REF ) for a review ) , these patterns allow :"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "method:The main benefits of this acquisition technique lie in the inferred patterns. Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #TARGET_REF ) for a review ) , these patterns allow :",
        "output": "method:The main benefits of this acquisition technique lie in the inferred patterns. Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #TARGET_REF<BACK/> ) for a review ) , these patterns allow :"
    },
    {
        "gold": {
            "text": [
                "Several works have proposed discriminative techniques to train log-linear model for SMT.",
                "(#REF;#REF) used maximum likelihood estimation to learn weights for MT.",
                "( #TARGET_REF ; #REF ; #REF ; #REF ) employed an evaluation metric as a loss function and directly optimized it .",
                "(#REF;#REF;#REF) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Several works have proposed discriminative techniques to train log-linear model for SMT. (#REF;#REF) used maximum likelihood estimation to learn weights for MT. ( #TARGET_REF ; #REF ; #REF ; #REF ) employed an evaluation metric as a loss function and directly optimized it . (#REF;#REF;#REF) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.",
        "output": "related work:Several works have proposed discriminative techniques to train log-linear model for SMT. (#REF;#REF) used maximum likelihood estimation to learn weights for MT. ( #TARGET_REF<USE/> ; #REF ; #REF ; #REF ) employed an evaluation metric as a loss function and directly optimized it . (#REF;#REF;#REF) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions."
    },
    {
        "gold": {
            "text": [
                "For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group ( #TARGET_REF ) containing approximately 1.7 B word tokens .",
                "We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length shorter than 100.",
                "The resulting corpus has 1,038,883 documents consisting of 75,678 word types and 466M word tokens."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group ( #TARGET_REF ) containing approximately 1.7 B word tokens . We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length shorter than 100. The resulting corpus has 1,038,883 documents consisting of 75,678 word types and 466M word tokens.",
        "output": "experiments:For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group ( #TARGET_REF<FUT/> ) containing approximately 1.7 B word tokens . We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length shorter than 100. The resulting corpus has 1,038,883 documents consisting of 75,678 word types and 466M word tokens."
    },
    {
        "gold": {
            "text": [
                "Minimality.",
                "Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form.",
                "In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Minimality. Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form. In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( #TARGET_REF ) .",
        "output": "nan:Minimality. Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form. In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "#REF employed a Bayesian method to learn discontinuous SCFG rules.",
                "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.",
                "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.",
                "#REF and #REF focused on joint parsing and alignment.",
                "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.",
                "#TARGET_REF adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .",
                "#REF re-trained the linguistic parsers bilingually based on word alignment.",
                "#REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.",
                "Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.",
                "#REF substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.",
                "#REF further labeled the SCFG rules with POS tags and unsupervised word classes.",
                "Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:#REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #REF focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. #TARGET_REF adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . #REF re-trained the linguistic parsers bilingually based on word alignment. #REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. #REF substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. #REF further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.",
        "output": "related work:#REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #REF focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. #TARGET_REF<USE/> adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees . #REF re-trained the linguistic parsers bilingually based on word alignment. #REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. #REF substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. #REF further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules."
    },
    {
        "gold": {
            "text": [
                "Maximum-posterior estimation tries to maximize P (θ) • i f θ (x i , y i ) where P (θ) is a prior probability.",
                "In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Maximum-posterior estimation tries to maximize P (θ) • i f θ (x i , y i ) where P (θ) is a prior probability. In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #TARGET_REF ) .",
        "output": "nan:Maximum-posterior estimation tries to maximize P (θ) • i f θ (x i , y i ) where P (θ) is a prior probability. In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.",
                "#TARGET_REF explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .",
                "The extraction procedure utilizes a head percolation table as introduced by #REF in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct.",
                "This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.",
                "The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.",
                "#REF also presents a similar method for the extraction of a TAG from the Penn Treebank.",
                "The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #REF and #REF."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar. #TARGET_REF explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing . The extraction procedure utilizes a head percolation table as introduced by #REF in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct. This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. #REF also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #REF and #REF.",
        "output": "related work:As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar. #TARGET_REF<BACK/> explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing . The extraction procedure utilizes a head percolation table as introduced by #REF in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct. This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. #REF also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #REF and #REF."
    },
    {
        "gold": {
            "text": [
                "â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and #REF ; Jijkoun and de #REF ) , or the corpus itself was significantly smaller than ours ( #TARGET_REF ; #REF ) .",
                "The representativeness of the sample size was not discussed in any of these studies."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and #REF ; Jijkoun and de #REF ) , or the corpus itself was significantly smaller than ours ( #TARGET_REF ; #REF ) . The representativeness of the sample size was not discussed in any of these studies.",
        "output": "method:â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and #REF ; Jijkoun and de #REF ) , or the corpus itself was significantly smaller than ours ( #TARGET_REF<USE/> ; #REF ) . The representativeness of the sample size was not discussed in any of these studies."
    },
    {
        "gold": {
            "text": [
                "As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.",
                "We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonly-used MUC scorer (#REF), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.",
                "Following #TARGET_REF , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .",
                "In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts. We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonly-used MUC scorer (#REF), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved. Following #TARGET_REF , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition . In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder.",
        "output": "nan:As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts. We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonly-used MUC scorer (#REF), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved. Following #TARGET_REF<FUT/> , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition . In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder."
    },
    {
        "gold": {
            "text": [
                "There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( #REF ; #REF ; #TARGET_REF ; #REF ) .",
                "However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.",
                "Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.",
                "On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (#REF).",
                "Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( #REF ; #REF ; #TARGET_REF ; #REF ) . However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins. Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet. On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (#REF). Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.",
        "output": "introduction:There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( #REF ; #REF ; #TARGET_REF<BACK/> ; #REF ) . However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins. Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet. On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (#REF). Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages."
    },
    {
        "gold": {
            "text": [
                "All logical notions that we are going to consider, such as theory or model, will be finitary.",
                "For example, a model would typically contain fewer than a hundred elements of different logical sorts.",
                "Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.",
                "The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.",
                "This Principle of Finitism is also assumed by #REF , #REF , #TARGET_REF , and implicitly or explicitly by almost all researchers in computational linguistics .",
                "As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.",
                "#REF)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred elements of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by #REF , #REF , #TARGET_REF , and implicitly or explicitly by almost all researchers in computational linguistics . As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. #REF).",
        "output": "introduction:All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred elements of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by #REF , #REF , #TARGET_REF<USE/> , and implicitly or explicitly by almost all researchers in computational linguistics . As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. #REF)."
    },
    {
        "gold": {
            "text": [
                "• Transition-based: An implementation of the transition-based dependency parsing framework ( #REF ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #TARGET_REF with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.",
                "The features used by all models are: the part-ofspeech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available); dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available).",
                "All feature conjunctions are included."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:• Transition-based: An implementation of the transition-based dependency parsing framework ( #REF ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #TARGET_REF with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists. The features used by all models are: the part-ofspeech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available); dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available). All feature conjunctions are included.",
        "output": "experiments:• Transition-based: An implementation of the transition-based dependency parsing framework ( #REF ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #TARGET_REF<FUT/> with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists. The features used by all models are: the part-ofspeech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available); dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available). All feature conjunctions are included."
    },
    {
        "gold": {
            "text": [
                "Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #REF ; #REF ; #TARGET_REFa ) .",
                "However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.",
                "Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-#REF), certain machine-assisted translation tools (e.g.",
                "(#REF;#REFb)), concordancing for bilingual lexicography (#REF;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &#REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #REF ; #REF ; #TARGET_REFa ) . However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-#REF), certain machine-assisted translation tools (e.g. (#REF;#REFb)), concordancing for bilingual lexicography (#REF;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &#REF).",
        "output": "introduction:Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #REF ; #REF ; #TARGET_REF<BACK/>a ) . However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including \"crummy\" MT on the World Wide Web (Church & I-#REF), certain machine-assisted translation tools (e.g. (#REF;#REFb)), concordancing for bilingual lexicography (#REF;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &#REF)."
    },
    {
        "gold": {
            "text": [
                "Tateisi et al. also translated LTAG into HPSG ( #TARGET_REF ) .",
                "However, their method depended on translator�s intuitive analy- sis of the original grammar.",
                "Thus the transla- tion was manual and grammar dependent.",
                "The manual translation demanded considerable efforts from the translator, and obscures the equiva- lence between the original and obtained gram- mars.",
                "Other works (#REF; #REF) convert HPSG grammars into LTAG grammars.",
                "However, given the greater ex- pressive power of HPSG, it is impossible to con- vert an arbitrary HPSG grammar into an LTAG grammar.",
                "Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capac- ity.",
                "Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above ad- vantages."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Tateisi et al. also translated LTAG into HPSG ( #TARGET_REF ) . However, their method depended on translator�s intuitive analy- sis of the original grammar. Thus the transla- tion was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equiva- lence between the original and obtained gram- mars. Other works (#REF; #REF) convert HPSG grammars into LTAG grammars. However, given the greater ex- pressive power of HPSG, it is impossible to con- vert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capac- ity. Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above ad- vantages.",
        "output": "introduction:Tateisi et al. also translated LTAG into HPSG ( #TARGET_REF<USE/> ) . However, their method depended on translator�s intuitive analy- sis of the original grammar. Thus the transla- tion was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equiva- lence between the original and obtained gram- mars. Other works (#REF; #REF) convert HPSG grammars into LTAG grammars. However, given the greater ex- pressive power of HPSG, it is impossible to con- vert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capac- ity. Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above ad- vantages."
    },
    {
        "gold": {
            "text": [
                "We start by reporting the results in which we compare the full parser and the shallow parser on the \"clean\" WSJ data.",
                "Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #TARGET_REF ) terminology .",
                "The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.",
                "Next, we compared the performance of the parsers on the task of identifying atomic phrases 2 .",
                "Here, again, the shallow parser exhibits significantly better performance.",
                "Table 3 shows the results of extracting atomic phrases."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We start by reporting the results in which we compare the full parser and the shallow parser on the \"clean\" WSJ data. Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #TARGET_REF ) terminology . The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser. Next, we compared the performance of the parsers on the task of identifying atomic phrases 2 . Here, again, the shallow parser exhibits significantly better performance. Table 3 shows the results of extracting atomic phrases.",
        "output": "experiments:We start by reporting the results in which we compare the full parser and the shallow parser on the \"clean\" WSJ data. Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #TARGET_REF<FUT/> ) terminology . The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser. Next, we compared the performance of the parsers on the task of identifying atomic phrases 2 . Here, again, the shallow parser exhibits significantly better performance. Table 3 shows the results of extracting atomic phrases."
    },
    {
        "gold": {
            "text": [
                "Furthermore, let R ≥ ||Φ(y) − Φ(y )||, for all y, y .",
                "Assumption 1. Assume training set D is lossseparable with margin γ.",
                "Theorem 1.",
                "Given Assumption 1.",
                "Let m be the number of mistakes made when training the perceptron (Algorithm 2) with inline ranker loss (Algorithm 3) on D, where a mistake occurs for (x, y) ∈ D with parameter vector θ when ∃ŷ j ∈ F k-best θ (x) wherê y j =ŷ 1 and L(ŷ j , y) < L(ŷ 1 , y).",
                "If training is run indefinitely, then m ≤ R 2 γ 2 .",
                "Proof.",
                "Identical to the standard perceptron proof , e.g. , #TARGET_REF , by inserting in loss-separability for normal separability ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Furthermore, let R ≥ ||Φ(y) − Φ(y )||, for all y, y . Assumption 1. Assume training set D is lossseparable with margin γ. Theorem 1. Given Assumption 1. Let m be the number of mistakes made when training the perceptron (Algorithm 2) with inline ranker loss (Algorithm 3) on D, where a mistake occurs for (x, y) ∈ D with parameter vector θ when ∃ŷ j ∈ F k-best θ (x) wherê y j =ŷ 1 and L(ŷ j , y) < L(ŷ 1 , y). If training is run indefinitely, then m ≤ R 2 γ 2 . Proof. Identical to the standard perceptron proof , e.g. , #TARGET_REF , by inserting in loss-separability for normal separability .",
        "output": "introduction:Furthermore, let R ≥ ||Φ(y) − Φ(y )||, for all y, y . Assumption 1. Assume training set D is lossseparable with margin γ. Theorem 1. Given Assumption 1. Let m be the number of mistakes made when training the perceptron (Algorithm 2) with inline ranker loss (Algorithm 3) on D, where a mistake occurs for (x, y) ∈ D with parameter vector θ when ∃ŷ j ∈ F k-best θ (x) wherê y j =ŷ 1 and L(ŷ j , y) < L(ŷ 1 , y). If training is run indefinitely, then m ≤ R 2 γ 2 . Proof. Identical to the standard perceptron proof , e.g. , #TARGET_REF<BACK/> , by inserting in loss-separability for normal separability ."
    },
    {
        "gold": {
            "text": [
                "The example used to illustrate the power of ATNs ( #TARGET_REF ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator .",
                "Cases of crossed traces are automatically blocked because the second CURRENT-FOCUS gets moved into the FLOAT-OBJECT position at the time of the second activator, overriding the preexisting FLOAT-OBJECT set up by the earlier activator.",
                "The wrong FLOAT-OBJECT is available at the position of the first trace, and the parse dies: *(Which books)/did you ask John (where)j Bill bought (ti) (tj)?",
                "The CURRENT-FOCUS slot is not restricted to nodes that represent nouns.",
                "Some of the generators are adverbial or adjectival parts of speech (pos).",
                "An absorber checks for agreement in POS before it can accept the FLOAT-OBJECT as its subparse.",
                "As an example, the question, \"(How oily)/do you like your salad dressing (ti)?\" contains a [q-subject] \"how oily\" that is an adjective."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The example used to illustrate the power of ATNs ( #TARGET_REF ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator . Cases of crossed traces are automatically blocked because the second CURRENT-FOCUS gets moved into the FLOAT-OBJECT position at the time of the second activator, overriding the preexisting FLOAT-OBJECT set up by the earlier activator. The wrong FLOAT-OBJECT is available at the position of the first trace, and the parse dies: *(Which books)/did you ask John (where)j Bill bought (ti) (tj)? The CURRENT-FOCUS slot is not restricted to nodes that represent nouns. Some of the generators are adverbial or adjectival parts of speech (pos). An absorber checks for agreement in POS before it can accept the FLOAT-OBJECT as its subparse. As an example, the question, \"(How oily)/do you like your salad dressing (ti)?\" contains a [q-subject] \"how oily\" that is an adjective.",
        "output": "nan:The example used to illustrate the power of ATNs ( #TARGET_REF<USE/> ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator . Cases of crossed traces are automatically blocked because the second CURRENT-FOCUS gets moved into the FLOAT-OBJECT position at the time of the second activator, overriding the preexisting FLOAT-OBJECT set up by the earlier activator. The wrong FLOAT-OBJECT is available at the position of the first trace, and the parse dies: *(Which books)/did you ask John (where)j Bill bought (ti) (tj)? The CURRENT-FOCUS slot is not restricted to nodes that represent nouns. Some of the generators are adverbial or adjectival parts of speech (pos). An absorber checks for agreement in POS before it can accept the FLOAT-OBJECT as its subparse. As an example, the question, \"(How oily)/do you like your salad dressing (ti)?\" contains a [q-subject] \"how oily\" that is an adjective."
    },
    {
        "gold": {
            "text": [
                "In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #TARGET_REFa ) .",
                "Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator.",
                "Our user study assessed the response-generation methods from the following perspectives, which yield information that is beyond the F-score and precision measures obtained in the automatic evaluation."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #TARGET_REFa ) . Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator. Our user study assessed the response-generation methods from the following perspectives, which yield information that is beyond the F-score and precision measures obtained in the automatic evaluation.",
        "output": "method:In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #TARGET_REF<FUT/>a ) . Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator. Our user study assessed the response-generation methods from the following perspectives, which yield information that is beyond the F-score and precision measures obtained in the automatic evaluation."
    },
    {
        "gold": {
            "text": [
                "Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.",
                "#REF run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.",
                "The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.",
                "#REF employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames.",
                "#TARGET_REF attempts to improve on the approach of #REF by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .",
                "He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.",
                "The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following #REF.",
                "Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames. #REF run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes. The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. #REF employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #TARGET_REF attempts to improve on the approach of #REF by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur . He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following #REF. Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.",
        "output": "related work:Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames. #REF run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes. The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. #REF employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #TARGET_REF<BACK/> attempts to improve on the approach of #REF by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur . He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following #REF. Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb."
    },
    {
        "gold": {
            "text": [
                "Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #TARGET_REF ; #REF ) .",
                "We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.",
                "Consider, for instance, the list of properties L = size > 3 cm, size < 9 cm .",
                "If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm).",
                "Clearly, sophisticated use of gradable adjectives requires a separation between CD and linguistic realization, unless one is willing to complicate linguistic realization considerably."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #TARGET_REF ; #REF ) . We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult. Consider, for instance, the list of properties L = size > 3 cm, size < 9 cm . If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm). Clearly, sophisticated use of gradable adjectives requires a separation between CD and linguistic realization, unless one is willing to complicate linguistic realization considerably.",
        "output": "nan:Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #TARGET_REF<USE/> ; #REF ) . We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult. Consider, for instance, the list of properties L = size > 3 cm, size < 9 cm . If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm). Clearly, sophisticated use of gradable adjectives requires a separation between CD and linguistic realization, unless one is willing to complicate linguistic realization considerably."
    },
    {
        "gold": {
            "text": [
                "The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by #TARGET_REF ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by #TARGET_REF .",
        "output": "experiments:The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by #TARGET_REF<FUT/> ."
    },
    {
        "gold": {
            "text": [
                "Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #TARGET_REF )",
                "3 (#REF) propose a compilation of lexical rules into T~r/: definite clauses 2This view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in (#REF).",
                "Typed feature structures as normal form ir~'~E terms are merely syntactic objects."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #TARGET_REF ) 3 (#REF) propose a compilation of lexical rules into T~r/: definite clauses 2This view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in (#REF). Typed feature structures as normal form ir~'~E terms are merely syntactic objects.",
        "output": "introduction:Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #TARGET_REF<BACK/> ) 3 (#REF) propose a compilation of lexical rules into T~r/: definite clauses 2This view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in (#REF). Typed feature structures as normal form ir~'~E terms are merely syntactic objects."
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .",
                "However, in our situation, there is not always one single winner (two methods can perform similarly well for a given request), and there are different ways to pick winners (for example, based on F-score or precision).",
                "Therefore, such an approach would require the utilization of subjective heuristics for creating labels, which would significantly influence what is being learned.",
                "Instead, we adopt an unsupervised approach that finds patterns in the data-confidence values coupled with performance scores (Section 6.1)-and then attempts to fit unseen data to these patterns (Section 6.2).",
                "Heuristics are still needed in order to decide which response-generation method to apply to an unseen case, but they are applied only after the learning is complete (Section 6.3).",
                "In other words, the subjective process of setting performance criteria (which should be conducted by the organization running the helpdesk) does not influence the machine learning process."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Following #TARGET_REF , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases . However, in our situation, there is not always one single winner (two methods can perform similarly well for a given request), and there are different ways to pick winners (for example, based on F-score or precision). Therefore, such an approach would require the utilization of subjective heuristics for creating labels, which would significantly influence what is being learned. Instead, we adopt an unsupervised approach that finds patterns in the data-confidence values coupled with performance scores (Section 6.1)-and then attempts to fit unseen data to these patterns (Section 6.2). Heuristics are still needed in order to decide which response-generation method to apply to an unseen case, but they are applied only after the learning is complete (Section 6.3). In other words, the subjective process of setting performance criteria (which should be conducted by the organization running the helpdesk) does not influence the machine learning process.",
        "output": "nan:Following #TARGET_REF<USE/> , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases . However, in our situation, there is not always one single winner (two methods can perform similarly well for a given request), and there are different ways to pick winners (for example, based on F-score or precision). Therefore, such an approach would require the utilization of subjective heuristics for creating labels, which would significantly influence what is being learned. Instead, we adopt an unsupervised approach that finds patterns in the data-confidence values coupled with performance scores (Section 6.1)-and then attempts to fit unseen data to these patterns (Section 6.2). Heuristics are still needed in order to decide which response-generation method to apply to an unseen case, but they are applied only after the learning is complete (Section 6.3). In other words, the subjective process of setting performance criteria (which should be conducted by the organization running the helpdesk) does not influence the machine learning process."
    },
    {
        "gold": {
            "text": [
                "The infrastructure will be implemented in C/C++.",
                "Templates will be used heavily to provide generality without significantly impacting on efficiency.",
                "However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces.",
                "To provide the required configurability in the static version of the code we will use policy templates ( #TARGET_REF ) , and for the dynamic version we will use configuration classes ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The infrastructure will be implemented in C/C++. Templates will be used heavily to provide generality without significantly impacting on efficiency. However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces. To provide the required configurability in the static version of the code we will use policy templates ( #TARGET_REF ) , and for the dynamic version we will use configuration classes .",
        "output": "experiments:The infrastructure will be implemented in C/C++. Templates will be used heavily to provide generality without significantly impacting on efficiency. However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces. To provide the required configurability in the static version of the code we will use policy templates ( #TARGET_REF<FUT/> ) , and for the dynamic version we will use configuration classes ."
    },
    {
        "gold": {
            "text": [
                "Combining control strategies depends on a way to differentiate between types of constraints.",
                "Proceedings of EACL '99 example , the ALE parser ( #TARGET_REF ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown .",
                "In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted.",
                "A literal (goal) is considered a parse lype literal (goal) if it has as its single argument a typed feature structure of a type specified as a parse type.",
                "1° All types in the type hierarchy can be used as parse types.",
                "This way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering.",
                "However, in the remainder we will concentrate on a specific class of parse types: We assume the specification of type sign and its subtypes as parse types.",
                "11 This choice is based on the observation that the constraints on type sign and its sub-types play an important guiding role in the parsing process and are best interpreted bottom-up given the lexical orientation of I-IPSG."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Combining control strategies depends on a way to differentiate between types of constraints. Proceedings of EACL '99 example , the ALE parser ( #TARGET_REF ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown . In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted. A literal (goal) is considered a parse lype literal (goal) if it has as its single argument a typed feature structure of a type specified as a parse type. 1° All types in the type hierarchy can be used as parse types. This way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering. However, in the remainder we will concentrate on a specific class of parse types: We assume the specification of type sign and its subtypes as parse types. 11 This choice is based on the observation that the constraints on type sign and its sub-types play an important guiding role in the parsing process and are best interpreted bottom-up given the lexical orientation of I-IPSG.",
        "output": "nan:Combining control strategies depends on a way to differentiate between types of constraints. Proceedings of EACL '99 example , the ALE parser ( #TARGET_REF<BACK/> ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown . In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted. A literal (goal) is considered a parse lype literal (goal) if it has as its single argument a typed feature structure of a type specified as a parse type. 1° All types in the type hierarchy can be used as parse types. This way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering. However, in the remainder we will concentrate on a specific class of parse types: We assume the specification of type sign and its subtypes as parse types. 11 This choice is based on the observation that the constraints on type sign and its sub-types play an important guiding role in the parsing process and are best interpreted bottom-up given the lexical orientation of I-IPSG."
    },
    {
        "gold": {
            "text": [
                "The direct prediction of surface forms is limited to those forms observed in the training data, which is a significant limitation.",
                "However, it is reasonable to expect that the use of features (and morphological generation) could also be problematic as this requires the use of morphologically-aware syntactic parsers to annotate the training data with such features, and additionally depends on the coverage of morphological analysis and generation.",
                "Despite this, our research clearly shows that the feature-based approach is superior for English-to-German SMT.",
                "This is a striking result considering state-of-theart performance of German parsing is poor compared with the best performance on English parsing.",
                "As parsing performance improves, the performance of linguistic-feature-based approaches will increase.",
                "#REF , #REF , #REF , #TARGET_REF , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .",
                "However, this does not deal directly with linguistic features marked by inflection.",
                "In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features.",
                "So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:The direct prediction of surface forms is limited to those forms observed in the training data, which is a significant limitation. However, it is reasonable to expect that the use of features (and morphological generation) could also be problematic as this requires the use of morphologically-aware syntactic parsers to annotate the training data with such features, and additionally depends on the coverage of morphological analysis and generation. Despite this, our research clearly shows that the feature-based approach is superior for English-to-German SMT. This is a striking result considering state-of-theart performance of German parsing is poor compared with the best performance on English parsing. As parsing performance improves, the performance of linguistic-feature-based approaches will increase. #REF , #REF , #REF , #TARGET_REF , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation . However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features. So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing.",
        "output": "related work:The direct prediction of surface forms is limited to those forms observed in the training data, which is a significant limitation. However, it is reasonable to expect that the use of features (and morphological generation) could also be problematic as this requires the use of morphologically-aware syntactic parsers to annotate the training data with such features, and additionally depends on the coverage of morphological analysis and generation. Despite this, our research clearly shows that the feature-based approach is superior for English-to-German SMT. This is a striking result considering state-of-theart performance of German parsing is poor compared with the best performance on English parsing. As parsing performance improves, the performance of linguistic-feature-based approaches will increase. #REF , #REF , #REF , #TARGET_REF<USE/> , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation . However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features. So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing."
    },
    {
        "gold": {
            "text": [
                "All experiments have been performed using MaltParser ( #TARGET_REF ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .1"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:All experiments have been performed using MaltParser ( #TARGET_REF ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .1",
        "output": "introduction:All experiments have been performed using MaltParser ( #TARGET_REF<FUT/> ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .1"
    },
    {
        "gold": {
            "text": [
                "A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language (#REFb).",
                "There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one language but not the other (e.g., English He walked and French Il est allé), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly.",
                "Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #TARGET_REF ) .",
                "The top row of Figure 1 shows two word alignments between an English-French sentence pair.",
                "We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language (#REFb). There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one language but not the other (e.g., English He walked and French Il est allé), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly. Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #TARGET_REF ) . The top row of Figure 1 shows two word alignments between an English-French sentence pair. We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns.",
        "output": "introduction:A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language (#REFb). There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one language but not the other (e.g., English He walked and French Il est allé), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly. Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #TARGET_REF<BACK/> ) . The top row of Figure 1 shows two word alignments between an English-French sentence pair. We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns."
    },
    {
        "gold": {
            "text": [
                "#REF employed a Bayesian method to learn discontinuous SCFG rules.",
                "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.",
                "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.",
                "#REF and #REF focused on joint parsing and alignment.",
                "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.",
                "adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.",
                "#REF re-trained the linguistic parsers bilingually based on word alignment.",
                "#REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.",
                "Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.",
                "#REF substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.",
                "#TARGET_REF further labeled the SCFG rules with POS tags and unsupervised word classes .",
                "Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:#REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #REF focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #REF re-trained the linguistic parsers bilingually based on word alignment. #REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. #REF substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. #TARGET_REF further labeled the SCFG rules with POS tags and unsupervised word classes . Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.",
        "output": "related work:#REF employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. #REF and #REF focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. #REF re-trained the linguistic parsers bilingually based on word alignment. #REF utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. #REF substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. #TARGET_REF<USE/> further labeled the SCFG rules with POS tags and unsupervised word classes . Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules."
    },
    {
        "gold": {
            "text": [
                "The experiments are conducted on Chinese-to-English translation.",
                "The training data are the FBIS corpus with approximately 7.1 million Chinese words and 9.2 million English words.",
                "We obtain the bidirectional word alignment with GIZA++, and then adopt the grow-diag-final-and strategy to obtain the final symmetric alignment.",
                "We train a 5gram language model on the Xinhua portion of the English Gigaword corpus and the English part of the training data.",
                "For tuning and testing, we use the NIST MT 2003 evaluation data as the development set, and use the NIST MT04 and MT05 data as the test set.",
                "We use MERT (#REF) to tune parameters.",
                "Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set.",
                "The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty.",
                "The statistical significance test is performed by the re-sampling approach ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The experiments are conducted on Chinese-to-English translation. The training data are the FBIS corpus with approximately 7.1 million Chinese words and 9.2 million English words. We obtain the bidirectional word alignment with GIZA++, and then adopt the grow-diag-final-and strategy to obtain the final symmetric alignment. We train a 5gram language model on the Xinhua portion of the English Gigaword corpus and the English part of the training data. For tuning and testing, we use the NIST MT 2003 evaluation data as the development set, and use the NIST MT04 and MT05 data as the test set. We use MERT (#REF) to tune parameters. Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set. The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach ( #TARGET_REF ) .",
        "output": "experiments:The experiments are conducted on Chinese-to-English translation. The training data are the FBIS corpus with approximately 7.1 million Chinese words and 9.2 million English words. We obtain the bidirectional word alignment with GIZA++, and then adopt the grow-diag-final-and strategy to obtain the final symmetric alignment. We train a 5gram language model on the Xinhua portion of the English Gigaword corpus and the English part of the training data. For tuning and testing, we use the NIST MT 2003 evaluation data as the development set, and use the NIST MT04 and MT05 data as the test set. We use MERT (#REF) to tune parameters. Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set. The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "The need for information systems to support physicians at the point of care has been well studied (#REF;#REF;#REF).",
                "Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.",
                "Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments.",
                "Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( #REF ) , and the availability of software that leverages this knowledge -- MetaMap ( #TARGET_REF ) for concept identification and SemRep ( #REF ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .",
                "#REF have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.",
                "Building on the work of #REF in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.",
                "(#REF).",
                "Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The need for information systems to support physicians at the point of care has been well studied (#REF;#REF;#REF). Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( #REF ) , and the availability of software that leverages this knowledge -- MetaMap ( #TARGET_REF ) for concept identification and SemRep ( #REF ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . #REF have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of #REF in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (#REF). Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.",
        "output": "introduction:The need for information systems to support physicians at the point of care has been well studied (#REF;#REF;#REF). Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence. Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments. Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( #REF ) , and the availability of software that leverages this knowledge -- MetaMap ( #TARGET_REF<BACK/> ) for concept identification and SemRep ( #REF ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks . #REF have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of #REF in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (#REF). Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well."
    },
    {
        "gold": {
            "text": [
                "A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.",
                "The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.",
                "Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.",
                "For instance , #TARGET_REF report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .",
                "This is a relatively small training set that can be manually marked in a few hours' time.",
                "But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention. The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation. Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance. For instance , #TARGET_REF report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words . This is a relatively small training set that can be manually marked in a few hours' time. But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%).",
        "output": "conclusion:A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention. The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation. Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance. For instance , #TARGET_REF<USE/> report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words . This is a relatively small training set that can be manually marked in a few hours' time. But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%)."
    },
    {
        "gold": {
            "text": [
                "Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities ( #TARGET_REF ) , we can efficiently compute the probability of the sentence , P ( w | G ) .",
                "Similarly, the algorithm can be modified to compute the quantity"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities ( #TARGET_REF ) , we can efficiently compute the probability of the sentence , P ( w | G ) . Similarly, the algorithm can be modified to compute the quantity",
        "output": "nan:Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities ( #TARGET_REF<FUT/> ) , we can efficiently compute the probability of the sentence , P ( w | G ) . Similarly, the algorithm can be modified to compute the quantity"
    },
    {
        "gold": {
            "text": [
                "An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries.",
                "This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary.",
                "Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and #REF ; #TARGET_REF ; #REF ; Malik , Subramaniam , and #REF ) .",
                "A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues.",
                "Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.",
                "Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and #REF ; #TARGET_REF ; #REF ; Malik , Subramaniam , and #REF ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses. Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard.",
        "output": "introduction:An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries. This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary. Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and #REF ; #TARGET_REF<BACK/> ; #REF ; Malik , Subramaniam , and #REF ) . A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues. Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses. Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard."
    },
    {
        "gold": {
            "text": [
                "Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (#REF), the Buckwalter morphological analyzer ( #TARGET_REF ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (#REF; Habash, Rambow, and #REF).",
                "The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.",
                "In this article, we use an in-house system which provides functional gender, number, and rationality features (#REF).",
                "See Section 5.2 for more details."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (#REF), the Buckwalter morphological analyzer ( #TARGET_REF ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (#REF; Habash, Rambow, and #REF). The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. In this article, we use an in-house system which provides functional gender, number, and rationality features (#REF). See Section 5.2 for more details.",
        "output": "experiments:Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (#REF), the Buckwalter morphological analyzer ( #TARGET_REF<USE/> ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (#REF; Habash, Rambow, and #REF). The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. In this article, we use an in-house system which provides functional gender, number, and rationality features (#REF). See Section 5.2 for more details."
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .",
                "Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).",
                "The approach therefore estimates the probability that a query Q is generated, given the document D is relevant.",
                "(A glossary of symbols used appears below.)"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Following #TARGET_REF , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) . Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ). The approach therefore estimates the probability that a query Q is generated, given the document D is relevant. (A glossary of symbols used appears below.)",
        "output": "nan:Following #TARGET_REF<FUT/> , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) . Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ). The approach therefore estimates the probability that a query Q is generated, given the document D is relevant. (A glossary of symbols used appears below.)"
    },
    {
        "gold": {
            "text": [
                "An example of psycholinguistically oriented research work can be found in #REF.",
                "These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.",
                "Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.",
                "Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #TARGET_REF and #REF ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:An example of psycholinguistically oriented research work can be found in #REF. These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information. Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #TARGET_REF and #REF .",
        "output": "introduction:An example of psycholinguistically oriented research work can be found in #REF. These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information. Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #TARGET_REF<BACK/> and #REF ."
    },
    {
        "gold": {
            "text": [
                "For compound splitting, we follow #REF, using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.",
                "Other approaches use less deep linguistic resources ( e.g. , POS-tags #TARGET_REF ) or are ( almost ) knowledge-free ( e.g. , #REF ) .",
                "Compound merging is less well studied.",
                "#REF used a simple, list-based merging approach, merging all consecutive words included in a merging list.",
                "This approach resulted in too many compounds.",
                "We follow #REF, for compound merging.",
                "We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:For compound splitting, we follow #REF, using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources ( e.g. , POS-tags #TARGET_REF ) or are ( almost ) knowledge-free ( e.g. , #REF ) . Compound merging is less well studied. #REF used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow #REF, for compound merging. We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets.",
        "output": "related work:For compound splitting, we follow #REF, using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources ( e.g. , POS-tags #TARGET_REF<USE/> ) or are ( almost ) knowledge-free ( e.g. , #REF ) . Compound merging is less well studied. #REF used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow #REF, for compound merging. We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets."
    },
    {
        "gold": {
            "text": [
                "The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #TARGET_REF ) .",
                "The resulting list of POS-tagged lemmas is weighted using the SMART 'ltc' 8 tf.idf-weighting scheme (#REF)."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #TARGET_REF ) . The resulting list of POS-tagged lemmas is weighted using the SMART 'ltc' 8 tf.idf-weighting scheme (#REF).",
        "output": "experiments:The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #TARGET_REF<FUT/> ) . The resulting list of POS-tagged lemmas is weighted using the SMART 'ltc' 8 tf.idf-weighting scheme (#REF)."
    },
    {
        "gold": {
            "text": [
                "Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) .",
                "However, most existing systems use pre-authored tutor responses for addressing student errors.",
                "The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.",
                "The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.",
                "It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #TARGET_REF ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state. It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.",
        "output": "introduction:Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #REF ; #TARGET_REF<BACK/> ; #REF ; #REF ; #REF ; #REF ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( #REF ; #REF ; #REF ; #REF ) . However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state. It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations."
    },
    {
        "gold": {
            "text": [
                "A second method is to structure the translated query, separating the translations for one term from translations for other terms.",
                "This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.",
                "There are several variations of such a method ( #REF ; #TARGET_REF ; #REF ) .",
                "One such method is to treat different translations of the same term as synonyms.",
                "Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.",
                "However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.",
                "By contrast, our HMM approach supports translation probabilities.",
                "The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function.",
                "Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "method:A second method is to structure the translated query, separating the translations for one term from translations for other terms. This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. There are several variations of such a method ( #REF ; #TARGET_REF ; #REF ) . One such method is to treat different translations of the same term as synonyms. Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms. However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other. By contrast, our HMM approach supports translation probabilities. The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function. Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations.",
        "output": "method:A second method is to structure the translated query, separating the translations for one term from translations for other terms. This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. There are several variations of such a method ( #REF ; #TARGET_REF<USE/> ; #REF ) . One such method is to treat different translations of the same term as synonyms. Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms. However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other. By contrast, our HMM approach supports translation probabilities. The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function. Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations."
    },
    {
        "gold": {
            "text": [
                "We perform our comparison using two state-ofthe-art parsers.",
                "For the full parser , we use the one developed by Michael Collins ( #REF ; #TARGET_REF ) -- one of the most accurate full parsers around .",
                "It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.",
                "Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.",
                "After that, it will choose the candidate parse tree with the highest probability as output.",
                "The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank.",
                "The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (#REF)."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We perform our comparison using two state-ofthe-art parsers. For the full parser , we use the one developed by Michael Collins ( #REF ; #TARGET_REF ) -- one of the most accurate full parsers around . It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them. Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence. After that, it will choose the candidate parse tree with the highest probability as output. The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank. The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (#REF).",
        "output": "experiments:We perform our comparison using two state-ofthe-art parsers. For the full parser , we use the one developed by Michael Collins ( #REF ; #TARGET_REF<FUT/> ) -- one of the most accurate full parsers around . It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them. Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence. After that, it will choose the candidate parse tree with the highest probability as output. The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank. The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (#REF)."
    },
    {
        "gold": {
            "text": [
                "Additional metadata are associated with each MEDLINE citation.",
                "The most important of these is the controlled vocabulary terms assigned by human indexers.",
                "NLM's controlled vocabulary thesaurus, Medical Subject Headings (MeSH), 2 contains approximately 23,000 descriptors arranged in a hierarchical structure and more than 151,000 Supplementary Concept Records (additional chemical substance names) within a separate thesaurus.",
                "Indexing is performed by approximately 100 indexers with at least bachelor's degrees in life sciences and formal training in indexing provided by NLM.",
                "Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( #TARGET_REF ) .",
                "Nevertheless, the indexing process remains firmly human-centered."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Additional metadata are associated with each MEDLINE citation. The most important of these is the controlled vocabulary terms assigned by human indexers. NLM's controlled vocabulary thesaurus, Medical Subject Headings (MeSH), 2 contains approximately 23,000 descriptors arranged in a hierarchical structure and more than 151,000 Supplementary Concept Records (additional chemical substance names) within a separate thesaurus. Indexing is performed by approximately 100 indexers with at least bachelor's degrees in life sciences and formal training in indexing provided by NLM. Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( #TARGET_REF ) . Nevertheless, the indexing process remains firmly human-centered.",
        "output": "nan:Additional metadata are associated with each MEDLINE citation. The most important of these is the controlled vocabulary terms assigned by human indexers. NLM's controlled vocabulary thesaurus, Medical Subject Headings (MeSH), 2 contains approximately 23,000 descriptors arranged in a hierarchical structure and more than 151,000 Supplementary Concept Records (additional chemical substance names) within a separate thesaurus. Indexing is performed by approximately 100 indexers with at least bachelor's degrees in life sciences and formal training in indexing provided by NLM. Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( #TARGET_REF<BACK/> ) . Nevertheless, the indexing process remains firmly human-centered."
    },
    {
        "gold": {
            "text": [
                "Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome.",
                "More specifically, it belongs to Burke's switching sub-category, where a single method is selected on a case-by-case basis.",
                "A similar approach is taken in Rotaru and Litman's (2005) reading comprehension system, but their system does not perform any learning.",
                "Instead it uses a voting mechanism to select the answer given by the majority of methods.",
                "The question answering system developed by #TARGET_REF belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .",
                "Because the results of all the methods are comparable, no learning is required: At each stage of the \"cascade of methods,\" the method that performs best is selected.",
                "In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics.",
                "Therefore, we need to learn from experience when to use each method."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome. More specifically, it belongs to Burke's switching sub-category, where a single method is selected on a case-by-case basis. A similar approach is taken in Rotaru and Litman's (2005) reading comprehension system, but their system does not perform any learning. Instead it uses a voting mechanism to select the answer given by the majority of methods. The question answering system developed by #TARGET_REF belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) . Because the results of all the methods are comparable, no learning is required: At each stage of the \"cascade of methods,\" the method that performs best is selected. In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics. Therefore, we need to learn from experience when to use each method.",
        "output": "nan:Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome. More specifically, it belongs to Burke's switching sub-category, where a single method is selected on a case-by-case basis. A similar approach is taken in Rotaru and Litman's (2005) reading comprehension system, but their system does not perform any learning. Instead it uses a voting mechanism to select the answer given by the majority of methods. The question answering system developed by #TARGET_REF<USE/> belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) . Because the results of all the methods are comparable, no learning is required: At each stage of the \"cascade of methods,\" the method that performs best is selected. In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics. Therefore, we need to learn from experience when to use each method."
    },
    {
        "gold": {
            "text": [
                "In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).",
                "The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the fo-cus of several recent investigations (#REF;#REF;#REF;#REF;#REF;#REF;#REF), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL'02 and CoNLL'03 shared tasks.",
                "Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.",
                "Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #TARGET_REF ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .",
                "John Mayor), nominal (the president) or pronominal (she, it).",
                "An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the fo-cus of several recent investigations (#REF;#REF;#REF;#REF;#REF;#REF;#REF), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL'02 and CoNLL'03 shared tasks. Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #TARGET_REF ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) . John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.",
        "output": "introduction:In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004). The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the fo-cus of several recent investigations (#REF;#REF;#REF;#REF;#REF;#REF;#REF), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL'02 and CoNLL'03 shared tasks. Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences. Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #TARGET_REF<FUT/> ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) . John Mayor), nominal (the president) or pronominal (she, it). An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity."
    },
    {
        "gold": {
            "text": [
                "I A more detailed discussion of various aspects of the proposed parser can be found in ( #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:I A more detailed discussion of various aspects of the proposed parser can be found in ( #TARGET_REF ) .",
        "output": "introduction:I A more detailed discussion of various aspects of the proposed parser can be found in ( #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.",
                "This is so since the lexical rule in Figure 2 \"(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.\"",
                "(Pollard and Sag [1994, 314], following Flickinger [1987]).",
                "This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( #TARGET_REF ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .",
                "Not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up (#REF)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output. This is so since the lexical rule in Figure 2 \"(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.\" (Pollard and Sag [1994, 314], following Flickinger [1987]). This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( #TARGET_REF ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule . Not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up (#REF).",
        "output": "introduction:Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output. This is so since the lexical rule in Figure 2 \"(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.\" (Pollard and Sag [1994, 314], following Flickinger [1987]). This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( #TARGET_REF<USE/> ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule . Not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up (#REF)."
    },
    {
        "gold": {
            "text": [
                "The potential highest level of the strength of evidence for a given citation can be identified using the Publication Type (a metadata field) and MeSH terms pertaining to the type of the clinical study.",
                "Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy ( #TARGET_REF ) ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The potential highest level of the strength of evidence for a given citation can be identified using the Publication Type (a metadata field) and MeSH terms pertaining to the type of the clinical study. Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy ( #TARGET_REF ) .",
        "output": "nan:The potential highest level of the strength of evidence for a given citation can be identified using the Publication Type (a metadata field) and MeSH terms pertaining to the type of the clinical study. Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy ( #TARGET_REF<FUT/> ) ."
    },
    {
        "gold": {
            "text": [
                "In the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.",
                "Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (#REF).",
                "In some systems such dependencies are learned from labeled examples ( #TARGET_REF ) .",
                "The advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class.",
                "The disadvantage is in the cost of building a wide-coverage set of contextual clues manually or producing annotated training data.",
                "Also, the contextual clues are usually highly specific to the domain and text genre, making such systems very difficult to port."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:In the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name. Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (#REF). In some systems such dependencies are learned from labeled examples ( #TARGET_REF ) . The advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class. The disadvantage is in the cost of building a wide-coverage set of contextual clues manually or producing annotated training data. Also, the contextual clues are usually highly specific to the domain and text genre, making such systems very difficult to port.",
        "output": "nan:In the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name. Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (#REF). In some systems such dependencies are learned from labeled examples ( #TARGET_REF<BACK/> ) . The advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class. The disadvantage is in the cost of building a wide-coverage set of contextual clues manually or producing annotated training data. Also, the contextual clues are usually highly specific to the domain and text genre, making such systems very difficult to port."
    },
    {
        "gold": {
            "text": [
                "Although there are other discussions of the paragraph as a central element of discourse (e.g.",
                "#TARGET_REF , #REF , #REF , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .",
                "Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.",
                "Our interest, however, lies precisely in that area."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Although there are other discussions of the paragraph as a central element of discourse (e.g. #TARGET_REF , #REF , #REF , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area.",
        "output": "introduction:Although there are other discussions of the paragraph as a central element of discourse (e.g. #TARGET_REF<USE/> , #REF , #REF , #REF ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure . Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area."
    },
    {
        "gold": {
            "text": [
                "Our baseline coreference system uses the C4.5 decision tree learner (#REF) to acquire a classifier on the training texts for determining whether two NPs are coreferent.",
                "Following previous work ( e.g. , #TARGET_REF and #REF ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .",
                "., NP j−1 .",
                "Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as #REF and #REF, as described below."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Our baseline coreference system uses the C4.5 decision tree learner (#REF) to acquire a classifier on the training texts for determining whether two NPs are coreferent. Following previous work ( e.g. , #TARGET_REF and #REF ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 . ., NP j−1 . Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as #REF and #REF, as described below.",
        "output": "nan:Our baseline coreference system uses the C4.5 decision tree learner (#REF) to acquire a classifier on the training texts for determining whether two NPs are coreferent. Following previous work ( e.g. , #TARGET_REF<FUT/> and #REF ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 . ., NP j−1 . Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as #REF and #REF, as described below."
    },
    {
        "gold": {
            "text": [
                "There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( #REF ; #TARGET_REF ; Grainger , et al. , 1991 ; #REF ) .",
                "However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.",
                "Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.",
                "On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (#REF).",
                "Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( #REF ; #TARGET_REF ; Grainger , et al. , 1991 ; #REF ) . However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins. Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet. On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (#REF). Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.",
        "output": "introduction:There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( #REF ; #TARGET_REF<BACK/> ; Grainger , et al. , 1991 ; #REF ) . However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins. Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet. On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (#REF). Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages."
    },
    {
        "gold": {
            "text": [
                "Table 2 shows the average parsing time with the LTAG and HPSG parsers.",
                "In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (#REF) without features (phase 1), and then executes feature unification (phase 2).",
                "TNT refers to the HPSG parser ( #TARGET_REF ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .",
                "Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.",
                "This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.",
                "We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.",
                "Another paper  describes the detailed analysis on the factor of the difference of parsing performance."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Table 2 shows the average parsing time with the LTAG and HPSG parsers. In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (#REF) without features (phase 1), and then executes feature unification (phase 2). TNT refers to the HPSG parser ( #TARGET_REF ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) . Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser. This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing. Another paper  describes the detailed analysis on the factor of the difference of parsing performance.",
        "output": "experiments:Table 2 shows the average parsing time with the LTAG and HPSG parsers. In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (#REF) without features (phase 1), and then executes feature unification (phase 2). TNT refers to the HPSG parser ( #TARGET_REF<USE/> ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) . Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser. This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing. Another paper  describes the detailed analysis on the factor of the difference of parsing performance."
    },
    {
        "gold": {
            "text": [
                "In general, dialogue act, agreement and turn anno- tations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.",
                "However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement.",
                "A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL.",
                "Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #REF ) 1 and corrected kappa ( #TARGET_REF ) 2 .",
                "Anvil divides the annotations in slices and compares each slice.",
                "We used slices of 0.04 seconds.",
                "The inter-coder agreement figures obtained for the three types of annotation are given in Table 2."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:In general, dialogue act, agreement and turn anno- tations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator. However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #REF ) 1 and corrected kappa ( #TARGET_REF ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds. The inter-coder agreement figures obtained for the three types of annotation are given in Table 2.",
        "output": "introduction:In general, dialogue act, agreement and turn anno- tations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator. However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement. A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL. Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #REF ) 1 and corrected kappa ( #TARGET_REF<FUT/> ) 2 . Anvil divides the annotations in slices and compares each slice. We used slices of 0.04 seconds. The inter-coder agreement figures obtained for the three types of annotation are given in Table 2."
    },
    {
        "gold": {
            "text": [
                "As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.",
                "The first work to do this with topic models is Feng and Lapata (2010b).",
                "They use a Bag of Visual Words (BoVW) model (#REF) to create a bimodal vocabulary describing documents.",
                "The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.",
                "Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.",
                "Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.",
                "More recently , #TARGET_REF show that visual attribute classifiers , which have been immensely successful in object recognition ( #REF ) , act as excellent substitutes for feature",
                "Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (#REF;#REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (#REF) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently , #TARGET_REF show that visual attribute classifiers , which have been immensely successful in object recognition ( #REF ) , act as excellent substitutes for feature Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (#REF;#REF).",
        "output": "related work:As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (#REF) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently , #TARGET_REF<BACK/> show that visual attribute classifiers , which have been immensely successful in object recognition ( #REF ) , act as excellent substitutes for feature Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (#REF;#REF)."
    },
    {
        "gold": {
            "text": [
                "Another common approach to lexical rules is to encode them as unary phrase structure rules.",
                "This approach is taken , for example , in LKB ( #REF ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #TARGET_REF , 31 ) .",
                "A similar method is included in PATR-II (#REF) and can be used to encode lexical rules as binary relations in the CUF system (#REF; D6rre and #REFb) or the TFS system (#REF;#REF).",
                "The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken , for example , in LKB ( #REF ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #TARGET_REF , 31 ) . A similar method is included in PATR-II (#REF) and can be used to encode lexical rules as binary relations in the CUF system (#REF; D6rre and #REFb) or the TFS system (#REF;#REF). The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.",
        "output": "related work:Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken , for example , in LKB ( #REF ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #TARGET_REF<USE/> , 31 ) . A similar method is included in PATR-II (#REF) and can be used to encode lexical rules as binary relations in the CUF system (#REF; D6rre and #REFb) or the TFS system (#REF;#REF). The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."
    },
    {
        "gold": {
            "text": [
                "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #TARGET_REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #TARGET_REF , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .",
        "output": "nan:Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly #REF , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #TARGET_REF<FUT/> , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] ."
    },
    {
        "gold": {
            "text": [
                "\"Semantic grammars\" already exist which describe not only the syntax but also the semantics of natural language.",
                "Thus for instance , ( #REF ; #REF ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #TARGET_REF ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:\"Semantic grammars\" already exist which describe not only the syntax but also the semantics of natural language. Thus for instance , ( #REF ; #REF ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #TARGET_REF ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .",
        "output": "nan:\"Semantic grammars\" already exist which describe not only the syntax but also the semantics of natural language. Thus for instance , ( #REF ; #REF ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #TARGET_REF<BACK/> ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics ."
    },
    {
        "gold": {
            "text": [
                "19 The paper by #TARGET_REF presents additional , more sophisticated models that we do not use in this article ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:19 The paper by #TARGET_REF presents additional , more sophisticated models that we do not use in this article .",
        "output": "related work:19 The paper by #TARGET_REF<USE/> presents additional , more sophisticated models that we do not use in this article ."
    },
    {
        "gold": {
            "text": [
                "For Spanish, we downloaded a bilingual English-Spanish lexicon from the Internet (http://www.activa.arrakis.es)",
                "containing around 22,000 English words (16,000 English stems) and processed it similarly.",
                "Each English word has around 1.5 translations on average.",
                "A cooccurrence based stemmer ( #TARGET_REF ) was used to stem Spanish words .",
                "One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.",
                "This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:For Spanish, we downloaded a bilingual English-Spanish lexicon from the Internet (http://www.activa.arrakis.es) containing around 22,000 English words (16,000 English stems) and processed it similarly. Each English word has around 1.5 translations on average. A cooccurrence based stemmer ( #TARGET_REF ) was used to stem Spanish words . One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon. This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon.",
        "output": "experiments:For Spanish, we downloaded a bilingual English-Spanish lexicon from the Internet (http://www.activa.arrakis.es) containing around 22,000 English words (16,000 English stems) and processed it similarly. Each English word has around 1.5 translations on average. A cooccurrence based stemmer ( #TARGET_REF<FUT/> ) was used to stem Spanish words . One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon. This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon."
    },
    {
        "gold": {
            "text": [
                "Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.",
                "Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?)",
                "While corpus driven efforts along the PARSEVAL lines ( #TARGET_REF ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .",
                "Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.",
                "For english, there is for instance the 15 year old Hewlett- Packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (#REF); and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (#REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar. Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?) While corpus driven efforts along the PARSEVAL lines ( #TARGET_REF ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation . Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english, there is for instance the 15 year old Hewlett- Packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (#REF); and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (#REF).",
        "output": "nan:Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar. Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?) While corpus driven efforts along the PARSEVAL lines ( #TARGET_REF<BACK/> ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation . Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english, there is for instance the 15 year old Hewlett- Packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (#REF); and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (#REF)."
    },
    {
        "gold": {
            "text": [
                "The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #REF ) and case-based reasoning ( #TARGET_REF ) .",
                "Such technologies require significant human input, and are difficult to create and maintain (#REF).",
                "In contrast, the techniques examined in this article are corpus-based and data-driven.",
                "The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #REF ) and case-based reasoning ( #TARGET_REF ) . Such technologies require significant human input, and are difficult to create and maintain (#REF). In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus.",
        "output": "nan:The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #REF ) and case-based reasoning ( #TARGET_REF<USE/> ) . Such technologies require significant human input, and are difficult to create and maintain (#REF). In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus."
    },
    {
        "gold": {
            "text": [
                "Here, y u,v = 1 iff mentions u, v are directly linked.",
                "Thus, we can construct a forest and the mentions in the same connected component (i.e., in the same tree) are co-referred.",
                "For this mention-pair coreference model Ï ( u , v ) , we use the same set of features used in #TARGET_REF ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:Here, y u,v = 1 iff mentions u, v are directly linked. Thus, we can construct a forest and the mentions in the same connected component (i.e., in the same tree) are co-referred. For this mention-pair coreference model Ï ( u , v ) , we use the same set of features used in #TARGET_REF .",
        "output": "nan:Here, y u,v = 1 iff mentions u, v are directly linked. Thus, we can construct a forest and the mentions in the same connected component (i.e., in the same tree) are co-referred. For this mention-pair coreference model Ï ( u , v ) , we use the same set of features used in #TARGET_REF<FUT/> ."
    },
    {
        "gold": {
            "text": [
                "There are a number of generalised NLP systems in the literature.",
                "Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #TARGET_REF ) and the Alembic Workbench ( #REF ) ) as well as NLP tools and resources that can be manipulated from the GUI .",
                "For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (#REF).",
                "GATE goes beyond earlier systems by using a component-based infrastructure (#REF) which the GUI is built on top of.",
                "This allows components to be highly configurable and simplifies the addition of new components to the system."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #TARGET_REF ) and the Alembic Workbench ( #REF ) ) as well as NLP tools and resources that can be manipulated from the GUI . For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (#REF). GATE goes beyond earlier systems by using a component-based infrastructure (#REF) which the GUI is built on top of. This allows components to be highly configurable and simplifies the addition of new components to the system.",
        "output": "experiments:There are a number of generalised NLP systems in the literature. Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #TARGET_REF<BACK/> ) and the Alembic Workbench ( #REF ) ) as well as NLP tools and resources that can be manipulated from the GUI . For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (#REF). GATE goes beyond earlier systems by using a component-based infrastructure (#REF) which the GUI is built on top of. This allows components to be highly configurable and simplifies the addition of new components to the system."
    },
    {
        "gold": {
            "text": [
                "The tree converter module is a core module of the system, which is an implementation of the grammar conversion algorithm given in Section 3. The type hierarchy extractor module extracts the symbols of the node, features, and feature values from the LTAG elementary tree templates and lexicon, and construct the type hierarchy from them.",
                "The lexicon converter module converts LTAG elementary tree templates into HPSG lexical entries.",
                "The derivation translator module takes HPSG parse  (#REF).",
                "However, their method depended on translator's intuitive analysis of the original grammar.",
                "Thus the translation was manual and grammar dependent.",
                "The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.",
                "Other works ( #REF ; #TARGET_REF ) convert HPSG grammars into LTAG grammars .",
                "However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar.",
                "Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity.",
                "Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:The tree converter module is a core module of the system, which is an implementation of the grammar conversion algorithm given in Section 3. The type hierarchy extractor module extracts the symbols of the node, features, and feature values from the LTAG elementary tree templates and lexicon, and construct the type hierarchy from them. The lexicon converter module converts LTAG elementary tree templates into HPSG lexical entries. The derivation translator module takes HPSG parse  (#REF). However, their method depended on translator's intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works ( #REF ; #TARGET_REF ) convert HPSG grammars into LTAG grammars . However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity. Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages.",
        "output": "introduction:The tree converter module is a core module of the system, which is an implementation of the grammar conversion algorithm given in Section 3. The type hierarchy extractor module extracts the symbols of the node, features, and feature values from the LTAG elementary tree templates and lexicon, and construct the type hierarchy from them. The lexicon converter module converts LTAG elementary tree templates into HPSG lexical entries. The derivation translator module takes HPSG parse  (#REF). However, their method depended on translator's intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works ( #REF ; #TARGET_REF<USE/> ) convert HPSG grammars into LTAG grammars . However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity. Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages."
    },
    {
        "gold": {
            "text": [
                "The system is trained on the Arabic ACE 2003 and part of the 2004 data.",
                "We introduce here a clearly defined and replicable split of the #TARGET_REF data , so that future investigations can accurately and correctly compare against the results presented here ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The system is trained on the Arabic ACE 2003 and part of the 2004 data. We introduce here a clearly defined and replicable split of the #TARGET_REF data , so that future investigations can accurately and correctly compare against the results presented here .",
        "output": "experiments:The system is trained on the Arabic ACE 2003 and part of the 2004 data. We introduce here a clearly defined and replicable split of the #TARGET_REF<FUT/> data , so that future investigations can accurately and correctly compare against the results presented here ."
    },
    {
        "gold": {
            "text": [
                "FindBestValue selects the 'best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity.",
                "The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #TARGET_REF ) .",
                "IA Plur can refer to individuals as well as sets, since reference to a target individual r can be modeled as reference to the singleton set {r}."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:FindBestValue selects the 'best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity. The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #TARGET_REF ) . IA Plur can refer to individuals as well as sets, since reference to a target individual r can be modeled as reference to the singleton set {r}.",
        "output": "introduction:FindBestValue selects the 'best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity. The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #TARGET_REF<BACK/> ) . IA Plur can refer to individuals as well as sets, since reference to a target individual r can be modeled as reference to the singleton set {r}."
    },
    {
        "gold": {
            "text": [
                "Prefer is misclassified as Object Raising, rather than as Object Equi, because the relevant code field contains a T5 code, as well as a V3 code.",
                "The T5 code is marked as 'rare', and the occurrence of prefer with a tensed sentential complement, as opposed to with an infinitive, is certainly marginal:  This example also highlights a deficiency in the LDOCE coding system since prefer occurs much more naturally with a sentential complement if it collocates with a modal such as \"would\".",
                "This deficiency is rectified in the verb classification system employed by #TARGET_REF in the Brandeis verb catalogue ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Prefer is misclassified as Object Raising, rather than as Object Equi, because the relevant code field contains a T5 code, as well as a V3 code. The T5 code is marked as 'rare', and the occurrence of prefer with a tensed sentential complement, as opposed to with an infinitive, is certainly marginal:  This example also highlights a deficiency in the LDOCE coding system since prefer occurs much more naturally with a sentential complement if it collocates with a modal such as \"would\". This deficiency is rectified in the verb classification system employed by #TARGET_REF in the Brandeis verb catalogue .",
        "output": "nan:Prefer is misclassified as Object Raising, rather than as Object Equi, because the relevant code field contains a T5 code, as well as a V3 code. The T5 code is marked as 'rare', and the occurrence of prefer with a tensed sentential complement, as opposed to with an infinitive, is certainly marginal:  This example also highlights a deficiency in the LDOCE coding system since prefer occurs much more naturally with a sentential complement if it collocates with a modal such as \"would\". This deficiency is rectified in the verb classification system employed by #TARGET_REF<USE/> in the Brandeis verb catalogue ."
    },
    {
        "gold": {
            "text": [
                "The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.",
                "To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (#REF).18",
                "This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender).",
                "We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #TARGET_REF ) .19",
                "The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input).",
                "The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values. To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (#REF).18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #TARGET_REF ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts.",
        "output": "related work:The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values. To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (#REF).18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #TARGET_REF<FUT/> ) .19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts."
    },
    {
        "gold": {
            "text": [
                "In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #REF ; #REF ; #TARGET_REF ] , head-driven phrase structure grammar [ HPSG ] [ #REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #REF ; #REF ; #TARGET_REF ] , head-driven phrase structure grammar [ HPSG ] [ #REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .",
        "output": "introduction:In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #REF ; #REF ; #TARGET_REF<BACK/> ] , head-driven phrase structure grammar [ HPSG ] [ #REF ] , tree-adjoining grammar [ TAG ] [ #REF ] , and combinatory categorial grammar [ CCG ] [ #REF ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information ."
    },
    {
        "gold": {
            "text": [
                "Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.",
                "State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.",
                "The best performance on the WSJ corpus was achieved by a combination of the SATZ system (#REF) with the Alembic system (#REF): a 0.5% error rate.",
                "The best performance on the Brown corpus, a 0.2% error rate, was reported by #REF, who trained a decision tree classifier on a 25-million-word corpus.",
                "In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #TARGET_REF .",
                "We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system (#REF) with the Alembic system (#REF): a 0.5% error rate. The best performance on the Brown corpus, a 0.2% error rate, was reported by #REF, who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #TARGET_REF . We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.",
        "output": "nan:Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system (#REF) with the Alembic system (#REF): a 0.5% error rate. The best performance on the Brown corpus, a 0.2% error rate, was reported by #REF, who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #TARGET_REF<USE/> . We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus."
    },
    {
        "gold": {
            "text": [
                "To evaluate our end-to-end system, we perform the well-studied task of news translation, using the Moses SMT package.",
                "We use the English/German data released for the 2009 ACL Workshop on Machine Translation shared task on translation. 7",
                "There are 82,740 parallel sentences from news-commentary09.de-en and 1,418,115 parallel sentences from europarl-v4.de-en.",
                "The monolingual data contains 9.8 M sentences. 8",
                "o build the baseline, the data was tokenized using the Moses tokenizer and lowercased.",
                "We use GIZA++ to generate alignments, by running 5 iterations of Model 1, 5 iterations of the HMM Model, and 4 iterations of Model 4. We symmetrize using the \"grow-diag-final-and\" heuristic.",
                "Our Moses systems use default settings.",
                "The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #TARGET_REF ) .",
                "We run MERT separately for each system.",
                "The recaser used is the same for all systems.",
                "It is the standard recaser supplied with Moses, trained on all German training data."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:To evaluate our end-to-end system, we perform the well-studied task of news translation, using the Moses SMT package. We use the English/German data released for the 2009 ACL Workshop on Machine Translation shared task on translation. 7 There are 82,740 parallel sentences from news-commentary09.de-en and 1,418,115 parallel sentences from europarl-v4.de-en. The monolingual data contains 9.8 M sentences. 8 o build the baseline, the data was tokenized using the Moses tokenizer and lowercased. We use GIZA++ to generate alignments, by running 5 iterations of Model 1, 5 iterations of the HMM Model, and 4 iterations of Model 4. We symmetrize using the \"grow-diag-final-and\" heuristic. Our Moses systems use default settings. The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #TARGET_REF ) . We run MERT separately for each system. The recaser used is the same for all systems. It is the standard recaser supplied with Moses, trained on all German training data.",
        "output": "experiments:To evaluate our end-to-end system, we perform the well-studied task of news translation, using the Moses SMT package. We use the English/German data released for the 2009 ACL Workshop on Machine Translation shared task on translation. 7 There are 82,740 parallel sentences from news-commentary09.de-en and 1,418,115 parallel sentences from europarl-v4.de-en. The monolingual data contains 9.8 M sentences. 8 o build the baseline, the data was tokenized using the Moses tokenizer and lowercased. We use GIZA++ to generate alignments, by running 5 iterations of Model 1, 5 iterations of the HMM Model, and 4 iterations of Model 4. We symmetrize using the \"grow-diag-final-and\" heuristic. Our Moses systems use default settings. The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #TARGET_REF<FUT/> ) . We run MERT separately for each system. The recaser used is the same for all systems. It is the standard recaser supplied with Moses, trained on all German training data."
    },
    {
        "gold": {
            "text": [
                "Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.",
                "\"And,\" \"or,\" and \"but\" are the three main coordinating connectives in English.",
                "However, \"but\" does not behave quite like the other two--semantically, \"but\" signals a contradiction, and in this role it seems to have three subfunctions: . .",
                "Opposition (called \"adversative\" or \"contrary-to-expectation\" by #REF;cf. also #TARGET_REF , p. 672 ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units. \"And,\" \"or,\" and \"but\" are the three main coordinating connectives in English. However, \"but\" does not behave quite like the other two--semantically, \"but\" signals a contradiction, and in this role it seems to have three subfunctions: . . Opposition (called \"adversative\" or \"contrary-to-expectation\" by #REF;cf. also #TARGET_REF , p. 672 ) .",
        "output": "introduction:Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units. \"And,\" \"or,\" and \"but\" are the three main coordinating connectives in English. However, \"but\" does not behave quite like the other two--semantically, \"but\" signals a contradiction, and in this role it seems to have three subfunctions: . . Opposition (called \"adversative\" or \"contrary-to-expectation\" by #REF;cf. also #TARGET_REF<BACK/> , p. 672 ) ."
    },
    {
        "gold": {
            "text": [
                "An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.",
                "before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.",
                "If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.",
                "Typical examples are Bulgarian (#REF;#REF), Chinese (#REF), Danish (#REF), and Swedish .",
                "Japanese ( #TARGET_REF ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6. Typical examples are Bulgarian (#REF;#REF), Chinese (#REF), Danish (#REF), and Swedish . Japanese ( #TARGET_REF ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .",
        "output": "experiments:An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page. before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length. If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6. Typical examples are Bulgarian (#REF;#REF), Chinese (#REF), Danish (#REF), and Swedish . Japanese ( #TARGET_REF<USE/> ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances ."
    },
    {
        "gold": {
            "text": [
                "We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #TARGET_REF ) .",
                "Two concerns motivated our implementation.",
                "First, we hoped the system would provide us with a research tool for testing our ideas about syntax and phrasing against a large unrestricted collection of sentences.",
                "Second, we wished to investigate how well our approach would work for determining prosodic phrasing in a text-to-speech synthesizer."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #TARGET_REF ) . Two concerns motivated our implementation. First, we hoped the system would provide us with a research tool for testing our ideas about syntax and phrasing against a large unrestricted collection of sentences. Second, we wished to investigate how well our approach would work for determining prosodic phrasing in a text-to-speech synthesizer.",
        "output": "experiments:We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #TARGET_REF<FUT/> ) . Two concerns motivated our implementation. First, we hoped the system would provide us with a research tool for testing our ideas about syntax and phrasing against a large unrestricted collection of sentences. Second, we wished to investigate how well our approach would work for determining prosodic phrasing in a text-to-speech synthesizer."
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .",
                "In their model, a document consists of a set of (word, feature) pairs, 4 rather than just words, and documents are still modeled as mixtures of shared topics.",
                "Topics consist of multinomial distributions over words, β k , but are extended to also include multinomial distributions over features, ψ k .",
                "The generative process is amended to include these feature distributions:"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "method:#TARGET_REF extend LDA to allow for the inference of document and topic distributions in a multimodal corpus . In their model, a document consists of a set of (word, feature) pairs, 4 rather than just words, and documents are still modeled as mixtures of shared topics. Topics consist of multinomial distributions over words, β k , but are extended to also include multinomial distributions over features, ψ k . The generative process is amended to include these feature distributions:",
        "output": "method:#TARGET_REF<BACK/> extend LDA to allow for the inference of document and topic distributions in a multimodal corpus . In their model, a document consists of a set of (word, feature) pairs, 4 rather than just words, and documents are still modeled as mixtures of shared topics. Topics consist of multinomial distributions over words, β k , but are extended to also include multinomial distributions over features, ψ k . The generative process is amended to include these feature distributions:"
    },
    {
        "gold": {
            "text": [
                "In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647.",
                "#TARGET_REF reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .",
                "The values may again not be compared directly.",
                "Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647. #TARGET_REF reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs . The values may again not be compared directly. Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low.",
        "output": "experiments:In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647. #TARGET_REF<USE/> reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs . The values may again not be compared directly. Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low."
    },
    {
        "gold": {
            "text": [
                "In these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand-corrected tags which come with the corpus.",
                "We used a publicly available tagger ( #TARGET_REF ) to tag the words and then used these in the input to the system ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:In these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand-corrected tags which come with the corpus. We used a publicly available tagger ( #TARGET_REF ) to tag the words and then used these in the input to the system .",
        "output": "experiments:In these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand-corrected tags which come with the corpus. We used a publicly available tagger ( #TARGET_REF<FUT/> ) to tag the words and then used these in the input to the system ."
    },
    {
        "gold": {
            "text": [
                "One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #TARGET_REF ; #REF ) .",
                "Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position.",
                "Interestingly, vague properties tend to be realized before others.",
                "#REF, for example, report that \"adjectives denoting size, length, and height normally precede other nonderived adjectives\" (e.g., the small round table is usually preferred to the round small table).",
                "Semantically, this does not come as a surprise.",
                "In a noun phrase of the form the three small (-est) [N], for example, the words preceding N select the three smallest elements of [N].",
                "It follows that, to denote the three smallest elements of the set of round tables, the only option is to say the three small round tables, rather than the three round small tables.",
                "The latter would mean something else, namely, the three round ones among the n small(est) tables (where n is not specified)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #TARGET_REF ; #REF ) . Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position. Interestingly, vague properties tend to be realized before others. #REF, for example, report that \"adjectives denoting size, length, and height normally precede other nonderived adjectives\" (e.g., the small round table is usually preferred to the round small table). Semantically, this does not come as a surprise. In a noun phrase of the form the three small (-est) [N], for example, the words preceding N select the three smallest elements of [N]. It follows that, to denote the three smallest elements of the set of round tables, the only option is to say the three small round tables, rather than the three round small tables. The latter would mean something else, namely, the three round ones among the n small(est) tables (where n is not specified).",
        "output": "nan:One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #TARGET_REF<BACK/> ; #REF ) . Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position. Interestingly, vague properties tend to be realized before others. #REF, for example, report that \"adjectives denoting size, length, and height normally precede other nonderived adjectives\" (e.g., the small round table is usually preferred to the round small table). Semantically, this does not come as a surprise. In a noun phrase of the form the three small (-est) [N], for example, the words preceding N select the three smallest elements of [N]. It follows that, to denote the three smallest elements of the set of round tables, the only option is to say the three small round tables, rather than the three round small tables. The latter would mean something else, namely, the three round ones among the n small(est) tables (where n is not specified)."
    },
    {
        "gold": {
            "text": [
                "In addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs.",
                "The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and #REF ; #TARGET_REF ) .",
                "Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine.",
                "Patient information is no doubt important to answering clinical questions, and our work could certainly benefit from experiences gained in the PERSIVAL project."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:In addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs. The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and #REF ; #TARGET_REF ) . Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine. Patient information is no doubt important to answering clinical questions, and our work could certainly benefit from experiences gained in the PERSIVAL project.",
        "output": "related work:In addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs. The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and #REF ; #TARGET_REF<USE/> ) . Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine. Patient information is no doubt important to answering clinical questions, and our work could certainly benefit from experiences gained in the PERSIVAL project."
    },
    {
        "gold": {
            "text": [
                "These edges are then used as weak supervision when training a generative or discriminative dependency parser.",
                "In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side.",
                "By changing the threshold in MBR decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges.",
                "We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and #REF) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish.",
                "Following Ganchev, Gillenwater, and #REF, we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).",
                "results are based on a corpus of movie subtitles ( #TARGET_REF ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( #REF ) .",
                "We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:These edges are then used as weak supervision when training a generative or discriminative dependency parser. In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side. By changing the threshold in MBR decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges. We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and #REF) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish. Following Ganchev, Gillenwater, and #REF, we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es). results are based on a corpus of movie subtitles ( #TARGET_REF ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( #REF ) . We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.",
        "output": "nan:These edges are then used as weak supervision when training a generative or discriminative dependency parser. In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side. By changing the threshold in MBR decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges. We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and #REF) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish. Following Ganchev, Gillenwater, and #REF, we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es). results are based on a corpus of movie subtitles ( #TARGET_REF<FUT/> ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( #REF ) . We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM."
    },
    {
        "gold": {
            "text": [
                "Figure 1: Example text shown in standard and ASR format ation which is not available in ASR output is sentence boundary information.",
                "However, knowledge of sentence boundaries is required by many NLP technologies.",
                "Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger ( #TARGET_REF ) ) and parsers generally aim to produce a tree spanning each sentence ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Figure 1: Example text shown in standard and ASR format ation which is not available in ASR output is sentence boundary information. However, knowledge of sentence boundaries is required by many NLP technologies. Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger ( #TARGET_REF ) ) and parsers generally aim to produce a tree spanning each sentence .",
        "output": "nan:Figure 1: Example text shown in standard and ASR format ation which is not available in ASR output is sentence boundary information. However, knowledge of sentence boundaries is required by many NLP technologies. Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger ( #TARGET_REF<BACK/> ) ) and parsers generally aim to produce a tree spanning each sentence ."
    },
    {
        "gold": {
            "text": [
                "Our results are lower than those of full parsers , e.g. , #TARGET_REF as might be expected since much less structural data , and no lexical data are being used ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:Our results are lower than those of full parsers , e.g. , #TARGET_REF as might be expected since much less structural data , and no lexical data are being used .",
        "output": "nan:Our results are lower than those of full parsers , e.g. , #TARGET_REF<USE/> as might be expected since much less structural data , and no lexical data are being used ."
    },
    {
        "gold": {
            "text": [
                "To test our hypothesis that DCA can be used as a complement to a local-context approach, we combined our main configuration (evaluated in row D of Table 4) with a POS tagger.",
                "Unlike other POS taggers , this POS tagger ( #TARGET_REF ) was also trained to disambiguate sentence boundaries ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:To test our hypothesis that DCA can be used as a complement to a local-context approach, we combined our main configuration (evaluated in row D of Table 4) with a POS tagger. Unlike other POS taggers , this POS tagger ( #TARGET_REF ) was also trained to disambiguate sentence boundaries .",
        "output": "nan:To test our hypothesis that DCA can be used as a complement to a local-context approach, we combined our main configuration (evaluated in row D of Table 4) with a POS tagger. Unlike other POS taggers , this POS tagger ( #TARGET_REF<FUT/> ) was also trained to disambiguate sentence boundaries ."
    },
    {
        "gold": {
            "text": [
                "Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.",
                "Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #TARGET_REF ) , neural networks ( #REF ) , and maximum-entropy modeling ( #REF ) .",
                "Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign.",
                "Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.",
                "This implies an investment in the data annotation phase."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #TARGET_REF ) , neural networks ( #REF ) , and maximum-entropy modeling ( #REF ) . Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign. Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training. This implies an investment in the data annotation phase.",
        "output": "nan:Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language. Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #TARGET_REF<BACK/> ) , neural networks ( #REF ) , and maximum-entropy modeling ( #REF ) . Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign. Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training. This implies an investment in the data annotation phase."
    },
    {
        "gold": {
            "text": [
                "There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.",
                "This includes work on generalized expectation ( #REF ) , posterior regularization ( #REF ) and constraint driven learning ( #TARGET_REF ; #REF ) .",
                "The work of #REF on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.",
                "In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.",
                "For our setting this would mean using weak application specific signals to improve dependency parsing.",
                "Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation ( #REF ) , posterior regularization ( #REF ) and constraint driven learning ( #TARGET_REF ; #REF ) . The work of #REF on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics. For our setting this would mean using weak application specific signals to improve dependency parsing. Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training.",
        "output": "introduction:There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation ( #REF ) , posterior regularization ( #REF ) and constraint driven learning ( #TARGET_REF<USE/> ; #REF ) . The work of #REF on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics. For our setting this would mean using weak application specific signals to improve dependency parsing. Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training."
    },
    {
        "gold": {
            "text": [
                "The candidate feature templates include : Voice from #TARGET_REF ."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The candidate feature templates include : Voice from #TARGET_REF .",
        "output": "nan:The candidate feature templates include : Voice from #TARGET_REF<FUT/> ."
    },
    {
        "gold": {
            "text": [
                "#REF employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames.",
                "#REF attempts to improve on the approach of #REF by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.",
                "He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.",
                "The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #TARGET_REF .",
                "Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.",
                "#REF predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and #REF) and ANLT (#REF) dictionaries and adding around 30 frames found by manual inspection."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "related work:#REF employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #REF attempts to improve on the approach of #REF by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur. He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #TARGET_REF . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. #REF predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and #REF) and ANLT (#REF) dictionaries and adding around 30 frames found by manual inspection.",
        "output": "related work:#REF employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames. #REF attempts to improve on the approach of #REF by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur. He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #TARGET_REF<BACK/> . Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. #REF predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and #REF) and ANLT (#REF) dictionaries and adding around 30 frames found by manual inspection."
    },
    {
        "gold": {
            "text": [
                "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF.",
                "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF, assertional statements as in #REF, or semantic nets as in #REF.",
                "That is, the current system learns procedures rather than data structures.",
                "There is some literature on procedure acquisition such as the LISP synthesis work described in #TARGET_REF and the PROLOG synthesis method of #REF ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF. The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF, assertional statements as in #REF, or semantic nets as in #REF. That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in #TARGET_REF and the PROLOG synthesis method of #REF .",
        "output": "nan:The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #REF. The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #REF, assertional statements as in #REF, or semantic nets as in #REF. That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in #TARGET_REF<USE/> and the PROLOG synthesis method of #REF ."
    },
    {
        "gold": {
            "text": [
                "The system utilizes several large size biological databases including three NCBI databases ( #REF , RefSeq #TARGET_REF , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and",
                "Additionally, several model organism databases or nomenclature databases were used.",
                "Correspondences among records from these databases are identified using the rich cross-reference information provided by the iProClass database of PIR [14].",
                "The following provides a brief description of each of the database."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The system utilizes several large size biological databases including three NCBI databases ( #REF , RefSeq #TARGET_REF , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and Additionally, several model organism databases or nomenclature databases were used. Correspondences among records from these databases are identified using the rich cross-reference information provided by the iProClass database of PIR [14]. The following provides a brief description of each of the database.",
        "output": "nan:The system utilizes several large size biological databases including three NCBI databases ( #REF , RefSeq #TARGET_REF<FUT/> , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and Additionally, several model organism databases or nomenclature databases were used. Correspondences among records from these databases are identified using the rich cross-reference information provided by the iProClass database of PIR [14]. The following provides a brief description of each of the database."
    },
    {
        "gold": {
            "text": [
                "A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , #REF , #REF , #TARGET_REF , #REF , and #REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , #REF , #REF , #TARGET_REF , #REF , and #REF ) .",
        "output": "introduction:A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , #REF , #REF , #TARGET_REF<BACK/> , #REF , and #REF ) ."
    },
    {
        "gold": {
            "text": [
                "The system was trained on the Penn Treebank ( #REF ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #REF , #REF , and #TARGET_REF , and became a common testbed ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:The system was trained on the Penn Treebank ( #REF ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #REF , #REF , and #TARGET_REF , and became a common testbed .",
        "output": "nan:The system was trained on the Penn Treebank ( #REF ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #REF , #REF , and #TARGET_REF<USE/> , and became a common testbed ."
    },
    {
        "gold": {
            "text": [
                "We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank ( #TARGET_REF ) and Penn Chinese Treebank (Xue, Chiou, and #REF), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (#REF) and Chinese (Burke, Lam, et al. 2004).",
                "The lexical resources, however, have not yet been evaluated. This, and much else, has to await further research."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank ( #TARGET_REF ) and Penn Chinese Treebank (Xue, Chiou, and #REF), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (#REF) and Chinese (Burke, Lam, et al. 2004). The lexical resources, however, have not yet been evaluated. This, and much else, has to await further research.",
        "output": "nan:We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank ( #TARGET_REF<FUT/> ) and Penn Chinese Treebank (Xue, Chiou, and #REF), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (#REF) and Chinese (Burke, Lam, et al. 2004). The lexical resources, however, have not yet been evaluated. This, and much else, has to await further research."
    },
    {
        "gold": {
            "text": [
                "The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.",
                "This includes work on question answering (#REF), sentiment analysis (#REF), MT reordering (#REF), and many other tasks.",
                "In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #REF ; #REF ; #REF ; #TARGET_REF ) .",
                "But these accuracies are measured with respect to gold-standard out-of-domain parse trees.",
                "There are few tasks that actually depend on the complete parse tree.",
                "Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.",
                "While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.",
                "The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (#REF), sentiment analysis (#REF), MT reordering (#REF), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #REF ; #REF ; #REF ; #TARGET_REF ) . But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training. The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.",
        "output": "introduction:The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (#REF), sentiment analysis (#REF), MT reordering (#REF), and many other tasks. In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #REF ; #REF ; #REF ; #TARGET_REF<BACK/> ) . But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training. The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure."
    },
    {
        "gold": {
            "text": [
                "The situation is worse if there are English words (e.g., adjectives) separating his and brother.",
                "This required mapping is a significant problem for generalization.",
                "We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection).",
                "We apply a \"split in preprocessing and resynthesize in postprocessing\" approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et.",
                "al.",
                "The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008), which deals with verbal morphology and attached pronouns.",
                "There has been other work on solving inflection.",
                "#REF introduced factored SMT.",
                "We use more complex context features.",
                "#TARGET_REF tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .",
                "#REF improved on this by marking prepositions with the case they mark (one of the most important markups in our system).",
                "Both efforts were ineffective on large data sets.",
                "#REF used unification in an SMT system to model some of the agreement phenomena that we model.",
                "Our CRF framework allows us to use more complex context features."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:The situation is worse if there are English words (e.g., adjectives) separating his and brother. This required mapping is a significant problem for generalization. We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection). We apply a \"split in preprocessing and resynthesize in postprocessing\" approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al. The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008), which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. #REF introduced factored SMT. We use more complex context features. #TARGET_REF tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . #REF improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. #REF used unification in an SMT system to model some of the agreement phenomena that we model. Our CRF framework allows us to use more complex context features.",
        "output": "related work:The situation is worse if there are English words (e.g., adjectives) separating his and brother. This required mapping is a significant problem for generalization. We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection). We apply a \"split in preprocessing and resynthesize in postprocessing\" approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al. The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008), which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. #REF introduced factored SMT. We use more complex context features. #TARGET_REF<USE/> tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms . #REF improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. #REF used unification in an SMT system to model some of the agreement phenomena that we model. Our CRF framework allows us to use more complex context features."
    },
    {
        "gold": {
            "text": [
                "The Collins-Brooks PP-attachment classification algorithm.",
                "preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification.",
                "For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb).",
                "The head words can be automatically extracted using a heuristic table lookup in the manner described by #TARGET_REF .",
                "For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n.",
                "In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples.",
                "A characteristic tuple is some subset of the four head words in the example, with the condition that one of the elements must be the preposition.",
                "Each training example forms eight characteristic tuples:"
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The Collins-Brooks PP-attachment classification algorithm. preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification. For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb). The head words can be automatically extracted using a heuristic table lookup in the manner described by #TARGET_REF . For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n. In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples. A characteristic tuple is some subset of the four head words in the example, with the condition that one of the elements must be the preposition. Each training example forms eight characteristic tuples:",
        "output": "nan:The Collins-Brooks PP-attachment classification algorithm. preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification. For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb). The head words can be automatically extracted using a heuristic table lookup in the manner described by #TARGET_REF<FUT/> . For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n. In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples. A characteristic tuple is some subset of the four head words in the example, with the condition that one of the elements must be the preposition. Each training example forms eight characteristic tuples:"
    },
    {
        "gold": {
            "text": [
                "The efforts required for performing morphological analysis vary from language to language.",
                "For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator.",
                "mers ( #TARGET_REF ; #REF ) demonstrably improve retrieval performance .",
                "This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;#REF;#REF;Ekmekçioglu et al., 1995;#REF;#REF).",
                "When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.",
                "This is particularly true for the medical domain.",
                "From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (#REF;#REF;#REF;#REF;#REF;#REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The efforts required for performing morphological analysis vary from language to language. For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator. mers ( #TARGET_REF ; #REF ) demonstrably improve retrieval performance . This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;#REF;#REF;Ekmekçioglu et al., 1995;#REF;#REF). When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain. From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (#REF;#REF;#REF;#REF;#REF;#REF).",
        "output": "introduction:The efforts required for performing morphological analysis vary from language to language. For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator. mers ( #TARGET_REF<BACK/> ; #REF ) demonstrably improve retrieval performance . This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;#REF;#REF;Ekmekçioglu et al., 1995;#REF;#REF). When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist. This is particularly true for the medical domain. From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (#REF;#REF;#REF;#REF;#REF;#REF)."
    },
    {
        "gold": {
            "text": [
                "In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.",
                "Consequently a POS tagger can naturally treat them similarly to any other ambiguous words.",
                "There is, however, one difference in the implementation of such a tagger.",
                "Normally, a POS tagger operates on text spans that form a sentence.",
                "This requires resolving sentence boundaries before tagging.",
                "We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #TARGET_REF ] , Brill 's [ #REFa ] , and MaxEnt [ #REF ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .",
                "The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.",
                "This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations. Consequently a POS tagger can naturally treat them similarly to any other ambiguous words. There is, however, one difference in the implementation of such a tagger. Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #TARGET_REF ] , Brill 's [ #REFa ] , and MaxEnt [ #REF ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.",
        "output": "nan:In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations. Consequently a POS tagger can naturally treat them similarly to any other ambiguous words. There is, however, one difference in the implementation of such a tagger. Normally, a POS tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #TARGET_REF<USE/> ] , Brill 's [ #REFa ] , and MaxEnt [ #REF ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens . The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions."
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document .",
                "Since, as seen in section 2.1, Brown clusters are hierarchical, we use features corresponding to prefixes of the path from the root to the leaf for each word type."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:Following #TARGET_REF , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document . Since, as seen in section 2.1, Brown clusters are hierarchical, we use features corresponding to prefixes of the path from the root to the leaf for each word type.",
        "output": "introduction:Following #TARGET_REF<FUT/> , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document . Since, as seen in section 2.1, Brown clusters are hierarchical, we use features corresponding to prefixes of the path from the root to the leaf for each word type."
    },
    {
        "gold": {
            "text": [
                "w,x,y P (v|w)P (w, x)P (y|x)P (z|y), implemented by composing 4 machines. 6,7",
                "here are also procedures for defining weighted FSTs that are not probabilistic (#REF).",
                "Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs).",
                "A more subtle example is weighted FSAs that approximate PCFGs ( #TARGET_REF ; #REF ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .",
                "These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:w,x,y P (v|w)P (w, x)P (y|x)P (z|y), implemented by composing 4 machines. 6,7 here are also procedures for defining weighted FSTs that are not probabilistic (#REF). Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs). A more subtle example is weighted FSAs that approximate PCFGs ( #TARGET_REF ; #REF ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation . These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution.",
        "output": "introduction:w,x,y P (v|w)P (w, x)P (y|x)P (z|y), implemented by composing 4 machines. 6,7 here are also procedures for defining weighted FSTs that are not probabilistic (#REF). Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs). A more subtle example is weighted FSAs that approximate PCFGs ( #TARGET_REF<BACK/> ; #REF ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation . These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution."
    },
    {
        "gold": {
            "text": [
                "Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #TARGET_REF ; #REF ) .",
                "Our task is closer to the work of #REF, who looked at the problem of intellectual attribution in scientific texts."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "related work:Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #TARGET_REF ; #REF ) . Our task is closer to the work of #REF, who looked at the problem of intellectual attribution in scientific texts.",
        "output": "related work:Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #TARGET_REF<USE/> ; #REF ) . Our task is closer to the work of #REF, who looked at the problem of intellectual attribution in scientific texts."
    },
    {
        "gold": {
            "text": [
                "The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features.",
                "We first identified the most informative unigrams and bigrams using the information gain measure ( #REF ) , and then selected only the positive outcome predictors using odds ratio ( #TARGET_REF ) .",
                "Diseasespecific terms, such as rheumatoid arthritis, were then manually removed.",
                "Finally, the list of features was revised by the registered nurse who participated in the annotation effort.",
                "This classifier also outputs the probability of a class assignment."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "nan:The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features. We first identified the most informative unigrams and bigrams using the information gain measure ( #REF ) , and then selected only the positive outcome predictors using odds ratio ( #TARGET_REF ) . Diseasespecific terms, such as rheumatoid arthritis, were then manually removed. Finally, the list of features was revised by the registered nurse who participated in the annotation effort. This classifier also outputs the probability of a class assignment.",
        "output": "nan:The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features. We first identified the most informative unigrams and bigrams using the information gain measure ( #REF ) , and then selected only the positive outcome predictors using odds ratio ( #TARGET_REF<FUT/> ) . Diseasespecific terms, such as rheumatoid arthritis, were then manually removed. Finally, the list of features was revised by the registered nurse who participated in the annotation effort. This classifier also outputs the probability of a class assignment."
    },
    {
        "gold": {
            "text": [
                "The language grounding problem has come in many different flavors with just as many different approaches.",
                "Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (#REF).",
                "Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (#REF) or robot commands (#REF;#REF).",
                "Some efforts have tackled tasks such as automatic image caption generation ( #REFa ; #REF ) , text illustration ( #REF ) , or automatic location identification of Twitter users ( #REF ; #REF ; #TARGET_REF ) ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (#REF). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (#REF) or robot commands (#REF;#REF). Some efforts have tackled tasks such as automatic image caption generation ( #REFa ; #REF ) , text illustration ( #REF ) , or automatic location identification of Twitter users ( #REF ; #REF ; #TARGET_REF ) .",
        "output": "introduction:The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (#REF). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (#REF) or robot commands (#REF;#REF). Some efforts have tackled tasks such as automatic image caption generation ( #REFa ; #REF ) , text illustration ( #REF ) , or automatic location identification of Twitter users ( #REF ; #REF ; #TARGET_REF<BACK/> ) ."
    },
    {
        "gold": {
            "text": [
                "Our results also confirm the insights gained by #TARGET_REF , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .",
                "Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "experiments:Our results also confirm the insights gained by #TARGET_REF , who observed that in crossdomain polarity analysis adding more training data is not always beneficial . Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data.",
        "output": "experiments:Our results also confirm the insights gained by #TARGET_REF<USE/> , who observed that in crossdomain polarity analysis adding more training data is not always beneficial . Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data."
    },
    {
        "gold": {
            "text": [
                "In this paper, we show that the strongly equivalent grammars enable the sharing of \"parsing techniques\", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.",
                "We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #TARGET_REF ) , which is a large-scale FB-LTAG grammar .",
                "A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .",
                "This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.",
                "We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "introduction:In this paper, we show that the strongly equivalent grammars enable the sharing of \"parsing techniques\", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities. We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #TARGET_REF ) , which is a large-scale FB-LTAG grammar . A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser . This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.",
        "output": "introduction:In this paper, we show that the strongly equivalent grammars enable the sharing of \"parsing techniques\", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities. We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #TARGET_REF<FUT/> ) , which is a large-scale FB-LTAG grammar . A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser . This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing."
    },
    {
        "gold": {
            "text": [
                "Many statistical parsers ( #REF ; #TARGET_REF ; #REF ) are based on a history-based probability model ( #REF ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .",
                "A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.",
                "Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (#REF;#REF;#REF).",
                "In the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.",
                "We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (#REF;#REF).",
                "Because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.",
                "The resulting parser achieves performance far greater than previous approaches to neural network parsing (#REF;#REF), and only marginally below the current state-of-the-art for parsing the Penn Treebank."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:Many statistical parsers ( #REF ; #TARGET_REF ; #REF ) are based on a history-based probability model ( #REF ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse . A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated. Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (#REF;#REF;#REF). In the work presented here, we automatically induce a finite set of features to represent the unbounded parse history. We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (#REF;#REF). Because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation. The resulting parser achieves performance far greater than previous approaches to neural network parsing (#REF;#REF), and only marginally below the current state-of-the-art for parsing the Penn Treebank.",
        "output": "introduction:Many statistical parsers ( #REF ; #TARGET_REF<BACK/> ; #REF ) are based on a history-based probability model ( #REF ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse . A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated. Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (#REF;#REF;#REF). In the work presented here, we automatically induce a finite set of features to represent the unbounded parse history. We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (#REF;#REF). Because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation. The resulting parser achieves performance far greater than previous approaches to neural network parsing (#REF;#REF), and only marginally below the current state-of-the-art for parsing the Penn Treebank."
    },
    {
        "gold": {
            "text": [
                "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (#REF), \"domain closure as- sumption\" (ibid.), \"domain circumscription\" (cf. #TARGET_REF), and their kin.",
                "Similarly, the notion of R+ M-abduction is spiritually related to the \"abduc- tive inference\" of #REF, the \"diagnosis from first principles\" of #REF, \"explainability\" of #REF, and the subset principle of #REF.",
                "But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.",
                "These connections are being examined elsewhere (Zadrozny forthcoming)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (#REF), \"domain closure as- sumption\" (ibid.), \"domain circumscription\" (cf. #TARGET_REF), and their kin. Similarly, the notion of R+ M-abduction is spiritually related to the \"abduc- tive inference\" of #REF, the \"diagnosis from first principles\" of #REF, \"explainability\" of #REF, and the subset principle of #REF. But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming).",
        "output": "introduction:Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (#REF), \"domain closure as- sumption\" (ibid.), \"domain circumscription\" (cf. #TARGET_REF<USE/>), and their kin. Similarly, the notion of R+ M-abduction is spiritually related to the \"abduc- tive inference\" of #REF, the \"diagnosis from first principles\" of #REF, \"explainability\" of #REF, and the subset principle of #REF. But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming)."
    },
    {
        "gold": {
            "text": [
                "The first feature represents the part of speech of the word.",
                "We use an in-house statistical tagger ( based on ( #TARGET_REF ) ) to tag the text in which the unknown word occurs .",
                "The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD).",
                "The tag set contains just one tag to identify nouns."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The first feature represents the part of speech of the word. We use an in-house statistical tagger ( based on ( #TARGET_REF ) ) to tag the text in which the unknown word occurs . The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD). The tag set contains just one tag to identify nouns.",
        "output": "experiments:The first feature represents the part of speech of the word. We use an in-house statistical tagger ( based on ( #TARGET_REF<FUT/> ) ) to tag the text in which the unknown word occurs . The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD). The tag set contains just one tag to identify nouns."
    },
    {
        "gold": {
            "text": [
                "We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.",
                "Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary.",
                "(#REF contains further description and discussion of LDOCE.)",
                "In this paper we focus on the exploitation of the LDOCE grammar coding system ; #REF and #TARGET_REF describe further research in Cambridge utilising different types of information available in LDOCE ."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system. Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary. (#REF contains further description and discussion of LDOCE.) In this paper we focus on the exploitation of the LDOCE grammar coding system ; #REF and #TARGET_REF describe further research in Cambridge utilising different types of information available in LDOCE .",
        "output": "introduction:We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system. Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary. (#REF contains further description and discussion of LDOCE.) In this paper we focus on the exploitation of the LDOCE grammar coding system ; #REF and #TARGET_REF<BACK/> describe further research in Cambridge utilising different types of information available in LDOCE ."
    },
    {
        "gold": {
            "text": [
                "With all its strong points, there are a number of restrictions to the proposed approach.",
                "First, in its present form it is suitable only for processing of reasonably \"wellbehaved\" texts that consistently use capitalization (mixed case) and do not contain much noisy data.",
                "Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts.",
                "We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.",
                "This is where robust syntactic systems like SATZ ( #TARGET_REF ) or the POS tagger reported in #REF , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage ."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "conclusion:With all its strong points, there are a number of restrictions to the proposed approach. First, in its present form it is suitable only for processing of reasonably \"wellbehaved\" texts that consistently use capitalization (mixed case) and do not contain much noisy data. Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts. We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. This is where robust syntactic systems like SATZ ( #TARGET_REF ) or the POS tagger reported in #REF , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .",
        "output": "conclusion:With all its strong points, there are a number of restrictions to the proposed approach. First, in its present form it is suitable only for processing of reasonably \"wellbehaved\" texts that consistently use capitalization (mixed case) and do not contain much noisy data. Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts. We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. This is where robust syntactic systems like SATZ ( #TARGET_REF<USE/> ) or the POS tagger reported in #REF , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage ."
    },
    {
        "gold": {
            "text": [
                "The RenTAL system is implemented in LiL-FeS (#REF) 2 .",
                "LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (#REF;.",
                "We applied our system to the XTAG English grammar (The XTAG Research #REF) 3 , which is a large-scale FB-LTAG grammar for English.",
                "The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #TARGET_REF ) 6 ( the average length is 6.32 words ) .",
                "This result empirically attested the strong equivalence of our algorithm."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:The RenTAL system is implemented in LiL-FeS (#REF) 2 . LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (#REF;. We applied our system to the XTAG English grammar (The XTAG Research #REF) 3 , which is a large-scale FB-LTAG grammar for English. The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #TARGET_REF ) 6 ( the average length is 6.32 words ) . This result empirically attested the strong equivalence of our algorithm.",
        "output": "experiments:The RenTAL system is implemented in LiL-FeS (#REF) 2 . LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (#REF;. We applied our system to the XTAG English grammar (The XTAG Research #REF) 3 , which is a large-scale FB-LTAG grammar for English. The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #TARGET_REF<FUT/> ) 6 ( the average length is 6.32 words ) . This result empirically attested the strong equivalence of our algorithm."
    },
    {
        "gold": {
            "text": [
                "The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.",
                "Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.",
                "#TARGET_REF claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct .",
                "#REF and #REF take a similar approach, but within a different theoretical framework.",
                "Previous versions of our work, as described in #REF also assume that phrasing is dependent on predicate-argument structure.",
                "The problem here is that the phrasing in observed data often ignores the argument status of constituents.",
                "In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.",
                "All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.",
                "(The complement in 17a and the adjuncts in 17b-f are italicized.)",
                "The relation between discourse and prosodic phrasing has been examined in some detail by #REF, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "introduction:The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations. Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing. #TARGET_REF claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct . #REF and #REF take a similar approach, but within a different theoretical framework. Previous versions of our work, as described in #REF also assume that phrasing is dependent on predicate-argument structure. The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts. All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others. (The complement in 17a and the adjuncts in 17b-f are italicized.) The relation between discourse and prosodic phrasing has been examined in some detail by #REF, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.",
        "output": "introduction:The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations. Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing. #TARGET_REF<BACK/> claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct . #REF and #REF take a similar approach, but within a different theoretical framework. Previous versions of our work, as described in #REF also assume that phrasing is dependent on predicate-argument structure. The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts. All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others. (The complement in 17a and the adjuncts in 17b-f are italicized.) The relation between discourse and prosodic phrasing has been examined in some detail by #REF, who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse."
    },
    {
        "gold": {
            "text": [
                "Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing \"there exists\" by \"all\" in the above definitions (cf. #TARGET_REFb ).",
                "We will have, however, no need for \"strong\" notions in this paper.",
                "Also, in a practical system, \"satisfies\" should be probably replaced by \"violates fewest.\""
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing \"there exists\" by \"all\" in the above definitions (cf. #TARGET_REFb ). We will have, however, no need for \"strong\" notions in this paper. Also, in a practical system, \"satisfies\" should be probably replaced by \"violates fewest.\"",
        "output": "introduction:Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing \"there exists\" by \"all\" in the above definitions (cf. #TARGET_REF<USE/>b ). We will have, however, no need for \"strong\" notions in this paper. Also, in a practical system, \"satisfies\" should be probably replaced by \"violates fewest.\""
    },
    {
        "gold": {
            "text": [
                "For compound splitting , we follow #TARGET_REF , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .",
                "Other approaches use less deep linguistic resources (e.g., POS-tags #REF) or are (almost) knowledge-free (e.g., #REF).",
                "Compound merging is less well studied.",
                "#REF used a simple, list-based merging approach, merging all consecutive words included in a merging list.",
                "This approach resulted in too many compounds.",
                "We follow #REF, for compound merging.",
                "We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "related work:For compound splitting , we follow #TARGET_REF , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies . Other approaches use less deep linguistic resources (e.g., POS-tags #REF) or are (almost) knowledge-free (e.g., #REF). Compound merging is less well studied. #REF used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow #REF, for compound merging. We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets.",
        "output": "related work:For compound splitting , we follow #TARGET_REF<FUT/> , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies . Other approaches use less deep linguistic resources (e.g., POS-tags #REF) or are (almost) knowledge-free (e.g., #REF). Compound merging is less well studied. #REF used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow #REF, for compound merging. We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets."
    },
    {
        "gold": {
            "text": [
                "This section has given an overview of the approach to history-based expectation processing.",
                "The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented.",
                "The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve.",
                "The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.",
                "An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #REF , #TARGET_REF ) .",
                "The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.",
                "The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, #REF).",
                "[The current system should be distinguished from an earlier voice system (VNLC, #REF), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.]"
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "experiments:This section has given an overview of the approach to history-based expectation processing. The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented. The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #REF , #TARGET_REF ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, #REF). [The current system should be distinguished from an earlier voice system (VNLC, #REF), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.]",
        "output": "experiments:This section has given an overview of the approach to history-based expectation processing. The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented. The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #REF , #TARGET_REF<BACK/> ) . The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, #REF). [The current system should be distinguished from an earlier voice system (VNLC, #REF), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.]"
    },
    {
        "gold": {
            "text": [
                "We decided to train the tagger with the minimum of preannotated resources.",
                "First, we used 20,000 tagged words to \"bootstrap\" the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision.",
                "We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%.",
                "This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #REF ] or Brill 's [ #TARGET_REFb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns .",
                "Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.",
                "Periods as many other closed-class words cannot be successfully covered by such technique."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "nan:We decided to train the tagger with the minimum of preannotated resources. First, we used 20,000 tagged words to \"bootstrap\" the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision. We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #REF ] or Brill 's [ #TARGET_REFb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent. Periods as many other closed-class words cannot be successfully covered by such technique.",
        "output": "nan:We decided to train the tagger with the minimum of preannotated resources. First, we used 20,000 tagged words to \"bootstrap\" the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision. We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #REF ] or Brill 's [ #TARGET_REF<USE/>b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns . Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent. Periods as many other closed-class words cannot be successfully covered by such technique."
    },
    {
        "gold": {
            "text": [
                "The probability model we use is generative and history-based.",
                "Generative models are expressed in terms of a stochastic process which generates both the phrase structure tree and the input sentence.",
                "At each step, the process chooses a characteristic of the tree or predicts a word in the sentence.",
                "This sequence of decisions is the derivation of the tree, which we will denote d1,..., dm .",
                "Because there is a one-to-one mapping from phrase structure trees to our derivations, the probability of a derivation P(di,..., dm) is equal to the joint probability of the derivation's tree and the input sentence.",
                "The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation.",
                "in history-based models ( #TARGET_REF ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .",
                "This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "method:The probability model we use is generative and history-based. Generative models are expressed in terms of a stochastic process which generates both the phrase structure tree and the input sentence. At each step, the process chooses a characteristic of the tree or predicts a word in the sentence. This sequence of decisions is the derivation of the tree, which we will denote d1,..., dm . Because there is a one-to-one mapping from phrase structure trees to our derivations, the probability of a derivation P(di,..., dm) is equal to the joint probability of the derivation's tree and the input sentence. The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation. in history-based models ( #TARGET_REF ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i . This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions.",
        "output": "method:The probability model we use is generative and history-based. Generative models are expressed in terms of a stochastic process which generates both the phrase structure tree and the input sentence. At each step, the process chooses a characteristic of the tree or predicts a word in the sentence. This sequence of decisions is the derivation of the tree, which we will denote d1,..., dm . Because there is a one-to-one mapping from phrase structure trees to our derivations, the probability of a derivation P(di,..., dm) is equal to the joint probability of the derivation's tree and the input sentence. The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation. in history-based models ( #TARGET_REF<FUT/> ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i . This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions."
    },
    {
        "gold": {
            "text": [
                "Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #TARGET_REFa ) .",
                "Thus the system can respond immediately after user pauses when the user has the initiative.",
                "When the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (#REF)."
            ],
            "label": [
                "BACK"
            ]
        },
        "input": "nan:Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #TARGET_REFa ) . Thus the system can respond immediately after user pauses when the user has the initiative. When the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (#REF).",
        "output": "nan:Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #TARGET_REF<BACK/>a ) . Thus the system can respond immediately after user pauses when the user has the initiative. When the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (#REF)."
    },
    {
        "gold": {
            "text": [
                "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (#REF), \"domain closure assumption\" (ibid.),",
                "\"domain circumscription\" (cf.",
                "#REF), and their kin.",
                "Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of #REF, the \"diagnosis from first principles\" of #REF, \"explainability\" of #REF, and the subset principle of #TARGET_REF .",
                "But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.",
                "These connections are being examined elsewhere (Zadrozny forthcoming)."
            ],
            "label": [
                "USE"
            ]
        },
        "input": "introduction:Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (#REF), \"domain closure assumption\" (ibid.), \"domain circumscription\" (cf. #REF), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of #REF, the \"diagnosis from first principles\" of #REF, \"explainability\" of #REF, and the subset principle of #TARGET_REF . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming).",
        "output": "introduction:Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (#REF), \"domain closure assumption\" (ibid.), \"domain circumscription\" (cf. #REF), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of #REF, the \"diagnosis from first principles\" of #REF, \"explainability\" of #REF, and the subset principle of #TARGET_REF<USE/> . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming)."
    },
    {
        "gold": {
            "text": [
                "Association Norms ( AN ) is a collection of association norms collected by Schulte im #TARGET_REF .",
                "In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.",
                "With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.",
                "After removing responses given only once in the entire study, the data set contains a total of 95,214 cue-response pairs for 1,012 nouns and 5,716 response types."
            ],
            "label": [
                "FUT"
            ]
        },
        "input": "experiments:Association Norms ( AN ) is a collection of association norms collected by Schulte im #TARGET_REF . In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind. With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words. After removing responses given only once in the entire study, the data set contains a total of 95,214 cue-response pairs for 1,012 nouns and 5,716 response types.",
        "output": "experiments:Association Norms ( AN ) is a collection of association norms collected by Schulte im #TARGET_REF<FUT/> . In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind. With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words. After removing responses given only once in the entire study, the data set contains a total of 95,214 cue-response pairs for 1,012 nouns and 5,716 response types."
    }
]