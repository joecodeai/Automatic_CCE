[
    {
        "gold": {
            "text": [
                "This paper considers the reading comprehension task in which some discrete-reasoning abilities are needed to correctly answer questions.",
                "Specifically, we focus on a new reading comprehension dataset called DROP #TARGET_REF , which requires Discrete Reasoning Over the content of Paragraphs to obtain the final answer.",
                "Unlike previous benchmarks such as CNN/DM (#REF) and SQuAD (#REF) that have been well solved #REF) , DROP is substantially more challenging in three ways.",
                "First, the answers to 1 https://github.com/huminghao16/MTMSN the questions involve a wide range of types such as numbers, dates, or text strings.",
                "Therefore, various kinds of prediction strategies are required to successfully find the answers."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "This paper considers the reading comprehension task in which some discrete-reasoning abilities are needed to correctly answer questions. Specifically, we focus on a new reading comprehension dataset called DROP #TARGET_REF , which requires Discrete Reasoning Over the content of Paragraphs to obtain the final answer. Unlike previous benchmarks such as CNN/DM (#REF) and SQuAD (#REF) that have been well solved #REF) , DROP is substantially more challenging in three ways. First, the answers to 1 https://github.com/huminghao16/MTMSN the questions involve a wide range of types such as numbers, dates, or text strings. Therefore, various kinds of prediction strategies are required to successfully find the answers.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Specifically, we focus on a new reading comprehension dataset called DROP #TARGET_REF , which requires Discrete Reasoning Over the content of Paragraphs to obtain the final answer.\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 2 gives an overview of our model that aims to combine neural reading comprehension with numerical reasoning.",
                "Our model uses BERT (#REF) as encoder: we map word embeddings into contextualized representations using pre-trained Transformer blocks (#REF ) ( §3.1).",
                "Based on the representations, we employ a multi-type answer predictor that is able to produce four answer types: (1) span from the text; (2) arithmetic expression; (3) count number; (4) negation on numbers ( §3.2).",
                "Following #TARGET_REF , we first predict the answer type of a given passage-question pair, and then adopt individual prediction strategies.",
                "To support multispan extraction ( §3.3), the model explicitly predicts the number of answer spans."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Figure 2 gives an overview of our model that aims to combine neural reading comprehension with numerical reasoning. Our model uses BERT (#REF) as encoder: we map word embeddings into contextualized representations using pre-trained Transformer blocks (#REF ) ( §3.1). Based on the representations, we employ a multi-type answer predictor that is able to produce four answer types: (1) span from the text; (2) arithmetic expression; (3) count number; (4) negation on numbers ( §3.2). Following #TARGET_REF , we first predict the answer type of a given passage-question pair, and then adopt individual prediction strategies. To support multispan extraction ( §3.3), the model explicitly predicts the number of answer spans.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following #TARGET_REF , we first predict the answer type of a given passage-question pair, and then adopt individual prediction strategies.\"]}"
    },
    {
        "gold": {
            "text": [
                "Rather than restricting the answer to always be a span of text, the discrete-reasoning reading comprehension task involves different answer types (e.g., number, date, span of text).",
                "Following #TARGET_REF , we design a multi-type answer predictor to selectively produce different kinds of answers such as span, count number, and arithmetic expression.",
                "To further increase answer coverage, we propose adding a new answer type to support logical negation.",
                "Moreover, unlike prior work that separately predicts passage spans and question spans, our approach directly extracts spans from the input sequence.",
                "Answer type prediction Inspired by the Augmented QANet model (#REF) , we use the contextualized token representations from the last four blocks (H L−3 , ..., H L ) as the inputs to our answer predictor, which are denoted as M 0 , M 1 , M 2 , M 3 , respectively."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Rather than restricting the answer to always be a span of text, the discrete-reasoning reading comprehension task involves different answer types (e.g., number, date, span of text). Following #TARGET_REF , we design a multi-type answer predictor to selectively produce different kinds of answers such as span, count number, and arithmetic expression. To further increase answer coverage, we propose adding a new answer type to support logical negation. Moreover, unlike prior work that separately predicts passage spans and question spans, our approach directly extracts spans from the input sequence. Answer type prediction Inspired by the Augmented QANet model (#REF) , we use the contextualized token representations from the last four blocks (H L−3 , ..., H L ) as the inputs to our answer predictor, which are denoted as M 0 , M 1 , M 2 , M 3 , respectively.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following #TARGET_REF , we design a multi-type answer predictor to selectively produce different kinds of answers such as span, count number, and arithmetic expression.\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #REF , we design a multi-type answer predictor to selectively produce different kinds of answers such as span, count number, and arithmetic expression.",
                "To further increase answer coverage, we propose adding a new answer type to support logical negation.",
                "Moreover, unlike prior work that separately predicts passage spans and question spans, our approach directly extracts spans from the input sequence.",
                "Answer type prediction Inspired by the Augmented QANet model #TARGET_REF , we use the contextualized token representations from the last four blocks (H L−3 , ..., H L ) as the inputs to our answer predictor, which are denoted as M 0 , M 1 , M 2 , M 3 , respectively.",
                "To predict the answer type, we first split the representation M 2 into a question representation Q 2 and a passage representation P 2 according to the index of intermediate [SEP] token."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Following #REF , we design a multi-type answer predictor to selectively produce different kinds of answers such as span, count number, and arithmetic expression. To further increase answer coverage, we propose adding a new answer type to support logical negation. Moreover, unlike prior work that separately predicts passage spans and question spans, our approach directly extracts spans from the input sequence. Answer type prediction Inspired by the Augmented QANet model #TARGET_REF , we use the contextualized token representations from the last four blocks (H L−3 , ..., H L ) as the inputs to our answer predictor, which are denoted as M 0 , M 1 , M 2 , M 3 , respectively. To predict the answer type, we first split the representation M 2 into a question representation Q 2 and a passage representation P 2 according to the index of intermediate [SEP] token.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Answer type prediction Inspired by the Augmented QANet model #TARGET_REF , we use the contextualized token representations from the last four blocks (H L\\u22123 , ..., H L ) as the inputs to our answer predictor, which are denoted as M 0 , M 1 , M 2 , M 3 , respectively.\"]}"
    },
    {
        "gold": {
            "text": [
                "Since DROP does not indicate the answer type but only provides the answer string, we therefore adopt the weakly supervised annotation scheme, as suggested in #REF; #TARGET_REF .",
                "We find all possible annotations that point to the gold answer, including matching spans, arithmetic expressions, correct count numbers, negation operations, and the number of spans.",
                "We use simple rules to search over all mentioned numbers to find potential negations.",
                "That is, if 100 minus a number is equal to the answer, then a negation occurs on this number.",
                "Besides, we only search the addition/subtraction of three numbers at most due to the exponential search space."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Since DROP does not indicate the answer type but only provides the answer string, we therefore adopt the weakly supervised annotation scheme, as suggested in #REF; #TARGET_REF . We find all possible annotations that point to the gold answer, including matching spans, arithmetic expressions, correct count numbers, negation operations, and the number of spans. We use simple rules to search over all mentioned numbers to find potential negations. That is, if 100 minus a number is equal to the answer, then a negation occurs on this number. Besides, we only search the addition/subtraction of three numbers at most due to the exponential search space.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Since DROP does not indicate the answer type but only provides the answer string, we therefore adopt the weakly supervised annotation scheme, as suggested in #REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Dataset We consider the reading comprehension benchmark that requires Discrete Reasoning Over Paragraphs (DROP) #TARGET_REF prehensive understanding of the context as well as the ability of numerical reasoning are required.",
                "Model settings We build our model upon two publicly available uncased versions of BERT: BERT BASE and BERT LARGE 2 , and refer readers to #REF for details on model sizes.",
                "We use Adam optimizer with a learning rate of 3e-5 and warmup over the first 5% steps to train.",
                "The maximum number of epochs is set to 10 for base models and 5 for large models, while the batch size is 12 or 24 respectively.",
                "A dropout probability of 0.1 is used unless stated otherwise."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Dataset We consider the reading comprehension benchmark that requires Discrete Reasoning Over Paragraphs (DROP) #TARGET_REF prehensive understanding of the context as well as the ability of numerical reasoning are required. Model settings We build our model upon two publicly available uncased versions of BERT: BERT BASE and BERT LARGE 2 , and refer readers to #REF for details on model sizes. We use Adam optimizer with a learning rate of 3e-5 and warmup over the first 5% steps to train. The maximum number of epochs is set to 10 for base models and 5 for large models, while the batch size is 12 or 24 respectively. A dropout probability of 0.1 is used unless stated otherwise.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Dataset We consider the reading comprehension benchmark that requires Discrete Reasoning Over Paragraphs (DROP) #TARGET_REF prehensive understanding of the context as well as the ability of numerical reasoning are required.\"]}"
    },
    {
        "gold": {
            "text": [
                "Despite their success, these datasets only require shallow pattern matching and simple logical reasoning, thus being well solved #REF) .",
                "Recently, #TARGET_REF released a new benchmark named DROP that demands discrete reasoning as well as deeper paragraph understanding to find the answers.",
                "#REF introduced a dataset consisting of different types of mathematics problems to focuses on mathematical computation.",
                "We choose to work on DROP to test both the numerical reasoning and linguistic comprehension abilities.",
                "Neural reading models Previous neural reading models, such as BiDAF (#REF) , R-Net (#REF) , QANet (#REF) , Reinforced Mreader (#REF) , are usually designed to extract a continuous span of text as the answer."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                1,
                0
            ]
        },
        "input": "Despite their success, these datasets only require shallow pattern matching and simple logical reasoning, thus being well solved #REF) . Recently, #TARGET_REF released a new benchmark named DROP that demands discrete reasoning as well as deeper paragraph understanding to find the answers. #REF introduced a dataset consisting of different types of mathematics problems to focuses on mathematical computation. We choose to work on DROP to test both the numerical reasoning and linguistic comprehension abilities. Neural reading models Previous neural reading models, such as BiDAF (#REF) , R-Net (#REF) , QANet (#REF) , Reinforced Mreader (#REF) , are usually designed to extract a continuous span of text as the answer.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Recently, #TARGET_REF released a new benchmark named DROP that demands discrete reasoning as well as deeper paragraph understanding to find the answers.\", \"We choose to work on DROP to test both the numerical reasoning and linguistic comprehension abilities.\"]}"
    },
    {
        "gold": {
            "text": [
                "where ⊗ denotes the outer product between the vector g and each token representation in M.",
                "Arithmetic expression In order to model the process of performing addition or subtraction among multiple numbers mentioned in the passage, we assign a three-way categorical variable (plus, minus, or zero) for each number to indicate its sign, similar to #TARGET_REF .",
                "As a result, an arithmetic expression that has a number as the final answer can be obtained and easily evaluated.",
                "Specifically, for each number mentioned in the passage, we gather its corresponding representation from the concatenation of M 2 and M 3 , eventually yielding U = (u 1 , ..., u N ) ∈ R N ×2 * D where N numbers exist.",
                "Then the probabilities of the i-th number being assigned a plus, minus or zero is computed as:"
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "where ⊗ denotes the outer product between the vector g and each token representation in M. Arithmetic expression In order to model the process of performing addition or subtraction among multiple numbers mentioned in the passage, we assign a three-way categorical variable (plus, minus, or zero) for each number to indicate its sign, similar to #TARGET_REF . As a result, an arithmetic expression that has a number as the final answer can be obtained and easily evaluated. Specifically, for each number mentioned in the passage, we gather its corresponding representation from the concatenation of M 2 and M 3 , eventually yielding U = (u 1 , ..., u N ) ∈ R N ×2 * D where N numbers exist. Then the probabilities of the i-th number being assigned a plus, minus or zero is computed as:",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Arithmetic expression In order to model the process of performing addition or subtraction among multiple numbers mentioned in the passage, we assign a three-way categorical variable (plus, minus, or zero) for each number to indicate its sign, similar to #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Baselines Following the implementation of Augmented QANet (NAQANet) #TARGET_REF , we introduce a similar baseline called Augmented BERT (NABERT).",
                "The main difference is that we replace the encoder of QANet (#REF) with the pre-trained Transformer blocks (#REF) .",
                "Moreover, it also supports the prediction of various answer types such as span, arithmetic expression, and count number."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "Baselines Following the implementation of Augmented QANet (NAQANet) #TARGET_REF , we introduce a similar baseline called Augmented BERT (NABERT). The main difference is that we replace the encoder of QANet (#REF) with the pre-trained Transformer blocks (#REF) . Moreover, it also supports the prediction of various answer types such as span, arithmetic expression, and count number.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Baselines Following the implementation of Augmented QANet (NAQANet) #TARGET_REF , we introduce a similar baseline called Augmented BERT (NABERT).\"]}"
    },
    {
        "gold": {
            "text": [
                "Polysynthetic languages pose a challenge for morphological analysis due to the rootmorpheme complexity and to the word class \"squish\".",
                "In addition, many of these polysynthetic languages are low-resource.",
                "We propose unsupervised approaches for morphological segmentation of low-resource polysynthetic languages based on Adaptor Grammars (AG) #TARGET_REF .",
                "We experiment with four languages from the Uto-Aztecan family.",
                "Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages."
            ],
            "label": [
                "USE",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                1
            ]
        },
        "input": "Polysynthetic languages pose a challenge for morphological analysis due to the rootmorpheme complexity and to the word class \"squish\". In addition, many of these polysynthetic languages are low-resource. We propose unsupervised approaches for morphological segmentation of low-resource polysynthetic languages based on Adaptor Grammars (AG) #TARGET_REF . We experiment with four languages from the Uto-Aztecan family. Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages.",
        "output": "{\"label\": [\"USE\", \"EXT\"], \"context\": [\"We propose unsupervised approaches for morphological segmentation of low-resource polysynthetic languages based on Adaptor Grammars (AG) #TARGET_REF .\", \"We experiment with four languages from the Uto-Aztecan family.\", \"Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages.\"]}"
    },
    {
        "gold": {
            "text": [
                "Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (#REF; #REF; #REF #REF .",
                "Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal.",
                "We use the datasets introduced by #REF in an unsupervised fashion (unsegmented words).",
                "We design several AG learning setups: 1) use the best-on-average AG setup from #TARGET_REF ; 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from #REF ; 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3).",
                "We show that the AG-based approaches outperform other unsupervised methods -M orf essor (#REF) and M orphoChain (#REF) ) -, and that for two of the languages (NH and YN), the best AG-based approaches outperform the best supervised methods (Section 4)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (#REF; #REF; #REF #REF . Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal. We use the datasets introduced by #REF in an unsupervised fashion (unsegmented words). We design several AG learning setups: 1) use the best-on-average AG setup from #TARGET_REF ; 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from #REF ; 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3). We show that the AG-based approaches outperform other unsupervised methods -M orf essor (#REF) and M orphoChain (#REF) ) -, and that for two of the languages (NH and YN), the best AG-based approaches outperform the best supervised methods (Section 4).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We design several AG learning setups: 1) use the best-on-average AG setup from #TARGET_REF ; 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from #REF ; 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3).\"]}"
    },
    {
        "gold": {
            "text": [
                "An Adaptor Grammar is typically composed of a PCFG and an adaptor that adapts the probabilities of individual subtrees.",
                "For morphological segmentation, a PCFG is a morphological grammar that specifies word structure, where AGs learn latent tree structures given a list of words.",
                "In this paper, we experiment with the grammars and the learning setups proposed by #TARGET_REF , which we outline briefly below.",
                "Grammars.",
                "We use the nine grammars from Eskander et al. (2016 Eskander et al. ( , 2018 that were designed based on three dimensions: 1) how the grammar models word structure (e.g., prefix-stem-suffix vs. morphemes), 2) the level of abstraction in nonterminals (e.g., compounds, morphemes and submorphemes) and 3) how the output boundaries are specified (see Table 2 for a sample grammars)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "An Adaptor Grammar is typically composed of a PCFG and an adaptor that adapts the probabilities of individual subtrees. For morphological segmentation, a PCFG is a morphological grammar that specifies word structure, where AGs learn latent tree structures given a list of words. In this paper, we experiment with the grammars and the learning setups proposed by #TARGET_REF , which we outline briefly below. Grammars. We use the nine grammars from Eskander et al. (2016 Eskander et al. ( , 2018 that were designed based on three dimensions: 1) how the grammar models word structure (e.g., prefix-stem-suffix vs. morphemes), 2) the level of abstraction in nonterminals (e.g., compounds, morphemes and submorphemes) and 3) how the output boundaries are specified (see Table 2 for a sample grammars).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In this paper, we experiment with the grammars and the learning setups proposed by #TARGET_REF , which we outline briefly below.\"]}"
    },
    {
        "gold": {
            "text": [
                "An Adaptor Grammar is typically composed of a PCFG and an adaptor that adapts the probabilities of individual subtrees.",
                "For morphological segmentation, a PCFG is a morphological grammar that specifies word structure, where AGs learn latent tree structures given a list of words.",
                "In this paper, we experiment with the grammars and the learning setups proposed by #REF , which we outline briefly below.",
                "Grammars.",
                "We use the nine grammars from #TARGET_REF Eskander et al. ( , 2018 that were designed based on three dimensions: 1) how the grammar models word structure (e.g., prefix-stem-suffix vs. morphemes), 2) the level of abstraction in nonterminals (e.g., compounds, morphemes and submorphemes) and 3) how the output boundaries are specified (see Table 2 for a sample grammars)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "An Adaptor Grammar is typically composed of a PCFG and an adaptor that adapts the probabilities of individual subtrees. For morphological segmentation, a PCFG is a morphological grammar that specifies word structure, where AGs learn latent tree structures given a list of words. In this paper, we experiment with the grammars and the learning setups proposed by #REF , which we outline briefly below. Grammars. We use the nine grammars from #TARGET_REF Eskander et al. ( , 2018 that were designed based on three dimensions: 1) how the grammar models word structure (e.g., prefix-stem-suffix vs. morphemes), 2) the level of abstraction in nonterminals (e.g., compounds, morphemes and submorphemes) and 3) how the output boundaries are specified (see Table 2 for a sample grammars).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use the nine grammars from #TARGET_REF Eskander et al. ( , 2018 that were designed based on three dimensions: 1) how the grammar models word structure (e.g., prefix-stem-suffix vs. morphemes), 2) the level of abstraction in nonterminals (e.g., compounds, morphemes and submorphemes) and 3) how the output boundaries are specified (see Table 2 for a sample grammars).\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the nine grammars from Eskander et al. (2016 Eskander et al. ( , 2018 that were designed based on three dimensions: 1) how the grammar models word structure (e.g., prefix-stem-suffix vs. morphemes), 2) the level of abstraction in nonterminals (e.g., compounds, morphemes and submorphemes) and 3) how the output boundaries are specified (see Table 2 for a sample grammars).",
                "For example, the PrStSu+SM grammar models the Table 2 : Sample grammar setups used by Eskander et al. (2018 #TARGET_REF .",
                "Compound = Upper level representation of the word as a sequence of compounds; Morph = affix/morpheme representation as a sequence of morphemes.",
                "SubMorph (SM) = Lower level representation of characters as a sequence of sub-morphemes. \"+\" denotes one or more.",
                "word as a complex prefix, a stem and a complex suffix, where the complex prefix and suffix are composed of zero or more morphemes, and a morpheme is a sequence of sub-morphemes."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We use the nine grammars from Eskander et al. (2016 Eskander et al. ( , 2018 that were designed based on three dimensions: 1) how the grammar models word structure (e.g., prefix-stem-suffix vs. morphemes), 2) the level of abstraction in nonterminals (e.g., compounds, morphemes and submorphemes) and 3) how the output boundaries are specified (see Table 2 for a sample grammars). For example, the PrStSu+SM grammar models the Table 2 : Sample grammar setups used by Eskander et al. (2018 #TARGET_REF . Compound = Upper level representation of the word as a sequence of compounds; Morph = affix/morpheme representation as a sequence of morphemes. SubMorph (SM) = Lower level representation of characters as a sequence of sub-morphemes. \"+\" denotes one or more. word as a complex prefix, a stem and a complex suffix, where the complex prefix and suffix are composed of zero or more morphemes, and a morpheme is a sequence of sub-morphemes.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For example, the PrStSu+SM grammar models the Table 2 : Sample grammar setups used by Eskander et al. (2018 #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Learning Settings.",
                "The input to the learner is a grammar and a vocabulary of unsegmented words.",
                "We consider the three learning settings in #TARGET_REF : Standard, Scholarseeded Knowledge and Cascaded.",
                "The Standard setting is language-independent and fully unsupervised, while in the Scholar-seeded-Knowledge setting, some linguistic knowledge (in the form of affixes taken from grammar books) is seeded into the grammar trees before learning takes place.",
                "The Cascaded setting simulates the effect of seeding scholar knowledge in a language-independent manner by first running an AG of high precision to derive a set of affixes, and then seeding those affixes into the grammars."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Learning Settings. The input to the learner is a grammar and a vocabulary of unsegmented words. We consider the three learning settings in #TARGET_REF : Standard, Scholarseeded Knowledge and Cascaded. The Standard setting is language-independent and fully unsupervised, while in the Scholar-seeded-Knowledge setting, some linguistic knowledge (in the form of affixes taken from grammar books) is seeded into the grammar trees before learning takes place. The Cascaded setting simulates the effect of seeding scholar knowledge in a language-independent manner by first running an AG of high precision to derive a set of affixes, and then seeding those affixes into the grammars.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We consider the three learning settings in #TARGET_REF : Standard, Scholarseeded Knowledge and Cascaded.\"]}"
    },
    {
        "gold": {
            "text": [
                "Language-Independent Morphological Segmenter.",
                "LIMS is the best-on-average AG setup obtained by #TARGET_REF when trained on six languages (English, German, Finnish, Estonian, Turkish and Zulu), which is the Cascaded PrStSu+SM configuration.",
                "We use this AG setup for each of the four languages.",
                "We refer to this system as AG LIM S .",
                "Best AG Configuration per Language."
            ],
            "label": [
                "USE",
                "BACK"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Language-Independent Morphological Segmenter. LIMS is the best-on-average AG setup obtained by #TARGET_REF when trained on six languages (English, German, Finnish, Estonian, Turkish and Zulu), which is the Cascaded PrStSu+SM configuration. We use this AG setup for each of the four languages. We refer to this system as AG LIM S . Best AG Configuration per Language.",
        "output": "{\"label\": [\"USE\", \"BACK\"], \"context\": [\"LIMS is the best-on-average AG setup obtained by #TARGET_REF when trained on six languages (English, German, Finnish, Estonian, Turkish and Zulu), which is the Cascaded PrStSu+SM configuration.\", \"We use this AG setup for each of the four languages.\"]}"
    },
    {
        "gold": {
            "text": [
                "We use this AG setup for each of the four languages.",
                "We refer to this system as AG LIM S .",
                "Best AG Configuration per Language.",
                "In this experimental setup, we consider all nine grammars from #TARGET_REF using both the Standard and the Cascaded approaches and choosing the one that is best for each polysynthetic language by training on the training set and evaluating on the development set.",
                "We denote this system as AG BestL ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We use this AG setup for each of the four languages. We refer to this system as AG LIM S . Best AG Configuration per Language. In this experimental setup, we consider all nine grammars from #TARGET_REF using both the Standard and the Cascaded approaches and choosing the one that is best for each polysynthetic language by training on the training set and evaluating on the development set. We denote this system as AG BestL .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In this experimental setup, we consider all nine grammars from #TARGET_REF using both the Standard and the Cascaded approaches and choosing the one that is best for each polysynthetic language by training on the training set and evaluating on the development set.\"]}"
    },
    {
        "gold": {
            "text": [
                "Unsupervised approaches based on Adaptor Grammars show promise for morphological segmentation of low-resource polysynthetic languages.",
                "We worked with the AG grammars developed by #TARGET_REF Eskander et al. ( , 2018 for languages that are not polysynthetic.",
                "We showed that even when using these approaches and very little data, we can obtain encouraging results, and that using additional unsupervised data is a promising path."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0
            ]
        },
        "input": "Unsupervised approaches based on Adaptor Grammars show promise for morphological segmentation of low-resource polysynthetic languages. We worked with the AG grammars developed by #TARGET_REF Eskander et al. ( , 2018 for languages that are not polysynthetic. We showed that even when using these approaches and very little data, we can obtain encouraging results, and that using additional unsupervised data is a promising path.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We worked with the AG grammars developed by #TARGET_REF Eskander et al. ( , 2018 for languages that are not polysynthetic.\"]}"
    },
    {
        "gold": {
            "text": [
                "We denote this system as AG BestL .",
                "Using Seeded Knowledge.",
                "To approximate the effect of Scholar-seeded-Knowledge in #TARGET_REF, we used the training set to derive affixes and use them as scholar-seeded knowledge added to the grammars (before the learning happens).",
                "However, since affixes and stems are not distinguished in the training annotations from #REF , we only consider the first and last morphemes that appear at least five times.",
                "We call this setup AG Scholar BestL ."
            ],
            "label": [
                "EXT",
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "We denote this system as AG BestL . Using Seeded Knowledge. To approximate the effect of Scholar-seeded-Knowledge in #TARGET_REF, we used the training set to derive affixes and use them as scholar-seeded knowledge added to the grammars (before the learning happens). However, since affixes and stems are not distinguished in the training annotations from #REF , we only consider the first and last morphemes that appear at least five times. We call this setup AG Scholar BestL .",
        "output": "{\"label\": [\"EXT\", \"SIM\"], \"context\": [\"To approximate the effect of Scholar-seeded-Knowledge in #TARGET_REF, we used the training set to derive affixes and use them as scholar-seeded knowledge added to the grammars (before the learning happens).\", \"However, since affixes and stems are not distinguished in the training annotations from #REF , we only consider the first and last morphemes that appear at least five times.\"]}"
    },
    {
        "gold": {
            "text": [
                "We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor Grammars (#REF) .",
                "We experiment with four UtoAztecan languages: Mexicanero (MX), Nahuatl (NH), Wixarika (WX) and Yorem Nokki (YN) (#REF) .",
                "Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (#REF; #REF; #TARGET_REF #REF .",
                "Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal.",
                "We use the datasets introduced by #REF in an unsupervised fashion (unsegmented words)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor Grammars (#REF) . We experiment with four UtoAztecan languages: Mexicanero (MX), Nahuatl (NH), Wixarika (WX) and Yorem Nokki (YN) (#REF) . Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (#REF; #REF; #TARGET_REF #REF . Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal. We use the datasets introduced by #REF in an unsupervised fashion (unsegmented words).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (#REF; #REF; #TARGET_REF #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Evaluating different AG setups.",
                "Table 3 shows the performance of our AG setups on the four languages.",
                "The best AG setup learned for each of the four polysynthetic languages (AG BestL ) is the PrStSu+SM grammar using the Cascaded learning setup.",
                "This is an interesting finding as the Cascaded PrSTSu+SM setup is in fact AG LIM S -the best-on-average AG setup obtained by #TARGET_REF Table 4 : Best AG results compared to supervised approaches from #REF .",
                "Bold indicates best scores."
            ],
            "label": [
                "BACK",
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Evaluating different AG setups. Table 3 shows the performance of our AG setups on the four languages. The best AG setup learned for each of the four polysynthetic languages (AG BestL ) is the PrStSu+SM grammar using the Cascaded learning setup. This is an interesting finding as the Cascaded PrSTSu+SM setup is in fact AG LIM S -the best-on-average AG setup obtained by #TARGET_REF Table 4 : Best AG results compared to supervised approaches from #REF . Bold indicates best scores.",
        "output": "{\"label\": [\"BACK\", \"SIM\"], \"context\": [\"The best AG setup learned for each of the four polysynthetic languages (AG BestL ) is the PrStSu+SM grammar using the Cascaded learning setup.\", \"This is an interesting finding as the Cascaded PrSTSu+SM setup is in fact AG LIM S -the best-on-average AG setup obtained by #TARGET_REF Table 4 : Best AG results compared to supervised approaches from #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Indeed, principled methods are now required that allow us to measure, understand and remove biases in our data in order for these systems to be truly accepted as a prominent part of our lives.",
                "In the domain of text, many modern approaches often begin by embedding the input text data into an embedding space that is used as the first layer in a subsequent deep network [4] , [14] .",
                "These word embeddings have been shown to contain the same biases #TARGET_REF , due to the source data from which they are trained.",
                "In effect, biases from the source data, such as in the differences in representation for men and women, that have been found in many different large-scale studies [5] , [10] , [12] , carry through to the semantic relations in the word embeddings, which become baked into the learning systems that are built on top of them.",
                "In this paper, we make three contributions towards addressing these concerns."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "Indeed, principled methods are now required that allow us to measure, understand and remove biases in our data in order for these systems to be truly accepted as a prominent part of our lives. In the domain of text, many modern approaches often begin by embedding the input text data into an embedding space that is used as the first layer in a subsequent deep network [4] , [14] . These word embeddings have been shown to contain the same biases #TARGET_REF , due to the source data from which they are trained. In effect, biases from the source data, such as in the differences in representation for men and women, that have been found in many different large-scale studies [5] , [10] , [12] , carry through to the semantic relations in the word embeddings, which become baked into the learning systems that are built on top of them. In this paper, we make three contributions towards addressing these concerns.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"In the domain of text, many modern approaches often begin by embedding the input text data into an embedding space that is used as the first layer in a subsequent deep network [4] , [14] .\", \"These word embeddings have been shown to contain the same biases #TARGET_REF , due to the source data from which they are trained.\", \"In effect, biases from the source data, such as in the differences in representation for men and women, that have been found in many different large-scale studies [5] , [10] , [12] , carry through to the semantic relations in the word embeddings, which become baked into the learning systems that are built on top of them.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we make three contributions towards addressing these concerns.",
                "First we propose a new version of the Word Embedding Association Tests (WEATs) studied in #TARGET_REF , designed to demonstrate and quantify bias in word embeddings, which puts them on a firm foundation by using the Linguistic Inquiry and Word Count (LIWC) lexica [17] to systematically detect and measure embedding biases.",
                "With this improved experimental setting, we find that European-American names are viewed more positively than African-American names, male names are more associated with work while female names are more associated with family, and that the academic disciplines of science and maths are more associated with male terms than the arts, which are more associated with female terms.",
                "Using this new methodology, we then find that there is a gender bias in the way different occupations are represented by the embedding.",
                "Furthermore, we use the latest official employment statistics in the UK, and find that there is a correlation between the ratio of men and women working in different occupation roles and how those roles are associated with gender in the word embeddings."
            ],
            "label": [
                "BACK",
                "MOT",
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "In this paper, we make three contributions towards addressing these concerns. First we propose a new version of the Word Embedding Association Tests (WEATs) studied in #TARGET_REF , designed to demonstrate and quantify bias in word embeddings, which puts them on a firm foundation by using the Linguistic Inquiry and Word Count (LIWC) lexica [17] to systematically detect and measure embedding biases. With this improved experimental setting, we find that European-American names are viewed more positively than African-American names, male names are more associated with work while female names are more associated with family, and that the academic disciplines of science and maths are more associated with male terms than the arts, which are more associated with female terms. Using this new methodology, we then find that there is a gender bias in the way different occupations are represented by the embedding. Furthermore, we use the latest official employment statistics in the UK, and find that there is a correlation between the ratio of men and women working in different occupation roles and how those roles are associated with gender in the word embeddings.",
        "output": "{\"label\": [\"BACK\", \"MOT\", \"DIFFER\"], \"context\": [\"First we propose a new version of the Word Embedding Association Tests (WEATs) studied in #TARGET_REF , designed to demonstrate and quantify bias in word embeddings, which puts them on a firm foundation by using the Linguistic Inquiry and Word Count (LIWC) lexica [17] to systematically detect and measure embedding biases.\", \"With this improved experimental setting, we find that European-American names are viewed more positively than African-American names, male names are more associated with work while female names are more associated with family, and that the academic disciplines of science and maths are more associated with male terms than the arts, which are more associated with female terms.\", \"Using this new methodology, we then find that there is a gender bias in the way different occupations are represented by the embedding.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we conduct three experiments on semantic word embeddings.",
                "We first propose a new version of the Word Embedding Association Tests studied in #TARGET_REF by using the LIWC lexica to systematically detect and measure the biases within the embedding, keeping the tests comparable with the same set of target words.",
                "We further extend this work using additional sets of target words, and compare sentiment across male and female names.",
                "Furthermore, we investigate gender bias in words that represent different occupations, comparing these associations with UK national employment statistics.",
                "In the last experiment, we use orthogonal projections [2] to debias our word embeddings, and measure the reduction in the biases demonstrated in the previous two experiments."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "In this paper, we conduct three experiments on semantic word embeddings. We first propose a new version of the Word Embedding Association Tests studied in #TARGET_REF by using the LIWC lexica to systematically detect and measure the biases within the embedding, keeping the tests comparable with the same set of target words. We further extend this work using additional sets of target words, and compare sentiment across male and female names. Furthermore, we investigate gender bias in words that represent different occupations, comparing these associations with UK national employment statistics. In the last experiment, we use orthogonal projections [2] to debias our word embeddings, and measure the reduction in the biases demonstrated in the previous two experiments.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"We first propose a new version of the Word Embedding Association Tests studied in #TARGET_REF by using the LIWC lexica to systematically detect and measure the biases within the embedding, keeping the tests comparable with the same set of target words.\", \"We further extend this work using additional sets of target words, and compare sentiment across male and female names.\", \"Furthermore, we investigate gender bias in words that represent different occupations, comparing these associations with UK national employment statistics.\"]}"
    },
    {
        "gold": {
            "text": [
                "Using the list of occupations from the previous section, we compared their association with each of the genders with the ratio of the actual number of men and women working in those roles, as recorded in the official statistics [15] , where 1 indicates only men work in this role, and 0 only women.",
                "We found that there is a strong, significant correlation (ρ = 0.57, p-value < 10 −6 ) between the word embedding association between gender and occupation and the number of people of each gender in the United Kingdom working in those roles.",
                "This supports a similar finding for U.S. employment statistics using an independent set of occupations found in #TARGET_REF ."
            ],
            "label": [
                "BACK",
                "SIM"
            ],
            "context": [
                0,
                1,
                1
            ]
        },
        "input": "Using the list of occupations from the previous section, we compared their association with each of the genders with the ratio of the actual number of men and women working in those roles, as recorded in the official statistics [15] , where 1 indicates only men work in this role, and 0 only women. We found that there is a strong, significant correlation (ρ = 0.57, p-value < 10 −6 ) between the word embedding association between gender and occupation and the number of people of each gender in the United Kingdom working in those roles. This supports a similar finding for U.S. employment statistics using an independent set of occupations found in #TARGET_REF .",
        "output": "{\"label\": [\"BACK\", \"SIM\"], \"context\": [\"We found that there is a strong, significant correlation (\\u03c1 = 0.57, p-value < 10 \\u22126 ) between the word embedding association between gender and occupation and the number of people of each gender in the United Kingdom working in those roles.\", \"This supports a similar finding for U.S. employment statistics using an independent set of occupations found in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "If we want AI to take a central position in society, we need to be able to detect and remove any source of possible discrimination, to ensure fairness and transparency, and ultimately trust in these learning systems.",
                "Principled methods to measure biases will certainly need to play a central role in this, as will an understanding of the origins of biases, and new developments in methods that can be used to remove biases once detected.",
                "In this paper, we have introduced the LIWC-WEAT, a set of objective tests extending the association tests in #TARGET_REF by using the LIWC lexica to measure bias within word embeddings.",
                "We found bias in both the associations of gender and race, as first described in [3] , while additionally finding that male names have a slightly higher positive association than female names.",
                "Biases found in the embedding were also shown to reflect biases in the real world and the media, where we found a correlation between the number of men and women in an occupation and its association with each set of male and female names."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "If we want AI to take a central position in society, we need to be able to detect and remove any source of possible discrimination, to ensure fairness and transparency, and ultimately trust in these learning systems. Principled methods to measure biases will certainly need to play a central role in this, as will an understanding of the origins of biases, and new developments in methods that can be used to remove biases once detected. In this paper, we have introduced the LIWC-WEAT, a set of objective tests extending the association tests in #TARGET_REF by using the LIWC lexica to measure bias within word embeddings. We found bias in both the associations of gender and race, as first described in [3] , while additionally finding that male names have a slightly higher positive association than female names. Biases found in the embedding were also shown to reflect biases in the real world and the media, where we found a correlation between the number of men and women in an occupation and its association with each set of male and female names.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"In this paper, we have introduced the LIWC-WEAT, a set of objective tests extending the association tests in #TARGET_REF by using the LIWC lexica to measure bias within word embeddings.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we have introduced the LIWC-WEAT, a set of objective tests extending the association tests in [3] by using the LIWC lexica to measure bias within word embeddings.",
                "We found bias in both the associations of gender and race, as first described in #TARGET_REF , while additionally finding that male names have a slightly higher positive association than female names.",
                "Biases found in the embedding were also shown to reflect biases in the real world and the media, where we found a correlation between the number of men and women in an occupation and its association with each set of male and female names.",
                "Finally, using a projection algorithm [2] , we were able to reduce the gender bias shown in the embeddings, resulting in a decrease in the difference between associations for all tests based upon gender.",
                "Further work in this direction will include removing bias in n-gram embeddings, embeddings that include multiple languages and new procedures for both generating better projections to remove a given bias, using debiased embeddings as an input to an upstream system and testing performance, and learning word embeddings which can be generated without chosen directions by construction."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "In this paper, we have introduced the LIWC-WEAT, a set of objective tests extending the association tests in [3] by using the LIWC lexica to measure bias within word embeddings. We found bias in both the associations of gender and race, as first described in #TARGET_REF , while additionally finding that male names have a slightly higher positive association than female names. Biases found in the embedding were also shown to reflect biases in the real world and the media, where we found a correlation between the number of men and women in an occupation and its association with each set of male and female names. Finally, using a projection algorithm [2] , we were able to reduce the gender bias shown in the embeddings, resulting in a decrease in the difference between associations for all tests based upon gender. Further work in this direction will include removing bias in n-gram embeddings, embeddings that include multiple languages and new procedures for both generating better projections to remove a given bias, using debiased embeddings as an input to an upstream system and testing performance, and learning word embeddings which can be generated without chosen directions by construction.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"We found bias in both the associations of gender and race, as first described in #TARGET_REF , while additionally finding that male names have a slightly higher positive association than female names.\", \"Biases found in the embedding were also shown to reflect biases in the real world and the media, where we found a correlation between the number of men and women in an occupation and its association with each set of male and female names.\"]}"
    },
    {
        "gold": {
            "text": [
                "Taking the list of target European-American and African-American names used in #TARGET_REF , we tested each of them for their associated with the positive and negative emotion concepts found in [17] by using the methodology described by Eq. 3 in Sec. II-B, replacing the short list of words used to originally represent pleasant and unpleasant attribute sets.",
                "Our test found that while both European-American names and African-American names are more associated with positive emotions than negative emotions, the test showed that European-American names are more associated with positive emotions than their African-American counterparts, as shown in Fig. 1a .",
                "This finding supports the association test in [3] , where they also found that European-American names were more pleasant than African-American names."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "Taking the list of target European-American and African-American names used in #TARGET_REF , we tested each of them for their associated with the positive and negative emotion concepts found in [17] by using the methodology described by Eq. 3 in Sec. II-B, replacing the short list of words used to originally represent pleasant and unpleasant attribute sets. Our test found that while both European-American names and African-American names are more associated with positive emotions than negative emotions, the test showed that European-American names are more associated with positive emotions than their African-American counterparts, as shown in Fig. 1a . This finding supports the association test in [3] , where they also found that European-American names were more pleasant than African-American names.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Taking the list of target European-American and African-American names used in #TARGET_REF , we tested each of them for their associated with the positive and negative emotion concepts found in [17] by using the methodology described by Eq. 3 in Sec. II-B, replacing the short list of words used to originally represent pleasant and unpleasant attribute sets.\"]}"
    },
    {
        "gold": {
            "text": [
                "The results of our test again support the findings of [3] , with Maths and Science terms being more closely associated with males, while Arts terms are more closely associated with females, as shown in Fig. 1b.",
                "3) Association of Gender with Career and Family : Taking the list of target gendered names used in #TARGET_REF , we tested each of them for their associated with the career and family concepts using the categories of 'work' and 'family' found in LIWC [17] .",
                "As shown in Fig. 1c , we found that the set of male names was more associated with the concept of work, while the female names were more associated with family, mirroring the results found in [3] .",
                "Extending this test, we generated a much larger set of male and female target names from an online list of baby names 1 .",
                "Repeating the same test on this larger set of names, we found that male and female names were much less separated than suggested by previous results, with only minor differences between the two, as shown in Fig. 1d ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The results of our test again support the findings of [3] , with Maths and Science terms being more closely associated with males, while Arts terms are more closely associated with females, as shown in Fig. 1b. 3) Association of Gender with Career and Family : Taking the list of target gendered names used in #TARGET_REF , we tested each of them for their associated with the career and family concepts using the categories of 'work' and 'family' found in LIWC [17] . As shown in Fig. 1c , we found that the set of male names was more associated with the concept of work, while the female names were more associated with family, mirroring the results found in [3] . Extending this test, we generated a much larger set of male and female target names from an online list of baby names 1 . Repeating the same test on this larger set of names, we found that male and female names were much less separated than suggested by previous results, with only minor differences between the two, as shown in Fig. 1d .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"3) Association of Gender with Career and Family : Taking the list of target gendered names used in #TARGET_REF , we tested each of them for their associated with the career and family concepts using the categories of 'work' and 'family' found in LIWC [17] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Taking the list of target European-American and African-American names used in [3] , we tested each of them for their associated with the positive and negative emotion concepts found in [17] by using the methodology described by Eq. 3 in Sec. II-B, replacing the short list of words used to originally represent pleasant and unpleasant attribute sets.",
                "Our test found that while both European-American names and African-American names are more associated with positive emotions than negative emotions, the test showed that European-American names are more associated with positive emotions than their African-American counterparts, as shown in Fig. 1a .",
                "This finding supports the association test in #TARGET_REF , where they also found that European-American names were more pleasant than African-American names."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                1
            ]
        },
        "input": "Taking the list of target European-American and African-American names used in [3] , we tested each of them for their associated with the positive and negative emotion concepts found in [17] by using the methodology described by Eq. 3 in Sec. II-B, replacing the short list of words used to originally represent pleasant and unpleasant attribute sets. Our test found that while both European-American names and African-American names are more associated with positive emotions than negative emotions, the test showed that European-American names are more associated with positive emotions than their African-American counterparts, as shown in Fig. 1a . This finding supports the association test in #TARGET_REF , where they also found that European-American names were more pleasant than African-American names.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Our test found that while both European-American names and African-American names are more associated with positive emotions than negative emotions, the test showed that European-American names are more associated with positive emotions than their African-American counterparts, as shown in Fig. 1a .\", \"This finding supports the association test in #TARGET_REF , where they also found that European-American names were more pleasant than African-American names.\"]}"
    },
    {
        "gold": {
            "text": [
                "A further test was conducted to find the association between words related to different subject disciplines (e.g. arts, maths, science) with each of the genders using the 'he' and 'she' categories from LIWC [17] .",
                "The results of our test again support the findings of #TARGET_REF , with Maths and Science terms being more closely associated with males, while Arts terms are more closely associated with females, as shown in Fig. 1b.",
                "3) Association of Gender with Career and Family : Taking the list of target gendered names used in [3] , we tested each of them for their associated with the career and family concepts using the categories of 'work' and 'family' found in LIWC [17] .",
                "As shown in Fig. 1c , we found that the set of male names was more associated with the concept of work, while the female names were more associated with family, mirroring the results found in [3] .",
                "Extending this test, we generated a much larger set of male and female target names from an online list of baby names 1 ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "A further test was conducted to find the association between words related to different subject disciplines (e.g. arts, maths, science) with each of the genders using the 'he' and 'she' categories from LIWC [17] . The results of our test again support the findings of #TARGET_REF , with Maths and Science terms being more closely associated with males, while Arts terms are more closely associated with females, as shown in Fig. 1b. 3) Association of Gender with Career and Family : Taking the list of target gendered names used in [3] , we tested each of them for their associated with the career and family concepts using the categories of 'work' and 'family' found in LIWC [17] . As shown in Fig. 1c , we found that the set of male names was more associated with the concept of work, while the female names were more associated with family, mirroring the results found in [3] . Extending this test, we generated a much larger set of male and female target names from an online list of baby names 1 .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"The results of our test again support the findings of #TARGET_REF , with Maths and Science terms being more closely associated with males, while Arts terms are more closely associated with females, as shown in Fig. 1b.\"]}"
    },
    {
        "gold": {
            "text": [
                "This included the association of science, mathematics and the arts with gender, the association of male and females names with sentiment, work and family, and the ranking of occupations by their gender association.",
                "In Experiment 1, we previously found that the disciplines of science and maths were more associated with male terms in the embedding, while the arts were closer to female terms.",
                "The association of each of these subject disciplines with gender after orthogonal projection was found to be more balanced, with closer to equal association for both male and female terms, shown in Fig. 3a .",
                "Male and Females names tested in #TARGET_REF showed a clear distinction in their association with work and family respectively, with our replication of the test in Sec. III-B3 finding the same results.",
                "Performing the same tests again after applying the gender projection to both name lists, we wished to quantify the change in associations."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "This included the association of science, mathematics and the arts with gender, the association of male and females names with sentiment, work and family, and the ranking of occupations by their gender association. In Experiment 1, we previously found that the disciplines of science and maths were more associated with male terms in the embedding, while the arts were closer to female terms. The association of each of these subject disciplines with gender after orthogonal projection was found to be more balanced, with closer to equal association for both male and female terms, shown in Fig. 3a . Male and Females names tested in #TARGET_REF showed a clear distinction in their association with work and family respectively, with our replication of the test in Sec. III-B3 finding the same results. Performing the same tests again after applying the gender projection to both name lists, we wished to quantify the change in associations.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Male and Females names tested in #TARGET_REF showed a clear distinction in their association with work and family respectively, with our replication of the test in Sec. III-B3 finding the same results.\"]}"
    },
    {
        "gold": {
            "text": [
                "This transcription bottleneck problem can be handled by translating into a widely spoken language to ensure subsequent interpretability of the collected recordings, and such parallel corpora have been recently created by aligning the collected audio with translations in a well-resourced language (#REF; #REF; #REF) .",
                "Moreover, some linguists suggested that more than one translation should be collected to capture deeper layers of meaning (#REF) .",
                "This work is a contribution to the Computational Language Documentation (CLD) research field, that aims to replace part of the manual steps performed by linguists during language documentation initiatives by automatic approaches.",
                "Here we investigate the unsupervised word discovery and segmentation task, using the bilingual-rooted approach from #TARGET_REF .",
                "There, words in the well-resourced language are aligned to unsegmented phonemes in the endangered language in order to identify group of phonemes, and to cluster them into word-like units."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "This transcription bottleneck problem can be handled by translating into a widely spoken language to ensure subsequent interpretability of the collected recordings, and such parallel corpora have been recently created by aligning the collected audio with translations in a well-resourced language (#REF; #REF; #REF) . Moreover, some linguists suggested that more than one translation should be collected to capture deeper layers of meaning (#REF) . This work is a contribution to the Computational Language Documentation (CLD) research field, that aims to replace part of the manual steps performed by linguists during language documentation initiatives by automatic approaches. Here we investigate the unsupervised word discovery and segmentation task, using the bilingual-rooted approach from #TARGET_REF . There, words in the well-resourced language are aligned to unsegmented phonemes in the endangered language in order to identify group of phonemes, and to cluster them into word-like units.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Here we investigate the unsupervised word discovery and segmentation task, using the bilingual-rooted approach from #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 1 shows some statistics for the produced Multilingual Mboshi parallel corpus.",
                "2 Bilingual Unsupervised Word Segmentation/Discovery Approach: We use the bilingual neuralbased Unsupervised Word Segmentation (UWS) approach from #TARGET_REF to discover words in Mboshi.",
                "In this approach, Neural Machine Translation (NMT) models are trained between language pairs, using as source language the translation (word-level) and as target, the language to document (unsegmented phonemic sequence).",
                "Due to the attention mechanism present in these networks (#REF) , posterior to training, it is possible to retrieve soft-alignment probability matrices between source and target sequences.",
                "These matrices give us sentence-level source-to-target alignment information, and by using it for clustering neighbor phonemes aligned to the same translation word, we are able to create segmentation in the target side."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Table 1 shows some statistics for the produced Multilingual Mboshi parallel corpus. 2 Bilingual Unsupervised Word Segmentation/Discovery Approach: We use the bilingual neuralbased Unsupervised Word Segmentation (UWS) approach from #TARGET_REF to discover words in Mboshi. In this approach, Neural Machine Translation (NMT) models are trained between language pairs, using as source language the translation (word-level) and as target, the language to document (unsegmented phonemic sequence). Due to the attention mechanism present in these networks (#REF) , posterior to training, it is possible to retrieve soft-alignment probability matrices between source and target sequences. These matrices give us sentence-level source-to-target alignment information, and by using it for clustering neighbor phonemes aligned to the same translation word, we are able to create segmentation in the target side.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"2 Bilingual Unsupervised Word Segmentation/Discovery Approach: We use the bilingual neuralbased Unsupervised Word Segmentation (UWS) approach from #TARGET_REF to discover words in Mboshi.\"]}"
    },
    {
        "gold": {
            "text": [
                "These matrices give us sentence-level source-to-target alignment information, and by using it for clustering neighbor phonemes aligned to the same translation word, we are able to create segmentation in the target side.",
                "The product of this approach is a set of (discovered-units, translation words) pairs.",
                "Multilingual Leveraging: In this work we apply two simple methods for including multilingual information into the bilingual models from #TARGET_REF .",
                "The first one, Multilingual Voting, consists of merging the information learned by models trained with different language pairs by performing a voting over the final discovered boundaries.",
                "The voting is performed by applying an agreement threshold T over the output boundaries."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "These matrices give us sentence-level source-to-target alignment information, and by using it for clustering neighbor phonemes aligned to the same translation word, we are able to create segmentation in the target side. The product of this approach is a set of (discovered-units, translation words) pairs. Multilingual Leveraging: In this work we apply two simple methods for including multilingual information into the bilingual models from #TARGET_REF . The first one, Multilingual Voting, consists of merging the information learned by models trained with different language pairs by performing a voting over the final discovered boundaries. The voting is performed by applying an agreement threshold T over the output boundaries.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Multilingual Leveraging: In this work we apply two simple methods for including multilingual information into the bilingual models from #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., #REF with 92.4 F 1 on Penn Treebank constituency parsing and #TARGET_REF with 92.8 F 1 .",
                "In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F 1 , with a comparatively simple architecture.",
                "In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem.",
                "Section 2 looks more closely at three of the most relevant previous papers.",
                "We then describe our exact model (Section 3), followed by the experimental setup and results (Sections 4 and 5)."
            ],
            "label": [
                "BACK",
                "USE"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., #REF with 92.4 F 1 on Penn Treebank constituency parsing and #TARGET_REF with 92.8 F 1 . In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F 1 , with a comparatively simple architecture. In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem. Section 2 looks more closely at three of the most relevant previous papers. We then describe our exact model (Section 3), followed by the experimental setup and results (Sections 4 and 5).",
        "output": "{\"label\": [\"BACK\", \"USE\"], \"context\": [\"Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., #REF with 92.4 F 1 on Penn Treebank constituency parsing and #TARGET_REF with 92.8 F 1 .\", \"In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F 1 , with a comparatively simple architecture.\"]}"
    },
    {
        "gold": {
            "text": [
                "A generative parsing model parses a sentence (x) into its phrasal structure (y) according to",
                "where Y(x) lists all possible structures of x. If we think of a tree (x, y) as a sequence (z) #TARGET_REF as illustrated in Figure 1 , we can define a probability distribution over (x, y) as follows:",
                "which is equivalent to Equation (1).",
                "We have reduced parsing to language modeling and can use language modeling techniques of estimating P (z t |z 1 , · · · , z t−1 ) for parsing."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0
            ]
        },
        "input": "A generative parsing model parses a sentence (x) into its phrasal structure (y) according to where Y(x) lists all possible structures of x. If we think of a tree (x, y) as a sequence (z) #TARGET_REF as illustrated in Figure 1 , we can define a probability distribution over (x, y) as follows: which is equivalent to Equation (1). We have reduced parsing to language modeling and can use language modeling techniques of estimating P (z t |z 1 , · · · , z t−1 ) for parsing.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"where Y(x) lists all possible structures of x. If we think of a tree (x, y) as a sequence (z) #TARGET_REF as illustrated in Figure 1 , we can define a probability distribution over (x, y) as follows:\"]}"
    },
    {
        "gold": {
            "text": [
                "We look here at three neural net (NN) models closest to our research along various dimensions.",
                "The first (#REF) gives the basic language modeling architecture that we have adopted, while the other two #TARGET_REF; #REF) are parsing models that have the current best results in NN parsing."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "We look here at three neural net (NN) models closest to our research along various dimensions. The first (#REF) gives the basic language modeling architecture that we have adopted, while the other two #TARGET_REF; #REF) are parsing models that have the current best results in NN parsing.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The first (#REF) gives the basic language modeling architecture that we have adopted, while the other two #TARGET_REF; #REF) are parsing models that have the current best results in NN parsing.\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the Wall Street Journal (WSJ) of the Penn Treebank (#REF) for training (2-21), development (24) and testing (23) and millions of auto-parsed \"silver\" trees (#REF; #REF; #TARGET_REF for tritraining.",
                "To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (#REF ) with a product of eight Berkeley parsers (#REF) 2 and ZPar (#REF) and select 24 million trees on which both parsers agree (#REF) .",
                "We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et 1 The code and trained models used for experiments are available at github.com/cdg720/emnlp2016.",
                "2 We use the reimplementation by #REF .",
                "(#REF) performed better when trained on all of 24 million trees than when trained on resampled two million trees."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We use the Wall Street Journal (WSJ) of the Penn Treebank (#REF) for training (2-21), development (24) and testing (23) and millions of auto-parsed \"silver\" trees (#REF; #REF; #TARGET_REF for tritraining. To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (#REF ) with a product of eight Berkeley parsers (#REF) 2 and ZPar (#REF) and select 24 million trees on which both parsers agree (#REF) . We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et 1 The code and trained models used for experiments are available at github.com/cdg720/emnlp2016. 2 We use the reimplementation by #REF . (#REF) performed better when trained on all of 24 million trees than when trained on resampled two million trees.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use the Wall Street Journal (WSJ) of the Penn Treebank (#REF) for training (2-21), development (24) and testing (23) and millions of auto-parsed \\\"silver\\\" trees (#REF; #REF; #TARGET_REF for tritraining.\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus 4 (HC) #TARGET_REF ; and an ensemble of six one-to-many sequence models trained on the HC and 4.5 millions of EnglishGerman translation sentence pairs (#REF) .",
                "We also compare LSTM-LM (GS) to best performing non-NN parsers in the literature.",
                "Parsers' parsing performance along with their training data is reported in Table 3 .",
                "LSTM-LM (GS) outperforms all the other parsers with 93.1 F 1 ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus 4 (HC) #TARGET_REF ; and an ensemble of six one-to-many sequence models trained on the HC and 4.5 millions of EnglishGerman translation sentence pairs (#REF) . We also compare LSTM-LM (GS) to best performing non-NN parsers in the literature. Parsers' parsing performance along with their training data is reported in Table 3 . LSTM-LM (GS) outperforms all the other parsers with 93.1 F 1 .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus 4 (HC) #TARGET_REF ; and an ensemble of six one-to-many sequence models trained on the HC and 4.5 millions of EnglishGerman translation sentence pairs (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We suspect building large models with character embeddings would lead to further improvement as in language modeling (#REF; #REF) .",
                "We also wish to develop a complete parsing model using the LSTM-LM framework.",
                "Table 3 : Evaluation of models trained on the WSJ and additional resources.",
                "Note that the numbers of #TARGET_REF and #REF are not directly comparable as their models are evaluated on OntoNotesstyle trees instead of PTB-style trees.",
                "E(LSTM-LMs (GS)) is an ensemble of eight LSTM-LMs (GS)."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We suspect building large models with character embeddings would lead to further improvement as in language modeling (#REF; #REF) . We also wish to develop a complete parsing model using the LSTM-LM framework. Table 3 : Evaluation of models trained on the WSJ and additional resources. Note that the numbers of #TARGET_REF and #REF are not directly comparable as their models are evaluated on OntoNotesstyle trees instead of PTB-style trees. E(LSTM-LMs (GS)) is an ensemble of eight LSTM-LMs (GS).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Note that the numbers of #TARGET_REF and #REF are not directly comparable as their models are evaluated on OntoNotesstyle trees instead of PTB-style trees.\"]}"
    },
    {
        "gold": {
            "text": [
                "For that reason, de #REF suggest that those representations could be modified for the purpose of parsing, thus creating a parsing representation.",
                "Transforming tree representations for the purpose of parsing is not a new idea.",
                "It has been done for constituency parsing for example by #REF but also for dependency parsing for example by #TARGET_REF .",
                "#REF modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.",
                "In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by #REF on UD treebanks to find out whether or not the alternative representation is useful for parsing with UD."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "For that reason, de #REF suggest that those representations could be modified for the purpose of parsing, thus creating a parsing representation. Transforming tree representations for the purpose of parsing is not a new idea. It has been done for constituency parsing for example by #REF but also for dependency parsing for example by #TARGET_REF . #REF modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy. In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by #REF on UD treebanks to find out whether or not the alternative representation is useful for parsing with UD.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"It has been done for constituency parsing for example by #REF but also for dependency parsing for example by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "For that reason, de #REF suggest that those representations could be modified for the purpose of parsing, thus creating a parsing representation.",
                "Transforming tree representations for the purpose of parsing is not a new idea.",
                "It has been done for constituency parsing for example by #REF but also for dependency parsing for example by #REF .",
                "#TARGET_REF modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.",
                "In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by #REF on UD treebanks to find out whether or not the alternative representation is useful for parsing with UD."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "For that reason, de #REF suggest that those representations could be modified for the purpose of parsing, thus creating a parsing representation. Transforming tree representations for the purpose of parsing is not a new idea. It has been done for constituency parsing for example by #REF but also for dependency parsing for example by #REF . #TARGET_REF modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy. In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by #REF on UD treebanks to find out whether or not the alternative representation is useful for parsing with UD.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.\"]}"
    },
    {
        "gold": {
            "text": [
                "2. Train a model on that transformed data.",
                "3. Parse the test data.",
                "4. Transform the parsed data back to the original representation (for comparison with the original gold standard).",
                "#TARGET_REF have shown that these same modifications as well as the modification of nonprojective structures helps parsing in four languages.",
                "#REF conducted a study over the alternative representations of 6 constructions across 5 parsing models for English and found that some of them are easier to parse than others."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "2. Train a model on that transformed data. 3. Parse the test data. 4. Transform the parsed data back to the original representation (for comparison with the original gold standard). #TARGET_REF have shown that these same modifications as well as the modification of nonprojective structures helps parsing in four languages. #REF conducted a study over the alternative representations of 6 constructions across 5 parsing models for English and found that some of them are easier to parse than others.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF have shown that these same modifications as well as the modification of nonprojective structures helps parsing in four languages.\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF show that making the auxiliary the head of the dependency as in Figure 2 is useful for parsing Czech and Slovenian.",
                "#REF verb groups are easier to parse when the auxiliary is the head (as in PDT) than when the verb is the head (as in MS).",
                "Since UD adopts the PDT style representation of verb groups, it would be interesting to find out whether or not transforming them to MS could also improve parsing.",
                "This is what will be attempted in this study.",
                "describe algorithms for such a transformation as well as its back transformation."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "#TARGET_REF show that making the auxiliary the head of the dependency as in Figure 2 is useful for parsing Czech and Slovenian. #REF verb groups are easier to parse when the auxiliary is the head (as in PDT) than when the verb is the head (as in MS). Since UD adopts the PDT style representation of verb groups, it would be interesting to find out whether or not transforming them to MS could also improve parsing. This is what will be attempted in this study. describe algorithms for such a transformation as well as its back transformation.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF show that making the auxiliary the head of the dependency as in Figure 2 is useful for parsing Czech and Slovenian.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we have attempted to reproduce a study by #TARGET_REF that has shown that making auxiliaries heads in verb groups improves parsing but failed to show that those results port to parsing with Universal Dependencies.",
                "Contrary to expectations, the study has given evidence that main verbs should stay heads of auxiliary dependency relations for parsing with UD.",
                "The benefits of error analyses for such a study have been highlighted because they allow us to shed more light on the different ways in which the transformations affect the parsing output.",
                "Experiments suggest that gains obtained from verb group transformations in previous studies have been obtained mainly because those transformations help disambiguating between main verbs and auxiliaries.",
                "It is however still an open question why the VG transformation hurts parsing accuracy in the case of UD."
            ],
            "label": [
                "BACK",
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In this paper, we have attempted to reproduce a study by #TARGET_REF that has shown that making auxiliaries heads in verb groups improves parsing but failed to show that those results port to parsing with Universal Dependencies. Contrary to expectations, the study has given evidence that main verbs should stay heads of auxiliary dependency relations for parsing with UD. The benefits of error analyses for such a study have been highlighted because they allow us to shed more light on the different ways in which the transformations affect the parsing output. Experiments suggest that gains obtained from verb group transformations in previous studies have been obtained mainly because those transformations help disambiguating between main verbs and auxiliaries. It is however still an open question why the VG transformation hurts parsing accuracy in the case of UD.",
        "output": "{\"label\": [\"BACK\", \"USE\"], \"context\": [\"In this paper, we have attempted to reproduce a study by #TARGET_REF that has shown that making auxiliaries heads in verb groups improves parsing but failed to show that those results port to parsing with Universal Dependencies.\"]}"
    },
    {
        "gold": {
            "text": [
                "It has been done for constituency parsing for example by #REF but also for dependency parsing for example by #REF .",
                "#REF modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.",
                "In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by #TARGET_REF on UD treebanks to find out whether or not the alternative representation is useful for parsing with UD.",
                "have shown that modifying coordination constructions and verb groups from their representation in the Prague Dependency Treebank (henceforth PDT) to a representation described in Melčuk (1988) (Mel'čuk style, henceforth MS) improves dependency parsing for Czech.",
                "The procedure they follow is as follows:"
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "It has been done for constituency parsing for example by #REF but also for dependency parsing for example by #REF . #REF modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy. In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by #TARGET_REF on UD treebanks to find out whether or not the alternative representation is useful for parsing with UD. have shown that modifying coordination constructions and verb groups from their representation in the Prague Dependency Treebank (henceforth PDT) to a representation described in Melčuk (1988) (Mel'čuk style, henceforth MS) improves dependency parsing for Czech. The procedure they follow is as follows:",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by #TARGET_REF on UD treebanks to find out whether or not the alternative representation is useful for parsing with UD.\"]}"
    },
    {
        "gold": {
            "text": [
                "We will follow the methodology from #TARGET_REF , that is, to transform, parse and then detransform the data so as to compare the original and the transformed model on the original gold standard.",
                "The method from #REF which consists in comparing the baseline and the transformed data on their respective gold standard is less relevant here because UD is believed to be a useful representation and that the aim will be to improve parsing within that representation.",
                "However, as was argued in that study, their method can give an indication of the learnability of a construction and can potentially be used to understand the results obtained by the parse-transform-detransform method.",
                "For this reason, this method will also be attempted.",
                "In addition, the original parsed data will also be transformed into the MS gold standard for comparison with the MS parsed data on the MS gold standard."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We will follow the methodology from #TARGET_REF , that is, to transform, parse and then detransform the data so as to compare the original and the transformed model on the original gold standard. The method from #REF which consists in comparing the baseline and the transformed data on their respective gold standard is less relevant here because UD is believed to be a useful representation and that the aim will be to improve parsing within that representation. However, as was argued in that study, their method can give an indication of the learnability of a construction and can potentially be used to understand the results obtained by the parse-transform-detransform method. For this reason, this method will also be attempted. In addition, the original parsed data will also be transformed into the MS gold standard for comparison with the MS parsed data on the MS gold standard.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We will follow the methodology from #TARGET_REF , that is, to transform, parse and then detransform the data so as to compare the original and the transformed model on the original gold standard.\"]}"
    },
    {
        "gold": {
            "text": [
                "Dutch was discarded because the back transformation accuracy was low (90%).",
                "This is due to inconsistencies in the annotation: verb groups are annotated as a chain of dependency relations.",
                "This leaves us with a total of 25 out of the 37 treebanks.",
                "For comparability with the study in #TARGET_REF , and because we used a slightly modified version of their algorithm, we also tested the approach on the versions of the Czech and Slovenian treebanks that they worked on, respectively version 1.0 of the PDT (#REF ) and the 2006 version of SDT (#REF) .",
                "overview of the data used for the experiments."
            ],
            "label": [
                "USE",
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Dutch was discarded because the back transformation accuracy was low (90%). This is due to inconsistencies in the annotation: verb groups are annotated as a chain of dependency relations. This leaves us with a total of 25 out of the 37 treebanks. For comparability with the study in #TARGET_REF , and because we used a slightly modified version of their algorithm, we also tested the approach on the versions of the Czech and Slovenian treebanks that they worked on, respectively version 1.0 of the PDT (#REF ) and the 2006 version of SDT (#REF) . overview of the data used for the experiments.",
        "output": "{\"label\": [\"USE\", \"DIFFER\"], \"context\": [\"For comparability with the study in #TARGET_REF , and because we used a slightly modified version of their algorithm, we also tested the approach on the versions of the Czech and Slovenian treebanks that they worked on, respectively version 1.0 of the PDT (#REF ) and the 2006 version of SDT (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Another aspect that draws interest in translation models is the effective computation of sentence representations using the translation task as an auxiliary semantic signal (#REF; #REF; #REF; #REF ).",
                "An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings #TARGET_REF .",
                "However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear.",
                "Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (#REF) , on translation quality (#REF) , on speed comparison (#REF) , or only exploring a bilingual scenario (Cífka and #REF) .",
                "In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Another aspect that draws interest in translation models is the effective computation of sentence representations using the translation task as an auxiliary semantic signal (#REF; #REF; #REF; #REF ). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings #TARGET_REF . However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear. Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (#REF) , on translation quality (#REF) , on speed comparison (#REF) , or only exploring a bilingual scenario (Cífka and #REF) . In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings #TARGET_REF .\", \"However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear.\"]}"
    },
    {
        "gold": {
            "text": [
                "Another aspect that draws interest in translation models is the effective computation of sentence representations using the translation task as an auxiliary semantic signal (#REF; #REF; #REF; #REF ).",
                "An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings (Cífka and #REF) .",
                "However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear.",
                "Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (#REF) , on translation quality (#REF) , on speed comparison (#REF) , or only exploring a bilingual scenario #TARGET_REF .",
                "In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Another aspect that draws interest in translation models is the effective computation of sentence representations using the translation task as an auxiliary semantic signal (#REF; #REF; #REF; #REF ). An important feature that enables an immediate use of the MT-based representations in other downstream tasks is the creation of fixed-sized sentence embeddings (Cífka and #REF) . However, the effects of the size of sentence embeddings and the relation between translation performance and meaning representation quality are not entirely clear. Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (#REF) , on translation quality (#REF) , on speed comparison (#REF) , or only exploring a bilingual scenario #TARGET_REF . In this paper, we are interested in exploring a cross-lingual intermediate shared layer (called attention bridge) in an attentive encoder-decoder MT model.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Recent studies based on NMT either focus entirely on the use of MT-based sentence embeddings in other tasks (#REF) , on translation quality (#REF) , on speed comparison (#REF) , or only exploring a bilingual scenario #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "However, we augment the network with language specific encoders and decoders to enable multilingual training as in #REF , plus we introduce an inner-attention layer (#REF; #REF) that summarizes the encoder information in a fixed-size vector representation that can easily be shared among different translation tasks with the language-specific encoders and decoders connecting to it.",
                "The overall architecture is illustrated in Figure 1 (see also Vázquez et al., 2019) .",
                "Due to the attentive connection between encoders and decoders we call this layer attention bridge, and its architecture is an adaptation from the model proposed by #TARGET_REF .",
                "Finally, each decoder follows a common attention mechanism in NMT, with the only exception that the context vector is computed on the attention bridge, and the initialization is performed by a mean pooling over it.",
                "Hence, the decoder receives the information only through the shared attention bridge."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "However, we augment the network with language specific encoders and decoders to enable multilingual training as in #REF , plus we introduce an inner-attention layer (#REF; #REF) that summarizes the encoder information in a fixed-size vector representation that can easily be shared among different translation tasks with the language-specific encoders and decoders connecting to it. The overall architecture is illustrated in Figure 1 (see also Vázquez et al., 2019) . Due to the attentive connection between encoders and decoders we call this layer attention bridge, and its architecture is an adaptation from the model proposed by #TARGET_REF . Finally, each decoder follows a common attention mechanism in NMT, with the only exception that the context vector is computed on the attention bridge, and the initialization is performed by a mean pooling over it. Hence, the decoder receives the information only through the shared attention bridge.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Due to the attentive connection between encoders and decoders we call this layer attention bridge, and its architecture is an adaptation from the model proposed by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "(2) The second outcome is the positive effect 3 Due to the large number of SentEval tasks, we report results on natural language inference (SNLI, SICK-E/SICK-R) and the average of all tasks.",
                "Table 2 : Results from supervised similarity tasks (SICK-R and STSB), measured using Pearson's (r) and Spearman's (ρ) correlation coefficients (r/ρ).",
                "The average across unsupervised similarity tasks on Pearson's measures are displayed in the right-most column.",
                "Results with † taken from #TARGET_REF",
                "of multilingual training."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "(2) The second outcome is the positive effect 3 Due to the large number of SentEval tasks, we report results on natural language inference (SNLI, SICK-E/SICK-R) and the average of all tasks. Table 2 : Results from supervised similarity tasks (SICK-R and STSB), measured using Pearson's (r) and Spearman's (ρ) correlation coefficients (r/ρ). The average across unsupervised similarity tasks on Pearson's measures are displayed in the right-most column. Results with † taken from #TARGET_REF of multilingual training.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Results with \\u2020 taken from #TARGET_REF\", \"of multilingual training.\"]}"
    },
    {
        "gold": {
            "text": [
                "For this, we adopt the in-domain development and evaluation dataset from the ACL-WMT07 shared task.",
                "Sentences are encoded using Byte-Pair Encoding (#REF) , with 32,000 merge operations for each language.",
                "4 SentEval: Classification tasks Table 1 shows the performance of our models on two popular tasks (SNLI and SICK-E) as in #TARGET_REF as well as the average of all 10 SentEval downstream tasks.",
                "The experiments reveal two important findings:",
                "(1) In contrast with the results from Cífka and #REF, our scores demonstrate that an increasing number of attention heads is beneficial for classification-based downstream tasks."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "For this, we adopt the in-domain development and evaluation dataset from the ACL-WMT07 shared task. Sentences are encoded using Byte-Pair Encoding (#REF) , with 32,000 merge operations for each language. 4 SentEval: Classification tasks Table 1 shows the performance of our models on two popular tasks (SNLI and SICK-E) as in #TARGET_REF as well as the average of all 10 SentEval downstream tasks. The experiments reveal two important findings: (1) In contrast with the results from Cífka and #REF, our scores demonstrate that an increasing number of attention heads is beneficial for classification-based downstream tasks.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"4 SentEval: Classification tasks Table 1 shows the performance of our models on two popular tasks (SNLI and SICK-E) as in #TARGET_REF as well as the average of all 10 SentEval downstream tasks.\"]}"
    },
    {
        "gold": {
            "text": [
                "This is in line with the findings of #TARGET_REF and could also be expected as the model is more strongly pushed into a dense semantic abstraction that is beneficial for measuring similarities without further training.",
                "More surprising is the negative effect of the multilingual models.",
                "We believe that the multilingual information encoded jointly in the attention bridge hampers the results for the monolingual semantic similarity measured with the cosine distance, while it becomes easier in a bilingual scenario where the vector encodes only one source language, English in this case.",
                "ii) On the supervised textual similarity tasks, we find a similar trend as in the previous section for SICK: both a higher number of attention heads and multilinguality contribute to better scores, while for STSB, we notice a different pattern.",
                "This general discrepancy between results in supervised and unsupervised tasks is not new in the literature (#REF) ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "This is in line with the findings of #TARGET_REF and could also be expected as the model is more strongly pushed into a dense semantic abstraction that is beneficial for measuring similarities without further training. More surprising is the negative effect of the multilingual models. We believe that the multilingual information encoded jointly in the attention bridge hampers the results for the monolingual semantic similarity measured with the cosine distance, while it becomes easier in a bilingual scenario where the vector encodes only one source language, English in this case. ii) On the supervised textual similarity tasks, we find a similar trend as in the previous section for SICK: both a higher number of attention heads and multilinguality contribute to better scores, while for STSB, we notice a different pattern. This general discrepancy between results in supervised and unsupervised tasks is not new in the literature (#REF) .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"This is in line with the findings of #TARGET_REF and could also be expected as the model is more strongly pushed into a dense semantic abstraction that is beneficial for measuring similarities without further training.\"]}"
    },
    {
        "gold": {
            "text": [
                "These approaches typically involve end-to-end supervised learning to generate questions.",
                "#REF proposed sequence-to-sequence learning for question generation from text passages.",
                "#TARGET_REF utilized the answer-position, and linguistic features such as named entity recognition (NER) and parts of speech (POS) information to further improve the QG performance as the model is aware that for which answer a question need to be generated.",
                "In the work of a multi-perspective context matching algorithm is employed.",
                "#REF use a set of rich linguistic features along with a NQG model."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "These approaches typically involve end-to-end supervised learning to generate questions. #REF proposed sequence-to-sequence learning for question generation from text passages. #TARGET_REF utilized the answer-position, and linguistic features such as named entity recognition (NER) and parts of speech (POS) information to further improve the QG performance as the model is aware that for which answer a question need to be generated. In the work of a multi-perspective context matching algorithm is employed. #REF use a set of rich linguistic features along with a NQG model.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF utilized the answer-position, and linguistic features such as named entity recognition (NER) and parts of speech (POS) information to further improve the QG performance as the model is aware that for which answer a question need to be generated.\"]}"
    },
    {
        "gold": {
            "text": [
                "In previous works #TARGET_REF; #REF) , named entity type features have been used.",
                "These features, however, only allow for the encoding of coarse level information such as knowledge of if an entity belongs to a set of predefined categories such as 'PERSON', 'LOCATION' and 'ORGANI-ZATION'.",
                "To alleviate this, we use the knowledge in the form of linked entities.",
                "In our experiments, we use Wikipedia as the knowledge base for which to link entities.",
                "This specific task (also known as Wikification (#REF) ) is the task of identifying concepts and entities in text and disambiguation them into the most specific corresponding Wikipedia pages."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In previous works #TARGET_REF; #REF) , named entity type features have been used. These features, however, only allow for the encoding of coarse level information such as knowledge of if an entity belongs to a set of predefined categories such as 'PERSON', 'LOCATION' and 'ORGANI-ZATION'. To alleviate this, we use the knowledge in the form of linked entities. In our experiments, we use Wikipedia as the knowledge base for which to link entities. This specific task (also known as Wikification (#REF) ) is the task of identifying concepts and entities in text and disambiguation them into the most specific corresponding Wikipedia pages.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"In previous works #TARGET_REF; #REF) , named entity type features have been used.\", \"These features, however, only allow for the encoding of coarse level information such as knowledge of if an entity belongs to a set of predefined categories such as 'PERSON', 'LOCATION' and 'ORGANI-ZATION'.\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluated the performance of our approach on SQuAD (#REF) and MS MARCO v2.1 (#REF) .",
                "SQuAD is composed of more than 100K questions posed by crowd workers on 536 Wikipedia articles.",
                "We used the same split as #TARGET_REF .",
                "MS MARCO datasets contains 1 million queries with corresponding answers and passages.",
                "All questions are sampled from real anonymized user queries and context passages are extracted from real web documents."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We evaluated the performance of our approach on SQuAD (#REF) and MS MARCO v2.1 (#REF) . SQuAD is composed of more than 100K questions posed by crowd workers on 536 Wikipedia articles. We used the same split as #TARGET_REF . MS MARCO datasets contains 1 million queries with corresponding answers and passages. All questions are sampled from real anonymized user queries and context passages are extracted from real web documents.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"We used the same split as #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "PredPatt 1 #TARGET_REF ) is a pattern-based framework for predicate-argument extraction.",
                "It defines a set of interpretable, extensible and non-lexicalized patterns based on Universal Dependencies (UD) (de #REF) , and extracts predicates and arguments through these manual patterns.",
                "Figure 1 shows the predicates and arguments extracted by PredPatt from the sentence: \"Chris, the designer, wants to launch a new brand.\" The underlying predicate-argument structure constructed by PredPatt is a directed graph, where a special dependency ARG is built between a predicate head token and its arguments' head tokens, and the original UD relations are retained within predicate phrases and argument phrases.",
                "For example, Figure 2 shows the directed graph for the predicate-argument extraction (1) and (2) in Figure 1 .",
                "Compared to other existing systems for predicate-argument extraction (#REF; #REF; #REF) , the use of manual language-agnostic patterns on UD makes PredPatt a well-founded component across languages."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "PredPatt 1 #TARGET_REF ) is a pattern-based framework for predicate-argument extraction. It defines a set of interpretable, extensible and non-lexicalized patterns based on Universal Dependencies (UD) (de #REF) , and extracts predicates and arguments through these manual patterns. Figure 1 shows the predicates and arguments extracted by PredPatt from the sentence: \"Chris, the designer, wants to launch a new brand.\" The underlying predicate-argument structure constructed by PredPatt is a directed graph, where a special dependency ARG is built between a predicate head token and its arguments' head tokens, and the original UD relations are retained within predicate phrases and argument phrases. For example, Figure 2 shows the directed graph for the predicate-argument extraction (1) and (2) in Figure 1 . Compared to other existing systems for predicate-argument extraction (#REF; #REF; #REF) , the use of manual language-agnostic patterns on UD makes PredPatt a well-founded component across languages.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"PredPatt 1 #TARGET_REF ) is a pattern-based framework for predicate-argument extraction.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF adapts PredPatt to data generation for cross-lingual open information extraction.",
                "However, the evaluation of PredPatt has been restricted to manually-checked extractions over a small set of sentences #TARGET_REF , which lacks gold annotations to conduct an objective and reproducible evaluation, and inhibits the updates of patterns in PredPatt.",
                "Chris , the designer , wants to launch a new brand .",
                "In this work, we aim to conduct a large-scale and reproducible evaluation of PredPatt by introducing a large set of gold annotations gathered from PropBank (#REF) .",
                "We leverage these gold annotations to improve PredPatt and compare it with other prominent systems."
            ],
            "label": [
                "BACK",
                "MOT",
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                1,
                0
            ]
        },
        "input": "#REF adapts PredPatt to data generation for cross-lingual open information extraction. However, the evaluation of PredPatt has been restricted to manually-checked extractions over a small set of sentences #TARGET_REF , which lacks gold annotations to conduct an objective and reproducible evaluation, and inhibits the updates of patterns in PredPatt. Chris , the designer , wants to launch a new brand . In this work, we aim to conduct a large-scale and reproducible evaluation of PredPatt by introducing a large set of gold annotations gathered from PropBank (#REF) . We leverage these gold annotations to improve PredPatt and compare it with other prominent systems.",
        "output": "{\"label\": [\"BACK\", \"MOT\", \"EXT\"], \"context\": [\"However, the evaluation of PredPatt has been restricted to manually-checked extractions over a small set of sentences #TARGET_REF , which lacks gold annotations to conduct an objective and reproducible evaluation, and inhibits the updates of patterns in PredPatt.\", \"In this work, we aim to conduct a large-scale and reproducible evaluation of PredPatt by introducing a large set of gold annotations gathered from PropBank (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "From the auto-converted gold annotations, we create a held-out set by randomly sampling 10% sentences from EWT.",
                "We then update the existing PredPatt patterns and introduce new patterns by analyzing PredPatt annotations on the held-out set.",
                "PredPatt extracts predicates and arguments in four stages #TARGET_REF : (1) predicate and argument root identification, (2) argument resolution, (3) predicate and argument phrase extraction, and (4) optional post-processing.",
                "We analyze PredPatt extraction in each of these stages on the held-out set, and make 19 improvements to PredPatt patterns.",
                "Due to lack of space, we only highlight one improvement for each stage below."
            ],
            "label": [
                "BACK",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "From the auto-converted gold annotations, we create a held-out set by randomly sampling 10% sentences from EWT. We then update the existing PredPatt patterns and introduce new patterns by analyzing PredPatt annotations on the held-out set. PredPatt extracts predicates and arguments in four stages #TARGET_REF : (1) predicate and argument root identification, (2) argument resolution, (3) predicate and argument phrase extraction, and (4) optional post-processing. We analyze PredPatt extraction in each of these stages on the held-out set, and make 19 improvements to PredPatt patterns. Due to lack of space, we only highlight one improvement for each stage below.",
        "output": "{\"label\": [\"BACK\", \"EXT\"], \"context\": [\"PredPatt extracts predicates and arguments in four stages #TARGET_REF : (1) predicate and argument root identification, (2) argument resolution, (3) predicate and argument phrase extraction, and (4) optional post-processing.\", \"We analyze PredPatt extraction in each of these stages on the held-out set, and make 19 improvements to PredPatt patterns.\"]}"
    },
    {
        "gold": {
            "text": [
                "Neural machine translation (NMT) (#REF; #REF) is rapidly proving itself to be a strong competitor to other statistical machine translation methods.",
                "However, it still lags behind other statistical methods on very lowresource language pairs #TARGET_REF; #REF) .",
                "A common strategy to improve learning of lowresource languages is to use resources from related languages (#REF) .",
                "However, adapting these resources is not trivial.",
                "NMT offers some simple ways of doing this."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Neural machine translation (NMT) (#REF; #REF) is rapidly proving itself to be a strong competitor to other statistical machine translation methods. However, it still lags behind other statistical methods on very lowresource language pairs #TARGET_REF; #REF) . A common strategy to improve learning of lowresource languages is to use resources from related languages (#REF) . However, adapting these resources is not trivial. NMT offers some simple ways of doing this.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Neural machine translation (NMT) (#REF; #REF) is rapidly proving itself to be a strong competitor to other statistical machine translation methods.\", \"However, it still lags behind other statistical methods on very lowresource language pairs #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular, they showed that a French-English model could be used to improve translation on a wide range of low-resource language pairs such as Hausa-, Turkish-, and Uzbek-English.",
                "In this paper, we explore the opposite scenario, where the parent language pair is also lowresource, but related to the child language pair.",
                "We show that, at least in the case of three Turkic languages (Turkish, Uzbek, and Uyghur), the original method of #TARGET_REF does not always work, but it is still possible to use the parent model to considerably improve the child model.",
                "The basic idea is to exploit the relationship between the parent and child language lexicons.",
                "Zoph et al.'s original method makes no assumption about the relatedness of the parent and child languages, so it effectively makes a random assignment of the parent-language word embeddings to child-language words. But if we assume that the parent and child lexicons are related, it should be beneficial to transfer source word embeddings from parent-language words to their child-language equivalents."
            ],
            "label": [
                "MOT",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In particular, they showed that a French-English model could be used to improve translation on a wide range of low-resource language pairs such as Hausa-, Turkish-, and Uzbek-English. In this paper, we explore the opposite scenario, where the parent language pair is also lowresource, but related to the child language pair. We show that, at least in the case of three Turkic languages (Turkish, Uzbek, and Uyghur), the original method of #TARGET_REF does not always work, but it is still possible to use the parent model to considerably improve the child model. The basic idea is to exploit the relationship between the parent and child language lexicons. Zoph et al.'s original method makes no assumption about the relatedness of the parent and child languages, so it effectively makes a random assignment of the parent-language word embeddings to child-language words. But if we assume that the parent and child lexicons are related, it should be beneficial to transfer source word embeddings from parent-language words to their child-language equivalents.",
        "output": "{\"label\": [\"MOT\", \"USE\"], \"context\": [\"We show that, at least in the case of three Turkic languages (Turkish, Uzbek, and Uyghur), the original method of #TARGET_REF does not always work, but it is still possible to use the parent model to considerably improve the child model.\"]}"
    },
    {
        "gold": {
            "text": [
                "We follow the transfer learning approach proposed by #TARGET_REF .",
                "In their work, a parent model is first trained on a high-resource language pair.",
                "Then the child model's parameter values are copied from the parent's and are fine-tuned on its low-resource data.",
                "The source word embeddings are copied with the rest of the model, with the ith parent-language word embedding being assigned to the ith childlanguage word.",
                "Because the parent and child source languages have different vocabularies, this amounts to randomly assigning parent source word embeddings to child source words."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We follow the transfer learning approach proposed by #TARGET_REF . In their work, a parent model is first trained on a high-resource language pair. Then the child model's parameter values are copied from the parent's and are fine-tuned on its low-resource data. The source word embeddings are copied with the rest of the model, with the ith parent-language word embedding being assigned to the ith childlanguage word. Because the parent and child source languages have different vocabularies, this amounts to randomly assigning parent source word embeddings to child source words.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We follow the transfer learning approach proposed by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The basic idea of our method is to extend the transfer method of #TARGET_REF to share the parent and child's source vocabularies, so that when source word embeddings are transferred, a word that appears in both vocabularies keeps its embedding.",
                "In order for this to work, it must be the case that the parent and child languages have considerable vocabulary overlap, and that when a word occurs in both languages, it often has a similar meaning in both languages.",
                "Thus, we need to process the data to make these two assumptions hold as much as possible."
            ],
            "label": [
                "USE",
                "EXT"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "The basic idea of our method is to extend the transfer method of #TARGET_REF to share the parent and child's source vocabularies, so that when source word embeddings are transferred, a word that appears in both vocabularies keeps its embedding. In order for this to work, it must be the case that the parent and child languages have considerable vocabulary overlap, and that when a word occurs in both languages, it often has a similar meaning in both languages. Thus, we need to process the data to make these two assumptions hold as much as possible.",
        "output": "{\"label\": [\"USE\", \"EXT\"], \"context\": [\"The basic idea of our method is to extend the transfer method of #TARGET_REF to share the parent and child's source vocabularies, so that when source word embeddings are transferred, a word that appears in both vocabularies keeps its embedding.\"]}"
    },
    {
        "gold": {
            "text": [
                "We also optimized the vocabulary size and the number of BPE operations for the word-based and BPEbased systems, respectively, to maximize the tokenized BLEU on the development set.",
                "After translation at test time, we rejoined BPE segments, recased, and detokenized.",
                "Finally, we evaluated using case-sensitive BLEU.",
                "As a baseline, we trained a child model using BPE but without transfer (that is, with weights randomly initialized).",
                "We also compared against a word-based baseline (without transfer) and two word-based systems using transfer without vocabulary-sharing, corresponding with the method of #TARGET_REF ( §2.2): one where the target word embeddings are fine-tuned, and one where they are frozen."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "We also optimized the vocabulary size and the number of BPE operations for the word-based and BPEbased systems, respectively, to maximize the tokenized BLEU on the development set. After translation at test time, we rejoined BPE segments, recased, and detokenized. Finally, we evaluated using case-sensitive BLEU. As a baseline, we trained a child model using BPE but without transfer (that is, with weights randomly initialized). We also compared against a word-based baseline (without transfer) and two word-based systems using transfer without vocabulary-sharing, corresponding with the method of #TARGET_REF ( §2.2): one where the target word embeddings are fine-tuned, and one where they are frozen.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We also compared against a word-based baseline (without transfer) and two word-based systems using transfer without vocabulary-sharing, corresponding with the method of #TARGET_REF ( \\u00a72.2): one where the target word embeddings are fine-tuned, and one where they are frozen.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we have shown that the transfer learning method of #TARGET_REF , while appealing, might not always work in a low-resource context.",
                "However, by combining it with BPE, we can improve NMT performance on a low-resource language pair by exploiting its lexical similarity with another related, low-resource language.",
                "Our results show consistent improvement in two Turkic languages.",
                "Our approach, which relies on segmenting words into subwords, seems well suited to agglutinative languages; further investigation would be needed to confirm whether our method works on other types of languages."
            ],
            "label": [
                "USE",
                "EXT"
            ],
            "context": [
                1,
                1,
                0,
                0
            ]
        },
        "input": "In this paper, we have shown that the transfer learning method of #TARGET_REF , while appealing, might not always work in a low-resource context. However, by combining it with BPE, we can improve NMT performance on a low-resource language pair by exploiting its lexical similarity with another related, low-resource language. Our results show consistent improvement in two Turkic languages. Our approach, which relies on segmenting words into subwords, seems well suited to agglutinative languages; further investigation would be needed to confirm whether our method works on other types of languages.",
        "output": "{\"label\": [\"USE\", \"EXT\"], \"context\": [\"In this paper, we have shown that the transfer learning method of #TARGET_REF , while appealing, might not always work in a low-resource context.\", \"However, by combining it with BPE, we can improve NMT performance on a low-resource language pair by exploiting its lexical similarity with another related, low-resource language.\"]}"
    },
    {
        "gold": {
            "text": [
                "NMT offers some simple ways of doing this.",
                "For example, #TARGET_REF train a parent model on a (possibly unrelated) high-resource language pair, then use this model to initialize a child model which is further trained on a low-resource language pair.",
                "In particular, they showed that a French-English model could be used to improve translation on a wide range of low-resource language pairs such as Hausa-, Turkish-, and Uzbek-English.",
                "In this paper, we explore the opposite scenario, where the parent language pair is also lowresource, but related to the child language pair.",
                "We show that, at least in the case of three Turkic languages (Turkish, Uzbek, and Uyghur), the original method of #REF does not always work, but it is still possible to use the parent model to considerably improve the child model."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "NMT offers some simple ways of doing this. For example, #TARGET_REF train a parent model on a (possibly unrelated) high-resource language pair, then use this model to initialize a child model which is further trained on a low-resource language pair. In particular, they showed that a French-English model could be used to improve translation on a wide range of low-resource language pairs such as Hausa-, Turkish-, and Uzbek-English. In this paper, we explore the opposite scenario, where the parent language pair is also lowresource, but related to the child language pair. We show that, at least in the case of three Turkic languages (Turkish, Uzbek, and Uyghur), the original method of #REF does not always work, but it is still possible to use the parent model to considerably improve the child model.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"For example, #TARGET_REF train a parent model on a (possibly unrelated) high-resource language pair, then use this model to initialize a child model which is further trained on a low-resource language pair.\"]}"
    },
    {
        "gold": {
            "text": [
                "Phoneme-only E2E systems have been shown to have inferior performance compared to grapheme or wordpiece models (WPM) in general #TARGET_REF 17] , but shows better recognition of rare words and proper nouns.",
                "In this work we propose to incorporate phonemes to a wordpiece E2E model as modeling units and use phoneme-level FST for contextual biasing.",
                "We propose a word-frequency based sampling strategy to randomly tokenize rare words into phonemes in the target sequence using a lexicon.",
                "This approach also mitigates accuracy regressions that have been observed when using phoneme-only E2E models [16, 17] .",
                "We train our model using only American English data and thus its wordpieces and phoneme set (no data from foreign languages)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Phoneme-only E2E systems have been shown to have inferior performance compared to grapheme or wordpiece models (WPM) in general #TARGET_REF 17] , but shows better recognition of rare words and proper nouns. In this work we propose to incorporate phonemes to a wordpiece E2E model as modeling units and use phoneme-level FST for contextual biasing. We propose a word-frequency based sampling strategy to randomly tokenize rare words into phonemes in the target sequence using a lexicon. This approach also mitigates accuracy regressions that have been observed when using phoneme-only E2E models [16, 17] . We train our model using only American English data and thus its wordpieces and phoneme set (no data from foreign languages).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Phoneme-only E2E systems have been shown to have inferior performance compared to grapheme or wordpiece models (WPM) in general #TARGET_REF 17] , but shows better recognition of rare words and proper nouns.\"]}"
    },
    {
        "gold": {
            "text": [
                "Phoneme-only E2E systems have been shown to have inferior performance compared to grapheme or wordpiece models (WPM) in general [16, 17] , but shows better recognition of rare words and proper nouns.",
                "In this work we propose to incorporate phonemes to a wordpiece E2E model as modeling units and use phoneme-level FST for contextual biasing.",
                "We propose a word-frequency based sampling strategy to randomly tokenize rare words into phonemes in the target sequence using a lexicon.",
                "This approach also mitigates accuracy regressions that have been observed when using phoneme-only E2E models #TARGET_REF 17] .",
                "We train our model using only American English data and thus its wordpieces and phoneme set (no data from foreign languages)."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Phoneme-only E2E systems have been shown to have inferior performance compared to grapheme or wordpiece models (WPM) in general [16, 17] , but shows better recognition of rare words and proper nouns. In this work we propose to incorporate phonemes to a wordpiece E2E model as modeling units and use phoneme-level FST for contextual biasing. We propose a word-frequency based sampling strategy to randomly tokenize rare words into phonemes in the target sequence using a lexicon. This approach also mitigates accuracy regressions that have been observed when using phoneme-only E2E models #TARGET_REF 17] . We train our model using only American English data and thus its wordpieces and phoneme set (no data from foreign languages).",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"We propose a word-frequency based sampling strategy to randomly tokenize rare words into phonemes in the target sequence using a lexicon.\", \"This approach also mitigates accuracy regressions that have been observed when using phoneme-only E2E models #TARGET_REF 17] .\"]}"
    },
    {
        "gold": {
            "text": [
                "We use a pronunciation lexicon to obtain phoneme sequences of words.",
                "Since phonemes show strength in recognizing rare words #TARGET_REF , we want to present these words as phonemes more often.",
                "In a target sentence, we decide to randomly present the i th word as phonemes with a probability",
                ", 1.0) where p0 and T are constants and c(i) is an integer representing the number of time the word appears in our entire training corpus.",
                "Therefore, the words that appear T times or less will be presented as phonemes with probability p0."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We use a pronunciation lexicon to obtain phoneme sequences of words. Since phonemes show strength in recognizing rare words #TARGET_REF , we want to present these words as phonemes more often. In a target sentence, we decide to randomly present the i th word as phonemes with a probability , 1.0) where p0 and T are constants and c(i) is an integer representing the number of time the word appears in our entire training corpus. Therefore, the words that appear T times or less will be presented as phonemes with probability p0.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Since phonemes show strength in recognizing rare words #TARGET_REF , we want to present these words as phonemes more often.\"]}"
    },
    {
        "gold": {
            "text": [
                ", 1.0) where p0 and T are constants and c(i) is an integer representing the number of time the word appears in our entire training corpus.",
                "Therefore, the words that appear T times or less will be presented as phonemes with probability p0.",
                "For words that appear more than T times, the more frequent they are, the less likely they are presented as phonemes 2 .",
                "Note that the decision of whether to use wordpieces or phonemes is made randomly at each gradient iteration, and thus a given sentence could have different target sequences at different epochs.",
                "We use context-independent phonemes as in #TARGET_REF ."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": ", 1.0) where p0 and T are constants and c(i) is an integer representing the number of time the word appears in our entire training corpus. Therefore, the words that appear T times or less will be presented as phonemes with probability p0. For words that appear more than T times, the more frequent they are, the less likely they are presented as phonemes 2 . Note that the decision of whether to use wordpieces or phonemes is made randomly at each gradient iteration, and thus a given sentence could have different target sequences at different epochs. We use context-independent phonemes as in #TARGET_REF .",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"We use context-independent phonemes as in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "To generate words as outputs, we search through a decoding graph similar to #TARGET_REF but accept both phonemes and wordpieces.",
                "An example is shown in Figure 2 .",
                "The decoding FST has wordpiece loops around state 0 (we show only a few for simplicity), but also has a pronunciation section (states 1 through 14) .",
                "The pronunciation section is a prefix tree with phonemes as inputs, and outputs are wordpieces of the corresponding word produced by the WPM in Section 3.1.",
                "Specifically, for each word in the biasing list, we look up pronunciations from the lexicon and split the word into its constituent wordpieces."
            ],
            "label": [
                "SIM",
                "DIFFER"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "To generate words as outputs, we search through a decoding graph similar to #TARGET_REF but accept both phonemes and wordpieces. An example is shown in Figure 2 . The decoding FST has wordpiece loops around state 0 (we show only a few for simplicity), but also has a pronunciation section (states 1 through 14) . The pronunciation section is a prefix tree with phonemes as inputs, and outputs are wordpieces of the corresponding word produced by the WPM in Section 3.1. Specifically, for each word in the biasing list, we look up pronunciations from the lexicon and split the word into its constituent wordpieces.",
        "output": "{\"label\": [\"SIM\", \"DIFFER\"], \"context\": [\"To generate words as outputs, we search through a decoding graph similar to #TARGET_REF but accept both phonemes and wordpieces.\"]}"
    },
    {
        "gold": {
            "text": [
                "The WER reductions range from 9%-23% relatively for different models when compared to the no-bias case.",
                "Comparing different biasing strategies, we find that the wordpiece-phoneme model performs the best: 16% relatively better than the grapheme model, and 8.3% better than the wordpiece model.",
                "We attribute the superior per- formance of the wordpiece-phoneme model to the robustness of phonemes to OOV words, as observed in #TARGET_REF .",
                "Since the wordpiece-phoneme model contains both wordpieces and phonemes as modeling units, we can further perform wordpiece biasing in addition to phoneme-based biasing by building a wordpiece FST in parallel to the phoneme FST.",
                "This further reduces the WER by 2%, as shown in the bottom row in Table 1 ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The WER reductions range from 9%-23% relatively for different models when compared to the no-bias case. Comparing different biasing strategies, we find that the wordpiece-phoneme model performs the best: 16% relatively better than the grapheme model, and 8.3% better than the wordpiece model. We attribute the superior per- formance of the wordpiece-phoneme model to the robustness of phonemes to OOV words, as observed in #TARGET_REF . Since the wordpiece-phoneme model contains both wordpieces and phonemes as modeling units, we can further perform wordpiece biasing in addition to phoneme-based biasing by building a wordpiece FST in parallel to the phoneme FST. This further reduces the WER by 2%, as shown in the bottom row in Table 1 .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We attribute the superior per- formance of the wordpiece-phoneme model to the robustness of phonemes to OOV words, as observed in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The wordpiecephoneme model performs a little better than the grapheme model, and we attribute that to the higher frequency of wordpieces during training.",
                "Compared to the wordpiece model, the wordpiece-phoneme model has a slight degradation (0.1% absolute WER).",
                "This is due to the introduction of phonemes in modeling.",
                "One potential approach to improve regression is to incorporate an English external language model for phonemes in rescoring, similarly to the wordpiece-based rescoring in [10] .",
                "However, we note that the regression is significantly smaller than the all-phoneme model in #TARGET_REF ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The wordpiecephoneme model performs a little better than the grapheme model, and we attribute that to the higher frequency of wordpieces during training. Compared to the wordpiece model, the wordpiece-phoneme model has a slight degradation (0.1% absolute WER). This is due to the introduction of phonemes in modeling. One potential approach to improve regression is to incorporate an English external language model for phonemes in rescoring, similarly to the wordpiece-based rescoring in [10] . However, we note that the regression is significantly smaller than the all-phoneme model in #TARGET_REF .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"However, we note that the regression is significantly smaller than the all-phoneme model in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The final output symbols, which are always wordpieces, are concatenated into words.",
                "Figure 2 : Decoding graph for the words \"crèche\" (daycare) with English cross lingual pronunciation \"k r\\ E S\" and \"créteil\" (a city) with pronunciation \"k r\\ E t E j\".",
                "For clarity, we omitted most wordpieces for the state 0.",
                "Based on #TARGET_REF , we add two improvements to the decoding strategy.",
                "First, during decoding we consume as many input epsilon arcs as possible thus guaranteeing that all wordpieces in word are produced when all corresponding phonemes are seen in the input."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The final output symbols, which are always wordpieces, are concatenated into words. Figure 2 : Decoding graph for the words \"crèche\" (daycare) with English cross lingual pronunciation \"k r\\ E S\" and \"créteil\" (a city) with pronunciation \"k r\\ E t E j\". For clarity, we omitted most wordpieces for the state 0. Based on #TARGET_REF , we add two improvements to the decoding strategy. First, during decoding we consume as many input epsilon arcs as possible thus guaranteeing that all wordpieces in word are produced when all corresponding phonemes are seen in the input.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"Based on #TARGET_REF , we add two improvements to the decoding strategy.\"]}"
    },
    {
        "gold": {
            "text": [
                "This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (#REF; #REF) to document summarization (#REF ), text classification (#REF; #REF) , text segmentation (#REF) , information extraction (#REF) , image caption generation (#REF; #REF) , sentence generation (#REFb) , dialogue control (#REF; #REFa) , sentiment classification, recommendation system, question answering (#REF) and machine translation , to name a few.",
                "Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model.",
                "The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.",
                "The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.",
                "This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #TARGET_REF , memory-augmented neural network (#REF; #REF) , stochastic neural network #REF) , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (#REF; #REF) to document summarization (#REF ), text classification (#REF; #REF) , text segmentation (#REF) , information extraction (#REF) , image caption generation (#REF; #REF) , sentence generation (#REFb) , dialogue control (#REF; #REFa) , sentiment classification, recommendation system, question answering (#REF) and machine translation , to name a few. Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model. The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs. The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference. This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #TARGET_REF , memory-augmented neural network (#REF; #REF) , stochastic neural network #REF) , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #TARGET_REF , memory-augmented neural network (#REF; #REF) , stochastic neural network #REF) , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, in \"20 people were wounded in Tuesday's airport blast\", \"wounded\" is a trigger of an Injure event and \"blast\" is a trigger of an Attack.",
                "The task both detects trigger tokens and classifies them to appropriate event types.",
                "While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling.",
                "Most state-of-the-art event trigger labeling approaches (#REF; #REFb; #REF; #TARGET_REF follow the standard supervised learning paradigm.",
                "For each event type, experts first write annotation guidelines."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "For example, in \"20 people were wounded in Tuesday's airport blast\", \"wounded\" is a trigger of an Injure event and \"blast\" is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (#REF; #REFb; #REF; #TARGET_REF follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Most state-of-the-art event trigger labeling approaches (#REF; #REFb; #REF; #TARGET_REF follow the standard supervised learning paradigm.\"]}"
    },
    {
        "gold": {
            "text": [
                "Using eventindependent features allows us to train the system only once, at system setup phase, requiring annotated triggers in a training set for just a few preselected event types.",
                "Then, whenever a new event type is introduced for labeling, we only need to collect a seed list for it from its description, and provide it as input to the system.",
                "We developed a seed-based system (Section 3), based on a state-of-the-art fully-supervised event extraction system #TARGET_REF .",
                "When evaluated on the ACE-2005 dataset, 1 our system outperforms the fully-supervised one (Section 4), even though we don't utilize any annotated triggers of the test events during the labeling phase, and only Figure 1 : Flow of the seed-based approach use the seed triggers appearing in the ACE annotation guidelines.",
                "This result contributes to the broader line of research on avoiding or reducing annotation cost in information extraction (Section 5)."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Using eventindependent features allows us to train the system only once, at system setup phase, requiring annotated triggers in a training set for just a few preselected event types. Then, whenever a new event type is introduced for labeling, we only need to collect a seed list for it from its description, and provide it as input to the system. We developed a seed-based system (Section 3), based on a state-of-the-art fully-supervised event extraction system #TARGET_REF . When evaluated on the ACE-2005 dataset, 1 our system outperforms the fully-supervised one (Section 4), even though we don't utilize any annotated triggers of the test events during the labeling phase, and only Figure 1 : Flow of the seed-based approach use the seed triggers appearing in the ACE annotation guidelines. This result contributes to the broader line of research on avoiding or reducing annotation cost in information extraction (Section 5).",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"We developed a seed-based system (Section 3), based on a state-of-the-art fully-supervised event extraction system #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "This section describes the method we designed to implement the seed-based approach.",
                "To assess our approach, we compare it (Section 4) with the common fully-supervised approach, which requires annotated triggers for each target event type.",
                "Therefore, we implemented our system by adapting the state-of-the-art fully-supervised event extraction system of #TARGET_REF , modifying mechanisms relevant for features and for trigger labels, as described below.",
                "Hence the systems are comparable with respect to using the same preprocessing and machine learning infrastructure."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0
            ]
        },
        "input": "This section describes the method we designed to implement the seed-based approach. To assess our approach, we compare it (Section 4) with the common fully-supervised approach, which requires annotated triggers for each target event type. Therefore, we implemented our system by adapting the state-of-the-art fully-supervised event extraction system of #TARGET_REF , modifying mechanisms relevant for features and for trigger labels, as described below. Hence the systems are comparable with respect to using the same preprocessing and machine learning infrastructure.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"Therefore, we implemented our system by adapting the state-of-the-art fully-supervised event extraction system of #TARGET_REF , modifying mechanisms relevant for features and for trigger labels, as described below.\"]}"
    },
    {
        "gold": {
            "text": [
                "To implement the seed-based approach for trigger labeling, we adapt only the trigger classification part in the #TARGET_REF fully-supervised system, ignoring arguments.",
                "Given a set of new target event types T we classify every test sentence once for each event type t ∈ T .",
                "Hence, when classifying a sentence for t, the labeling of each token x i is binary, where y i ∈ { , ⊥} marks whether x i is a trigger of type t ( ) or not (⊥).",
                "For instance x i =\"visited\" labeled as when classifying for t=Meet, means x i is labeled as a Meet trigger.",
                "To score the binary label assignment (x i , y i ), we use a small set of features that assess the similarity between x i and t's given seed list."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "To implement the seed-based approach for trigger labeling, we adapt only the trigger classification part in the #TARGET_REF fully-supervised system, ignoring arguments. Given a set of new target event types T we classify every test sentence once for each event type t ∈ T . Hence, when classifying a sentence for t, the labeling of each token x i is binary, where y i ∈ { , ⊥} marks whether x i is a trigger of type t ( ) or not (⊥). For instance x i =\"visited\" labeled as when classifying for t=Meet, means x i is labeled as a Meet trigger. To score the binary label assignment (x i , y i ), we use a small set of features that assess the similarity between x i and t's given seed list.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"To implement the seed-based approach for trigger labeling, we adapt only the trigger classification part in the #TARGET_REF fully-supervised system, ignoring arguments.\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by #REF (Section 3) .",
                "To maintain comparability, we use the ACE-2005 documents with the same split as in (#REF; #REFb; #REF) to 40 test documents and 559 training documents.",
                "However, some evaluation settings differ: #TARGET_REF train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types.",
                "Our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training.",
                "We next describe how this setup is addressed in our evaluation."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by #REF (Section 3) . To maintain comparability, we use the ACE-2005 documents with the same split as in (#REF; #REFb; #REF) to 40 test documents and 559 training documents. However, some evaluation settings differ: #TARGET_REF train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types. Our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training. We next describe how this setup is addressed in our evaluation.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"However, some evaluation settings differ: #TARGET_REF train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types.\"]}"
    },
    {
        "gold": {
            "text": [
                "The event extraction system of #TARGET_REF labels triggers and their arguments for a set of target event types L, for which annotated training documents are provided.",
                "The system utilizes a structured perceptron with beam search (#REF; .",
                "To label triggers, the system scans each sentence x, and creates candidate assignments y, that for each token x i assign each possible label y i ∈ L ∪ {⊥} (⊥ meaning x i is not a trigger at all).",
                "The score of an assignment (x i , y i ) is calculated as w · f , where f is the binary feature vector calculated for (x i , y i ), and w is the learned feature weight vector.",
                "The classifier's features capture various properties of x i and its context, such as its unigram and its containing bigrams."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The event extraction system of #TARGET_REF labels triggers and their arguments for a set of target event types L, for which annotated training documents are provided. The system utilizes a structured perceptron with beam search (#REF; . To label triggers, the system scans each sentence x, and creates candidate assignments y, that for each token x i assign each possible label y i ∈ L ∪ {⊥} (⊥ meaning x i is not a trigger at all). The score of an assignment (x i , y i ) is calculated as w · f , where f is the binary feature vector calculated for (x i , y i ), and w is the learned feature weight vector. The classifier's features capture various properties of x i and its context, such as its unigram and its containing bigrams.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"The event extraction system of #TARGET_REF labels triggers and their arguments for a set of target event types L, for which annotated training documents are provided.\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by #TARGET_REF (Section 3) .",
                "To maintain comparability, we use the ACE-2005 documents with the same split as in (#REF; #REFb; #REF) to 40 test documents and 559 training documents.",
                "However, some evaluation settings differ: #REF train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types.",
                "Our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training.",
                "We next describe how this setup is addressed in our evaluation."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by #TARGET_REF (Section 3) . To maintain comparability, we use the ACE-2005 documents with the same split as in (#REF; #REFb; #REF) to 40 test documents and 559 training documents. However, some evaluation settings differ: #REF train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types. Our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training. We next describe how this setup is addressed in our evaluation.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by #TARGET_REF (Section 3) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by #REF (Section 3) .",
                "To maintain comparability, we use the ACE-2005 documents with the same split as in (#REF; #REFb; #TARGET_REF to 40 test documents and 559 training documents.",
                "However, some evaluation settings differ: #REF train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types.",
                "Our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training.",
                "We next describe how this setup is addressed in our evaluation."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We evaluate our seed-based approach (Section 2) in comparison to the fully-supervised approach implemented by #REF (Section 3) . To maintain comparability, we use the ACE-2005 documents with the same split as in (#REF; #REFb; #TARGET_REF to 40 test documents and 559 training documents. However, some evaluation settings differ: #REF train a multi-class model for all 33 ACE-2005 event types, and classify all tokens in the test documents into these event types. Our approach, on the other hand, trains an eventindependent binary classifier, while testing on new event types that are different from those utilized for training. We next describe how this setup is addressed in our evaluation.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"To maintain comparability, we use the ACE-2005 documents with the same split as in (#REF; #REFb; #TARGET_REF to 40 test documents and 559 training documents.\"]}"
    },
    {
        "gold": {
            "text": [
                "To maintain consistency between different sets of training event types, we use a fixed total of 30 annotated trigger tokens for each set of",
                "80.6 67.1 73.2 0.04 #REF 73.7 62.3 67.5 - #REF 67.6 53.5 59.7 - Table 2 shows our system's precision, recall and F 1 , 7 and the average variance of F 1 within the 10 runs of each test event type.",
                "The very low variance indicates that the system's performance does not depend much on the choice of training event types.",
                "We compare our system's performance to the published trigger classification results of the baseline system of #TARGET_REF ) (its globally optimized run, when labeling both triggers and arguments).",
                "We also compare to the sentence-level system in (#REF) which uses the same dataset split."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "To maintain consistency between different sets of training event types, we use a fixed total of 30 annotated trigger tokens for each set of 80.6 67.1 73.2 0.04 #REF 73.7 62.3 67.5 - #REF 67.6 53.5 59.7 - Table 2 shows our system's precision, recall and F 1 , 7 and the average variance of F 1 within the 10 runs of each test event type. The very low variance indicates that the system's performance does not depend much on the choice of training event types. We compare our system's performance to the published trigger classification results of the baseline system of #TARGET_REF ) (its globally optimized run, when labeling both triggers and arguments). We also compare to the sentence-level system in (#REF) which uses the same dataset split.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We compare our system's performance to the published trigger classification results of the baseline system of #TARGET_REF ) (its globally optimized run, when labeling both triggers and arguments).\"]}"
    },
    {
        "gold": {
            "text": [
                "Representing sentences as numerical vectors while capturing their semantic context is an important and useful intermediate step in natural language processing.",
                "Representations that are both general and discriminative can serve as a tool for tackling various NLP tasks.",
                "While common sentence representation methods are unsupervised in nature, recently, an approach for learning universal sentence representation in a supervised setting was presented in #TARGET_REF .",
                "We argue that although promising results were obtained, an improvement can be reached by adding various unsupervised constraints that are motivated by auto-encoders and by language models.",
                "We show that by adding such constraints, superior sentence embeddings can be achieved."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Representing sentences as numerical vectors while capturing their semantic context is an important and useful intermediate step in natural language processing. Representations that are both general and discriminative can serve as a tool for tackling various NLP tasks. While common sentence representation methods are unsupervised in nature, recently, an approach for learning universal sentence representation in a supervised setting was presented in #TARGET_REF . We argue that although promising results were obtained, an improvement can be reached by adding various unsupervised constraints that are motivated by auto-encoders and by language models. We show that by adding such constraints, superior sentence embeddings can be achieved.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"While common sentence representation methods are unsupervised in nature, recently, an approach for learning universal sentence representation in a supervised setting was presented in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "FastSent (#REF) learns to predicts a Bag-Of-Word (BOW) representation of adjacent sentences given a BOW representation of some sentence.",
                "In (#REF) a Hybrid Gaussian Laplacian density function is fitted to the sentence to derive Fisher Vectors.",
                "While previous methods train sentence embeddings in an unsupervised manner, a recent work #TARGET_REF argued that better representations can be achieved via supervised training on a general sentence inference dataset (#REF) .",
                "To this end, the authors use the Stanford Natural Language Inference (SNLI) dataset (#REF) to train different Table 1 : Sentence embedding results.",
                "BiLSTM refers to the original BiLSTM followed by MaxPooling implementation of (#REF) which is the baseline for our work."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "FastSent (#REF) learns to predicts a Bag-Of-Word (BOW) representation of adjacent sentences given a BOW representation of some sentence. In (#REF) a Hybrid Gaussian Laplacian density function is fitted to the sentence to derive Fisher Vectors. While previous methods train sentence embeddings in an unsupervised manner, a recent work #TARGET_REF argued that better representations can be achieved via supervised training on a general sentence inference dataset (#REF) . To this end, the authors use the Stanford Natural Language Inference (SNLI) dataset (#REF) to train different Table 1 : Sentence embedding results. BiLSTM refers to the original BiLSTM followed by MaxPooling implementation of (#REF) which is the baseline for our work.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"While previous methods train sentence embeddings in an unsupervised manner, a recent work #TARGET_REF argued that better representations can be achieved via supervised training on a general sentence inference dataset (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "FastSent (#REF) learns to predicts a Bag-Of-Word (BOW) representation of adjacent sentences given a BOW representation of some sentence.",
                "In (#REF) a Hybrid Gaussian Laplacian density function is fitted to the sentence to derive Fisher Vectors.",
                "While previous methods train sentence embeddings in an unsupervised manner, a recent work (#REF) argued that better representations can be achieved via supervised training on a general sentence inference dataset (#REF) .",
                "To this end, the authors use the Stanford Natural Language Inference (SNLI) dataset (#REF) to train different Table 1 : Sentence embedding results.",
                "BiLSTM refers to the original BiLSTM followed by MaxPooling implementation of #TARGET_REF which is the baseline for our work."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "FastSent (#REF) learns to predicts a Bag-Of-Word (BOW) representation of adjacent sentences given a BOW representation of some sentence. In (#REF) a Hybrid Gaussian Laplacian density function is fitted to the sentence to derive Fisher Vectors. While previous methods train sentence embeddings in an unsupervised manner, a recent work (#REF) argued that better representations can be achieved via supervised training on a general sentence inference dataset (#REF) . To this end, the authors use the Stanford Natural Language Inference (SNLI) dataset (#REF) to train different Table 1 : Sentence embedding results. BiLSTM refers to the original BiLSTM followed by MaxPooling implementation of #TARGET_REF which is the baseline for our work.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"BiLSTM refers to the original BiLSTM followed by MaxPooling implementation of #TARGET_REF which is the baseline for our work.\"]}"
    },
    {
        "gold": {
            "text": [
                "More concretely, given a sequence of T words, {w t } t=1,...,T with given word embedding (#REF; #REF) {v t } t=1,...,T ,a bidirectional LSTM computes a set of T vectors {h t } t=1,...,T where each h t is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions.",
                "We denote { − → h t } and { ← − h t } as the hidden states of the left and right LSTM's respectively, where t = 1, . . . , T .",
                "The final sentence representation is obtained by taking the maximal value of each dimension of the {h t } hidden units (i.e.: max pooling).",
                "The original model of #TARGET_REF was trained on the SNLI dataset in a supervised fashion -given pairs of sentences s 1 and s 2 , denote their representation bys 1 and s 2 .",
                "During training, the concatenation ofs 1 ,s 2 , |s 1 −s 2 | ands 1 * s 2 is fed to a three layer fully connected network followed by a softmax classifier."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "More concretely, given a sequence of T words, {w t } t=1,...,T with given word embedding (#REF; #REF) {v t } t=1,...,T ,a bidirectional LSTM computes a set of T vectors {h t } t=1,...,T where each h t is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions. We denote { − → h t } and { ← − h t } as the hidden states of the left and right LSTM's respectively, where t = 1, . . . , T . The final sentence representation is obtained by taking the maximal value of each dimension of the {h t } hidden units (i.e.: max pooling). The original model of #TARGET_REF was trained on the SNLI dataset in a supervised fashion -given pairs of sentences s 1 and s 2 , denote their representation bys 1 and s 2 . During training, the concatenation ofs 1 ,s 2 , |s 1 −s 2 | ands 1 * s 2 is fed to a three layer fully connected network followed by a softmax classifier.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The original model of #TARGET_REF was trained on the SNLI dataset in a supervised fashion -given pairs of sentences s 1 and s 2 , denote their representation bys 1 and s 2 .\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF we have tested our approach on a wide array of classification tasks, including sentiment analysis (MR -#REF , SST -#REF ), question-type (TREC -#REF ), product reviews (CR - #REF ), subjectivity/objectivity (SUBJ - #REF ) and opinion polarity (MPQA -#REF ).",
                "We also tested our approach on semantic textual similarity (STS 14 - #REF ), paraphrase detection (MRPC - #REF ), entailment and semantic relatedness tasks (SICK-R and SICK-E - #REF ), though those tasks are more close in nature to the task of the SNLI dataset which the model was trained on.",
                "In our experiments we have set λ from eq. (1) and eq. (2) to be 1 and λ 1 , λ 2 from eq. (3) and eq. (4) to be 0.5.",
                "All other hyper-parameters and implementation details were left unchanged to provide a fair comparison to the baseline method of (#REF) .",
                "Our results are summarized in table 1."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Following #TARGET_REF we have tested our approach on a wide array of classification tasks, including sentiment analysis (MR -#REF , SST -#REF ), question-type (TREC -#REF ), product reviews (CR - #REF ), subjectivity/objectivity (SUBJ - #REF ) and opinion polarity (MPQA -#REF ). We also tested our approach on semantic textual similarity (STS 14 - #REF ), paraphrase detection (MRPC - #REF ), entailment and semantic relatedness tasks (SICK-R and SICK-E - #REF ), though those tasks are more close in nature to the task of the SNLI dataset which the model was trained on. In our experiments we have set λ from eq. (1) and eq. (2) to be 1 and λ 1 , λ 2 from eq. (3) and eq. (4) to be 0.5. All other hyper-parameters and implementation details were left unchanged to provide a fair comparison to the baseline method of (#REF) . Our results are summarized in table 1.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following #TARGET_REF we have tested our approach on a wide array of classification tasks, including sentiment analysis (MR -#REF , SST -#REF ), question-type (TREC -#REF ), product reviews (CR - #REF ), subjectivity/objectivity (SUBJ - #REF ) and opinion polarity (MPQA -#REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "Following that, (#REF) proposed a dropout augmented LSTM.",
                "We note that there exists a connection between those two problems and try to model it more explicitly.",
                "Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -#REF, CoVe -#REF Peters et al. (2017 , #REF ) -in this work we jointly train the unsupervised and supervised tasks.",
                "To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by #TARGET_REF .",
                "We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of (#REF) ."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Following that, (#REF) proposed a dropout augmented LSTM. We note that there exists a connection between those two problems and try to model it more explicitly. Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -#REF, CoVe -#REF Peters et al. (2017 , #REF ) -in this work we jointly train the unsupervised and supervised tasks. To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by #TARGET_REF . We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of (#REF) .",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Our approach builds upon the previous work of #TARGET_REF .",
                "Specifically, we use their BiLSTM model with max pooling.",
                "More concretely, given a sequence of T words, {w t } t=1,...,T with given word embedding (#REF; #REF) {v t } t=1,...,T ,a bidirectional LSTM computes a set of T vectors {h t } t=1,...,T where each h t is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions.",
                "We denote { − → h t } and { ← − h t } as the hidden states of the left and right LSTM's respectively, where t = 1, . . . , T .",
                "The final sentence representation is obtained by taking the maximal value of each dimension of the {h t } hidden units (i.e.: max pooling)."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Our approach builds upon the previous work of #TARGET_REF . Specifically, we use their BiLSTM model with max pooling. More concretely, given a sequence of T words, {w t } t=1,...,T with given word embedding (#REF; #REF) {v t } t=1,...,T ,a bidirectional LSTM computes a set of T vectors {h t } t=1,...,T where each h t is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions. We denote { − → h t } and { ← − h t } as the hidden states of the left and right LSTM's respectively, where t = 1, . . . , T . The final sentence representation is obtained by taking the maximal value of each dimension of the {h t } hidden units (i.e.: max pooling).",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"Our approach builds upon the previous work of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Following that, (#REF) proposed a dropout augmented LSTM.",
                "We note that there exists a connection between those two problems and try to model it more explicitly.",
                "Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -#REF, CoVe -#REF Peters et al. (2017 , #REF ) -in this work we jointly train the unsupervised and supervised tasks.",
                "To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by (#REF) .",
                "We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of #TARGET_REF ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Following that, (#REF) proposed a dropout augmented LSTM. We note that there exists a connection between those two problems and try to model it more explicitly. Recently, the incorporation of the hidden states of neural language models in downstream supervised-learning models have been shown to improve the results of the latter (e.g. ElMo -#REF, CoVe -#REF Peters et al. (2017 , #REF ) -in this work we jointly train the unsupervised and supervised tasks. To this end, we incorporate unsupervised regularization terms motivated by language modeling and auto-encoders in the training framework proposed by (#REF) . We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of #TARGET_REF .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We test our proposed model on a set of NLP tasks and show improved results over the baseline framework of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In our work, we have sought to connect unsupervised and supervised learning in the context of sentence embeddings.",
                "Leveraging supervision given by some general task aided in obtaining state-of-the-art sentence representations #TARGET_REF .",
                "However, every supervised learning tasks is prone to overfit.",
                "In this context, overfitting to the learning task will result in a model which generalizes less well to new tasks.",
                "We alleviate this problem by incorporating unsupervised regularization criteria in the model's loss function which are motivated by autoencoders and language models."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "In our work, we have sought to connect unsupervised and supervised learning in the context of sentence embeddings. Leveraging supervision given by some general task aided in obtaining state-of-the-art sentence representations #TARGET_REF . However, every supervised learning tasks is prone to overfit. In this context, overfitting to the learning task will result in a model which generalizes less well to new tasks. We alleviate this problem by incorporating unsupervised regularization criteria in the model's loss function which are motivated by autoencoders and language models.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Leveraging supervision given by some general task aided in obtaining state-of-the-art sentence representations #TARGET_REF .\", \"However, every supervised learning tasks is prone to overfit.\"]}"
    },
    {
        "gold": {
            "text": [
                "There are many methods for recognising timexes that are widely used in natural language engineering.",
                "For English (but not exclusively), in approaches based on supervised learning, sequence labelling methods are often used, especially Conditional Random #REF .",
                "A review of the methods in the article #TARGET_REF about the recognition of timexes for English and Spanish has shown a certain shift within the most popular solutions.",
                "As with the normalisation of timexes, the best results are still achieved with rule-based methods, many new solutions have been introduced in the area of recognition.",
                "The best systems listed in [35] , called TIP#REF and ClearTK [1] , use CRFs for recognition, so initially, we decided to apply the CRF-based approach for this task."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "There are many methods for recognising timexes that are widely used in natural language engineering. For English (but not exclusively), in approaches based on supervised learning, sequence labelling methods are often used, especially Conditional Random #REF . A review of the methods in the article #TARGET_REF about the recognition of timexes for English and Spanish has shown a certain shift within the most popular solutions. As with the normalisation of timexes, the best results are still achieved with rule-based methods, many new solutions have been introduced in the area of recognition. The best systems listed in [35] , called TIP#REF and ClearTK [1] , use CRFs for recognition, so initially, we decided to apply the CRF-based approach for this task.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"A review of the methods in the article #TARGET_REF about the recognition of timexes for English and Spanish has shown a certain shift within the most popular solutions.\"]}"
    },
    {
        "gold": {
            "text": [
                "A review of the methods in the article [35] about the recognition of timexes for English and Spanish has shown a certain shift within the most popular solutions.",
                "As with the normalisation of timexes, the best results are still achieved with rule-based methods, many new solutions have been introduced in the area of recognition.",
                "The best systems listed in #TARGET_REF , called TIP#REF and ClearTK [1] , use CRFs for recognition, so initially, we decided to apply the CRF-based approach for this task.",
                "The results were described in [12, 10] .",
                "In recent years, solutions based on deep neural networks, using word representation in the form of word embeddings, created with the use of large linguistic corpus, have begun to dominate in the field of recognition of word expressions."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "A review of the methods in the article [35] about the recognition of timexes for English and Spanish has shown a certain shift within the most popular solutions. As with the normalisation of timexes, the best results are still achieved with rule-based methods, many new solutions have been introduced in the area of recognition. The best systems listed in #TARGET_REF , called TIP#REF and ClearTK [1] , use CRFs for recognition, so initially, we decided to apply the CRF-based approach for this task. The results were described in [12, 10] . In recent years, solutions based on deep neural networks, using word representation in the form of word embeddings, created with the use of large linguistic corpus, have begun to dominate in the field of recognition of word expressions.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The best systems listed in #TARGET_REF , called TIP#REF and ClearTK [1] , use CRFs for recognition, so initially, we decided to apply the CRF-based approach for this task.\"]}"
    },
    {
        "gold": {
            "text": [
                "Experiments were carried out by the method proposed in #TARGET_REF .",
                "The first part is described as Task A, the purpose of which is to identify the boundaries of timexes and assign them to one of the following classes:",
                "9 http://nlp.pwr.edu.pl/ 10 https://github.com/CLARIN-PL/PolDeepNer date, time, duration, set.",
                "[%]  all  1635  100  train  1227  50  test  408  25   Table 5 : Evaluation data sets (source: KPWr)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "Experiments were carried out by the method proposed in #TARGET_REF . The first part is described as Task A, the purpose of which is to identify the boundaries of timexes and assign them to one of the following classes: 9 http://nlp.pwr.edu.pl/ 10 https://github.com/CLARIN-PL/PolDeepNer date, time, duration, set. [%]  all  1635  100  train  1227  50  test  408  25   Table 5 : Evaluation data sets (source: KPWr).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Experiments were carried out by the method proposed in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The results are presented in Tables 6, 7 and 8.",
                "We chose the best 3 results from each word embeddings group (EE, EP, EC) from Table 8 presenting F1-scores for all models.",
                "Then we evaluated these results using more detailed measures for timexes, presented in #TARGET_REF .",
                "The following measures were used to evaluate the quality of boundaries and class recognition, socalled strict match: strict precision (Str.P), strict recall (Str.R) and strict F1-score (Str.F1).",
                "A relaxed match (Rel."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The results are presented in Tables 6, 7 and 8. We chose the best 3 results from each word embeddings group (EE, EP, EC) from Table 8 presenting F1-scores for all models. Then we evaluated these results using more detailed measures for timexes, presented in #TARGET_REF . The following measures were used to evaluate the quality of boundaries and class recognition, socalled strict match: strict precision (Str.P), strict recall (Str.R) and strict F1-score (Str.F1). A relaxed match (Rel.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Then we evaluated these results using more detailed measures for timexes, presented in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "F1) evaluation has also been carried out to determine whether there is an overlap between the system entity and gold entity, e.g.",
                "[Sunday] and [Sunday morning] [35] .",
                "If there was an overlap, a relaxed type F1-score (Type.F1) was calculated #TARGET_REF .",
                "The results are presented in Table 9 .",
                "Table 6 : Evaluation results (precision) for 17 word embeddings models for each TIMEX3 class (date, time, duration and set)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "F1) evaluation has also been carried out to determine whether there is an overlap between the system entity and gold entity, e.g. [Sunday] and [Sunday morning] [35] . If there was an overlap, a relaxed type F1-score (Type.F1) was calculated #TARGET_REF . The results are presented in Table 9 . Table 6 : Evaluation results (precision) for 17 word embeddings models for each TIMEX3 class (date, time, duration and set).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"If there was an overlap, a relaxed type F1-score (Type.F1) was calculated #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The ability of the model to provide vector representation for the unknown words seems to be the most important.",
                "Also, previous models built using KGR10 (EP) are probably less accurate due to an incorrect tokenisation of the corpus.",
                "We used WCRFT tagger [29] , which utilises #REF to tokenise the input text before the creation of the embeddings model.",
                "The comparison of EC1 with previous results obtained using only CRF [9] show the significant improvement across all the tested metrics: 3.6pp increase in strict F1-score, 1.36pp increase in relaxed precision, 5.61pp increase in relaxed recall and 3.51pp increase in relaxed F1-score.",
                "Table 9 : Evaluation results for all TIMEX3 classes (total) for 9 word embeddings models (3 best models from each embeddings group: EE, EP, EC from Table 8 ) using the following measures from #TARGET_REF : strict precision, strict recall, strict F1-score, relaxed precision, relaxed recall, relaxed F1-score, type F1-score."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The ability of the model to provide vector representation for the unknown words seems to be the most important. Also, previous models built using KGR10 (EP) are probably less accurate due to an incorrect tokenisation of the corpus. We used WCRFT tagger [29] , which utilises #REF to tokenise the input text before the creation of the embeddings model. The comparison of EC1 with previous results obtained using only CRF [9] show the significant improvement across all the tested metrics: 3.6pp increase in strict F1-score, 1.36pp increase in relaxed precision, 5.61pp increase in relaxed recall and 3.51pp increase in relaxed F1-score. Table 9 : Evaluation results for all TIMEX3 classes (total) for 9 word embeddings models (3 best models from each embeddings group: EE, EP, EC from Table 8 ) using the following measures from #TARGET_REF : strict precision, strict recall, strict F1-score, relaxed precision, relaxed recall, relaxed F1-score, type F1-score.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Table 9 : Evaluation results for all TIMEX3 classes (total) for 9 word embeddings models (3 best models from each embeddings group: EE, EP, EC from Table 8 ) using the following measures from #TARGET_REF : strict precision, strict recall, strict F1-score, relaxed precision, relaxed recall, relaxed F1-score, type F1-score.\"]}"
    },
    {
        "gold": {
            "text": [
                "Leading recent approaches to domain adaptation in NLP are based on Neural Networks (NNs), and particularly on autoencoders (#REF; #REF) .",
                "These models are believed to extract features that are robust to cross-domain variations.",
                "However, while excelling on benchmark domain adaptation tasks such as cross-domain product sentiment classification #TARGET_REF , the reasons to this success are not entirely understood.",
                "In the pre-NN era, a prominent approach to domain adaptation in NLP, and particularly in sentiment classification, has been structural correspondence learning (SCL) (#REF (#REF .",
                "Following the auxiliary problems approach to semi-supervised learning (#REF) , this method identifies correspondences among features from different domains by modeling their correlations with pivot features: features that are frequent in both domains and are important for the NLP task."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Leading recent approaches to domain adaptation in NLP are based on Neural Networks (NNs), and particularly on autoencoders (#REF; #REF) . These models are believed to extract features that are robust to cross-domain variations. However, while excelling on benchmark domain adaptation tasks such as cross-domain product sentiment classification #TARGET_REF , the reasons to this success are not entirely understood. In the pre-NN era, a prominent approach to domain adaptation in NLP, and particularly in sentiment classification, has been structural correspondence learning (SCL) (#REF (#REF . Following the auxiliary problems approach to semi-supervised learning (#REF) , this method identifies correspondences among features from different domains by modeling their correlations with pivot features: features that are frequent in both domains and are important for the NLP task.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"However, while excelling on benchmark domain adaptation tasks such as cross-domain product sentiment classification #TARGET_REF , the reasons to this success are not entirely understood.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, while excelling on benchmark domain adaptation tasks such as cross-domain product sentiment classification (#REF) , the reasons to this success are not entirely understood.",
                "In the pre-NN era, a prominent approach to domain adaptation in NLP, and particularly in sentiment classification, has been structural correspondence learning (SCL) (#REF #TARGET_REF .",
                "Following the auxiliary problems approach to semi-supervised learning (#REF) , this method identifies correspondences among features from different domains by modeling their correlations with pivot features: features that are frequent in both domains and are important for the NLP task.",
                "Non-pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, providing a bridge between the domains.",
                "Elegant and well motivated as it may be, SCL does not form the state-of-the-art since the neural approaches took over."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "However, while excelling on benchmark domain adaptation tasks such as cross-domain product sentiment classification (#REF) , the reasons to this success are not entirely understood. In the pre-NN era, a prominent approach to domain adaptation in NLP, and particularly in sentiment classification, has been structural correspondence learning (SCL) (#REF #TARGET_REF . Following the auxiliary problems approach to semi-supervised learning (#REF) , this method identifies correspondences among features from different domains by modeling their correlations with pivot features: features that are frequent in both domains and are important for the NLP task. Non-pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, providing a bridge between the domains. Elegant and well motivated as it may be, SCL does not form the state-of-the-art since the neural approaches took over.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In the pre-NN era, a prominent approach to domain adaptation in NLP, and particularly in sentiment classification, has been structural correspondence learning (SCL) (#REF #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Pivot and Non-Pivot Features The definitions of this approach are given in Blitzer et al. (2006 #TARGET_REF , where SCL is presented in the context of POS tagging and sentiment classification, respectively.",
                "Fundamentally, the method divides the shared feature space of both the source and the target domains to the set of pivot features that are frequent in both domains and are prominent in the NLP task, and a complementary set of non-pivot features.",
                "In this section we abstract away from the actual feature space and its division to pivot and non-pivot subsets.",
                "In Section 4 we discuss this issue in the context of sentiment classification.",
                "For representation learning, SCL employs the pivot features in order to learn mappings from the original feature space of both domains to a shared, low-dimensional, real-valued feature space."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Pivot and Non-Pivot Features The definitions of this approach are given in Blitzer et al. (2006 #TARGET_REF , where SCL is presented in the context of POS tagging and sentiment classification, respectively. Fundamentally, the method divides the shared feature space of both the source and the target domains to the set of pivot features that are frequent in both domains and are prominent in the NLP task, and a complementary set of non-pivot features. In this section we abstract away from the actual feature space and its division to pivot and non-pivot subsets. In Section 4 we discuss this issue in the context of sentiment classification. For representation learning, SCL employs the pivot features in order to learn mappings from the original feature space of both domains to a shared, low-dimensional, real-valued feature space.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Pivot and Non-Pivot Features The definitions of this approach are given in Blitzer et al. (2006 #TARGET_REF , where SCL is presented in the context of POS tagging and sentiment classification, respectively.\"]}"
    },
    {
        "gold": {
            "text": [
                "An important observation of #TARGET_REF , is that some pivot features are similar to each other to the level that they indicate the same information with respect to the classification task.",
                "For example, in sentiment classification with word unigram features, the words (unigrams) great and excellent are likely to serve as pivot features, as the meaning of each of them is preserved across domains.",
                "At the same time, both features convey very similar (positive) sentiment information to the level that a sentiment classifier should treat them as equals.",
                "The AE-SCL-SR model is based on two crucial observations.",
                "First, in many NLP tasks the pivot features can be pre-embeded into a vector space where pivots with similar meaning have similar vectors."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "An important observation of #TARGET_REF , is that some pivot features are similar to each other to the level that they indicate the same information with respect to the classification task. For example, in sentiment classification with word unigram features, the words (unigrams) great and excellent are likely to serve as pivot features, as the meaning of each of them is preserved across domains. At the same time, both features convey very similar (positive) sentiment information to the level that a sentiment classifier should treat them as equals. The AE-SCL-SR model is based on two crucial observations. First, in many NLP tasks the pivot features can be pre-embeded into a vector space where pivots with similar meaning have similar vectors.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"An important observation of #TARGET_REF , is that some pivot features are similar to each other to the level that they indicate the same information with respect to the classification task.\"]}"
    },
    {
        "gold": {
            "text": [
                "As this is an end-to-end model that predicts the sentiment class jointly with the new feature representation, we do not employ any additional sentiment classifier.",
                "As in the other models, MSDA-DAN utilizes source domain labeled data as well as unlabeled data from both the source and the target domains at training time.",
                "We experiment with a 5-fold cross-validation on the source domain (#REF) : 1600 reviews for training and 400 reviews for development.",
                "The test set for each target domain of #TARGET_REF consists of all 2000 labeled reviews of that domain, and for the Blog domain it consists of the 7086 labeled sentences provided with the task dataset.",
                "In all five folds half of the training examples and half of the development examples are randomly selected from the positive reviews and the other halves from the negative reviews."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "As this is an end-to-end model that predicts the sentiment class jointly with the new feature representation, we do not employ any additional sentiment classifier. As in the other models, MSDA-DAN utilizes source domain labeled data as well as unlabeled data from both the source and the target domains at training time. We experiment with a 5-fold cross-validation on the source domain (#REF) : 1600 reviews for training and 400 reviews for development. The test set for each target domain of #TARGET_REF consists of all 2000 labeled reviews of that domain, and for the Blog domain it consists of the 7086 labeled sentences provided with the task dataset. In all five folds half of the training examples and half of the development examples are randomly selected from the positive reviews and the other halves from the negative reviews.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The test set for each target domain of #TARGET_REF consists of all 2000 labeled reviews of that domain, and for the Blog domain it consists of the 7086 labeled sentences provided with the task dataset.\"]}"
    },
    {
        "gold": {
            "text": [
                "As explained in Section 3, this approach encourages the model to learn similar hidden layers for documents that have different pivot features as long as these features have similar meaning.",
                "In sentiment classification, for example, although one positive review may use the unigram pivot feature excellent while another positive review uses the pivot great, as long as the embeddings of pivot features with similar meaning are similar (as expected from high quality embeddings) the hidden layers learned for both documents are biased to be similar.",
                "We experiment with the task of cross-domain product sentiment classification of #TARGET_REF , consisting of 4 domains (12 domain pairs) and further add an additional target domain, consisting of sentences extracted from social media blogs (total of 16 domain pairs).",
                "For pivot feature embedding in our advanced model, we employ the word2vec algorithm (#REF) .",
                "Our models substantially outperform strong baselines: the SCL algorithm, the marginalized stacked denoising autoencoder (MSDA) model (#REF) and the MSDA-DAN model (#REF) that combines the power of MSDA with a domain adversarial network (DAN)."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "As explained in Section 3, this approach encourages the model to learn similar hidden layers for documents that have different pivot features as long as these features have similar meaning. In sentiment classification, for example, although one positive review may use the unigram pivot feature excellent while another positive review uses the pivot great, as long as the embeddings of pivot features with similar meaning are similar (as expected from high quality embeddings) the hidden layers learned for both documents are biased to be similar. We experiment with the task of cross-domain product sentiment classification of #TARGET_REF , consisting of 4 domains (12 domain pairs) and further add an additional target domain, consisting of sentences extracted from social media blogs (total of 16 domain pairs). For pivot feature embedding in our advanced model, we employ the word2vec algorithm (#REF) . Our models substantially outperform strong baselines: the SCL algorithm, the marginalized stacked denoising autoencoder (MSDA) model (#REF) and the MSDA-DAN model (#REF) that combines the power of MSDA with a domain adversarial network (DAN).",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"We experiment with the task of cross-domain product sentiment classification of #TARGET_REF , consisting of 4 domains (12 domain pairs) and further add an additional target domain, consisting of sentences extracted from social media blogs (total of 16 domain pairs).\"]}"
    },
    {
        "gold": {
            "text": [
                "We denote the feature set in our problem with f , the subset of pivot features with f p ⊆ {1, . . . , |f |} and the subset of non-pivot features with f np ⊆ {1, . . . , |f |} such that f p ∪ f np = {1, . . . , |f |} and f p ∩ f np = ∅. We further denote the feature representation of an input example X with x. Following this notation, the vector of pivot features of X is denoted with x p while the vector of non-pivot features is denoted with x np .",
                "In order to learn a robust and compact feature representation for X we will aim to learn a nonlinear prediction function from x np to x p .",
                "As discussed in Section 4 the task we experiment with is cross-domain sentiment classification.",
                "Following previous work (e.g. (#REF #TARGET_REF #REF) our feature representation consists of binary indicators for the occurrence of word unigrams and bigrams in the represented document.",
                "In what follows we hence assume that the feature representation x of an example X is a binary vector, and hence so are x p and x np ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We denote the feature set in our problem with f , the subset of pivot features with f p ⊆ {1, . . . , |f |} and the subset of non-pivot features with f np ⊆ {1, . . . , |f |} such that f p ∪ f np = {1, . . . , |f |} and f p ∩ f np = ∅. We further denote the feature representation of an input example X with x. Following this notation, the vector of pivot features of X is denoted with x p while the vector of non-pivot features is denoted with x np . In order to learn a robust and compact feature representation for X we will aim to learn a nonlinear prediction function from x np to x p . As discussed in Section 4 the task we experiment with is cross-domain sentiment classification. Following previous work (e.g. (#REF #TARGET_REF #REF) our feature representation consists of binary indicators for the occurrence of word unigrams and bigrams in the represented document. In what follows we hence assume that the feature representation x of an example X is a binary vector, and hence so are x p and x np .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following previous work (e.g. (#REF #TARGET_REF #REF) our feature representation consists of binary indicators for the occurrence of word unigrams and bigrams in the represented document.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section we describe our experiments.",
                "To facilitate clarity, some details are not given here and instead are provided in the appendices.",
                "Cross-domain Sentiment Classification To demonstrate the power of our models for domain adaptation we experiment with the task of crossdomain sentiment classification #TARGET_REF .",
                "The data for this task consist of Amazon product reviews from four product domains: Books (B), DVDs (D), Electronic items (E) and Kitchen appliances (K).",
                "For each domain 2000 labeled reviews are provided: 1000 are classified as positive and 1000 as negative, and these are augmented with unlabeled reviews: 6000 (B), 34741 (D), 13153 (E) and 16785 (K)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In this section we describe our experiments. To facilitate clarity, some details are not given here and instead are provided in the appendices. Cross-domain Sentiment Classification To demonstrate the power of our models for domain adaptation we experiment with the task of crossdomain sentiment classification #TARGET_REF . The data for this task consist of Amazon product reviews from four product domains: Books (B), DVDs (D), Electronic items (E) and Kitchen appliances (K). For each domain 2000 labeled reviews are provided: 1000 are classified as positive and 1000 as negative, and these are augmented with unlabeled reviews: 6000 (B), 34741 (D), 13153 (E) and 16785 (K).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Cross-domain Sentiment Classification To demonstrate the power of our models for domain adaptation we experiment with the task of crossdomain sentiment classification #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We hence compare our models to three strong baselines, running all models under the same conditions.",
                "We aim to select baselines that represent the state-of-the-art in cross-domain sentiment classification in general, and in the two lines of work we focus at: pivot based and autoencoder based representation learning, in particular.",
                "The first baseline is SCL with pivot features selected using the mutual information criterion (SCL-MI, #TARGET_REF ).",
                "This is the SCL method where pivot features are frequent in the unlabeled data of both the source and the target do-mains, and among those features are the ones with the highest mutual information with the task (sentiment) label in the source domain labeled data.",
                "We implemented this method."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We hence compare our models to three strong baselines, running all models under the same conditions. We aim to select baselines that represent the state-of-the-art in cross-domain sentiment classification in general, and in the two lines of work we focus at: pivot based and autoencoder based representation learning, in particular. The first baseline is SCL with pivot features selected using the mutual information criterion (SCL-MI, #TARGET_REF ). This is the SCL method where pivot features are frequent in the unlabeled data of both the source and the target do-mains, and among those features are the ones with the highest mutual information with the task (sentiment) label in the source domain labeled data. We implemented this method.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The first baseline is SCL with pivot features selected using the mutual information criterion (SCL-MI, #TARGET_REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "MSDA-DAN gets an input representation that consists of a concatenation of the original and the MSDA-induced feature sets.",
                "As this is an end-to-end model that predicts the sentiment class jointly with the new feature representation, we do not employ any additional sentiment classifier.",
                "As in the other models, MSDA-DAN utilizes source domain labeled data as well as unlabeled data from both the source and the target domains at training time.",
                "We experiment with a 5-fold cross-validation on the source domain #TARGET_REF : 1600 reviews for training and 400 reviews for development.",
                "The test set for each target domain of #REF consists of all 2000 labeled reviews of that domain, and for the Blog domain it consists of the 7086 labeled sentences provided with the task dataset."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "MSDA-DAN gets an input representation that consists of a concatenation of the original and the MSDA-induced feature sets. As this is an end-to-end model that predicts the sentiment class jointly with the new feature representation, we do not employ any additional sentiment classifier. As in the other models, MSDA-DAN utilizes source domain labeled data as well as unlabeled data from both the source and the target domains at training time. We experiment with a 5-fold cross-validation on the source domain #TARGET_REF : 1600 reviews for training and 400 reviews for development. The test set for each target domain of #REF consists of all 2000 labeled reviews of that domain, and for the Blog domain it consists of the 7086 labeled sentences provided with the task dataset.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We experiment with a 5-fold cross-validation on the source domain #TARGET_REF : 1600 reviews for training and 400 reviews for development.\"]}"
    },
    {
        "gold": {
            "text": [
                "The number of pivots was chosen among {100, 200, . . . , 500} and the dimensionality of h among {100, 300, 500}. For the features induced by these models we take their w h x np vector.",
                "For AE-SCL-SR, embeddings for the unigram and bigram features were learned with word2vec (#REF) .",
                "Details about the software and the way we learn bigram representations are in the appendices.",
                "Baselines: For SCL-MI, following #TARGET_REF we tuned the number of pivot features (#REF; #REF) between 500 and 1000 and the SVD dimensions among 50,100 and 150.",
                "For MSDA we tuned the number of reconstructed features among {500, 1000, 2000, 5000, 10000}, the number of model layers among {1, 3, 5} and the corruption probability among {0.1, 0.2, . . . , 0.5}. For MSDA-DAN, we followed #REF : the λ adaptation parameter is chosen among 9 values between 10 −2 and 1 on a logarithmic scale, the hidden layer size l is chosen among {50, 100, 200} and the learning rate µ is 10 −3 ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The number of pivots was chosen among {100, 200, . . . , 500} and the dimensionality of h among {100, 300, 500}. For the features induced by these models we take their w h x np vector. For AE-SCL-SR, embeddings for the unigram and bigram features were learned with word2vec (#REF) . Details about the software and the way we learn bigram representations are in the appendices. Baselines: For SCL-MI, following #TARGET_REF we tuned the number of pivot features (#REF; #REF) between 500 and 1000 and the SVD dimensions among 50,100 and 150. For MSDA we tuned the number of reconstructed features among {500, 1000, 2000, 5000, 10000}, the number of model layers among {1, 3, 5} and the corruption probability among {0.1, 0.2, . . . , 0.5}. For MSDA-DAN, we followed #REF : the λ adaptation parameter is chosen among 9 values between 10 −2 and 1 on a logarithmic scale, the hidden layer size l is chosen among {50, 100, 200} and the learning rate µ is 10 −3 .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Baselines: For SCL-MI, following #TARGET_REF we tuned the number of pivot features (#REF; #REF) between 500 and 1000 and the SVD dimensions among 50,100 and 150.\"]}"
    },
    {
        "gold": {
            "text": [
                "Variants of the Product Review Data There are two releases of the datasets of the #TARGET_REF cross-domain product review task.",
                "We use the one from http://www.cs.jhu.",
                "edu/˜mdredze/datasets/sentiment/ index2.html where the data is imbalanced, consisting of more positive than negative reviews.",
                "We believe that our setup is more realistic as when collecting unlabeled data, it is hard to get a balanced set.",
                "Note that #REF used the other release where the unlabeled data consists of the same number of positive and negative reviews."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Variants of the Product Review Data There are two releases of the datasets of the #TARGET_REF cross-domain product review task. We use the one from http://www.cs.jhu. edu/˜mdredze/datasets/sentiment/ index2.html where the data is imbalanced, consisting of more positive than negative reviews. We believe that our setup is more realistic as when collecting unlabeled data, it is hard to get a balanced set. Note that #REF used the other release where the unlabeled data consists of the same number of positive and negative reviews.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Variants of the Product Review Data There are two releases of the datasets of the #TARGET_REF cross-domain product review task.\", \"We use the one from http://www.cs.jhu.\"]}"
    },
    {
        "gold": {
            "text": [
                "edu/˜mdredze/datasets/sentiment/ index2.html where the data is imbalanced, consisting of more positive than negative reviews.",
                "We believe that our setup is more realistic as when collecting unlabeled data, it is hard to get a balanced set.",
                "Note that #TARGET_REF used the other release where the unlabeled data consists of the same number of positive and negative reviews.",
                "Test Set Size While #REF used only 400 target domain reviews for test, we use the entire set of 2000 reviews.",
                "We believe that this decision yields more robust and statistically significant results."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "edu/˜mdredze/datasets/sentiment/ index2.html where the data is imbalanced, consisting of more positive than negative reviews. We believe that our setup is more realistic as when collecting unlabeled data, it is hard to get a balanced set. Note that #TARGET_REF used the other release where the unlabeled data consists of the same number of positive and negative reviews. Test Set Size While #REF used only 400 target domain reviews for test, we use the entire set of 2000 reviews. We believe that this decision yields more robust and statistically significant results.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We believe that our setup is more realistic as when collecting unlabeled data, it is hard to get a balanced set.\", \"Note that #TARGET_REF used the other release where the unlabeled data consists of the same number of positive and negative reviews.\"]}"
    },
    {
        "gold": {
            "text": [
                "edu/˜mdredze/datasets/sentiment/ index2.html where the data is imbalanced, consisting of more positive than negative reviews.",
                "We believe that our setup is more realistic as when collecting unlabeled data, it is hard to get a balanced set.",
                "Note that #REF used the other release where the unlabeled data consists of the same number of positive and negative reviews.",
                "Test Set Size While #TARGET_REF used only 400 target domain reviews for test, we use the entire set of 2000 reviews.",
                "We believe that this decision yields more robust and statistically significant results."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "edu/˜mdredze/datasets/sentiment/ index2.html where the data is imbalanced, consisting of more positive than negative reviews. We believe that our setup is more realistic as when collecting unlabeled data, it is hard to get a balanced set. Note that #REF used the other release where the unlabeled data consists of the same number of positive and negative reviews. Test Set Size While #TARGET_REF used only 400 target domain reviews for test, we use the entire set of 2000 reviews. We believe that this decision yields more robust and statistically significant results.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Test Set Size While #TARGET_REF used only 400 target domain reviews for test, we use the entire set of 2000 reviews.\"]}"
    },
    {
        "gold": {
            "text": [
                "There are only very few studies that attempt to apply neural network approaches to the MD task.",
                "Lee et al. (2017; #TARGET_REF first introduced a neural mention detector as a part of their end-to-end coreference system; however, the system does not output intermediate mentions, hence the mention detector cannot be used by other coreference systems directly.",
                "To the best of our knowledge, #REF introduced the only standalone neural mention detector.",
                "By using a modified version of the NER system of #REF , they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system (#REF) .",
                "To build a high accuracy standalone MD system is not only important for the downstream applications, but also beneficial for annotation tasks that require mentions (#REF; #REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "There are only very few studies that attempt to apply neural network approaches to the MD task. Lee et al. (2017; #TARGET_REF first introduced a neural mention detector as a part of their end-to-end coreference system; however, the system does not output intermediate mentions, hence the mention detector cannot be used by other coreference systems directly. To the best of our knowledge, #REF introduced the only standalone neural mention detector. By using a modified version of the NER system of #REF , they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system (#REF) . To build a high accuracy standalone MD system is not only important for the downstream applications, but also beneficial for annotation tasks that require mentions (#REF; #REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Lee et al. (2017; #TARGET_REF first introduced a neural mention detector as a part of their end-to-end coreference system; however, the system does not output intermediate mentions, hence the mention detector cannot be used by other coreference systems directly.\"]}"
    },
    {
        "gold": {
            "text": [
                "Many other coreference systems simply use all the NPs as the candidate mentions (Björkelund and #REF; #REF; #REF) .",
                "#REF first introduced a neural network based end-to-end coreference system in which the neural mention detection part is not separated.",
                "This move proved very effective; however, as a result the mention detection part of their system needs to be trained jointly with the coreference resolution part, hence can not be used separately.",
                "The system has been later extended by #REF and #TARGET_REF .",
                "#REF added biaffine attention to the coreference part of the #REF system, improving the system by 0.6%."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Many other coreference systems simply use all the NPs as the candidate mentions (Björkelund and #REF; #REF; #REF) . #REF first introduced a neural network based end-to-end coreference system in which the neural mention detection part is not separated. This move proved very effective; however, as a result the mention detection part of their system needs to be trained jointly with the coreference resolution part, hence can not be used separately. The system has been later extended by #REF and #TARGET_REF . #REF added biaffine attention to the coreference part of the #REF system, improving the system by 0.6%.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The system has been later extended by #REF and #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "After training the system with the new setting, we get an average F1 of 72.6% (see table 4), which narrows the performance gap between the end-to-end system and the model trained without the joint learning.",
                "This confirms our first hypothesis that by downgrading the system to a pipeline setting does harm the overall performance of the coreference resolution.",
                "For our second experiment, we used the #REF instead.",
                "The #TARGET_REF system is an extended version of the #REF system, hence they share most of the network architecture.",
                "The #REF has a lower performance on mention detection (93.5% recall when λ = 0.4), which creates a large (4%) difference when compared with the recall of our BIAFFINE MD."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "After training the system with the new setting, we get an average F1 of 72.6% (see table 4), which narrows the performance gap between the end-to-end system and the model trained without the joint learning. This confirms our first hypothesis that by downgrading the system to a pipeline setting does harm the overall performance of the coreference resolution. For our second experiment, we used the #REF instead. The #TARGET_REF system is an extended version of the #REF system, hence they share most of the network architecture. The #REF has a lower performance on mention detection (93.5% recall when λ = 0.4), which creates a large (4%) difference when compared with the recall of our BIAFFINE MD.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The #TARGET_REF system is an extended version of the #REF system, hence they share most of the network architecture.\"]}"
    },
    {
        "gold": {
            "text": [
                "The third system takes the outputs from BERT (#REF) and feeds them into a feed-forward neural network to classify candidates into mentions and non mentions.",
                "We evaluate these three models on both the CONLL and the CRAC data sets, with the following results.",
                "Firstly, we show that better mention performance of up to 1.5 percentage points 1 can be achieved by training the mention detector alone.",
                "Secondly, our best system achieves improvements of 5.3 and 6.5 percentage points when compared with #REF 's neural MD system on CONLL and CRAC respectively.",
                "Thirdly, by using better mentions from our mention detector, we can improve the end-to-end #TARGET_REF system and the Clark and Manning (2016a) pipeline system by up to 0.7% and 1.7% respectively."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The third system takes the outputs from BERT (#REF) and feeds them into a feed-forward neural network to classify candidates into mentions and non mentions. We evaluate these three models on both the CONLL and the CRAC data sets, with the following results. Firstly, we show that better mention performance of up to 1.5 percentage points 1 can be achieved by training the mention detector alone. Secondly, our best system achieves improvements of 5.3 and 6.5 percentage points when compared with #REF 's neural MD system on CONLL and CRAC respectively. Thirdly, by using better mentions from our mention detector, we can improve the end-to-end #TARGET_REF system and the Clark and Manning (2016a) pipeline system by up to 0.7% and 1.7% respectively.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"Thirdly, by using better mentions from our mention detector, we can improve the end-to-end #TARGET_REF system and the Clark and Manning (2016a) pipeline system by up to 0.7% and 1.7% respectively.\"]}"
    },
    {
        "gold": {
            "text": [
                "For the mention detection evaluation we use the #REF system as baseline.",
                "The baseline is trained end-toend on the coreference task and we use as baseline the mentions predicted by the system before carrying out coreference resolution.",
                "For the coreference evaluation we use the state-of-the-art #REF system as our baseline for the end-to-end system, and the Clark and Manning (2016a) system as our baseline for the pipeline system.",
                "During the evaluation, we slightly modified the #TARGET_REF system to allow the system to take the mentions predicted by our model instead of its internal mention detector.",
                "Other than that we keep the system unchanged."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "For the mention detection evaluation we use the #REF system as baseline. The baseline is trained end-toend on the coreference task and we use as baseline the mentions predicted by the system before carrying out coreference resolution. For the coreference evaluation we use the state-of-the-art #REF system as our baseline for the end-to-end system, and the Clark and Manning (2016a) system as our baseline for the pipeline system. During the evaluation, we slightly modified the #TARGET_REF system to allow the system to take the mentions predicted by our model instead of its internal mention detector. Other than that we keep the system unchanged.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"During the evaluation, we slightly modified the #TARGET_REF system to allow the system to take the mentions predicted by our model instead of its internal mention detector.\"]}"
    },
    {
        "gold": {
            "text": [
                "Evaluation on the CRAC data set 3 For the CRAC data set, we train the #REF system end-to-end on the reduced corpus with singleton mentions removed and extract mentions from the system by set λ = 0.4.",
                "We then train our models with the same λ but on the full corpus, since our mention detectors naturally support both mention 3 As the #TARGET_REF system does not predict singleton mentions, the results on CRAC data set in Table 2 are evaluated without singleton mentions.",
                "While the results reported in Table 3 are evaluated with singleton mentions included.",
                "88.0 89.7 89.1 Table 3 : Comparison between our BIAFFINE MD and the top performing systems on the mention detection task using the CONLL and CRAC data sets.",
                "types (singleton and non-singleton mentions)."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Evaluation on the CRAC data set 3 For the CRAC data set, we train the #REF system end-to-end on the reduced corpus with singleton mentions removed and extract mentions from the system by set λ = 0.4. We then train our models with the same λ but on the full corpus, since our mention detectors naturally support both mention 3 As the #TARGET_REF system does not predict singleton mentions, the results on CRAC data set in Table 2 are evaluated without singleton mentions. While the results reported in Table 3 are evaluated with singleton mentions included. 88.0 89.7 89.1 Table 3 : Comparison between our BIAFFINE MD and the top performing systems on the mention detection task using the CONLL and CRAC data sets. types (singleton and non-singleton mentions).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We then train our models with the same \\u03bb but on the full corpus, since our mention detectors naturally support both mention 3 As the #TARGET_REF system does not predict singleton mentions, the results on CRAC data set in Table 2 are evaluated without singleton mentions.\"]}"
    },
    {
        "gold": {
            "text": [
                "Evaluation with the end-to-end system.",
                "We first evaluate our BIAFFINE MD in combination with the end-to-end #TARGET_REF system.",
                "We slightly modified the system to feed the system mentions predicted by our mention detector.",
                "As a result, the original mention selection function is switched off, we keep all the other settings (include the mention scoring function) unchanged.",
                "We then train the modified system to obtain a new model."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "Evaluation with the end-to-end system. We first evaluate our BIAFFINE MD in combination with the end-to-end #TARGET_REF system. We slightly modified the system to feed the system mentions predicted by our mention detector. As a result, the original mention selection function is switched off, we keep all the other settings (include the mention scoring function) unchanged. We then train the modified system to obtain a new model.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We first evaluate our BIAFFINE MD in combination with the end-to-end #TARGET_REF system.\", \"We slightly modified the system to feed the system mentions predicted by our mention detector.\", \"As a result, the original mention selection function is switched off, we keep all the other settings (include the mention scoring function) unchanged.\"]}"
    },
    {
        "gold": {
            "text": [
                "The third type of projection, P Acc , integrates syntactic information in the edge weights calculated by the following formula: While the entity grid #TARGET_REF uses information about sentences which do not share entities by means of the \"--\" transition, the entity graph cannot employ this negative information.",
                "Here, we propose a normalization for the entity graph and its corresponding one-mode projections which is based on the relative importance of entities and, in turn, the relative importance of sentences.",
                "Including negative information allows to normalize the importance of entities according to sentence length (measured in terms of entity mentions), and hence to capture distance information between mentions of the same entity.",
                "This brings the entity graph closer to Stoddard's (1991, p.30 ) notion of cohesion: \"The relative cohesiveness of a text depends on the number of cohesive ties [...] and on the distance between the nodes and their associated cohesive elements.",
                "\" By using this information, edge weights are set less arbitrary which leads to the more sound method and higher performance in all tasks."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The third type of projection, P Acc , integrates syntactic information in the edge weights calculated by the following formula: While the entity grid #TARGET_REF uses information about sentences which do not share entities by means of the \"--\" transition, the entity graph cannot employ this negative information. Here, we propose a normalization for the entity graph and its corresponding one-mode projections which is based on the relative importance of entities and, in turn, the relative importance of sentences. Including negative information allows to normalize the importance of entities according to sentence length (measured in terms of entity mentions), and hence to capture distance information between mentions of the same entity. This brings the entity graph closer to Stoddard's (1991, p.30 ) notion of cohesion: \"The relative cohesiveness of a text depends on the number of cohesive ties [...] and on the distance between the nodes and their associated cohesive elements. \" By using this information, edge weights are set less arbitrary which leads to the more sound method and higher performance in all tasks.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"The third type of projection, P Acc , integrates syntactic information in the edge weights calculated by the following formula: While the entity grid #TARGET_REF uses information about sentences which do not share entities by means of the \\\"--\\\" transition, the entity graph cannot employ this negative information.\", \"Here, we propose a normalization for the entity graph and its corresponding one-mode projections which is based on the relative importance of entities and, in turn, the relative importance of sentences.\"]}"
    },
    {
        "gold": {
            "text": [
                "Experimental setup and data follow #REF (61 documents from the English test part of the CoNLL 2012 shared task (#REF) ).",
                "For discrimination we use 20 permutations of each text.",
                "Table 1 shows the results.",
                "Results for #REF , G&S, are reproduced, results for #TARGET_REF , B&L, and #REF , E&C, were reproduced by #REF .",
                "The unweighted graph, P U , does not need normalization."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Experimental setup and data follow #REF (61 documents from the English test part of the CoNLL 2012 shared task (#REF) ). For discrimination we use 20 permutations of each text. Table 1 shows the results. Results for #REF , G&S, are reproduced, results for #TARGET_REF , B&L, and #REF , E&C, were reproduced by #REF . The unweighted graph, P U , does not need normalization.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Results for #REF , G&S, are reproduced, results for #TARGET_REF , B&L, and #REF , E&C, were reproduced by #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In experiments, #TARGET_REF assume that articles taken from Encyclopedia Britannica are more difficult to read (less coherent) than the corresponding articles from Encyclopedia Britannica Elementary, its version for children.",
                "We follow them with regard to data (107 article pairs), experimental setup and evaluation.",
                "Sentences in the Britannica Elementary are simpler and shorter than in the Encyclopedia Britannica.",
                "The entity graph does not take into account the effect of entities not shared between sentences while the normalized entity graph assigns a lower weight if there are more of these entities.",
                "Hence, Britannica Elementary receives a higher cohesion score than Encyclopedia Britannica in our model."
            ],
            "label": [
                "BACK",
                "MOT",
                "USE"
            ],
            "context": [
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "In experiments, #TARGET_REF assume that articles taken from Encyclopedia Britannica are more difficult to read (less coherent) than the corresponding articles from Encyclopedia Britannica Elementary, its version for children. We follow them with regard to data (107 article pairs), experimental setup and evaluation. Sentences in the Britannica Elementary are simpler and shorter than in the Encyclopedia Britannica. The entity graph does not take into account the effect of entities not shared between sentences while the normalized entity graph assigns a lower weight if there are more of these entities. Hence, Britannica Elementary receives a higher cohesion score than Encyclopedia Britannica in our model.",
        "output": "{\"label\": [\"BACK\", \"MOT\", \"USE\"], \"context\": [\"In experiments, #TARGET_REF assume that articles taken from Encyclopedia Britannica are more difficult to read (less coherent) than the corresponding articles from Encyclopedia Britannica Elementary, its version for children.\", \"We follow them with regard to data (107 article pairs), experimental setup and evaluation.\", \"Sentences in the Britannica Elementary are simpler and shorter than in the Encyclopedia Britannica.\", \"The entity graph does not take into account the effect of entities not shared between sentences while the normalized entity graph assigns a lower weight if there are more of these entities.\", \"Hence, Britannica Elementary receives a higher cohesion score than Encyclopedia Britannica in our model.\"]}"
    },
    {
        "gold": {
            "text": [
                "We follow #TARGET_REF for evaluating whether the normalized entity graph can decide whether automatic or human summaries are more coherent (80 pairs of summaries extracted from DUC 2003).",
                "Human coherence scores are associated with each pair of summarized documents (#REF) .",
                "Table 3 displays reported results of B&L and reproduced results of the entity graph and our normalized entity graph.",
                "Normalizing significantly improves the results for P W and P Acc .",
                "P U is still slightly better than both, but in contrast to the entity graph, this difference is not statistically significant."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We follow #TARGET_REF for evaluating whether the normalized entity graph can decide whether automatic or human summaries are more coherent (80 pairs of summaries extracted from DUC 2003). Human coherence scores are associated with each pair of summarized documents (#REF) . Table 3 displays reported results of B&L and reproduced results of the entity graph and our normalized entity graph. Normalizing significantly improves the results for P W and P Acc . P U is still slightly better than both, but in contrast to the entity graph, this difference is not statistically significant.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We follow #TARGET_REF for evaluating whether the normalized entity graph can decide whether automatic or human summaries are more coherent (80 pairs of summaries extracted from DUC 2003).\"]}"
    },
    {
        "gold": {
            "text": [
                "We follow #REF for evaluating whether the normalized entity graph can decide whether automatic or human summaries are more coherent (80 pairs of summaries extracted from DUC 2003).",
                "Human coherence scores are associated with each pair of summarized documents #TARGET_REF .",
                "Table 3 displays reported results of B&L and reproduced results of the entity graph and our normalized entity graph.",
                "Normalizing significantly improves the results for P W and P Acc .",
                "P U is still slightly better than both, but in contrast to the entity graph, this difference is not statistically significant."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We follow #REF for evaluating whether the normalized entity graph can decide whether automatic or human summaries are more coherent (80 pairs of summaries extracted from DUC 2003). Human coherence scores are associated with each pair of summarized documents #TARGET_REF . Table 3 displays reported results of B&L and reproduced results of the entity graph and our normalized entity graph. Normalizing significantly improves the results for P W and P Acc . P U is still slightly better than both, but in contrast to the entity graph, this difference is not statistically significant.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Human coherence scores are associated with each pair of summarized documents #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We proposed a normalization method for the entity graph (#REF) .",
                "We compared our model to the entity graph and to the entity grid #TARGET_REF and showed that normalization improves the results significantly in most tasks.",
                "Future work will include adding more linguistic information, stronger weighting schemes and application to other readability datasets (#REF; De #REF) ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0
            ]
        },
        "input": "We proposed a normalization method for the entity graph (#REF) . We compared our model to the entity graph and to the entity grid #TARGET_REF and showed that normalization improves the results significantly in most tasks. Future work will include adding more linguistic information, stronger weighting schemes and application to other readability datasets (#REF; De #REF) .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We compared our model to the entity graph and to the entity grid #TARGET_REF and showed that normalization improves the results significantly in most tasks.\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, our setting resembles the established task of entity recognition (#REF; #TARGET_REF) , with the difference being that we focus on un-named entities.",
                "Contribution.",
                "One of the factors impeding progress in common sense information extraction is the lack of training data.",
                "It is relatively easy to obtain labeled data for named entities such as companies and people.",
                "Examples of such named entities can be found in structured forms on the Web, such as HTML lists and tables, and Wikipedia infoboxes (#REF; #REF) ."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Therefore, our setting resembles the established task of entity recognition (#REF; #TARGET_REF) , with the difference being that we focus on un-named entities. Contribution. One of the factors impeding progress in common sense information extraction is the lack of training data. It is relatively easy to obtain labeled data for named entities such as companies and people. Examples of such named entities can be found in structured forms on the Web, such as HTML lists and tables, and Wikipedia infoboxes (#REF; #REF) .",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"Therefore, our setting resembles the established task of entity recognition (#REF; #TARGET_REF) , with the difference being that we focus on un-named entities.\"]}"
    },
    {
        "gold": {
            "text": [
                "to words, and y refers to BIO labels.",
                "Conditional Random Fields (CRFs) (#REF ) have been widely used named entity recognition #TARGET_REF; #REF) , a task similar to our own.",
                "While the CRF models performed reasonably well on our task, we sought to obtain improvements by proposing and training variations of Long Short Memory (LSTM) recurrent neural networks (#REF).",
                "We found our variations of LSTM sequence classifiers to do better than the CRF model, and also better than standard LSTMs.",
                "Word and Character Features."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "to words, and y refers to BIO labels. Conditional Random Fields (CRFs) (#REF ) have been widely used named entity recognition #TARGET_REF; #REF) , a task similar to our own. While the CRF models performed reasonably well on our task, we sought to obtain improvements by proposing and training variations of Long Short Memory (LSTM) recurrent neural networks (#REF). We found our variations of LSTM sequence classifiers to do better than the CRF model, and also better than standard LSTMs. Word and Character Features.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Conditional Random Fields (CRFs) (#REF ) have been widely used named entity recognition #TARGET_REF; #REF) , a task similar to our own.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our task is related to entity recognition however in this paper we focused on novel types of entities, which can be used to improve extraction of common sense knowledge.",
                "Entity recognition systems are traditionally based on a sequential model, for example a CRF, and involve feature engineering (#REF; #TARGET_REF.",
                "More recently, neural approaches have been used for named entity recognition (#REF; #REF; dos Santos and Guimarães, 2015; #REF; #REF) .",
                "Like other neural approaches, our approach does not require feature engineering, the only features we use are word and character embeddings.",
                "Related to our proposed recurrence in the output layer is the work of (#REF) which introduced a CRF on top of LSTM for the task of named entity recognition."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Our task is related to entity recognition however in this paper we focused on novel types of entities, which can be used to improve extraction of common sense knowledge. Entity recognition systems are traditionally based on a sequential model, for example a CRF, and involve feature engineering (#REF; #TARGET_REF. More recently, neural approaches have been used for named entity recognition (#REF; #REF; dos Santos and Guimarães, 2015; #REF; #REF) . Like other neural approaches, our approach does not require feature engineering, the only features we use are word and character embeddings. Related to our proposed recurrence in the output layer is the work of (#REF) which introduced a CRF on top of LSTM for the task of named entity recognition.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Entity recognition systems are traditionally based on a sequential model, for example a CRF, and involve feature engineering (#REF; #TARGET_REF.\"]}"
    },
    {
        "gold": {
            "text": [
                "We would like to detect mentions of concepts discernible by sense.",
                "In this paper, we focus on mentions of audible (sound) and olfactible (smell) concepts.",
                "We treat sense recognition in text as a sequence labeling task where each sentence is a sequence of tokens labeled using the BIO tagging scheme #TARGET_REF.",
                "The BIO labels denote tokens at the beginning, inside, and outside of a relevant mention, respectively.",
                "Example BIO tagged sentences are shown in Figure 1 ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We would like to detect mentions of concepts discernible by sense. In this paper, we focus on mentions of audible (sound) and olfactible (smell) concepts. We treat sense recognition in text as a sequence labeling task where each sentence is a sequence of tokens labeled using the BIO tagging scheme #TARGET_REF. The BIO labels denote tokens at the beginning, inside, and outside of a relevant mention, respectively. Example BIO tagged sentences are shown in Figure 1 .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We treat sense recognition in text as a sequence labeling task where each sentence is a sequence of tokens labeled using the BIO tagging scheme #TARGET_REF.\"]}"
    },
    {
        "gold": {
            "text": [
                "With these α values, the combination approach produced 1,962 and 1,702 training instances for audible and olfactible concepts respectively Performance of the various models is shown in Table 4 .",
                "The abbreviations denote the following: LSTM refers to a vanilla LSTM model, using only word embeddings as features, + OR refers to the LSTM plus the output recurrence, + CHAR refers to the LSTM plus the character embeddings as features.",
                "+ OR + CHAR refers to the LSTM plus the output recurrence and character embeddings as features.",
                "For the CRF, we use the commonly used features for named entity recognition: words, prefix/suffices, and part-of-speech tag #TARGET_REF.",
                "We can see that for both senses, the model that uses both character embedding features, and an output recurrence layer yields the best F1 score."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "With these α values, the combination approach produced 1,962 and 1,702 training instances for audible and olfactible concepts respectively Performance of the various models is shown in Table 4 . The abbreviations denote the following: LSTM refers to a vanilla LSTM model, using only word embeddings as features, + OR refers to the LSTM plus the output recurrence, + CHAR refers to the LSTM plus the character embeddings as features. + OR + CHAR refers to the LSTM plus the output recurrence and character embeddings as features. For the CRF, we use the commonly used features for named entity recognition: words, prefix/suffices, and part-of-speech tag #TARGET_REF. We can see that for both senses, the model that uses both character embedding features, and an output recurrence layer yields the best F1 score.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For the CRF, we use the commonly used features for named entity recognition: words, prefix/suffices, and part-of-speech tag #TARGET_REF.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, neither of these are suitable for fact checking a claim made in natural language against a database.",
                "Previous works appropriate for this task operate on a limited domain and are not able to incorporate temporal information when checking time-dependent claims #TARGET_REF .",
                "In this paper we introduce our fact checking tool, describe its architecture and design decisions, evaluate its accuracy and discuss future work.",
                "We highlight the ease of incorporating new information sources to fact check, which may be unavailable during training.",
                "To validate the extensibility of the system, we complete an additional evaluation of the system using claims taken from #REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "However, neither of these are suitable for fact checking a claim made in natural language against a database. Previous works appropriate for this task operate on a limited domain and are not able to incorporate temporal information when checking time-dependent claims #TARGET_REF . In this paper we introduce our fact checking tool, describe its architecture and design decisions, evaluate its accuracy and discuss future work. We highlight the ease of incorporating new information sources to fact check, which may be unavailable during training. To validate the extensibility of the system, we complete an additional evaluation of the system using claims taken from #REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Previous works appropriate for this task operate on a limited domain and are not able to incorporate temporal information when checking time-dependent claims #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous works appropriate for this task operate on a limited domain and are not able to incorporate temporal information when checking time-dependent claims (#REF) .",
                "In this paper we introduce our fact checking tool, describe its architecture and design decisions, evaluate its accuracy and discuss future work.",
                "We highlight the ease of incorporating new information sources to fact check, which may be unavailable during training.",
                "To validate the extensibility of the system, we complete an additional evaluation of the system using claims taken from #TARGET_REF .",
                "We make the source code publicly available to the community."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Previous works appropriate for this task operate on a limited domain and are not able to incorporate temporal information when checking time-dependent claims (#REF) . In this paper we introduce our fact checking tool, describe its architecture and design decisions, evaluate its accuracy and discuss future work. We highlight the ease of incorporating new information sources to fact check, which may be unavailable during training. To validate the extensibility of the system, we complete an additional evaluation of the system using claims taken from #TARGET_REF . We make the source code publicly available to the community.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"To validate the extensibility of the system, we complete an additional evaluation of the system using claims taken from #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In our system, this step is achieved by simply importing a CSV file and running a script to generate the new instances to train the relation matching classifier.",
                "Analysis of our entry to this competition showed that two errors were caused by incorrect initial source data and one partial error caused by recalling a correct property but making an incorrect deduction.",
                "Of numerical claims that we did not attempt, we observed that many required looking up multiple entries and performing a more complex deduction step which was beyond the scope of this project.",
                "We further validate the system by evaluating the ability of this fact checking system to make veracity assessments on simple numerical claims from the data set collected by #TARGET_REF .",
                "Of the 4,255 claims about numerical properties about countries and geographical areas in this data set, our KB contained information to fact check 3,418."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "In our system, this step is achieved by simply importing a CSV file and running a script to generate the new instances to train the relation matching classifier. Analysis of our entry to this competition showed that two errors were caused by incorrect initial source data and one partial error caused by recalling a correct property but making an incorrect deduction. Of numerical claims that we did not attempt, we observed that many required looking up multiple entries and performing a more complex deduction step which was beyond the scope of this project. We further validate the system by evaluating the ability of this fact checking system to make veracity assessments on simple numerical claims from the data set collected by #TARGET_REF . Of the 4,255 claims about numerical properties about countries and geographical areas in this data set, our KB contained information to fact check 3,418.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We further validate the system by evaluating the ability of this fact checking system to make veracity assessments on simple numerical claims from the data set collected by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The core capability of the system demonstration we presented is to fact check natural language claims against relations stored in a KB.",
                "Although the range of claims is limited, the system is a fieldtested prototype and has been evaluated on a published data set #TARGET_REF and on real-world claims presented as part of the HeroX fact checking challenge.",
                "In future work, we will extend the semantic parsing technique used and apply our system to more complex claim types.",
                "Additionally, further work is required to reduce the number of candidate relations recalled from the KB.",
                "While this was not an issue in our case, we believe that ameliorating this issue will enhance the ability of the system to assign a correct truth label where there exist properties with similar numerical values."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The core capability of the system demonstration we presented is to fact check natural language claims against relations stored in a KB. Although the range of claims is limited, the system is a fieldtested prototype and has been evaluated on a published data set #TARGET_REF and on real-world claims presented as part of the HeroX fact checking challenge. In future work, we will extend the semantic parsing technique used and apply our system to more complex claim types. Additionally, further work is required to reduce the number of candidate relations recalled from the KB. While this was not an issue in our case, we believe that ameliorating this issue will enhance the ability of the system to assign a correct truth label where there exist properties with similar numerical values.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Although the range of claims is limited, the system is a fieldtested prototype and has been evaluated on a published data set #TARGET_REF and on real-world claims presented as part of the HeroX fact checking challenge.\"]}"
    },
    {
        "gold": {
            "text": [
                "The types of claims the system presented can fact check was restricted to those which require looking up a value in a KB, similar to the one in Figure 1 .",
                "To learn a model to perform the KB look up (essentially a semantic parsing task), we extend the work of #TARGET_REF who used distant supervision (#REF ) to generate training data, obviating the need for manual labeling.",
                "In particular, we extend it to handle simple temporal expressions in order to fact check time-dependent claims appropriately, i. e. population in 2015.",
                "While the recently proposed semantic parser of #REF is also able to handle temporal expressions, it makes the assumption that the table against which the claim needs to be interpreted is known, which is unrealistic in the context of fact checking.",
                "Furthermore, the system we propose can predict relations from the KB on which the semantic parser has not been trained, a paradigm referred to as zero-shot learning (#REF) ."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The types of claims the system presented can fact check was restricted to those which require looking up a value in a KB, similar to the one in Figure 1 . To learn a model to perform the KB look up (essentially a semantic parsing task), we extend the work of #TARGET_REF who used distant supervision (#REF ) to generate training data, obviating the need for manual labeling. In particular, we extend it to handle simple temporal expressions in order to fact check time-dependent claims appropriately, i. e. population in 2015. While the recently proposed semantic parser of #REF is also able to handle temporal expressions, it makes the assumption that the table against which the claim needs to be interpreted is known, which is unrealistic in the context of fact checking. Furthermore, the system we propose can predict relations from the KB on which the semantic parser has not been trained, a paradigm referred to as zero-shot learning (#REF) .",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"To learn a model to perform the KB look up (essentially a semantic parsing task), we extend the work of #TARGET_REF who used distant supervision (#REF ) to generate training data, obviating the need for manual labeling.\"]}"
    },
    {
        "gold": {
            "text": [
                "Part-of-speech (POS) tagging has received a great deal of attention as it is a critical component of most natural language processing systems.",
                "As supervised POS tagging accuracies for English (measured on the Wall Street Journal portion of the PennTreebank (#REF) ) have converged to around 97.3% (#REF; #REF) , the attention has shifted to unsupervised approaches (#REF) .",
                "In particular, there has been growing interest in both multilingual POS induction ) and cross-lingual POS induction via treebank projection (#REF; #REF; #TARGET_REF .",
                "Underlying these studies is the idea that a set of (coarse) syntactic POS categories exist in similar forms across languages.",
                "These categories are often called universals to represent their cross-lingual nature (#REF; #REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Part-of-speech (POS) tagging has received a great deal of attention as it is a critical component of most natural language processing systems. As supervised POS tagging accuracies for English (measured on the Wall Street Journal portion of the PennTreebank (#REF) ) have converged to around 97.3% (#REF; #REF) , the attention has shifted to unsupervised approaches (#REF) . In particular, there has been growing interest in both multilingual POS induction ) and cross-lingual POS induction via treebank projection (#REF; #REF; #TARGET_REF . Underlying these studies is the idea that a set of (coarse) syntactic POS categories exist in similar forms across languages. These categories are often called universals to represent their cross-lingual nature (#REF; #REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Part-of-speech (POS) tagging has received a great deal of attention as it is a critical component of most natural language processing systems.\", \"In particular, there has been growing interest in both multilingual POS induction ) and cross-lingual POS induction via treebank projection (#REF; #REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Underlying these studies is the idea that a set of (coarse) syntactic POS categories exist in similar forms across languages.",
                "These categories are often called universals to represent their cross-lingual nature (#REF; #REF) .",
                "For example, used the Multext-East (#REF) corpus to evaluate their multi-lingual POS induction system, because it uses the same tagset for multiple languages.",
                "When corpora with common tagsets are unavailable, a standard approach is to manually define a mapping from language and treebank specific fine-grained tagsets to a predefined universal set.",
                "This was the approach taken by #TARGET_REF to evaluate their cross-lingual POS projection system for six different languages."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                1,
                0,
                1,
                1
            ]
        },
        "input": "Underlying these studies is the idea that a set of (coarse) syntactic POS categories exist in similar forms across languages. These categories are often called universals to represent their cross-lingual nature (#REF; #REF) . For example, used the Multext-East (#REF) corpus to evaluate their multi-lingual POS induction system, because it uses the same tagset for multiple languages. When corpora with common tagsets are unavailable, a standard approach is to manually define a mapping from language and treebank specific fine-grained tagsets to a predefined universal set. This was the approach taken by #TARGET_REF to evaluate their cross-lingual POS projection system for six different languages.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Underlying these studies is the idea that a set of (coarse) syntactic POS categories exist in similar forms across languages.\", \"These categories are often called universals to represent their cross-lingual nature (#REF; #REF) .\", \"When corpora with common tagsets are unavailable, a standard approach is to manually define a mapping from language and treebank specific fine-grained tagsets to a predefined universal set.\", \"This was the approach taken by #TARGET_REF to evaluate their cross-lingual POS projection system for six different languages.\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally, it also permits language technology practitioners to train POS taggers with common tagsets across multiple languages.",
                "This in turn facilitates downstream application development as there is no need to maintain language specific rules due to differences in treebank annotation guidelines.",
                "In this paper, we specifically highlight two use cases of this resource.",
                "First, using our universal tagset and mapping, we run an experiment comparing POS tag accuracies for 25 different treebanks to evaluate POS tagging accuracy on a single tagset.",
                "Second, we combine the cross-lingual projection part-of-speech taggers of #TARGET_REF with the grammar induction system of #REF -which requires a universal tagset -to produce a completely unsupervised grammar induction system for multiple languages, that does not require gold POS tags in the target language."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Finally, it also permits language technology practitioners to train POS taggers with common tagsets across multiple languages. This in turn facilitates downstream application development as there is no need to maintain language specific rules due to differences in treebank annotation guidelines. In this paper, we specifically highlight two use cases of this resource. First, using our universal tagset and mapping, we run an experiment comparing POS tag accuracies for 25 different treebanks to evaluate POS tagging accuracy on a single tagset. Second, we combine the cross-lingual projection part-of-speech taggers of #TARGET_REF with the grammar induction system of #REF -which requires a universal tagset -to produce a completely unsupervised grammar induction system for multiple languages, that does not require gold POS tags in the target language.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"Second, we combine the cross-lingual projection part-of-speech taggers of #TARGET_REF with the grammar induction system of #REF -which requires a universal tagset -to produce a completely unsupervised grammar induction system for multiple languages, that does not require gold POS tags in the target language.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, since this rule is reversed for other languages, we omit it in our tagset.",
                "Additionally, they also used refined categories in the form of CoNLL treebank tags.",
                "In our experiments, we did not make use of refined categories, as the POS tags induced by #REF were all coarse.",
                "We present results on the same eight IndoEuropean languages as #TARGET_REF , so that we can make use of their automatically projected POS tags.",
                "For all languages, we used the treebanks released as a part of the CoNLL-X (#REF) shared task."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "However, since this rule is reversed for other languages, we omit it in our tagset. Additionally, they also used refined categories in the form of CoNLL treebank tags. In our experiments, we did not make use of refined categories, as the POS tags induced by #REF were all coarse. We present results on the same eight IndoEuropean languages as #TARGET_REF , so that we can make use of their automatically projected POS tags. For all languages, we used the treebanks released as a part of the CoNLL-X (#REF) shared task.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We present results on the same eight IndoEuropean languages as #TARGET_REF , so that we can make use of their automatically projected POS tags.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, since this rule is reversed for other languages, we omit it in our tagset.",
                "Additionally, they also used refined categories in the form of CoNLL treebank tags.",
                "In our experiments, we did not make use of refined categories, as the POS tags induced by #TARGET_REF were all coarse.",
                "We present results on the same eight IndoEuropean languages as #REF , so that we can make use of their automatically projected POS tags.",
                "For all languages, we used the treebanks released as a part of the CoNLL-X (#REF) shared task."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "However, since this rule is reversed for other languages, we omit it in our tagset. Additionally, they also used refined categories in the form of CoNLL treebank tags. In our experiments, we did not make use of refined categories, as the POS tags induced by #TARGET_REF were all coarse. We present results on the same eight IndoEuropean languages as #REF , so that we can make use of their automatically projected POS tags. For all languages, we used the treebanks released as a part of the CoNLL-X (#REF) shared task.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In our experiments, we did not make use of refined categories, as the POS tags induced by #TARGET_REF were all coarse.\"]}"
    },
    {
        "gold": {
            "text": [
                "We provide wrappers around most PyTorch optimizers and an implementation of Adafactor (#REF) , which is a memory-efficient variant of Adam.",
                "Learning Rate Schedulers update the learning rate over the course of training.",
                "We provide several popular schedulers, e.g., the inverse square-root scheduler from #TARGET_REF and cyclical schedulers based on warm restarts (#REF) .",
                "Reproducibility and forward compatibility.",
                "FAIRSEQ includes features designed to improve reproducibility and forward compatibility."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We provide wrappers around most PyTorch optimizers and an implementation of Adafactor (#REF) , which is a memory-efficient variant of Adam. Learning Rate Schedulers update the learning rate over the course of training. We provide several popular schedulers, e.g., the inverse square-root scheduler from #TARGET_REF and cyclical schedulers based on warm restarts (#REF) . Reproducibility and forward compatibility. FAIRSEQ includes features designed to improve reproducibility and forward compatibility.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"We provide several popular schedulers, e.g., the inverse square-root scheduler from #TARGET_REF and cyclical schedulers based on warm restarts (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We provide reference implementations of several popular sequence-to-sequence models which can be used for machine translation, including LSTM (#REF) , convolutional models (#REF; #REF) and Transformer (#REF) .",
                "We evaluate a \"big\" Transformer encoderdecoder model on two language pairs, WMT English to German (En-De) and WMT English to French (En-Fr).",
                "For En-De we replicate the setup of #TARGET_REF which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14.",
                "The 32K vocabulary is based on a joint source and target byte pair encoding (BPE; #REF ).",
                "For En-Fr, we train on WMT'14 and borrow the setup of #REF with 36M training sentence pairs."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We provide reference implementations of several popular sequence-to-sequence models which can be used for machine translation, including LSTM (#REF) , convolutional models (#REF; #REF) and Transformer (#REF) . We evaluate a \"big\" Transformer encoderdecoder model on two language pairs, WMT English to German (En-De) and WMT English to French (En-Fr). For En-De we replicate the setup of #TARGET_REF which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14. The 32K vocabulary is based on a joint source and target byte pair encoding (BPE; #REF ). For En-Fr, we train on WMT'14 and borrow the setup of #REF with 36M training sentence pairs.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"For En-De we replicate the setup of #TARGET_REF which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14.\"]}"
    },
    {
        "gold": {
            "text": [
                "The 40K vocabulary is based on a joint source and target BPE.",
                "We measure case-sensitive tokenized BLEU with multi-bleu (#REF) and detokenized BLEU with SacreBLEU 1 (#REF) .",
                "All results use beam search with a beam width of 4 and length penalty of 0.6, following #TARGET_REF .",
                "FAIRSEQ results are summarized in Table 2 .",
                "We reported improved BLEU scores over #REF by training with a bigger batch size and an increased learning rate ."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The 40K vocabulary is based on a joint source and target BPE. We measure case-sensitive tokenized BLEU with multi-bleu (#REF) and detokenized BLEU with SacreBLEU 1 (#REF) . All results use beam search with a beam width of 4 and length penalty of 0.6, following #TARGET_REF . FAIRSEQ results are summarized in Table 2 . We reported improved BLEU scores over #REF by training with a bigger batch size and an increased learning rate .",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"All results use beam search with a beam width of 4 and length penalty of 0.6, following #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "This scales the loss right after the forward pass to fit into the FP16 range while the backward pass is left unchanged.",
                "After the FP16 gradients are synchronized between workers, we convert them to FP32, restore the original scale, and update the weights.",
                "Inference.",
                "FAIRSEQ provides fast inference for non-recurrent models (#REF; #TARGET_REF; #REFb; #REF) through incremental decoding, where the model states of previously generated tokens are cached in each active beam and re-used.",
                "This can speed up a naïve implementation without caching by up to an order of magnitude, since only new states are computed for each token."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "This scales the loss right after the forward pass to fit into the FP16 range while the backward pass is left unchanged. After the FP16 gradients are synchronized between workers, we convert them to FP32, restore the original scale, and update the weights. Inference. FAIRSEQ provides fast inference for non-recurrent models (#REF; #TARGET_REF; #REFb; #REF) through incremental decoding, where the model states of previously generated tokens are cached in each active beam and re-used. This can speed up a naïve implementation without caching by up to an order of magnitude, since only new states are computed for each token.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"FAIRSEQ provides fast inference for non-recurrent models (#REF; #TARGET_REF; #REFb; #REF) through incremental decoding, where the model states of previously generated tokens are cached in each active beam and re-used.\"]}"
    },
    {
        "gold": {
            "text": [
                "We provide reference implementations of several popular sequence-to-sequence models which can be used for machine translation, including LSTM (#REF) , convolutional models (#REF; #REF) and Transformer #TARGET_REF .",
                "We evaluate a \"big\" Transformer encoderdecoder model on two language pairs, WMT English to German (En-De) and WMT English to French (En-Fr).",
                "For En-De we replicate the setup of #REF which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14.",
                "The 32K vocabulary is based on a joint source and target byte pair encoding (BPE; #REF ).",
                "For En-Fr, we train on WMT'14 and borrow the setup of #REF with 36M training sentence pairs."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We provide reference implementations of several popular sequence-to-sequence models which can be used for machine translation, including LSTM (#REF) , convolutional models (#REF; #REF) and Transformer #TARGET_REF . We evaluate a \"big\" Transformer encoderdecoder model on two language pairs, WMT English to German (En-De) and WMT English to French (En-Fr). For En-De we replicate the setup of #REF which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14. The 32K vocabulary is based on a joint source and target byte pair encoding (BPE; #REF ). For En-Fr, we train on WMT'14 and borrow the setup of #REF with 36M training sentence pairs.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"We provide reference implementations of several popular sequence-to-sequence models which can be used for machine translation, including LSTM (#REF) , convolutional models (#REF; #REF) and Transformer #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "FAIRSEQ supports language modeling with gated convolutional models and Transformer models #TARGET_REF .",
                "Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. #REF 25.2 40.5 b. #REF 28.4 41.0 c. #REF 28.9 41.4 d. #REF 29 et al., 2016), adaptive softmax (#REF) , and adaptive inputs .",
                "We also provide tutorials and pre-trained models that replicate the results of and"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "FAIRSEQ supports language modeling with gated convolutional models and Transformer models #TARGET_REF . Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. #REF 25.2 40.5 b. #REF 28.4 41.0 c. #REF 28.9 41.4 d. #REF 29 et al., 2016), adaptive softmax (#REF) , and adaptive inputs . We also provide tutorials and pre-trained models that replicate the results of and",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"FAIRSEQ supports language modeling with gated convolutional models and Transformer models #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "FAIRSEQ supports language modeling with gated convolutional models and Transformer models (#REF) .",
                "Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. #REF 25.2 40.5 b. #TARGET_REF 28.4 41.0 c. #REF 28.9 41.4 d. #REF 29 et al., 2016), adaptive softmax (#REF) , and adaptive inputs .",
                "We also provide tutorials and pre-trained models that replicate the results of and"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0
            ]
        },
        "input": "FAIRSEQ supports language modeling with gated convolutional models and Transformer models (#REF) . Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. #REF 25.2 40.5 b. #TARGET_REF 28.4 41.0 c. #REF 28.9 41.4 d. #REF 29 et al., 2016), adaptive softmax (#REF) , and adaptive inputs . We also provide tutorials and pre-trained models that replicate the results of and",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim 1 En-De En-Fr a. #REF 25.2 40.5 b. #TARGET_REF 28.4 41.0 c. #REF 28.9 41.4 d. #REF 29 et al., 2016), adaptive softmax (#REF) , and adaptive inputs .\"]}"
    },
    {
        "gold": {
            "text": [
                "The 40K vocabulary is based on a joint source and target BPE.",
                "We measure case-sensitive tokenized BLEU with multi-bleu (#REF) and detokenized BLEU with SacreBLEU 1 (#REF) .",
                "All results use beam search with a beam width of 4 and length penalty of 0.6, following #REF .",
                "FAIRSEQ results are summarized in Table 2 .",
                "We reported improved BLEU scores over #TARGET_REF by training with a bigger batch size and an increased learning rate ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The 40K vocabulary is based on a joint source and target BPE. We measure case-sensitive tokenized BLEU with multi-bleu (#REF) and detokenized BLEU with SacreBLEU 1 (#REF) . All results use beam search with a beam width of 4 and length penalty of 0.6, following #REF . FAIRSEQ results are summarized in Table 2 . We reported improved BLEU scores over #TARGET_REF by training with a bigger batch size and an increased learning rate .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We reported improved BLEU scores over #TARGET_REF by training with a bigger batch size and an increased learning rate .\"]}"
    },
    {
        "gold": {
            "text": [
                "In the study of Habernal and Gurevych (2016b) , annotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency.",
                "This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches.",
                "Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (#REFa) and one with 17 reasons for quality differences phrased spontaneously in practice #TARGET_REF .",
                "In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4).",
                "We find that assessments of overall argumentation quality largely match in theory and practice."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In the study of Habernal and Gurevych (2016b) , annotators assessed Argument A as more convincing than B. When giving reasons for their assessment, though, they saw A as more credible and well thought through; that does not seem to be too far from the theoretical notion of cogency. This paper gives empirical answers to the question of how different the theoretical and practical views of argumentation quality actually are. Section 2 briefly reviews existing theories and practical approaches. Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (#REFa) and one with 17 reasons for quality differences phrased spontaneously in practice #TARGET_REF . In a crowdsourcing study, we test whether lay annotators achieve agreement on the theoretical quality dimensions (Section 4). We find that assessments of overall argumentation quality largely match in theory and practice.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Section 3 then empirically analyzes correlations in two recent argument corpora, one annotated for 15 well-defined quality dimensions taken from theory (#REFa) and one with 17 reasons for quality differences phrased spontaneously in practice #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Conv A is more convincing than B. Table 2 : The 17+1 practical reason labels given in the corpus of #TARGET_REF .",
                "covered.",
                "In Section 3, we use their absolute quality ratings from 1 (low) to 3 (high) annotated by three experts for each dimension of 304 arguments taken from the UKPConvArg1 corpus detailed below."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "Conv A is more convincing than B. Table 2 : The 17+1 practical reason labels given in the corpus of #TARGET_REF . covered. In Section 3, we use their absolute quality ratings from 1 (low) to 3 (high) annotated by three experts for each dimension of 304 arguments taken from the UKPConvArg1 corpus detailed below.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Conv A is more convincing than B. Table 2 : The 17+1 practical reason labels given in the corpus of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Based on the idea that relative assessment is easier, Habernal and Gurevych (2016b) crowdsourced the UKPConvArg1 corpus.",
                "Argument pairs (A, B) from a debate portal were classified as to which argument is more convincing.",
                "Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a follow-up study #TARGET_REF , these reasons were used to derive a hierarchical annotation scheme.",
                "9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- Wachsmuth et al. (2017a) given for each of the 17+1 reason labels of Habernal and Gurevych (2016a) .",
                "Bold/gray: Highest/lowest value in each column."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Based on the idea that relative assessment is easier, Habernal and Gurevych (2016b) crowdsourced the UKPConvArg1 corpus. Argument pairs (A, B) from a debate portal were classified as to which argument is more convincing. Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a follow-up study #TARGET_REF , these reasons were used to derive a hierarchical annotation scheme. 9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- Wachsmuth et al. (2017a) given for each of the 17+1 reason labels of Habernal and Gurevych (2016a) . Bold/gray: Highest/lowest value in each column.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a follow-up study #TARGET_REF , these reasons were used to derive a hierarchical annotation scheme.\"]}"
    },
    {
        "gold": {
            "text": [
                "Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a follow-up study (#REFa) , these reasons were used to derive a hierarchical annotation scheme.",
                "9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- Wachsmuth et al. (2017a) given for each of the 17+1 reason labels of #TARGET_REF .",
                "Bold/gray: Highest/lowest value in each column.",
                "Bottom row: The number of labels for each dimension.",
                "by crowd workers (UKPConvArg2)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Without giving any guidelines, the authors also asked for reasons as to why A is more convincing than B. In a follow-up study (#REFa) , these reasons were used to derive a hierarchical annotation scheme. 9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- Wachsmuth et al. (2017a) given for each of the 17+1 reason labels of #TARGET_REF . Bold/gray: Highest/lowest value in each column. Bottom row: The number of labels for each dimension. by crowd workers (UKPConvArg2).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"9111 argument pairs were then labeled with one or more of the 17 reason labels in Table 2 Negative Properties of Argument B Positive Properties of Argument A Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9- Wachsmuth et al. (2017a) given for each of the 17+1 reason labels of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The high τ 's of 8-5 (more credible) for local acceptability (.73) and of 9-4 (well thought through) for cogency (.75) confirm the match assumed in Section 1. Also, the values of 5-3 (unclear) for clarity (.91) and of 7-2 (non-sense) for reasonableness (.94) as well as the weaker correlation of 8-4 (objective) for emotional appeal (.35) makes sense.",
                "Only the comparably low τ of 6-1 (no credible evidence) for local acceptability (.49) and credibility (.52) seem really unexpected.",
                "Besides, the descriptions of 6-2 and 6-3 sound like local but cor- Table 4 : The mean rating for each quality dimension of those arguments from Wachsmuth et al. (2017a) given for each reason label #TARGET_REF .",
                "The bottom rows show that the minimum maximum mean ratings are consistently higher for the positive properties than for the negative properties.",
                "relate more with global relevance and sufficiency respectively."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The high τ 's of 8-5 (more credible) for local acceptability (.73) and of 9-4 (well thought through) for cogency (.75) confirm the match assumed in Section 1. Also, the values of 5-3 (unclear) for clarity (.91) and of 7-2 (non-sense) for reasonableness (.94) as well as the weaker correlation of 8-4 (objective) for emotional appeal (.35) makes sense. Only the comparably low τ of 6-1 (no credible evidence) for local acceptability (.49) and credibility (.52) seem really unexpected. Besides, the descriptions of 6-2 and 6-3 sound like local but cor- Table 4 : The mean rating for each quality dimension of those arguments from Wachsmuth et al. (2017a) given for each reason label #TARGET_REF . The bottom rows show that the minimum maximum mean ratings are consistently higher for the positive properties than for the negative properties. relate more with global relevance and sufficiency respectively.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Besides, the descriptions of 6-2 and 6-3 sound like local but cor- Table 4 : The mean rating for each quality dimension of those arguments from Wachsmuth et al. (2017a) given for each reason label #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 4 compares the mean ratings of \"negative labels\" (5-1 to 7-4) and \"positive\" ones (8-1 to 9-4).",
                "For all dimensions, the maximum and minimum value are higher for the positive than for the negative labels-a clear support of Hypothesis 3.",
                "3 Also, Table 4 reveals which reasons predict absolute differences most: The mean ratings of 7-3 (off-topic) are very low, indicating a strong negative impact, while 6-3 (irrelevant reasons) still shows rather 3 While the differences seem not very large, this is expected, as in many argument pairs from #TARGET_REF both arguments are strong or weak respectively.",
                "high values.",
                "Vice versa, especially 8-5 (more credible) and 9-4 (well thought through) are reflected in high ratings, whereas 9-2 (sticks to topic) does not have much positive impact."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Table 4 compares the mean ratings of \"negative labels\" (5-1 to 7-4) and \"positive\" ones (8-1 to 9-4). For all dimensions, the maximum and minimum value are higher for the positive than for the negative labels-a clear support of Hypothesis 3. 3 Also, Table 4 reveals which reasons predict absolute differences most: The mean ratings of 7-3 (off-topic) are very low, indicating a strong negative impact, while 6-3 (irrelevant reasons) still shows rather 3 While the differences seem not very large, this is expected, as in many argument pairs from #TARGET_REF both arguments are strong or weak respectively. high values. Vice versa, especially 8-5 (more credible) and 9-4 (well thought through) are reflected in high ratings, whereas 9-2 (sticks to topic) does not have much positive impact.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"3 Also, Table 4 reveals which reasons predict absolute differences most: The mean ratings of 7-3 (off-topic) are very low, indicating a strong negative impact, while 6-3 (irrelevant reasons) still shows rather 3 While the differences seem not very large, this is expected, as in many argument pairs from #TARGET_REF both arguments are strong or weak respectively.\"]}"
    },
    {
        "gold": {
            "text": [
                "Also, global sufficiency has the lowest agreement in both cases.",
                "In contrast, the experts hardly said \"cannot judge\" at all, whereas the crowd chose it for about 4% of all ratings (most often for global sufficiency), possibly due to a lack of training.",
                "Still, we conclude that the crowd generally handles the theory-based quality assessment almost as well as the experts.",
                "However, the complexity of the assessment is underlined by the generally limited agreement, suggesting that either simplification or stricter guidelines are needed.",
                "Regarding simplification, the most common practical reasons of #TARGET_REF imply what to focus on."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Also, global sufficiency has the lowest agreement in both cases. In contrast, the experts hardly said \"cannot judge\" at all, whereas the crowd chose it for about 4% of all ratings (most often for global sufficiency), possibly due to a lack of training. Still, we conclude that the crowd generally handles the theory-based quality assessment almost as well as the experts. However, the complexity of the assessment is underlined by the generally limited agreement, suggesting that either simplification or stricter guidelines are needed. Regarding simplification, the most common practical reasons of #TARGET_REF imply what to focus on.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Regarding simplification, the most common practical reasons of #TARGET_REF imply what to focus on.\"]}"
    },
    {
        "gold": {
            "text": [
                "For Hypotheses 1 and 2, we consider all 736 pairs of arguments from #TARGET_REF where both have been annotated by Wachsmuth et al. (2017a) .",
                "For each pair (A, B) with A being 1 Source code and annotated data: http://www.arguana.com more convincing than B, we check whether the ratings of A and B for each dimension (averaged over all annotators) show a concordant difference (i.e., a higher rating for A), a disconcordant difference (lower), or a tie.",
                "This way, we can correlate each dimension with all reason labels in Table 2 including Conv.",
                "In particular, we compute Kendall's τ based on all argument pairs given for each label.",
                "2  Table 3 presents all τ -values."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "For Hypotheses 1 and 2, we consider all 736 pairs of arguments from #TARGET_REF where both have been annotated by Wachsmuth et al. (2017a) . For each pair (A, B) with A being 1 Source code and annotated data: http://www.arguana.com more convincing than B, we check whether the ratings of A and B for each dimension (averaged over all annotators) show a concordant difference (i.e., a higher rating for A), a disconcordant difference (lower), or a tie. This way, we can correlate each dimension with all reason labels in Table 2 including Conv. In particular, we compute Kendall's τ based on all argument pairs given for each label. 2  Table 3 presents all τ -values.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"For Hypotheses 1 and 2, we consider all 736 pairs of arguments from #TARGET_REF where both have been annotated by Wachsmuth et al. (2017a) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The correlations found imply that the relative quality differences captured are reflected in absolute differences.",
                "For explicitness, we computed the mean rating for each quality dimension of all arguments from Wachsmuth et al. (2017a) with a particular reason label from #TARGET_REF .",
                "As each reason refers to one argument of a pair, this reveals whether the labels, although meant to signal relative differences, indicate absolute ratings.",
                "Table 4 compares the mean ratings of \"negative labels\" (5-1 to 7-4) and \"positive\" ones (8-1 to 9-4).",
                "For all dimensions, the maximum and minimum value are higher for the positive than for the negative labels-a clear support of Hypothesis 3."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The correlations found imply that the relative quality differences captured are reflected in absolute differences. For explicitness, we computed the mean rating for each quality dimension of all arguments from Wachsmuth et al. (2017a) with a particular reason label from #TARGET_REF . As each reason refers to one argument of a pair, this reveals whether the labels, although meant to signal relative differences, indicate absolute ratings. Table 4 compares the mean ratings of \"negative labels\" (5-1 to 7-4) and \"positive\" ones (8-1 to 9-4). For all dimensions, the maximum and minimum value are higher for the positive than for the negative labels-a clear support of Hypothesis 3.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"For explicitness, we computed the mean rating for each quality dimension of all arguments from Wachsmuth et al. (2017a) with a particular reason label from #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Abstract.",
                "Predicting mental health from smartphone and social media data on a longitudinal basis has recently attracted great interest, with very promising results being reported across many studies [3, 9, 13, #TARGET_REF .",
                "Such approaches have the potential to revolutionise mental health assessment, if their development and evaluation follows a real world deployment setting.",
                "In this work we take a closer look at state-of-the-art approaches, using different mental health datasets and indicators, different feature sources and multiple simulations, in order to assess their ability to generalise.",
                "We demonstrate that under a pragmatic evaluation framework, none of the approaches deliver or even approach the reported performances."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Abstract. Predicting mental health from smartphone and social media data on a longitudinal basis has recently attracted great interest, with very promising results being reported across many studies [3, 9, 13, #TARGET_REF . Such approaches have the potential to revolutionise mental health assessment, if their development and evaluation follows a real world deployment setting. In this work we take a closer look at state-of-the-art approaches, using different mental health datasets and indicators, different feature sources and multiple simulations, in order to assess their ability to generalise. We demonstrate that under a pragmatic evaluation framework, none of the approaches deliver or even approach the reported performances.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Predicting mental health from smartphone and social media data on a longitudinal basis has recently attracted great interest, with very promising results being reported across many studies [3, 9, 13, #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Establishing the right indicators of mental well-being is a grand challenge posed by the World Health #REF .",
                "Poor mental health is highly correlated with low motivation, lack of satisfaction, low productivity and a negative economic impact [20] .",
                "The current approach is to combine census data at the population level [19] , thus failing to capture well-being on an individual basis.",
                "The latter is only possible via self-reporting on the basis of established psychological scales, which are hard to acquire consistently on a longitudinal basis, and they capture long-term aggregates instead of the current state of the individual.",
                "The widespread use of smart-phones and social media offers new ways of assessing mental well-being, and recent research [1, 2, 3, 5, 9, 10, 13, 14, 22, 23, #TARGET_REF has started exploring the effectiveness of these modalities for automatically assessing"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Establishing the right indicators of mental well-being is a grand challenge posed by the World Health #REF . Poor mental health is highly correlated with low motivation, lack of satisfaction, low productivity and a negative economic impact [20] . The current approach is to combine census data at the population level [19] , thus failing to capture well-being on an individual basis. The latter is only possible via self-reporting on the basis of established psychological scales, which are hard to acquire consistently on a longitudinal basis, and they capture long-term aggregates instead of the current state of the individual. The widespread use of smart-phones and social media offers new ways of assessing mental well-being, and recent research [1, 2, 3, 5, 9, 10, 13, 14, 22, 23, #TARGET_REF has started exploring the effectiveness of these modalities for automatically assessing",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The widespread use of smart-phones and social media offers new ways of assessing mental well-being, and recent research [1, 2, 3, 5, 9, 10, 13, 14, 22, 23, #TARGET_REF has started exploring the effectiveness of these modalities for automatically assessing\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to overcome these issues, previous work [2, 5, 9, 10, 22, #TARGET_REF has combined the instances {X uj i , y uj i } from different individuals u j and performed evaluation using randomised cross validation (MIXED).",
                "While such approaches can attain optimistic performance, the corresponding models fail to generalise to the general population and also fail to ensure effective personalised assessment of the mental health state of a single individual.",
                "In this paper we demonstrate the challenges that current state-of-the-art models face, when tested in a real-world setting.",
                "We work on two longitudinal datasets with four mental health targets, using different features derived from a wide range of heterogeneous sources.",
                "Following the state-of-the-art experimental methods and evaluation settings, we achieve very promising results, regardless of the features we employ and the mental health target we aim to predict."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In order to overcome these issues, previous work [2, 5, 9, 10, 22, #TARGET_REF has combined the instances {X uj i , y uj i } from different individuals u j and performed evaluation using randomised cross validation (MIXED). While such approaches can attain optimistic performance, the corresponding models fail to generalise to the general population and also fail to ensure effective personalised assessment of the mental health state of a single individual. In this paper we demonstrate the challenges that current state-of-the-art models face, when tested in a real-world setting. We work on two longitudinal datasets with four mental health targets, using different features derived from a wide range of heterogeneous sources. Following the state-of-the-art experimental methods and evaluation settings, we achieve very promising results, regardless of the features we employ and the mental health target we aim to predict.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In order to overcome these issues, previous work [2, 5, 9, 10, 22, #TARGET_REF has combined the instances {X uj i , y uj i } from different individuals u j and performed evaluation using randomised cross validation (MIXED).\"]}"
    },
    {
        "gold": {
            "text": [
                "Research in assessing mental health on a longitudinal basis aims to make use of relevant features extracted from various modalities, in order to train models for automatically predicting a user's mental state (target), either in a classification or a regression manner [1, 2, 3, 9, 10, 13, #TARGET_REF .",
                "Examples of state-of-the-art work in this domain are listed in Table 2 , along with the number of subjects that was used and the method upon which evaluation took place.",
                "Most approaches have used the \"MIXED\" approach to evaluate models [1, 2, 5, 9, 10, 22, 26] , which, as we will show, is vulnerable to bias, due to the danger of recognising the user in the test set and thus simply inferring her average mood score.",
                "LOIOCV approaches that have not ensured that their train/test sets are independent are also vulnerable to bias in a realistic setting [3, 13] .",
                "From the works listed in Table 2 , only #REF achieves unbiased results with respect to model generalisability; however, the features employed for their prediction task are derived from self-reported questionnaires of the subjects and not by automatic means."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Research in assessing mental health on a longitudinal basis aims to make use of relevant features extracted from various modalities, in order to train models for automatically predicting a user's mental state (target), either in a classification or a regression manner [1, 2, 3, 9, 10, 13, #TARGET_REF . Examples of state-of-the-art work in this domain are listed in Table 2 , along with the number of subjects that was used and the method upon which evaluation took place. Most approaches have used the \"MIXED\" approach to evaluate models [1, 2, 5, 9, 10, 22, 26] , which, as we will show, is vulnerable to bias, due to the danger of recognising the user in the test set and thus simply inferring her average mood score. LOIOCV approaches that have not ensured that their train/test sets are independent are also vulnerable to bias in a realistic setting [3, 13] . From the works listed in Table 2 , only #REF achieves unbiased results with respect to model generalisability; however, the features employed for their prediction task are derived from self-reported questionnaires of the subjects and not by automatic means.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Research in assessing mental health on a longitudinal basis aims to make use of relevant features extracted from various modalities, in order to train models for automatically predicting a user's mental state (target), either in a classification or a regression manner [1, 2, 3, 9, 10, 13, #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Research in assessing mental health on a longitudinal basis aims to make use of relevant features extracted from various modalities, in order to train models for automatically predicting a user's mental state (target), either in a classification or a regression manner [1, 2, 3, 9, 10, 13, 26] .",
                "Examples of state-of-the-art work in this domain are listed in Table 2 , along with the number of subjects that was used and the method upon which evaluation took place.",
                "Most approaches have used the \"MIXED\" approach to evaluate models [1, 2, 5, 9, 10, 22, #TARGET_REF , which, as we will show, is vulnerable to bias, due to the danger of recognising the user in the test set and thus simply inferring her average mood score.",
                "LOIOCV approaches that have not ensured that their train/test sets are independent are also vulnerable to bias in a realistic setting [3, 13] .",
                "From the works listed in Table 2 , only #REF achieves unbiased results with respect to model generalisability; however, the features employed for their prediction task are derived from self-reported questionnaires of the subjects and not by automatic means."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Research in assessing mental health on a longitudinal basis aims to make use of relevant features extracted from various modalities, in order to train models for automatically predicting a user's mental state (target), either in a classification or a regression manner [1, 2, 3, 9, 10, 13, 26] . Examples of state-of-the-art work in this domain are listed in Table 2 , along with the number of subjects that was used and the method upon which evaluation took place. Most approaches have used the \"MIXED\" approach to evaluate models [1, 2, 5, 9, 10, 22, #TARGET_REF , which, as we will show, is vulnerable to bias, due to the danger of recognising the user in the test set and thus simply inferring her average mood score. LOIOCV approaches that have not ensured that their train/test sets are independent are also vulnerable to bias in a realistic setting [3, 13] . From the works listed in Table 2 , only #REF achieves unbiased results with respect to model generalisability; however, the features employed for their prediction task are derived from self-reported questionnaires of the subjects and not by automatic means.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Most approaches have used the \\\"MIXED\\\" approach to evaluate models [1, 2, 5, 9, 10, 22, #TARGET_REF , which, as we will show, is vulnerable to bias, due to the danger of recognising the user in the test set and thus simply inferring her average mood score.\"]}"
    },
    {
        "gold": {
            "text": [
                "To illustrate this problem we have followed the approach by #REF , as one of the pioneering works on predicting depression with GPS traces, on a longitudinal basis.",
                "P3 Predicting users instead of mood scores: Most approaches merge all the instances from different subjects, in an attempt to build user-agnostic models in a randomised cross-validation framework [2, 9, 10, #TARGET_REF .",
                "This is problematic, especially when dealing with a small number of subjects, whose behaviour (as captured through their data) and mental health scores differ on an individual basis.",
                "Such approaches are in danger of \"predicting\" the user in the test set, since her (test set) features might be highly correlated with her features in the training set, and thus infer her average well-being score, based on the corresponding observations of the training set.",
                "Such approaches cannot guarantee that they will generalise on either a population-wide (LOUOCV ) or a personalised (LOIOCV ) level."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "To illustrate this problem we have followed the approach by #REF , as one of the pioneering works on predicting depression with GPS traces, on a longitudinal basis. P3 Predicting users instead of mood scores: Most approaches merge all the instances from different subjects, in an attempt to build user-agnostic models in a randomised cross-validation framework [2, 9, 10, #TARGET_REF . This is problematic, especially when dealing with a small number of subjects, whose behaviour (as captured through their data) and mental health scores differ on an individual basis. Such approaches are in danger of \"predicting\" the user in the test set, since her (test set) features might be highly correlated with her features in the training set, and thus infer her average well-being score, based on the corresponding observations of the training set. Such approaches cannot guarantee that they will generalise on either a population-wide (LOUOCV ) or a personalised (LOIOCV ) level.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"P3 Predicting users instead of mood scores: Most approaches merge all the instances from different subjects, in an attempt to build user-agnostic models in a randomised cross-validation framework [2, 9, 10, #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Tsakalidis et al. #TARGET_REF monitored the behaviour of 19 individuals over four months.",
                "The subjects were asked to complete two psychological scales [25, 29] on a daily basis, leading to three target scores (positive, negative, mental well-being); various features from smartphones (e.g., time spent on the preferred locations) and textual features (e.g., ngrams) were extracted passively over the 24 hours preceding a mood form timestamp.",
                "Model training and evaluation was performed in a randomised (MIXED) cross-validation setup, leading to high accuracy (R 2 = 0.76).",
                "However, a case demonstrating the potential user bias is when the models are trained on the textual sources: initially the highest R 2 (0.22) is achieved when a model is applied to the mental-wellbeing target; by normalising the textual features on a per-user basis, the R 2 increases to 0.65.",
                "While this is likely to happen because the vocabulary used by different users is normalised, there is also the danger of over-fitting the trained model to the identity of the user."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Tsakalidis et al. #TARGET_REF monitored the behaviour of 19 individuals over four months. The subjects were asked to complete two psychological scales [25, 29] on a daily basis, leading to three target scores (positive, negative, mental well-being); various features from smartphones (e.g., time spent on the preferred locations) and textual features (e.g., ngrams) were extracted passively over the 24 hours preceding a mood form timestamp. Model training and evaluation was performed in a randomised (MIXED) cross-validation setup, leading to high accuracy (R 2 = 0.76). However, a case demonstrating the potential user bias is when the models are trained on the textual sources: initially the highest R 2 (0.22) is achieved when a model is applied to the mental-wellbeing target; by normalising the textual features on a per-user basis, the R 2 increases to 0.65. While this is likely to happen because the vocabulary used by different users is normalised, there is also the danger of over-fitting the trained model to the identity of the user.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Tsakalidis et al. #TARGET_REF monitored the behaviour of 19 individuals over four months.\"]}"
    },
    {
        "gold": {
            "text": [
                "Self-reported scores on a daily basis served as the ground truth.",
                "The authors labelled the instances with the top 30% of all the scores as \"happy\" and the lowest 30% as \"sad\" and randomly separated them into training, validation and test sets, leading to the same user bias issue.",
                "Since different users exhibit different mood scores on average #TARGET_REF , by selecting instances from the top and bottom scores, one might end up separating users and convert the mood prediction task into a user identification one.",
                "A more suitable task could have been to try to predict the highest and lowest scores of every individual separately, either in a LOIOCV or in a LOUOCV setup.",
                "While we focus on the works of #REF and #REF , similar experimental setups were also followed in [10] , using the median of scores to separate the instances and performing five-fold cross-validation, and by Bogomolov et al. in [2] , working on a user-agnostic validation setting on 117 subjects to predict their happiness levels, and in [1] , for the stress level classification task."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Self-reported scores on a daily basis served as the ground truth. The authors labelled the instances with the top 30% of all the scores as \"happy\" and the lowest 30% as \"sad\" and randomly separated them into training, validation and test sets, leading to the same user bias issue. Since different users exhibit different mood scores on average #TARGET_REF , by selecting instances from the top and bottom scores, one might end up separating users and convert the mood prediction task into a user identification one. A more suitable task could have been to try to predict the highest and lowest scores of every individual separately, either in a LOIOCV or in a LOUOCV setup. While we focus on the works of #REF and #REF , similar experimental setups were also followed in [10] , using the median of scores to separate the instances and performing five-fold cross-validation, and by Bogomolov et al. in [2] , working on a user-agnostic validation setting on 117 subjects to predict their happiness levels, and in [1] , for the stress level classification task.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Since different users exhibit different mood scores on average #TARGET_REF , by selecting instances from the top and bottom scores, one might end up separating users and convert the mood prediction task into a user identification one.\"]}"
    },
    {
        "gold": {
            "text": [
                "P3 Predicting users instead of mood scores: Most approaches merge all the instances from different subjects, in an attempt to build user-agnostic models in a randomised cross-validation framework [2, 9, 10, 26] .",
                "This is problematic, especially when dealing with a small number of subjects, whose behaviour (as captured through their data) and mental health scores differ on an individual basis.",
                "Such approaches are in danger of \"predicting\" the user in the test set, since her (test set) features might be highly correlated with her features in the training set, and thus infer her average well-being score, based on the corresponding observations of the training set.",
                "Such approaches cannot guarantee that they will generalise on either a population-wide (LOUOCV ) or a personalised (LOIOCV ) level.",
                "In order to examine this effect in both a regression and a classification setting, we have followed the experimental framework by Tsakalidis et al. #TARGET_REF and #REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "P3 Predicting users instead of mood scores: Most approaches merge all the instances from different subjects, in an attempt to build user-agnostic models in a randomised cross-validation framework [2, 9, 10, 26] . This is problematic, especially when dealing with a small number of subjects, whose behaviour (as captured through their data) and mental health scores differ on an individual basis. Such approaches are in danger of \"predicting\" the user in the test set, since her (test set) features might be highly correlated with her features in the training set, and thus infer her average well-being score, based on the corresponding observations of the training set. Such approaches cannot guarantee that they will generalise on either a population-wide (LOUOCV ) or a personalised (LOIOCV ) level. In order to examine this effect in both a regression and a classification setting, we have followed the experimental framework by Tsakalidis et al. #TARGET_REF and #REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In order to examine this effect in both a regression and a classification setting, we have followed the experimental framework by Tsakalidis et al. #TARGET_REF and #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We employed the dataset obtained by Tsakalidis et al. #TARGET_REF , a pioneering dataset which contains a mix of longitudinal textual and mobile phone usage data for 30 subjects.",
                "From a textual perspective, this dataset consists of social media posts (1,854/5,167 facebook/twitter posts) and private messages (64,221/132/47,043 facebook/twitter/ SMS messages) sent by the subjects.",
                "For our ground truth, we use the {positive, negative, mental well-being} mood scores (in the ranges of , , , respectively) derived from self-assessed psychological scales during the study period."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "We employed the dataset obtained by Tsakalidis et al. #TARGET_REF , a pioneering dataset which contains a mix of longitudinal textual and mobile phone usage data for 30 subjects. From a textual perspective, this dataset consists of social media posts (1,854/5,167 facebook/twitter posts) and private messages (64,221/132/47,043 facebook/twitter/ SMS messages) sent by the subjects. For our ground truth, we use the {positive, negative, mental well-being} mood scores (in the ranges of , , , respectively) derived from self-assessed psychological scales during the study period.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We employed the dataset obtained by Tsakalidis et al. #TARGET_REF , a pioneering dataset which contains a mix of longitudinal textual and mobile phone usage data for 30 subjects.\"]}"
    },
    {
        "gold": {
            "text": [
                "We followed the evaluation settings of two past works (see section 3.3), with the only difference being the use of 5-fold CV instead of a train/dev/test split that was used in [9] .",
                "The features of every instance are extracted from the past day before the completion of a mood form.",
                "In Experiment 1 we follow the setup in #TARGET_REF : we perform 5-fold CV (MIXED) using SVM (SVR RBF ) and evaluate performance based on R 2 and RM SE.",
                "We compare the performance when tested under the LOIOCV /LOUOCV setups, with and without the per-user feature normalisation step.",
                "We also compare the performance of the MIXED setting, when our model is trained on the one-hot-encoded user id only."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We followed the evaluation settings of two past works (see section 3.3), with the only difference being the use of 5-fold CV instead of a train/dev/test split that was used in [9] . The features of every instance are extracted from the past day before the completion of a mood form. In Experiment 1 we follow the setup in #TARGET_REF : we perform 5-fold CV (MIXED) using SVM (SVR RBF ) and evaluate performance based on R 2 and RM SE. We compare the performance when tested under the LOIOCV /LOUOCV setups, with and without the per-user feature normalisation step. We also compare the performance of the MIXED setting, when our model is trained on the one-hot-encoded user id only.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In Experiment 1 we follow the setup in #TARGET_REF : we perform 5-fold CV (MIXED) using SVM (SVR RBF ) and evaluate performance based on R 2 and RM SE.\"]}"
    },
    {
        "gold": {
            "text": [
                "Experiment 1: Table 7 shows the results based on the evaluation setup of Tsakalidis et al. #TARGET_REF .",
                "In the MIXED cases, the pattern is consistent with [26] , indicating that normalising the features on a per-user basis yields better results, when dealing with sparse textual features (positive, negative, wellbeing targets).",
                "The explanation of this effect lies within the danger of predicting the user's identity instead of her mood scores.",
                "This is why the per-user normalisation does not have any effect for the stress target, since for that we are using dense features derived from smartphones: the vocabulary used by the subjects for the other targets is more indicative of their identity.",
                "In order to further support this statement, we trained the SVR model using only the one-hot encoded user id as a feature, without any textual features."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Experiment 1: Table 7 shows the results based on the evaluation setup of Tsakalidis et al. #TARGET_REF . In the MIXED cases, the pattern is consistent with [26] , indicating that normalising the features on a per-user basis yields better results, when dealing with sparse textual features (positive, negative, wellbeing targets). The explanation of this effect lies within the danger of predicting the user's identity instead of her mood scores. This is why the per-user normalisation does not have any effect for the stress target, since for that we are using dense features derived from smartphones: the vocabulary used by the subjects for the other targets is more indicative of their identity. In order to further support this statement, we trained the SVR model using only the one-hot encoded user id as a feature, without any textual features.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Experiment 1: Table 7 shows the results based on the evaluation setup of Tsakalidis et al. #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In MIXED, by identifying who the user is, we have a rough estimate of her mood score, which is by itself a good predictor, if it is compared with the average predictor across all mood scores of all users.",
                "Thus, the effect of the features in this setting cannot be assessed with certainty.",
                "Table 7 .",
                "P3: Results following the evaluation setup in #TARGET_REF (MIXED), along with the results obtained in the LOIOCV and LOUOCV settings with (+) and without (-) per-user input normalisation.",
                "Table 8 displays our results based on #REF (see section 3.3)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "In MIXED, by identifying who the user is, we have a rough estimate of her mood score, which is by itself a good predictor, if it is compared with the average predictor across all mood scores of all users. Thus, the effect of the features in this setting cannot be assessed with certainty. Table 7 . P3: Results following the evaluation setup in #TARGET_REF (MIXED), along with the results obtained in the LOIOCV and LOUOCV settings with (+) and without (-) per-user input normalisation. Table 8 displays our results based on #REF (see section 3.3).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"P3: Results following the evaluation setup in #TARGET_REF (MIXED), along with the results obtained in the LOIOCV and LOUOCV settings with (+) and without (-) per-user input normalisation.\"]}"
    },
    {
        "gold": {
            "text": [
                "Experiment 1: Table 7 shows the results based on the evaluation setup of #REF .",
                "In the MIXED cases, the pattern is consistent with #TARGET_REF , indicating that normalising the features on a per-user basis yields better results, when dealing with sparse textual features (positive, negative, wellbeing targets).",
                "The explanation of this effect lies within the danger of predicting the user's identity instead of her mood scores.",
                "This is why the per-user normalisation does not have any effect for the stress target, since for that we are using dense features derived from smartphones: the vocabulary used by the subjects for the other targets is more indicative of their identity.",
                "In order to further support this statement, we trained the SVR model using only the one-hot encoded user id as a feature, without any textual features."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Experiment 1: Table 7 shows the results based on the evaluation setup of #REF . In the MIXED cases, the pattern is consistent with #TARGET_REF , indicating that normalising the features on a per-user basis yields better results, when dealing with sparse textual features (positive, negative, wellbeing targets). The explanation of this effect lies within the danger of predicting the user's identity instead of her mood scores. This is why the per-user normalisation does not have any effect for the stress target, since for that we are using dense features derived from smartphones: the vocabulary used by the subjects for the other targets is more indicative of their identity. In order to further support this statement, we trained the SVR model using only the one-hot encoded user id as a feature, without any textual features.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"In the MIXED cases, the pattern is consistent with #TARGET_REF , indicating that normalising the features on a per-user basis yields better results, when dealing with sparse textual features (positive, negative, wellbeing targets).\"]}"
    },
    {
        "gold": {
            "text": [
                "Automatically learning dialogue structure from corpora is an active area of research driven by a recognition of the value offered by data-driven approaches (e.g., #REF) .",
                "Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems #TARGET_REF , but also contribute to our understanding of the cognitive and affective processes involved in learning through tutoring (#REF) .",
                "Although traditional top-down approaches (e.g., #REF) and some empirical work on analyzing the structure of tutorial dialogue (#REF) have yielded significant results, the field is limited by the lack of an automatic, data-driven approach to identifying dialogue structure.",
                "An empirical approach to identifying tutorial dialogue strategies, or modes, could address this limitation by providing a mechanism for describing in succinct probabilistic terms the tutorial strategies that actually occur in a corpus.",
                "Just as early work on dialogue act interpretation utilized hidden Markov models (HMMs) to capture linguistic structure (#REF) , we propose a system that uses HMMs to capture the structure of tutorial dialogue implicit within sequences of already-tagged dialogue acts."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Automatically learning dialogue structure from corpora is an active area of research driven by a recognition of the value offered by data-driven approaches (e.g., #REF) . Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems #TARGET_REF , but also contribute to our understanding of the cognitive and affective processes involved in learning through tutoring (#REF) . Although traditional top-down approaches (e.g., #REF) and some empirical work on analyzing the structure of tutorial dialogue (#REF) have yielded significant results, the field is limited by the lack of an automatic, data-driven approach to identifying dialogue structure. An empirical approach to identifying tutorial dialogue strategies, or modes, could address this limitation by providing a mechanism for describing in succinct probabilistic terms the tutorial strategies that actually occur in a corpus. Just as early work on dialogue act interpretation utilized hidden Markov models (HMMs) to capture linguistic structure (#REF) , we propose a system that uses HMMs to capture the structure of tutorial dialogue implicit within sequences of already-tagged dialogue acts.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems #TARGET_REF , but also contribute to our understanding of the cognitive and affective processes involved in learning through tutoring (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems (#REF) , but also contribute to our understanding of the cognitive and affective processes involved in learning through tutoring (#REF) .",
                "Although traditional top-down approaches (e.g., #REF) and some empirical work on analyzing the structure of tutorial dialogue #TARGET_REF have yielded significant results, the field is limited by the lack of an automatic, data-driven approach to identifying dialogue structure.",
                "An empirical approach to identifying tutorial dialogue strategies, or modes, could address this limitation by providing a mechanism for describing in succinct probabilistic terms the tutorial strategies that actually occur in a corpus.",
                "Just as early work on dialogue act interpretation utilized hidden Markov models (HMMs) to capture linguistic structure (#REF) , we propose a system that uses HMMs to capture the structure of tutorial dialogue implicit within sequences of already-tagged dialogue acts.",
                "This approach operates on the premise that at any given point in the tutorial dialogue, the collaborative interaction is in a dialogue mode that characterizes the nature of the exchanges between tutor and student."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems (#REF) , but also contribute to our understanding of the cognitive and affective processes involved in learning through tutoring (#REF) . Although traditional top-down approaches (e.g., #REF) and some empirical work on analyzing the structure of tutorial dialogue #TARGET_REF have yielded significant results, the field is limited by the lack of an automatic, data-driven approach to identifying dialogue structure. An empirical approach to identifying tutorial dialogue strategies, or modes, could address this limitation by providing a mechanism for describing in succinct probabilistic terms the tutorial strategies that actually occur in a corpus. Just as early work on dialogue act interpretation utilized hidden Markov models (HMMs) to capture linguistic structure (#REF) , we propose a system that uses HMMs to capture the structure of tutorial dialogue implicit within sequences of already-tagged dialogue acts. This approach operates on the premise that at any given point in the tutorial dialogue, the collaborative interaction is in a dialogue mode that characterizes the nature of the exchanges between tutor and student.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Although traditional top-down approaches (e.g., #REF) and some empirical work on analyzing the structure of tutorial dialogue #TARGET_REF have yielded significant results, the field is limited by the lack of an automatic, data-driven approach to identifying dialogue structure.\"]}"
    },
    {
        "gold": {
            "text": [
                "The importance of adjacency pairs is wellestablished in natural language dialogue (e.g., Schlegoff & #REF) , and adjacency pair analysis has illuminated important phenomena in tutoring as well #TARGET_REF .",
                "For the current corpus, bigram analysis of dialogue acts yielded a set of commonly-occurring pairs.",
                "However, as noted in (#REF) , in order to establish that two dialogue acts are truly related as an adjacency pair, it is important to determine whether the presence of the first member of the pair is associated with a significantly higher probability of the second member occurring.",
                "For this analysis we utilize a χ 2 test for independence of the categorical variables act i and act i+1 for all two-way combinations of dialogue act tags.",
                "Only pairs in which speaker(act i )≠speaker(act i+1 ) were considered."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The importance of adjacency pairs is wellestablished in natural language dialogue (e.g., Schlegoff & #REF) , and adjacency pair analysis has illuminated important phenomena in tutoring as well #TARGET_REF . For the current corpus, bigram analysis of dialogue acts yielded a set of commonly-occurring pairs. However, as noted in (#REF) , in order to establish that two dialogue acts are truly related as an adjacency pair, it is important to determine whether the presence of the first member of the pair is associated with a significantly higher probability of the second member occurring. For this analysis we utilize a χ 2 test for independence of the categorical variables act i and act i+1 for all two-way combinations of dialogue act tags. Only pairs in which speaker(act i )≠speaker(act i+1 ) were considered.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The importance of adjacency pairs is wellestablished in natural language dialogue (e.g., Schlegoff & #REF) , and adjacency pair analysis has illuminated important phenomena in tutoring as well #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Emojis are the evolution of characterbased emoticons (#REF) , and are extensively used, not only as sentiment carriers or boosters, but more importantly, to express ideas about a myriad of topics, e.g., mood ( ), food ( ), sports ( ) or scenery ( ).",
                "Emoji modeling and prediction is, therefore, an important problem towards the end goal of properly capturing the intended meaning of a social media message.",
                "In fact, emoji prediction, i.e., given a (usually short) message, predict its most likely associated emoji(s), may help to improve different NLP tasks (#REF) , such as information retrieval, generation of emojienriched social media content or suggestion of emojis when writing text messages or sharing pictures online.",
                "It has furthermore proven to be useful for sentiment analysis, emotion recognition and irony detection #TARGET_REF .",
                "The problem of emoji prediction, albeit recent, has already seen important developments."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Emojis are the evolution of characterbased emoticons (#REF) , and are extensively used, not only as sentiment carriers or boosters, but more importantly, to express ideas about a myriad of topics, e.g., mood ( ), food ( ), sports ( ) or scenery ( ). Emoji modeling and prediction is, therefore, an important problem towards the end goal of properly capturing the intended meaning of a social media message. In fact, emoji prediction, i.e., given a (usually short) message, predict its most likely associated emoji(s), may help to improve different NLP tasks (#REF) , such as information retrieval, generation of emojienriched social media content or suggestion of emojis when writing text messages or sharing pictures online. It has furthermore proven to be useful for sentiment analysis, emotion recognition and irony detection #TARGET_REF . The problem of emoji prediction, albeit recent, has already seen important developments.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In fact, emoji prediction, i.e., given a (usually short) message, predict its most likely associated emoji(s), may help to improve different NLP tasks (#REF) , such as information retrieval, generation of emojienriched social media content or suggestion of emojis when writing text messages or sharing pictures online.\", \"It has furthermore proven to be useful for sentiment analysis, emotion recognition and irony detection #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The model also includes an attention module to increase its sensitivity to individual words during prediction.",
                "In general, attention mechanisms allow the model to focus on specific words of the input (#REF) , instead of having to memorize all the important features in a fixed-length vector.",
                "The main architectural difference with respect to the typical attention is illustrated in Figure 1 .",
                "In #TARGET_REF , attention is computed as follows:",
                "Here h i ∈ R d is the hidden representation of the LSTM corresponding to the i th word, with N the total number of words in the sentence."
            ],
            "label": [
                "BACK",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                1,
                1
            ]
        },
        "input": "The model also includes an attention module to increase its sensitivity to individual words during prediction. In general, attention mechanisms allow the model to focus on specific words of the input (#REF) , instead of having to memorize all the important features in a fixed-length vector. The main architectural difference with respect to the typical attention is illustrated in Figure 1 . In #TARGET_REF , attention is computed as follows: Here h i ∈ R d is the hidden representation of the LSTM corresponding to the i th word, with N the total number of words in the sentence.",
        "output": "{\"label\": [\"BACK\", \"USE\"], \"context\": [\"The main architectural difference with respect to the typical attention is illustrated in Figure 1 .\", \"In #TARGET_REF , attention is computed as follows:\", \"Here h i \\u2208 R d is the hidden representation of the LSTM corresponding to the i th word, with N the total number of words in the sentence.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our contribution in this paper is twofold.",
                "First, we use the proposed label-wise mechanism to analyze the behavior of neural emoji classifiers, exploiting the attention weights to uncover and interpret emoji usages.",
                "Second, we experimentally compare the effect of the label-wise mechanism on the performance of an emoji classifier.",
                "We observed a performance improvement over competitive baselines such as FastText (FT) (#REF) and Deepmoji #TARGET_REF , which is most noticeable in the case of infrequent emojis.",
                "This suggests that an attentive mechanism can be leveraged to make neural architectures more sensitive to instances of underrepresented classes."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Our contribution in this paper is twofold. First, we use the proposed label-wise mechanism to analyze the behavior of neural emoji classifiers, exploiting the attention weights to uncover and interpret emoji usages. Second, we experimentally compare the effect of the label-wise mechanism on the performance of an emoji classifier. We observed a performance improvement over competitive baselines such as FastText (FT) (#REF) and Deepmoji #TARGET_REF , which is most noticeable in the case of infrequent emojis. This suggests that an attentive mechanism can be leveraged to make neural architectures more sensitive to instances of underrepresented classes.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We observed a performance improvement over competitive baselines such as FastText (FT) (#REF) and Deepmoji #TARGET_REF , which is most noticeable in the case of infrequent emojis.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our base architecture is the Deepmoji model #TARGET_REF , which is based on two stacked word-based bi-directional LSTM recurrent neural networks with skip connections between the first and the second LSTM.",
                "The model also includes an attention module to increase its sensitivity to individual words during prediction.",
                "In general, attention mechanisms allow the model to focus on specific words of the input (#REF) , instead of having to memorize all the important features in a fixed-length vector.",
                "The main architectural difference with respect to the typical attention is illustrated in Figure 1 .",
                "In #REF , attention is computed as follows:"
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Our base architecture is the Deepmoji model #TARGET_REF , which is based on two stacked word-based bi-directional LSTM recurrent neural networks with skip connections between the first and the second LSTM. The model also includes an attention module to increase its sensitivity to individual words during prediction. In general, attention mechanisms allow the model to focus on specific words of the input (#REF) , instead of having to memorize all the important features in a fixed-length vector. The main architectural difference with respect to the typical attention is illustrated in Figure 1 . In #REF , attention is computed as follows:",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Our base architecture is the Deepmoji model #TARGET_REF , which is based on two stacked word-based bi-directional LSTM recurrent neural networks with skip connections between the first and the second LSTM.\"]}"
    },
    {
        "gold": {
            "text": [
                "Models.",
                "In order to put our proposed labelwise attention mechanism in context, we compare its performance with a set of baselines: (1 #TARGET_REF .",
                "Finally, we denote as 2-BiLSTMs l our proposed label-wise attentive Bi-LSTM architecture.",
                "Results.",
                "Table 1 shows the results of our model and the baselines in the emoji prediction task for the different evaluation splits."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Models. In order to put our proposed labelwise attention mechanism in context, we compare its performance with a set of baselines: (1 #TARGET_REF . Finally, we denote as 2-BiLSTMs l our proposed label-wise attentive Bi-LSTM architecture. Results. Table 1 shows the results of our model and the baselines in the emoji prediction task for the different evaluation splits.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In order to put our proposed labelwise attention mechanism in context, we compare its performance with a set of baselines: (1 #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The parallel training data provided for German-English is quite large (38M sentence pairs).",
                "Most of the parallel data is crawled from the Internet and is not in News domain.",
                "Out-ofdomain training data can hurt the translation performance on News test sets (#REF) and also significantly increase training time.",
                "Therefore, we trained neural language models on a large monolingual News corpus to perform data selection #TARGET_REF .",
                "Back-translation Large monolingual data in the News domain is provided for both German and 1 https://github.com/awslabs/sockeye English, which can be back-translated as additional parallel training data for our system (#REFa; #REF) ."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "The parallel training data provided for German-English is quite large (38M sentence pairs). Most of the parallel data is crawled from the Internet and is not in News domain. Out-ofdomain training data can hurt the translation performance on News test sets (#REF) and also significantly increase training time. Therefore, we trained neural language models on a large monolingual News corpus to perform data selection #TARGET_REF . Back-translation Large monolingual data in the News domain is provided for both German and 1 https://github.com/awslabs/sockeye English, which can be back-translated as additional parallel training data for our system (#REFa; #REF) .",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Out-ofdomain training data can hurt the translation performance on News test sets (#REF) and also significantly increase training time.\", \"Therefore, we trained neural language models on a large monolingual News corpus to perform data selection #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, we trained neural language models on a large monolingual News corpus to perform data selection (#REF) .",
                "Back-translation Large monolingual data in the News domain is provided for both German and 1 https://github.com/awslabs/sockeye English, which can be back-translated as additional parallel training data for our system (#REFa; #REF) .",
                "The back-translated parallel data is in the News domain, which is a big advantage compared to outof-domain parallel training data provided for the News task.",
                "In-domain Fine-tuning The Transformer models were finally fine-tuned using the small in-domain parallel data provided for the News task (#REF; #TARGET_REF .",
                "Note that the large back-translated parallel data is also in-domain, but it has relatively low quality due to translation errors."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Therefore, we trained neural language models on a large monolingual News corpus to perform data selection (#REF) . Back-translation Large monolingual data in the News domain is provided for both German and 1 https://github.com/awslabs/sockeye English, which can be back-translated as additional parallel training data for our system (#REFa; #REF) . The back-translated parallel data is in the News domain, which is a big advantage compared to outof-domain parallel training data provided for the News task. In-domain Fine-tuning The Transformer models were finally fine-tuned using the small in-domain parallel data provided for the News task (#REF; #TARGET_REF . Note that the large back-translated parallel data is also in-domain, but it has relatively low quality due to translation errors.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In-domain Fine-tuning The Transformer models were finally fine-tuned using the small in-domain parallel data provided for the News task (#REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We trained two Transformer models with different sizes, Transformer-base and Transformer-big.",
                "Our final submission is an ensemble of both models #TARGET_REF .",
                "The ensemble of both models outperformed a single base or big model most likely because the two models can capture somewhat different features for the translation task."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                1,
                0
            ]
        },
        "input": "We trained two Transformer models with different sizes, Transformer-base and Transformer-big. Our final submission is an ensemble of both models #TARGET_REF . The ensemble of both models outperformed a single base or big model most likely because the two models can capture somewhat different features for the translation task.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"Our final submission is an ensemble of both models #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Many services such as web search (#REF) , recommender systems (#REF) , targeted advertising (#REF) , and rapid disaster response (#REF) rely on the location of users to personalise information and extract actionable knowledge.",
                "Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (#REF) or some combination of these #TARGET_REF,a) .",
                "The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (#REF; #REF; #REF; #REF) or dialectology #REF) .",
                "In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (#REF) .",
                "Lexical dialectology is (in part) the converse of user geolocation (#REF) : given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Many services such as web search (#REF) , recommender systems (#REF) , targeted advertising (#REF) , and rapid disaster response (#REF) rely on the location of users to personalise information and extract actionable knowledge. Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (#REF) or some combination of these #TARGET_REF,a) . The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (#REF; #REF; #REF; #REF) or dialectology #REF) . In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (#REF) . Lexical dialectology is (in part) the converse of user geolocation (#REF) : given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (#REF) or some combination of these #TARGET_REF,a) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The number of regions, regularisation strength, hidden layer and mini-batch size are tuned over development data and set to (32, 10 −5 , 896, 100), (256, 10 −6 , 2048, 10000) and (930, 10 −6 , 3720, 10000) for GEOTEXT, TWITTER-US and TWITTER-WORLD, respectively.",
                "The parameters are optimised using Adamx (#REF) using Lasagne/Theano (Theano Development #REF) .",
                "Following #REF and #REF , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\").",
                "Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161.",
                "4 The results reported in #TARGET_REF; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The number of regions, regularisation strength, hidden layer and mini-batch size are tuned over development data and set to (32, 10 −5 , 896, 100), (256, 10 −6 , 2048, 10000) and (930, 10 −6 , 3720, 10000) for GEOTEXT, TWITTER-US and TWITTER-WORLD, respectively. The parameters are optimised using Adamx (#REF) using Lasagne/Theano (Theano Development #REF) . Following #REF and #REF , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\"). Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161. 4 The results reported in #TARGET_REF; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"4 The results reported in #TARGET_REF; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset.\"]}"
    },
    {
        "gold": {
            "text": [
                "The results are also compared with state-of-the-art text-based methods based on a flat #TARGET_REF; #REF) or hierarchical (#REF; #REF; #REF) geospatial representation.",
                "Our method outperforms both the flat and hierarchical text-based models by a large margin.",
                "Comparing the two discretisation strategies, k-means outperforms k-d tree by a reasonable margin.",
                "We also incorporated the MLP predictions into a network-based model based on the method of Rahimi et al. (2015a) , and improved upon their work.",
                "We analysed the Table 2 : Nearest neighbours of place names."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The results are also compared with state-of-the-art text-based methods based on a flat #TARGET_REF; #REF) or hierarchical (#REF; #REF; #REF) geospatial representation. Our method outperforms both the flat and hierarchical text-based models by a large margin. Comparing the two discretisation strategies, k-means outperforms k-d tree by a reasonable margin. We also incorporated the MLP predictions into a network-based model based on the method of Rahimi et al. (2015a) , and improved upon their work. We analysed the Table 2 : Nearest neighbours of place names.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The results are also compared with state-of-the-art text-based methods based on a flat #TARGET_REF; #REF) or hierarchical (#REF; #REF; #REF) geospatial representation.\", \"Our method outperforms both the flat and hierarchical text-based models by a large margin.\"]}"
    },
    {
        "gold": {
            "text": [
                "We quantitatively tested the quality of the geographical embeddings by calculating the micro-average recall of the k-nearest dialect terms (in terms of the proportion of retrieved dialect terms) given a dialect region, as shown in Figure 4 .",
                "Recall at 0.5% is about 3.6%, meaning that we were able to retrieve 3.6% of the dialect terms given the dialect region name in the geographical embedding space.",
                "The embeddings slightly outperform the output layer of logistic regression (LR) #TARGET_REF"
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1
            ]
        },
        "input": "We quantitatively tested the quality of the geographical embeddings by calculating the micro-average recall of the k-nearest dialect terms (in terms of the proportion of retrieved dialect terms) given a dialect region, as shown in Figure 4 . Recall at 0.5% is about 3.6%, meaning that we were able to retrieve 3.6% of the dialect terms given the dialect region name in the geographical embedding space. The embeddings slightly outperform the output layer of logistic regression (LR) #TARGET_REF",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The embeddings slightly outperform the output layer of logistic regression (LR) #TARGET_REF\"]}"
    },
    {
        "gold": {
            "text": [
                "If we compare the widely used Conditional Random Fields (CRF) with newly proposed \"deep architecture\" sequence models #TARGET_REF , there are two things changing: from linear architecture to non-linear, and from discrete feature representation to distributional.",
                "It is unclear, however, what utility nonlinearity offers in conventional featurebased models.",
                "In this study, we show the close connection between CRF and \"sequence model\" neural nets, and present an empirical investigation to compare their performance on two sequence labeling tasks -Named Entity Recognition and Syntactic Chunking.",
                "Our results suggest that non-linear models are highly effective in low-dimensional distributional spaces.",
                "Somewhat surprisingly, we find that a nonlinear architecture offers no benefits in a high-dimensional discrete feature space."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "If we compare the widely used Conditional Random Fields (CRF) with newly proposed \"deep architecture\" sequence models #TARGET_REF , there are two things changing: from linear architecture to non-linear, and from discrete feature representation to distributional. It is unclear, however, what utility nonlinearity offers in conventional featurebased models. In this study, we show the close connection between CRF and \"sequence model\" neural nets, and present an empirical investigation to compare their performance on two sequence labeling tasks -Named Entity Recognition and Syntactic Chunking. Our results suggest that non-linear models are highly effective in low-dimensional distributional spaces. Somewhat surprisingly, we find that a nonlinear architecture offers no benefits in a high-dimensional discrete feature space.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"If we compare the widely used Conditional Random Fields (CRF) with newly proposed \\\"deep architecture\\\" sequence models #TARGET_REF , there are two things changing: from linear architecture to non-linear, and from discrete feature representation to distributional.\"]}"
    },
    {
        "gold": {
            "text": [
                "Fortunately, we can use forward-backward style dynamic programming to compute the marginal probabilities efficiently.",
                "It is also worth pointing out that this model has in fact been introduced a few times in prior literature.",
                "It was termed Conditional Neural Fields by #REF , and later Neural Conditional Random Fields by #REF .",
                "Unfortunately, the connection to #REF was not recognized in either of these two studies; vice versa, neither of the above were referenced in #TARGET_REF .",
                "This model also appeared previously in the speech recognition literature in #REF ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Fortunately, we can use forward-backward style dynamic programming to compute the marginal probabilities efficiently. It is also worth pointing out that this model has in fact been introduced a few times in prior literature. It was termed Conditional Neural Fields by #REF , and later Neural Conditional Random Fields by #REF . Unfortunately, the connection to #REF was not recognized in either of these two studies; vice versa, neither of the above were referenced in #TARGET_REF . This model also appeared previously in the speech recognition literature in #REF .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Unfortunately, the connection to #REF was not recognized in either of these two studies; vice versa, neither of the above were referenced in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "All sequences of numbers are replaced with num (e.g., \"PS1\" would become \"PSnum\"), sentence boundaries are padded with token PAD, and unknown words are grouped into UNKNOWN.",
                "We attempt to replicate the model described in #TARGET_REF without task-specific fine-tuning, with a few exceptions: 1) we used the soft tanh activation function instead of hard tanh; 2) we use the BIO2 tagging scheme instead of BIOES; 3) we use L-BFGS optimization algorithm instead of stochastic gradient descent; 4) we did not use Gazetteer features; 5) #TARGET_REF mentioned 5 binary features that look at the capitalization pattern of words to append to the embedding as additional dimensions, but only 4 were described in the paper, which we implemented accordingly."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "All sequences of numbers are replaced with num (e.g., \"PS1\" would become \"PSnum\"), sentence boundaries are padded with token PAD, and unknown words are grouped into UNKNOWN. We attempt to replicate the model described in #TARGET_REF without task-specific fine-tuning, with a few exceptions: 1) we used the soft tanh activation function instead of hard tanh; 2) we use the BIO2 tagging scheme instead of BIOES; 3) we use L-BFGS optimization algorithm instead of stochastic gradient descent; 4) we did not use Gazetteer features; 5) #TARGET_REF mentioned 5 binary features that look at the capitalization pattern of words to append to the embedding as additional dimensions, but only 4 were described in the paper, which we implemented accordingly.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"We attempt to replicate the model described in #TARGET_REF without task-specific fine-tuning, with a few exceptions: 1) we used the soft tanh activation function instead of hard tanh; 2) we use the BIO2 tagging scheme instead of BIOES; 3) we use L-BFGS optimization algorithm instead of stochastic gradient descent; 4) we did not use Gazetteer features; 5) #TARGET_REF mentioned 5 binary features that look at the capitalization pattern of words to append to the embedding as additional dimensions, but only 4 were described in the paper, which we implemented accordingly.\"]}"
    },
    {
        "gold": {
            "text": [
                "Sequence labeling encompasses an important class of NLP problems that aim at annotating natural language texts with various syntactic and semantic information, such as part-of-speech tags and named-entity labels.",
                "Output from such systems can facilitate downstream applications such as Question Answering and Relation Extraction.",
                "Most methods developed so far for sequence labeling employ generalized linear statistical models, meaning methods that describe the data as a combination of linear basis functions, either directly in the input variables space (e.g., SVM) or through some transformation of the probability distributions (e.g., \"log-linear\" models).",
                "Recently, #TARGET_REF proposed \"deep architecture\" models for sequence labeling (named Sentence-level Likelihood Neural Nets, abbreviated as SLNN henceforth), and showed promising results on a range of tasks (POS tagging, NER, Chunking, and SRL).",
                "Two new changes were suggested: extending the model from a linear to non-linear architecture; and replacing discrete feature representations with distributional feature representations in a continuous space."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Sequence labeling encompasses an important class of NLP problems that aim at annotating natural language texts with various syntactic and semantic information, such as part-of-speech tags and named-entity labels. Output from such systems can facilitate downstream applications such as Question Answering and Relation Extraction. Most methods developed so far for sequence labeling employ generalized linear statistical models, meaning methods that describe the data as a combination of linear basis functions, either directly in the input variables space (e.g., SVM) or through some transformation of the probability distributions (e.g., \"log-linear\" models). Recently, #TARGET_REF proposed \"deep architecture\" models for sequence labeling (named Sentence-level Likelihood Neural Nets, abbreviated as SLNN henceforth), and showed promising results on a range of tasks (POS tagging, NER, Chunking, and SRL). Two new changes were suggested: extending the model from a linear to non-linear architecture; and replacing discrete feature representations with distributional feature representations in a continuous space.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Recently, #TARGET_REF proposed \\\"deep architecture\\\" models for sequence labeling (named Sentence-level Likelihood Neural Nets, abbreviated as SLNN henceforth), and showed promising results on a range of tasks (POS tagging, NER, Chunking, and SRL).\"]}"
    },
    {
        "gold": {
            "text": [
                "1 Normalizing locally in a logistic regression is equivalent to adding a softmax layer to the output layer of the IONN, which was commonly done in neural networks, such as in #TARGET_REF .",
                "We can add a hidden linear layer to this architecture to formulate a two-layer Linear Neural Network (LNN), as shown in the middle diagram of Figure 1 .",
                "The value of the node z j in the hidden layer is computed as",
                "The value y i for nodes in the output layer is computed as:",
                "where ω (k,j) and δ (j,i) are new parameters introduced in the model."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "1 Normalizing locally in a logistic regression is equivalent to adding a softmax layer to the output layer of the IONN, which was commonly done in neural networks, such as in #TARGET_REF . We can add a hidden linear layer to this architecture to formulate a two-layer Linear Neural Network (LNN), as shown in the middle diagram of Figure 1 . The value of the node z j in the hidden layer is computed as The value y i for nodes in the output layer is computed as: where ω (k,j) and δ (j,i) are new parameters introduced in the model.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"1 Normalizing locally in a logistic regression is equivalent to adding a softmax layer to the output layer of the IONN, which was commonly done in neural networks, such as in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The node potential function now becomes:",
                "This two-layer network is actually no more powerful than the previous model, since we can always compile it down to a single-layer IONN by making Θ = Ω∆. In the next step, we take the output of the hidden layer in the LNN, and send it through a non-linear activation function, such as a sigmoid or tanh, then we arrive at a two-layer Deep Neural Network (DNN) model.",
                "Unlike the previous two models, the DNN is non-linear, and thus capable of representing a more complex decision surface.",
                "So far we have extended the potential function used in node cliques of a CRF to a non-linear DNN. And if we keep the potential function for edge cliques the same as before, then in fact we have arrived at an identical model to the SLNN in Collobert et al. #TARGET_REF .",
                "The difference between a SLNN and an ordinary DNN model is that we need to take into consideration the influence of edge cliques, and therefore we can no longer normalize the clique factors at each position to calculate the local marginals, as we would do in a logistic regression."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The node potential function now becomes: This two-layer network is actually no more powerful than the previous model, since we can always compile it down to a single-layer IONN by making Θ = Ω∆. In the next step, we take the output of the hidden layer in the LNN, and send it through a non-linear activation function, such as a sigmoid or tanh, then we arrive at a two-layer Deep Neural Network (DNN) model. Unlike the previous two models, the DNN is non-linear, and thus capable of representing a more complex decision surface. So far we have extended the potential function used in node cliques of a CRF to a non-linear DNN. And if we keep the potential function for edge cliques the same as before, then in fact we have arrived at an identical model to the SLNN in Collobert et al. #TARGET_REF . The difference between a SLNN and an ordinary DNN model is that we need to take into consideration the influence of edge cliques, and therefore we can no longer normalize the clique factors at each position to calculate the local marginals, as we would do in a logistic regression.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"So far we have extended the potential function used in node cliques of a CRF to a non-linear DNN. And if we keep the potential function for edge cliques the same as before, then in fact we have arrived at an identical model to the SLNN in Collobert et al. #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We did not explicitly tune the features used in CRF to optimize for performance, since feature engineering is not the focus of this study.",
                "However, overall we found that the feature set we used is competitive with CRF results from earlier literature (#REF; #TARGET_REF .",
                "For models that embed hidden layers, we set the number of hidden nodes to 300.",
                "2 Results are reported on the standard evaluation metrics of entity/chunk precision, recall and F1 measure.",
                "For experiments with continuous space feature representations (a.k.a., word embeddings), we took the word embeddings (130K words, 50 dimensions) used in #REF , which were trained for 2 months over Wikipedia text."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We did not explicitly tune the features used in CRF to optimize for performance, since feature engineering is not the focus of this study. However, overall we found that the feature set we used is competitive with CRF results from earlier literature (#REF; #TARGET_REF . For models that embed hidden layers, we set the number of hidden nodes to 300. 2 Results are reported on the standard evaluation metrics of entity/chunk precision, recall and F1 measure. For experiments with continuous space feature representations (a.k.a., word embeddings), we took the word embeddings (130K words, 50 dimensions) used in #REF , which were trained for 2 months over Wikipedia text.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"However, overall we found that the feature set we used is competitive with CRF results from earlier literature (#REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We did not explicitly tune the features used in CRF to optimize for performance, since feature engineering is not the focus of this study.",
                "However, overall we found that the feature set we used is competitive with CRF results from earlier literature (#REF; #REF) .",
                "For models that embed hidden layers, we set the number of hidden nodes to 300.",
                "2 Results are reported on the standard evaluation metrics of entity/chunk precision, recall and F1 measure.",
                "For experiments with continuous space feature representations (a.k.a., word embeddings), we took the word embeddings (130K words, 50 dimensions) used in #TARGET_REF , which were trained for 2 months over Wikipedia text."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "We did not explicitly tune the features used in CRF to optimize for performance, since feature engineering is not the focus of this study. However, overall we found that the feature set we used is competitive with CRF results from earlier literature (#REF; #REF) . For models that embed hidden layers, we set the number of hidden nodes to 300. 2 Results are reported on the standard evaluation metrics of entity/chunk precision, recall and F1 measure. For experiments with continuous space feature representations (a.k.a., word embeddings), we took the word embeddings (130K words, 50 dimensions) used in #TARGET_REF , which were trained for 2 months over Wikipedia text.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"For experiments with continuous space feature representations (a.k.a., word embeddings), we took the word embeddings (130K words, 50 dimensions) used in #TARGET_REF , which were trained for 2 months over Wikipedia text.\"]}"
    },
    {
        "gold": {
            "text": [
                "For the next experiment, we replace the discrete input features with a continuous space representation by looking up the embedding of each word, and concatenate the embeddings of a five word window centered around the current position.",
                "Four binary features are also appended to each word embedding to capture capitalization patterns, as described in #TARGET_REF .",
                "Results of the CRF and SLNN under this setting for the NER task is show in Table 3 .",
                "With a continuous space representation, the SLNN model works significantly better than a CRF, by as much as 7% on the CoNLL development set, and 3.7% on ACE dataset.",
                "This suggests that there exist statistical dependencies within this low-dimensional (300) data that cannot be effectively captured by linear transformations, but can be modeled in the non-linear neural nets."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "For the next experiment, we replace the discrete input features with a continuous space representation by looking up the embedding of each word, and concatenate the embeddings of a five word window centered around the current position. Four binary features are also appended to each word embedding to capture capitalization patterns, as described in #TARGET_REF . Results of the CRF and SLNN under this setting for the NER task is show in Table 3 . With a continuous space representation, the SLNN model works significantly better than a CRF, by as much as 7% on the CoNLL development set, and 3.7% on ACE dataset. This suggests that there exist statistical dependencies within this low-dimensional (300) data that cannot be effectively captured by linear transformations, but can be modeled in the non-linear neural nets.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"Four binary features are also appended to each word embedding to capture capitalization patterns, as described in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We carefully compared and analyzed the nonlinear neural networks used in #TARGET_REF and the widely adopted CRF, and revealed their close relationship.",
                "Through extensive experiments on NER and Syntactic Chunking, we have shown that non-linear architectures are effective in low dimensional continuous input spaces, but that they are not better suited for conventional highdimensional discrete input spaces.",
                "Furthermore, both linear and non-linear models benefit greatly from the combination of continuous and discrete features, especially for out-of-domain datasets.",
                "This finding confirms earlier results that distributional representations can be used to achieve better generalization."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "We carefully compared and analyzed the nonlinear neural networks used in #TARGET_REF and the widely adopted CRF, and revealed their close relationship. Through extensive experiments on NER and Syntactic Chunking, we have shown that non-linear architectures are effective in low dimensional continuous input spaces, but that they are not better suited for conventional highdimensional discrete input spaces. Furthermore, both linear and non-linear models benefit greatly from the combination of continuous and discrete features, especially for out-of-domain datasets. This finding confirms earlier results that distributional representations can be used to achieve better generalization.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"We carefully compared and analyzed the nonlinear neural networks used in #TARGET_REF and the widely adopted CRF, and revealed their close relationship.\"]}"
    },
    {
        "gold": {
            "text": [
                "We use an encoder-decoder framework where the sandhied (unsegmented) and the unsandhied (segmented) sequences are treated as the input at the encoder and the output at the decoder, respectively.",
                "We train the model so as to maximise the conditional probability of predicting the unsandhied sequence given its corresponding sandhied sequence (#REF) .",
                "We propose a knowledge-lean data-centric approach for the segmentation task.",
                "Our approach will help to scale the segmentation process in comparison with the challenges posed by knowledge involved processes in the current systems #TARGET_REF .",
                "We only use parallel segmented and unsegmented sentences during training."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We use an encoder-decoder framework where the sandhied (unsegmented) and the unsandhied (segmented) sequences are treated as the input at the encoder and the output at the decoder, respectively. We train the model so as to maximise the conditional probability of predicting the unsandhied sequence given its corresponding sandhied sequence (#REF) . We propose a knowledge-lean data-centric approach for the segmentation task. Our approach will help to scale the segmentation process in comparison with the challenges posed by knowledge involved processes in the current systems #TARGET_REF . We only use parallel segmented and unsegmented sentences during training.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"Our approach will help to scale the segmentation process in comparison with the challenges posed by knowledge involved processes in the current systems #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "To further catalyse the research in word segmentation for Sanskrit, #REF has released a dataset for the word segmentation task.",
                "The work releases a dataset of 119,000 sentences in Sanskrit along with the lexical and morphological analysis from a shallow parser.",
                "The work emphasises the need for not just predicting the inflected word form but also the prediction of the associated morphological information of the word.",
                "The additional information will be beneficial in further processing of Sanskrit texts, such as Dependency parsing or summarisation #TARGET_REF .So far, no system successfully predicts the morphological information of the words in addition to the final word form.",
                "Though #REF has designed their system with this requirement in mind and outlined the possible extension of their system for the purpose, the system currently only predicts the final word-form."
            ],
            "label": [
                "FUT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "To further catalyse the research in word segmentation for Sanskrit, #REF has released a dataset for the word segmentation task. The work releases a dataset of 119,000 sentences in Sanskrit along with the lexical and morphological analysis from a shallow parser. The work emphasises the need for not just predicting the inflected word form but also the prediction of the associated morphological information of the word. The additional information will be beneficial in further processing of Sanskrit texts, such as Dependency parsing or summarisation #TARGET_REF .So far, no system successfully predicts the morphological information of the words in addition to the final word form. Though #REF has designed their system with this requirement in mind and outlined the possible extension of their system for the purpose, the system currently only predicts the final word-form.",
        "output": "{\"label\": [\"FUT\"], \"context\": [\"The additional information will be beneficial in further processing of Sanskrit texts, such as Dependency parsing or summarisation #TARGET_REF .So far, no system successfully predicts the morphological information of the words in addition to the final word form.\"]}"
    },
    {
        "gold": {
            "text": [
                "But predicting morphological information requires the knowledge of exact word boundaries.",
                "This should be seen as a multitask learning set up.",
                "One possible solution is to learn 'GibberishVocab' on the set of words rather than sentences. But this leads to increased vocabulary at decoder which is not beneficial, given the scarcity of the data we have.",
                "Given the importance of morphological segmentation in morphologically rich languages such as Hebrew and Arabic (Seeker and Ç etinoglu, 2015) , the same applies to the morphologically rich Sanskrit as well #TARGET_REF .",
                "But, we leave this work for future."
            ],
            "label": [
                "FUT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "But predicting morphological information requires the knowledge of exact word boundaries. This should be seen as a multitask learning set up. One possible solution is to learn 'GibberishVocab' on the set of words rather than sentences. But this leads to increased vocabulary at decoder which is not beneficial, given the scarcity of the data we have. Given the importance of morphological segmentation in morphologically rich languages such as Hebrew and Arabic (Seeker and Ç etinoglu, 2015) , the same applies to the morphologically rich Sanskrit as well #TARGET_REF . But, we leave this work for future.",
        "output": "{\"label\": [\"FUT\"], \"context\": [\"Given the importance of morphological segmentation in morphologically rich languages such as Hebrew and Arabic (Seeker and \\u00c7 etinoglu, 2015) , the same applies to the morphologically rich Sanskrit as well #TARGET_REF .\", \"But, we leave this work for future.\"]}"
    },
    {
        "gold": {
            "text": [
                "While there are plenty of sandhied texts available for Sanskrit, it is hard to find parallel or unsandhied texts alone, as it is deterministic to get sandhied text from unsandhied texts.",
                "In our case we use 105,000 parallel strings from the Digital Corpus of Sanskrit as released in #TARGET_REF .",
                "To handle the data sparsity, we adopt a purely engineering based approach for our model.",
                "Rather than relying on the real word boundaries, we use the 'sentencepiece' model, an unsupervised text tokeniser (#REF) to obtain a new vocabulary for the corpus.",
                "The method was originally proposed for segmentation problem in Japanese and Korean speech recognition systems."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "While there are plenty of sandhied texts available for Sanskrit, it is hard to find parallel or unsandhied texts alone, as it is deterministic to get sandhied text from unsandhied texts. In our case we use 105,000 parallel strings from the Digital Corpus of Sanskrit as released in #TARGET_REF . To handle the data sparsity, we adopt a purely engineering based approach for our model. Rather than relying on the real word boundaries, we use the 'sentencepiece' model, an unsupervised text tokeniser (#REF) to obtain a new vocabulary for the corpus. The method was originally proposed for segmentation problem in Japanese and Korean speech recognition systems.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In our case we use 105,000 parallel strings from the Digital Corpus of Sanskrit as released in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We used a dataset of 107,000 sentences from the #TARGET_REF #TARGET_REF .",
                "The dataset is a subset of the Digital Corpus of Sanskrit.",
                "From the dataset we only use the input sentences and the ground truth inflected word-forms.",
                "We ignore all the other morphological and lemma information available in the dataset."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "We used a dataset of 107,000 sentences from the #TARGET_REF #TARGET_REF . The dataset is a subset of the Digital Corpus of Sanskrit. From the dataset we only use the input sentences and the ground truth inflected word-forms. We ignore all the other morphological and lemma information available in the dataset.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We used a dataset of 107,000 sentences from the #TARGET_REF #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Scalability of such systems is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing.",
                "The systems by #REF and #TARGET_REF assume that the parser by #REF , identifies all the possible candidate chunks.",
                "Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence.",
                "#REF states that it is desirable to predict the morphological information of a word from along with the final word-form as the information will be helpful in further processing of Sanskrit.",
                "The segmentation task is seen as a means and not an end itself."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Scalability of such systems is further restricted as the sentences often need to undergo linguistically involved preprocessing steps that lead to human in the loop processing. The systems by #REF and #TARGET_REF assume that the parser by #REF , identifies all the possible candidate chunks. Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence. #REF states that it is desirable to predict the morphological information of a word from along with the final word-form as the information will be helpful in further processing of Sanskrit. The segmentation task is seen as a means and not an end itself.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The systems by #REF and #TARGET_REF assume that the parser by #REF , identifies all the possible candidate chunks.\", \"Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence.",
                "#TARGET_REF states that it is desirable to predict the morphological information of a word from along with the final word-form as the information will be helpful in further processing of Sanskrit.",
                "The segmentation task is seen as a means and not an end itself.",
                "Here, we overlook this aspect and see the segmentation task as an end in itself.",
                "So we achieve scalability at the cost of missing out on providing valuable linguistic information."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "Our proposed model is built with precisely one purpose in mind, which is to predict the final word-forms in a given sequence. #TARGET_REF states that it is desirable to predict the morphological information of a word from along with the final word-form as the information will be helpful in further processing of Sanskrit. The segmentation task is seen as a means and not an end itself. Here, we overlook this aspect and see the segmentation task as an end in itself. So we achieve scalability at the cost of missing out on providing valuable linguistic information.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"#TARGET_REF states that it is desirable to predict the morphological information of a word from along with the final word-form as the information will be helpful in further processing of Sanskrit.\", \"The segmentation task is seen as a means and not an end itself.\", \"Here, we overlook this aspect and see the segmentation task as an end in itself.\"]}"
    },
    {
        "gold": {
            "text": [
                "Translating Japanese to English is difficult because they belong to different language families.",
                "Naïve phrase-based statistical machine translation (SMT) often fails to address syntactic difference between Japanese and English.",
                "Preordering methods are one of the simple but effective approaches that can model reordering in a long distance, which is crucial in translating Japanese and English.",
                "Thus, we apply a predicate-argument structure-based preordering method to the Japanese-English statistical machine translation task of scientific papers.",
                "Our method is based on the method described in ( #TARGET_REF) , and extends #TARGET_REF rules to handle abbreviation and passivization frequently found in scientific papers."
            ],
            "label": [
                "USE",
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Translating Japanese to English is difficult because they belong to different language families. Naïve phrase-based statistical machine translation (SMT) often fails to address syntactic difference between Japanese and English. Preordering methods are one of the simple but effective approaches that can model reordering in a long distance, which is crucial in translating Japanese and English. Thus, we apply a predicate-argument structure-based preordering method to the Japanese-English statistical machine translation task of scientific papers. Our method is based on the method described in ( #TARGET_REF) , and extends #TARGET_REF rules to handle abbreviation and passivization frequently found in scientific papers.",
        "output": "{\"label\": [\"USE\", \"EXT\"], \"context\": [\"Our method is based on the method described in ( #TARGET_REF) , and extends #TARGET_REF rules to handle abbreviation and passivization frequently found in scientific papers.\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, academic writing in English traditionally relies on passive voice to give an objective impression, but one can use either passive construction or a zeropronoun in the Japanese translation of passive construction on the English side.",
                "It is not clear whether existing preordering rules are applicable to scientific domain due to such stylistic difference.",
                "Predicate-argument structure-based preordering is one of the promising approaches that can solve syntactic and stylistic difference between a language pair.",
                "Predicate-argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction.",
                "Following ( #TARGET_REF) , we perform predicate-argument structure analysis on the Japanese side to preorder Japanese sentences to form an SVO-like word order."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "For example, academic writing in English traditionally relies on passive voice to give an objective impression, but one can use either passive construction or a zeropronoun in the Japanese translation of passive construction on the English side. It is not clear whether existing preordering rules are applicable to scientific domain due to such stylistic difference. Predicate-argument structure-based preordering is one of the promising approaches that can solve syntactic and stylistic difference between a language pair. Predicate-argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction. Following ( #TARGET_REF) , we perform predicate-argument structure analysis on the Japanese side to preorder Japanese sentences to form an SVO-like word order.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following ( #TARGET_REF) , we perform predicate-argument structure analysis on the Japanese side to preorder Japanese sentences to form an SVO-like word order.\"]}"
    },
    {
        "gold": {
            "text": [
                "Since #TARGET_REF is the one we re-implemented in this paper, we will describe #TARGET_REF in detail below.",
                "Pseudo head-initialization Since Japanese is a head-final language but English is a head-initial language, this rule transforms a Japanese dependency tree as to become a head-initial phrase sequence.",
                "Concretely, we move the last phrase, which is a predicate of a Japanese sentence in almost all cases, to the beginning of the sentence.",
                "We then order each phrase as their children located immediately after them.",
                "Inter-chunk preordering We move a predicate of a sentence to an adequate place."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Since #TARGET_REF is the one we re-implemented in this paper, we will describe #TARGET_REF in detail below. Pseudo head-initialization Since Japanese is a head-final language but English is a head-initial language, this rule transforms a Japanese dependency tree as to become a head-initial phrase sequence. Concretely, we move the last phrase, which is a predicate of a Japanese sentence in almost all cases, to the beginning of the sentence. We then order each phrase as their children located immediately after them. Inter-chunk preordering We move a predicate of a sentence to an adequate place.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Since #TARGET_REF is the one we re-implemented in this paper, we will describe #TARGET_REF in detail below.\"]}"
    },
    {
        "gold": {
            "text": [
                "If a subject exists in a sentence 2 , the predicate is moved imme-Inter-chunk normalization We restore the order of coordinate expressions which are reversed by the first rule.",
                "Also, since a full stop is moved along with the predicate, we restore it back to the end of the sentence.",
                "Intra-chunk preordering We apply the phraselevel rule, which swaps function words and content words in a phrase.",
                "It will improve alignments because function words in Japanese (e.g. postposition) appear after content words while those in English (e.g. preposition) appear before content words.",
                "3 Extension to ( #TARGET_REF) Our proposed preordering model is based on ( #TARGET_REF) with three extensions to better handle academic writing in scientific papers."
            ],
            "label": [
                "USE",
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "If a subject exists in a sentence 2 , the predicate is moved imme-Inter-chunk normalization We restore the order of coordinate expressions which are reversed by the first rule. Also, since a full stop is moved along with the predicate, we restore it back to the end of the sentence. Intra-chunk preordering We apply the phraselevel rule, which swaps function words and content words in a phrase. It will improve alignments because function words in Japanese (e.g. postposition) appear after content words while those in English (e.g. preposition) appear before content words. 3 Extension to ( #TARGET_REF) Our proposed preordering model is based on ( #TARGET_REF) with three extensions to better handle academic writing in scientific papers.",
        "output": "{\"label\": [\"USE\", \"EXT\"], \"context\": [\"3 Extension to ( #TARGET_REF) Our proposed preordering model is based on ( #TARGET_REF) with three extensions to better handle academic writing in scientific papers.\"]}"
    },
    {
        "gold": {
            "text": [
                "We compared translation performance using a standard phrase-based statistical machine translation technique with three kinds of data:",
                "• original data (baseline),",
                "• preordered data by our re-implementation of ( #TARGET_REF) , and",
                "• preordered data by our proposed methods.",
                "We analyzed predicate-argument structure of only the last predicate for each sentence, regardless of the number of predicates in a sentence."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We compared translation performance using a standard phrase-based statistical machine translation technique with three kinds of data: • original data (baseline), • preordered data by our re-implementation of ( #TARGET_REF) , and • preordered data by our proposed methods. We analyzed predicate-argument structure of only the last predicate for each sentence, regardless of the number of predicates in a sentence.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We compared translation performance using a standard phrase-based statistical machine translation technique with three kinds of data:\", \"\\u2022 preordered data by our re-implementation of ( #TARGET_REF) , and\"]}"
    },
    {
        "gold": {
            "text": [
                "• original data (baseline),",
                "• preordered data by our re-implementation of (#REF) , and",
                "• preordered data by our proposed methods.",
                "We analyzed predicate-argument structure of only the last predicate for each sentence, regardless of the number of predicates in a sentence.",
                "Also, following ( #TARGET_REF) , we did not consider event nouns as predicates."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "• original data (baseline), • preordered data by our re-implementation of (#REF) , and • preordered data by our proposed methods. We analyzed predicate-argument structure of only the last predicate for each sentence, regardless of the number of predicates in a sentence. Also, following ( #TARGET_REF) , we did not consider event nouns as predicates.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Also, following ( #TARGET_REF) , we did not consider event nouns as predicates.\"]}"
    },
    {
        "gold": {
            "text": [
                "Some of the errors found in a translation result are due to the errors in predicate-argument structure analysis.",
                "We found that it is hard for predicateargument structure analyzer trained on a newswire Table 1 : Comparison of the preordering methods.",
                "All the preordering models using ( #TARGET_REF) are our re-implementation of their paper."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1
            ]
        },
        "input": "Some of the errors found in a translation result are due to the errors in predicate-argument structure analysis. We found that it is hard for predicateargument structure analyzer trained on a newswire Table 1 : Comparison of the preordering methods. All the preordering models using ( #TARGET_REF) are our re-implementation of their paper.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"All the preordering models using ( #TARGET_REF) are our re-implementation of their paper.\"]}"
    },
    {
        "gold": {
            "text": [
                "Predicate-argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction.",
                "Following (#REF) , we perform predicate-argument structure analysis on the Japanese side to preorder Japanese sentences to form an SVO-like word order.",
                "We propose three modifications to the preordering rules to extend their model to better handle translation of scientific papers.",
                "The main contribution of this work is as follows:",
                "• We propose an extension to ( #TARGET_REF) in order to deal with abbreviation and passivization frequently found in scientific papers."
            ],
            "label": [
                "EXT",
                "MOT"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Predicate-argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction. Following (#REF) , we perform predicate-argument structure analysis on the Japanese side to preorder Japanese sentences to form an SVO-like word order. We propose three modifications to the preordering rules to extend their model to better handle translation of scientific papers. The main contribution of this work is as follows: • We propose an extension to ( #TARGET_REF) in order to deal with abbreviation and passivization frequently found in scientific papers.",
        "output": "{\"label\": [\"EXT\", \"MOT\"], \"context\": [\"\\u2022 We propose an extension to ( #TARGET_REF) in order to deal with abbreviation and passivization frequently found in scientific papers.\"]}"
    },
    {
        "gold": {
            "text": [
                "Naïve phrase-based statistical machine translation (SMT) often fails to address syntactic difference between Japanese and English.",
                "Preordering methods are one of the simple but effective approaches that can model reordering in a long distance, which is crucial in translating Japanese and English.",
                "Thus, we apply a predicate-argument structure-based preordering method to the Japanese-English statistical machine translation task of scientific papers.",
                "Our method is based on the method described in (#REF) , and extends their rules to handle abbreviation and passivization frequently found in scientific papers.",
                "Experimental results show that our proposed method improves performance of both ( #TARGET_REF) 's system and our phrase-based SMT baseline without preordering."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Naïve phrase-based statistical machine translation (SMT) often fails to address syntactic difference between Japanese and English. Preordering methods are one of the simple but effective approaches that can model reordering in a long distance, which is crucial in translating Japanese and English. Thus, we apply a predicate-argument structure-based preordering method to the Japanese-English statistical machine translation task of scientific papers. Our method is based on the method described in (#REF) , and extends their rules to handle abbreviation and passivization frequently found in scientific papers. Experimental results show that our proposed method improves performance of both ( #TARGET_REF) 's system and our phrase-based SMT baseline without preordering.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Experimental results show that our proposed method improves performance of both ( #TARGET_REF) 's system and our phrase-based SMT baseline without preordering.\"]}"
    },
    {
        "gold": {
            "text": [
                "We performed minimum error rate training (#REF) optimized for BLEU using the development set (dev.txt) of the ASPEC corpus.",
                "We conducted all the experiments using the scripts distributed at KFTT Moses Baseline v1.4 11 .",
                "Table 1 shows the experimental results.",
                "In terms of BLEU, our re-implementation of ( #TARGET_REF) is below the baseline method while our proposed methods better than the baseline.",
                "In terms of RIBES, all preordering methods outperform the baseline, and our proposed method archieve the highest score."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We performed minimum error rate training (#REF) optimized for BLEU using the development set (dev.txt) of the ASPEC corpus. We conducted all the experiments using the scripts distributed at KFTT Moses Baseline v1.4 11 . Table 1 shows the experimental results. In terms of BLEU, our re-implementation of ( #TARGET_REF) is below the baseline method while our proposed methods better than the baseline. In terms of RIBES, all preordering methods outperform the baseline, and our proposed method archieve the highest score.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"In terms of BLEU, our re-implementation of ( #TARGET_REF) is below the baseline method while our proposed methods better than the baseline.\"]}"
    },
    {
        "gold": {
            "text": [
                "One of the advantages of preordering is that it can incorporate rich linguistic information on the source side, whilst off-the-shelf SMT toolkit can be plugged in without any modification.",
                "Preordering methods employ various kinds of linguistic information to achieve better alignment between source and target languages.",
                "Specifically, previous work in the literature uses morphological analysis (#REF) , dependency structure (#REF) and predicate-argument structure (#REF; #TARGET_REF) for preordering in Japanese-English statistical machine translation.",
                "However, these preordering methods are tested on limited domains: travel (#REF) and patent (#REF; #REF) corpora.",
                "Translating Japanese to English in a different domain such as scientific papers is still a big challenge for preordering-based approach."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "One of the advantages of preordering is that it can incorporate rich linguistic information on the source side, whilst off-the-shelf SMT toolkit can be plugged in without any modification. Preordering methods employ various kinds of linguistic information to achieve better alignment between source and target languages. Specifically, previous work in the literature uses morphological analysis (#REF) , dependency structure (#REF) and predicate-argument structure (#REF; #TARGET_REF) for preordering in Japanese-English statistical machine translation. However, these preordering methods are tested on limited domains: travel (#REF) and patent (#REF; #REF) corpora. Translating Japanese to English in a different domain such as scientific papers is still a big challenge for preordering-based approach.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Specifically, previous work in the literature uses morphological analysis (#REF) , dependency structure (#REF) and predicate-argument structure (#REF; #TARGET_REF) for preordering in Japanese-English statistical machine translation.\"]}"
    },
    {
        "gold": {
            "text": [
                "Unlike (#REF) , they also reverse all words in each phrase.",
                "Third, #TARGET_REF proposed predicate-argument structure-based preordering rules in two-level for the Japanese-English patent translation task.",
                "The first is sentence-level and the second is phrase-level.",
                "Furthermore, sentence-level preordering rules are divided into three parts.",
                "In total, sentences are reordered sequentially by four rules."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Unlike (#REF) , they also reverse all words in each phrase. Third, #TARGET_REF proposed predicate-argument structure-based preordering rules in two-level for the Japanese-English patent translation task. The first is sentence-level and the second is phrase-level. Furthermore, sentence-level preordering rules are divided into three parts. In total, sentences are reordered sequentially by four rules.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Third, #TARGET_REF proposed predicate-argument structure-based preordering rules in two-level for the Japanese-English patent translation task.\"]}"
    },
    {
        "gold": {
            "text": [
                "(explained . . .)\" can be either \". .",
                ". was explained\" or \"It was explained that . . .\".).",
                "#TARGET_REF proposed to move a predicate after the subject (inter-chunk preordering).",
                "However, if a subject is modified by other phrases, this rule moves the predicate to the middle of a subjective phrase composed of multiple phrases.",
                "Thus, we move a predicate to the end of the subjective phrase."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "(explained . . .)\" can be either \". . . was explained\" or \"It was explained that . . .\".). #TARGET_REF proposed to move a predicate after the subject (inter-chunk preordering). However, if a subject is modified by other phrases, this rule moves the predicate to the middle of a subjective phrase composed of multiple phrases. Thus, we move a predicate to the end of the subjective phrase.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF proposed to move a predicate after the subject (inter-chunk preordering).\"]}"
    },
    {
        "gold": {
            "text": [
                "Preordering methods employ various kinds of linguistic information to achieve better alignment between source and target languages.",
                "Specifically, previous work in the literature uses morphological analysis (#REF) , dependency structure (#REF) and predicate-argument structure (#REF; #REF) for preordering in Japanese-English statistical machine translation.",
                "However, these preordering methods are tested on limited domains: travel (#REF) and patent (#REF; #TARGET_REF) corpora.",
                "Translating Japanese to English in a different domain such as scientific papers is still a big challenge for preordering-based approach.",
                "For example, academic writing in English traditionally relies on passive voice to give an objective impression, but one can use either passive construction or a zeropronoun in the Japanese translation of passive construction on the English side."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Preordering methods employ various kinds of linguistic information to achieve better alignment between source and target languages. Specifically, previous work in the literature uses morphological analysis (#REF) , dependency structure (#REF) and predicate-argument structure (#REF; #REF) for preordering in Japanese-English statistical machine translation. However, these preordering methods are tested on limited domains: travel (#REF) and patent (#REF; #TARGET_REF) corpora. Translating Japanese to English in a different domain such as scientific papers is still a big challenge for preordering-based approach. For example, academic writing in English traditionally relies on passive voice to give an objective impression, but one can use either passive construction or a zeropronoun in the Japanese translation of passive construction on the English side.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"However, these preordering methods are tested on limited domains: travel (#REF) and patent (#REF; #TARGET_REF) corpora.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper we explore generalized pooling methods to enhance sentence embedding.",
                "Specifically, by extending scalar self-attention models such as those proposed in #TARGET_REF , we propose vectorbased multi-head attention, which includes the widely used max pooling, mean pooling, and scalar selfattention itself as special cases.",
                "On one hand, the proposed method allows for extracting different aspects of the sentence into multiple vector representations through the multi-head mechanism.",
                "On the other, it allows the models to focus on one of many possible interpretations of the words encoded in the context vector through the vector-based attention mechanism.",
                "In the proposed model we design penalization terms to reduce redundancy in multi-head attention."
            ],
            "label": [
                "SIM",
                "MOT"
            ],
            "context": [
                1,
                1,
                1,
                1,
                0
            ]
        },
        "input": "In this paper we explore generalized pooling methods to enhance sentence embedding. Specifically, by extending scalar self-attention models such as those proposed in #TARGET_REF , we propose vectorbased multi-head attention, which includes the widely used max pooling, mean pooling, and scalar selfattention itself as special cases. On one hand, the proposed method allows for extracting different aspects of the sentence into multiple vector representations through the multi-head mechanism. On the other, it allows the models to focus on one of many possible interpretations of the words encoded in the context vector through the vector-based attention mechanism. In the proposed model we design penalization terms to reduce redundancy in multi-head attention.",
        "output": "{\"label\": [\"SIM\", \"MOT\"], \"context\": [\"In this paper we explore generalized pooling methods to enhance sentence embedding.\", \"Specifically, by extending scalar self-attention models such as those proposed in #TARGET_REF , we propose vectorbased multi-head attention, which includes the widely used max pooling, mean pooling, and scalar selfattention itself as special cases.\", \"On one hand, the proposed method allows for extracting different aspects of the sentence into multiple vector representations through the multi-head mechanism.\", \"On the other, it allows the models to focus on one of many possible interpretations of the words encoded in the context vector through the vector-based attention mechanism.\"]}"
    },
    {
        "gold": {
            "text": [
                "Half of these 10 genres are used in training while the rest are not, resulting in-domain and cross-domain development and test sets used to test NLI systems.",
                "We use the same data split as in #REF , i.e., 392,702 samples for training, 9,815/9,832 samples for in-domain/cross-domain development, and 9,796/9,847 samples for in-domain/cross-domain testing.",
                "Note that, we do not use SNLI as an additional training/development set in our experiments.",
                "Age Dataset To compare our models with that of #TARGET_REF , we use the same Age dataset in our experiment here, which is an Author Profiling dataset.",
                "The dataset are extracted from the Author Profiling dataset 1 , which consists of tweets from English Twitter."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Half of these 10 genres are used in training while the rest are not, resulting in-domain and cross-domain development and test sets used to test NLI systems. We use the same data split as in #REF , i.e., 392,702 samples for training, 9,815/9,832 samples for in-domain/cross-domain development, and 9,796/9,847 samples for in-domain/cross-domain testing. Note that, we do not use SNLI as an additional training/development set in our experiments. Age Dataset To compare our models with that of #TARGET_REF , we use the same Age dataset in our experiment here, which is an Author Profiling dataset. The dataset are extracted from the Author Profiling dataset 1 , which consists of tweets from English Twitter.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Age Dataset To compare our models with that of #TARGET_REF , we use the same Age dataset in our experiment here, which is an Author Profiling dataset.\"]}"
    },
    {
        "gold": {
            "text": [
                "Age Dataset To compare our models with that of #REF , we use the same Age dataset in our experiment here, which is an Author Profiling dataset.",
                "The dataset are extracted from the Author Profiling dataset 1 , which consists of tweets from English Twitter.",
                "The task is to predict the age range of authors of input tweets.",
                "The age range are split into 5 classes: 18-24, 25-34, 35-49, 50-64, 65+.",
                "We use the same data split as in #TARGET_REF , i.e., 68,485 samples for training, 4,000 for development, and 4,000 for testing."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Age Dataset To compare our models with that of #REF , we use the same Age dataset in our experiment here, which is an Author Profiling dataset. The dataset are extracted from the Author Profiling dataset 1 , which consists of tweets from English Twitter. The task is to predict the age range of authors of input tweets. The age range are split into 5 classes: 18-24, 25-34, 35-49, 50-64, 65+. We use the same data split as in #TARGET_REF , i.e., 68,485 samples for training, 4,000 for development, and 4,000 for testing.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use the same data split as in #TARGET_REF , i.e., 68,485 samples for training, 4,000 for development, and 4,000 for testing.\"]}"
    },
    {
        "gold": {
            "text": [
                "The Yelp dataset 2 is a sentiment analysis task, which takes reviews as input and predicts the level of sentiment in terms of the number of stars, from 1 to 5 stars, where 5-star means the most positive.",
                "We use the same data split as in #TARGET_REF , i.e., 500,000 samples for training, 2,000 for development, and 2,000 for testing."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "The Yelp dataset 2 is a sentiment analysis task, which takes reviews as input and predicts the level of sentiment in terms of the number of stars, from 1 to 5 stars, where 5-star means the most positive. We use the same data split as in #TARGET_REF , i.e., 500,000 samples for training, 2,000 for development, and 2,000 for testing.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use the same data split as in #TARGET_REF , i.e., 500,000 samples for training, 2,000 for development, and 2,000 for testing.\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate the proposed approach on three different tasks: natural language inference, author profiling, and sentiment classification.",
                "The experiments show that the proposed model achieves significant improvement over strong sentence-encoding-based methods, resulting in state-of-the-art performances on four datasets.",
                "The proposed approach can be easily implemented for more problems than we discuss in this paper.",
                "Our future work includes exploring more effective MLP to use the structures of multi-head vectors, inspired by the idea from #TARGET_REF .",
                "Leveraging structure information from syntactic and semantic parses is another direction interesting to us."
            ],
            "label": [
                "FUT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We evaluate the proposed approach on three different tasks: natural language inference, author profiling, and sentiment classification. The experiments show that the proposed model achieves significant improvement over strong sentence-encoding-based methods, resulting in state-of-the-art performances on four datasets. The proposed approach can be easily implemented for more problems than we discuss in this paper. Our future work includes exploring more effective MLP to use the structures of multi-head vectors, inspired by the idea from #TARGET_REF . Leveraging structure information from syntactic and semantic parses is another direction interesting to us.",
        "output": "{\"label\": [\"FUT\"], \"context\": [\"Our future work includes exploring more effective MLP to use the structures of multi-head vectors, inspired by the idea from #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The discriminator decoder takes the hidden state at the last time step of a sequence concatenated with both the max-pooled and mean-pooled representation of the hidden states #TARGET_REF and outputs a number in the range [0, 1].",
                "The difficulty of using GANs in text generation comes from the discrete nature of text, making the model non-differentiable hence, we update parameters for the generator model with policy gradients as described in #REF .",
                "We utilize AWD-LSTM [21] and TransformerXL [22] based language models.",
                "For model hyperparameters please to refer to Supplementary Section Table 2 .",
                "We use Adam optimizer [23] with β1 = 0.7 and β2 = 0.8 similar to [20] and use a batch size of 50."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The discriminator decoder takes the hidden state at the last time step of a sequence concatenated with both the max-pooled and mean-pooled representation of the hidden states #TARGET_REF and outputs a number in the range [0, 1]. The difficulty of using GANs in text generation comes from the discrete nature of text, making the model non-differentiable hence, we update parameters for the generator model with policy gradients as described in #REF . We utilize AWD-LSTM [21] and TransformerXL [22] based language models. For model hyperparameters please to refer to Supplementary Section Table 2 . We use Adam optimizer [23] with β1 = 0.7 and β2 = 0.8 similar to [20] and use a batch size of 50.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The discriminator decoder takes the hidden state at the last time step of a sequence concatenated with both the max-pooled and mean-pooled representation of the hidden states #TARGET_REF and outputs a number in the range [0, 1].\"]}"
    },
    {
        "gold": {
            "text": [
                "The mix of linguistic styles within this corpus offers the potential for interesting variation during the generation phase.",
                "We use the same pre-processing as in earlier work #TARGET_REF 24] .",
                "We reserve 10% of our data for test set and another 10% for our validation set.",
                "We first pre-train our generator on the Gutenberg dataset [25] for 20 epochs and then fine-tune [20] them to our target datasets with a language modeling objective.",
                "The discriminator's encoder is initialized to the same weights as our fine-tuned language model."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The mix of linguistic styles within this corpus offers the potential for interesting variation during the generation phase. We use the same pre-processing as in earlier work #TARGET_REF 24] . We reserve 10% of our data for test set and another 10% for our validation set. We first pre-train our generator on the Gutenberg dataset [25] for 20 epochs and then fine-tune [20] them to our target datasets with a language modeling objective. The discriminator's encoder is initialized to the same weights as our fine-tuned language model.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use the same pre-processing as in earlier work #TARGET_REF 24] .\"]}"
    },
    {
        "gold": {
            "text": [
                "The mix of linguistic styles within this corpus offers the potential for interesting variation during the generation phase.",
                "We use the same pre-processing as in earlier work [20, 24] .",
                "We reserve 10% of our data for test set and another 10% for our validation set.",
                "We first pre-train our generator on the Gutenberg dataset [25] for 20 epochs and then fine-tune #TARGET_REF them to our target datasets with a language modeling objective.",
                "The discriminator's encoder is initialized to the same weights as our fine-tuned language model."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The mix of linguistic styles within this corpus offers the potential for interesting variation during the generation phase. We use the same pre-processing as in earlier work [20, 24] . We reserve 10% of our data for test set and another 10% for our validation set. We first pre-train our generator on the Gutenberg dataset [25] for 20 epochs and then fine-tune #TARGET_REF them to our target datasets with a language modeling objective. The discriminator's encoder is initialized to the same weights as our fine-tuned language model.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We first pre-train our generator on the Gutenberg dataset [25] for 20 epochs and then fine-tune #TARGET_REF them to our target datasets with a language modeling objective.\"]}"
    },
    {
        "gold": {
            "text": [
                "For model hyperparameters please to refer to Supplementary Section Table 2 .",
                "We use Adam optimizer [23] with β1 = 0.7 and β2 = 0.8 similar to #TARGET_REF and use a batch size of 50.",
                "Other practices for LM training were the same as [22] and [21] for Transformer-XL and AWD-LSTM respectively.",
                "We refer to our proposed GAN as Creative-GAN and compare it to a baseline (a language model equivalent to our pre-trained generator) and a GumbelGAN model [15] across all proposed datasets.",
                "We use three creative English datasets with distinct linguistic characteristics: (1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website 1 and (3) a corpus of 1500 song lyrics ranging across genres."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "For model hyperparameters please to refer to Supplementary Section Table 2 . We use Adam optimizer [23] with β1 = 0.7 and β2 = 0.8 similar to #TARGET_REF and use a batch size of 50. Other practices for LM training were the same as [22] and [21] for Transformer-XL and AWD-LSTM respectively. We refer to our proposed GAN as Creative-GAN and compare it to a baseline (a language model equivalent to our pre-trained generator) and a GumbelGAN model [15] across all proposed datasets. We use three creative English datasets with distinct linguistic characteristics: (1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website 1 and (3) a corpus of 1500 song lyrics ranging across genres.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We use Adam optimizer [23] with \\u03b21 = 0.7 and \\u03b22 = 0.8 similar to #TARGET_REF and use a batch size of 50.\"]}"
    },
    {
        "gold": {
            "text": [
                "Variational Autoencoder (VAE) is a powerful method for learning representations of highdimensional data.",
                "However, VAEs can suffer from an issue known as latent variable collapse (or KL loss vanishing), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.",
                "Such an issue is particularly prevalent when employing VAE-RNN architectures for text modelling #TARGET_REF .",
                "In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse.",
                "Compared to existing VAE-RNN architectures, we show that our model can achieve much more stable training process and can generate text with significantly better quality."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Variational Autoencoder (VAE) is a powerful method for learning representations of highdimensional data. However, VAEs can suffer from an issue known as latent variable collapse (or KL loss vanishing), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks. Such an issue is particularly prevalent when employing VAE-RNN architectures for text modelling #TARGET_REF . In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse. Compared to existing VAE-RNN architectures, we show that our model can achieve much more stable training process and can generate text with significantly better quality.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Such an issue is particularly prevalent when employing VAE-RNN architectures for text modelling #TARGET_REF .\", \"In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse.\"]}"
    },
    {
        "gold": {
            "text": [
                "With vMF, the KL loss only depends on the concentration parameter which is fixed during training and testing, and hence results in a constant KL loss.",
                "In a more recent work, #REF avoided latent variable collapse by including skip connections in the generative model, where the skip connections enforce strong links between the latent variables and the likelihood function.",
                "Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss #TARGET_REF; , or resort to designing more sophisticated model structures (#REF; #REF; #REF) .",
                "In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse.",
                "In contrast to existing VAE-RNN models for text modelling which merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation for all hidden states of the RNN encoder."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "With vMF, the KL loss only depends on the concentration parameter which is fixed during training and testing, and hence results in a constant KL loss. In a more recent work, #REF avoided latent variable collapse by including skip connections in the generative model, where the skip connections enforce strong links between the latent variables and the likelihood function. Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss #TARGET_REF; , or resort to designing more sophisticated model structures (#REF; #REF; #REF) . In this paper, we present a simple architecture called holistic regularisation VAE (HR-VAE), which can effectively avoid latent variable collapse. In contrast to existing VAE-RNN models for text modelling which merely impose a standard normal distribution prior on the last hidden state of the RNN encoder, our HR-VAE model imposes regularisation for all hidden states of the RNN encoder.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Although the aforementioned works show effectiveness in addressing the latent variable collapse issue to some extent, they either require carefully engineering to balance the weight between the reconstruction loss and KL loss #TARGET_REF; , or resort to designing more sophisticated model structures (#REF; #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section, we discuss the technical details of the proposed holistic regularisation VAE (HR-VAE) model, a general architecture which can effectively mitigate the KL vanishing phenomenon.",
                "Our model design is motivated by one noticeable defect shared by the VAE-RNN based models in previous works #TARGET_REF; #REF; #REF; #REF) .",
                "That is, all these models, as shown in Figure 1a , only impose a standard normal distribution prior on the last hidden state of the RNN encoder, which potentially leads to learning a suboptimal representation of the latent variable and results in model vulnerable to KL loss vanishing.",
                "Our hypothesis is that to learn a good representation of data and a good generative model, it is crucial to impose the standard normal prior on all the hidden states of the RNN-based encoder (see Figure 1b ), which allows a better regularisation of the model learning process.",
                "We implement the HR-VAE model using a twolayer LSTM for both the encoder and decoder."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In this section, we discuss the technical details of the proposed holistic regularisation VAE (HR-VAE) model, a general architecture which can effectively mitigate the KL vanishing phenomenon. Our model design is motivated by one noticeable defect shared by the VAE-RNN based models in previous works #TARGET_REF; #REF; #REF; #REF) . That is, all these models, as shown in Figure 1a , only impose a standard normal distribution prior on the last hidden state of the RNN encoder, which potentially leads to learning a suboptimal representation of the latent variable and results in model vulnerable to KL loss vanishing. Our hypothesis is that to learn a good representation of data and a good generative model, it is crucial to impose the standard normal prior on all the hidden states of the RNN-based encoder (see Figure 1b ), which allows a better regularisation of the model learning process. We implement the HR-VAE model using a twolayer LSTM for both the encoder and decoder.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Our model design is motivated by one noticeable defect shared by the VAE-RNN based models in previous works #TARGET_REF; #REF; #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Variational Autoencoder (VAE) (#REF ) is a powerful method for learning representations of high-dimensional data.",
                "However, recent attempts of applying VAEs to text modelling are still far less successful compared to its application to image and speech (#REF; #REF; #REF) .",
                "When applying VAEs for text modelling, recurrent neural networks (RNNs) 1 are commonly used as the architecture for both encoder and decoder #TARGET_REF; #REF; #REF) .",
                "While such a VAE-RNN based architecture allows encoding and generating sentences (in the decoding phase) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse (or KL loss vanishing) , where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.",
                "Various efforts have been made to alleviate the latent variable collapse issue."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Variational Autoencoder (VAE) (#REF ) is a powerful method for learning representations of high-dimensional data. However, recent attempts of applying VAEs to text modelling are still far less successful compared to its application to image and speech (#REF; #REF; #REF) . When applying VAEs for text modelling, recurrent neural networks (RNNs) 1 are commonly used as the architecture for both encoder and decoder #TARGET_REF; #REF; #REF) . While such a VAE-RNN based architecture allows encoding and generating sentences (in the decoding phase) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse (or KL loss vanishing) , where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks. Various efforts have been made to alleviate the latent variable collapse issue.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"When applying VAEs for text modelling, recurrent neural networks (RNNs) 1 are commonly used as the architecture for both encoder and decoder #TARGET_REF; #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "When applying VAEs for text modelling, recurrent neural networks (RNNs) 1 are commonly used as the architecture for both encoder and decoder (#REF; #REF; #REF) .",
                "While such a VAE-RNN based architecture allows encoding and generating sentences (in the decoding phase) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse (or KL loss vanishing) , where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.",
                "Various efforts have been made to alleviate the latent variable collapse issue.",
                "#TARGET_REF uses KL annealing, where a variable weight is added to the KL term in the cost function at training time.",
                "#REF discovered that there is a trade-off between the contextual capacity of the decoder and effective use of encoding information, and developed a dilated CNN as decoder which can vary the amount of conditioning context."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "When applying VAEs for text modelling, recurrent neural networks (RNNs) 1 are commonly used as the architecture for both encoder and decoder (#REF; #REF; #REF) . While such a VAE-RNN based architecture allows encoding and generating sentences (in the decoding phase) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse (or KL loss vanishing) , where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks. Various efforts have been made to alleviate the latent variable collapse issue. #TARGET_REF uses KL annealing, where a variable weight is added to the KL term in the cost function at training time. #REF discovered that there is a trade-off between the contextual capacity of the decoder and effective use of encoding information, and developed a dilated CNN as decoder which can vary the amount of conditioning context.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF uses KL annealing, where a variable weight is added to the KL term in the cost function at training time.\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate our model on two public datasets, namely, Penn Treebank (PTB) (#REF) and the end-to-end (E2E) text generation corpus (#REF) , which have been used in a number of previous works for text generation #TARGET_REF; #REF; #REF; #REF) .",
                "PTB consists of more than 40,000 sentences from Wall Street Journal articles whereas the E2E dataset contains over 50,000 sen-tences of restaurant reviews.",
                "The statistics of these two datasets are summarised in Table 1 ."
            ],
            "label": [
                "BACK",
                "USE"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "We evaluate our model on two public datasets, namely, Penn Treebank (PTB) (#REF) and the end-to-end (E2E) text generation corpus (#REF) , which have been used in a number of previous works for text generation #TARGET_REF; #REF; #REF; #REF) . PTB consists of more than 40,000 sentences from Wall Street Journal articles whereas the E2E dataset contains over 50,000 sen-tences of restaurant reviews. The statistics of these two datasets are summarised in Table 1 .",
        "output": "{\"label\": [\"BACK\", \"USE\"], \"context\": [\"We evaluate our model on two public datasets, namely, Penn Treebank (PTB) (#REF) and the end-to-end (E2E) text generation corpus (#REF) , which have been used in a number of previous works for text generation #TARGET_REF; #REF; #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures.",
                "We evaluate our model against several strong baselines which apply VAE for text modelling #TARGET_REF; #REF; #REF) .",
                "We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset (#REF) and the end-to-end (E2E) text generation dataset (#REF) .",
                "Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation.",
                "The code for our model is available online 2 ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "Another advantage of our model is that it is generic and can be applied to any existing VAE-RNN-based architectures. We evaluate our model against several strong baselines which apply VAE for text modelling #TARGET_REF; #REF; #REF) . We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset (#REF) and the end-to-end (E2E) text generation dataset (#REF) . Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation. The code for our model is available online 2 .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We evaluate our model against several strong baselines which apply VAE for text modelling #TARGET_REF; #REF; #REF) .\", \"We conducted experiments based on two public benchmark datasets, namely, the Penn Treebank dataset (#REF) and the end-to-end (E2E) text generation dataset (#REF) .\", \"Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation.\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally, the KL divergence between these two multivariate Gaussian distributions (i.e., Q φt and P (z t )) will contribute to the overall KL loss of the ELBO.",
                "By taking the average of the KL loss at each time stamp t, the resulting ELBO takes the following form",
                "KL(Q φt (z t |x) P (z t )).",
                "( 3) As can be seen in Eq. 3, our solution to the KL collapse issue does not require any engineering for balancing the weight between the reconstruction term and KL loss as commonly the case in existing works #TARGET_REF; .",
                "The weight between these two terms of our model is simply 1 : 1."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Finally, the KL divergence between these two multivariate Gaussian distributions (i.e., Q φt and P (z t )) will contribute to the overall KL loss of the ELBO. By taking the average of the KL loss at each time stamp t, the resulting ELBO takes the following form KL(Q φt (z t |x) P (z t )). ( 3) As can be seen in Eq. 3, our solution to the KL collapse issue does not require any engineering for balancing the weight between the reconstruction term and KL loss as commonly the case in existing works #TARGET_REF; . The weight between these two terms of our model is simply 1 : 1.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"( 3) As can be seen in Eq. 3, our solution to the KL collapse issue does not require any engineering for balancing the weight between the reconstruction term and KL loss as commonly the case in existing works #TARGET_REF; .\"]}"
    },
    {
        "gold": {
            "text": [
                "For the PTB dataset, we used the train-test split following #TARGET_REF; #REF) .",
                "For the E2E dataset, we used the train-test split from the original dataset (#REF) and indexed the words with a frequency higher than 3.",
                "We represent input data with 512-dimensional word2vec embeddings (#REF) .",
                "We set the dimension of the hidden layers of both encoder and decoder to 256.",
                "The Adam optimiser (#REF) was used for training with an initial learning rate of 0.0001."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "For the PTB dataset, we used the train-test split following #TARGET_REF; #REF) . For the E2E dataset, we used the train-test split from the original dataset (#REF) and indexed the words with a frequency higher than 3. We represent input data with 512-dimensional word2vec embeddings (#REF) . We set the dimension of the hidden layers of both encoder and decoder to 256. The Adam optimiser (#REF) was used for training with an initial learning rate of 0.0001.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For the PTB dataset, we used the train-test split following #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base 3 : A variational autoencoder model which uses LSTM for both encoder and decoder.",
                "KL annealing is used to tackled the latent variable collapse issue #TARGET_REF ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder (#REF) ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (#REF) .",
                "the decoder needs to predict the entire sequence with only the help of the given latent variable z.",
                "In this way, a high-quality representation abstracting the information of the input sentence is much needed for the decoder, and hence enforcing z to learn the required information.",
                "Overall performance."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We compare our HR-VAE model with three strong baselines using VAE for text modelling: VAE-LSTM-base 3 : A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue #TARGET_REF ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder (#REF) ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (#REF) . the decoder needs to predict the entire sequence with only the help of the given latent variable z. In this way, a high-quality representation abstracting the information of the input sentence is much needed for the decoder, and hence enforcing z to learn the required information. Overall performance.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"KL annealing is used to tackled the latent variable collapse issue #TARGET_REF ; VAE-CNN 4 : A variational autoencoder model with a LSTM encoder and a dilated CNN decoder (#REF) ; vMF-VAE 5 : A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Loss analysis.",
                "To conduct a more thorough evaluation, we further investigate model behaviours in terms of both reconstruction loss and KL loss, as shown in Figure 2 .",
                "These plots were obtained based on the E2E training set using the inputless setting.",
                "We can see that the KL loss of VAE-LSTMbase, which uses Sigmoid annealing #TARGET_REF , collapses to zero, leading to a poor generative performance as indicated by the high reconstruction loss.",
                "The KL loss for both VAE-CNN and vMF-VAE are nonzero, where the former mitigates the KL collapse issue with a KL loss clipping strategy and the latter by replacing the standard normal distribution for the prior with the vMF distribution (i.e., with the vMF distribution, the KL loss only depends on a fixed concentration parameter, and hence results in a constant KL loss)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Loss analysis. To conduct a more thorough evaluation, we further investigate model behaviours in terms of both reconstruction loss and KL loss, as shown in Figure 2 . These plots were obtained based on the E2E training set using the inputless setting. We can see that the KL loss of VAE-LSTMbase, which uses Sigmoid annealing #TARGET_REF , collapses to zero, leading to a poor generative performance as indicated by the high reconstruction loss. The KL loss for both VAE-CNN and vMF-VAE are nonzero, where the former mitigates the KL collapse issue with a KL loss clipping strategy and the latter by replacing the standard normal distribution for the prior with the vMF distribution (i.e., with the vMF distribution, the KL loss only depends on a fixed concentration parameter, and hence results in a constant KL loss).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We can see that the KL loss of VAE-LSTMbase, which uses Sigmoid annealing #TARGET_REF , collapses to zero, leading to a poor generative performance as indicated by the high reconstruction loss.\"]}"
    },
    {
        "gold": {
            "text": [
                "In natural language processing, the deep learning revolution has shifted the focus from conventional hand-crafted symbolic representations to dense inputs, which are adequate representations learned automatically from corpora.",
                "However, particularly when working with low-resource languages, small amounts of symbolic lexical resources such as user-generated lexicons are often available even when gold-standard corpora are not.",
                "Recent work has shown benefits of combining conventional lexical information into neural cross-lingual part-ofspeech (PoS) tagging #TARGET_REF .",
                "However, little is known on how complementary such additional information is, and to what extent improvements depend on the coverage and quality of these external resources.",
                "The contribution of this paper is in the analysis of the contributions of models' components (tagger transfer through annotation projection vs. the contribution of encoding lexical and morphosyntactic resources)."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "In natural language processing, the deep learning revolution has shifted the focus from conventional hand-crafted symbolic representations to dense inputs, which are adequate representations learned automatically from corpora. However, particularly when working with low-resource languages, small amounts of symbolic lexical resources such as user-generated lexicons are often available even when gold-standard corpora are not. Recent work has shown benefits of combining conventional lexical information into neural cross-lingual part-ofspeech (PoS) tagging #TARGET_REF . However, little is known on how complementary such additional information is, and to what extent improvements depend on the coverage and quality of these external resources. The contribution of this paper is in the analysis of the contributions of models' components (tagger transfer through annotation projection vs. the contribution of encoding lexical and morphosyntactic resources).",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Recent work has shown benefits of combining conventional lexical information into neural cross-lingual part-ofspeech (PoS) tagging #TARGET_REF .\", \"However, little is known on how complementary such additional information is, and to what extent improvements depend on the coverage and quality of these external resources.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, we use identical settings for each language which worked well and is less expensive, following #REF .",
                "For all experiments, we average over 3 randomly seeded runs, and provide mean accuracy.",
                "We use the off-the-shelf Polyglot word embeddings (#REF) .",
                "Word embedding initialization provides a consistent and considerable boost in this cross-lingual setup, up to 10% absolute improvements across 21 languages when only 500 projected training instances are available #TARGET_REF .",
                "Note that we em- pirically find it to be best to not update the word embeddings in this noisy training setup, as that results in better performance, see Section 4.4."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "However, we use identical settings for each language which worked well and is less expensive, following #REF . For all experiments, we average over 3 randomly seeded runs, and provide mean accuracy. We use the off-the-shelf Polyglot word embeddings (#REF) . Word embedding initialization provides a consistent and considerable boost in this cross-lingual setup, up to 10% absolute improvements across 21 languages when only 500 projected training instances are available #TARGET_REF . Note that we em- pirically find it to be best to not update the word embeddings in this noisy training setup, as that results in better performance, see Section 4.4.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"We use the off-the-shelf Polyglot word embeddings (#REF) .\", \"Word embedding initialization provides a consistent and considerable boost in this cross-lingual setup, up to 10% absolute improvements across 21 languages when only 500 projected training instances are available #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively).",
                "1 The sizes of the dictionaries vary considerably, from a few thousand entries (e.g., for Hindi and Bulgarian) to 2M entries (Finnish Uni-Morph).",
                "We study the impact of smaller dictionary sizes in Section 4.1.",
                "The tagger we analyze in this paper is an extension of the base tagger, called distant supervision from disparate sources (DSDS) tagger #TARGET_REF .",
                "It is trained on projected data and further differs from the base tagger by the integration of lexicon information."
            ],
            "label": [
                "USE",
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively). 1 The sizes of the dictionaries vary considerably, from a few thousand entries (e.g., for Hindi and Bulgarian) to 2M entries (Finnish Uni-Morph). We study the impact of smaller dictionary sizes in Section 4.1. The tagger we analyze in this paper is an extension of the base tagger, called distant supervision from disparate sources (DSDS) tagger #TARGET_REF . It is trained on projected data and further differs from the base tagger by the integration of lexicon information.",
        "output": "{\"label\": [\"USE\", \"EXT\"], \"context\": [\"The tagger we analyze in this paper is an extension of the base tagger, called distant supervision from disparate sources (DSDS) tagger #TARGET_REF .\", \"It is trained on projected data and further differs from the base tagger by the integration of lexicon information.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section we describe the baselines, the data and the tagger hyperparameters.",
                "Data We use the 12 Universal PoS tags (#REF) .",
                "The set of languages is motivated by accessibility to embeddings and dictionaries.",
                "We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by #TARGET_REF showing that DSDS provides a viable alternative.",
                "Annotation projection To build the taggers for new languages, we resort to annotation projection following Plank and Agić (2018) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "In this section we describe the baselines, the data and the tagger hyperparameters. Data We use the 12 Universal PoS tags (#REF) . The set of languages is motivated by accessibility to embeddings and dictionaries. We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by #TARGET_REF showing that DSDS provides a viable alternative. Annotation projection To build the taggers for new languages, we resort to annotation projection following Plank and Agić (2018) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by #TARGET_REF showing that DSDS provides a viable alternative.\"]}"
    },
    {
        "gold": {
            "text": [
                "We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by Plank and Agić (2018) showing that DSDS provides a viable alternative.",
                "Annotation projection To build the taggers for new languages, we resort to annotation projection following #TARGET_REF .",
                "In particular, they employ the approach by Agić et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences.",
                "The wide-coverage Watchtower corpus (WTC) by Agić et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following Plank and Agić (2018) .",
                "Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (#REF) and retrofitting initialization."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We here focus on 21 dev sets of the Universal Dependencies 2.1 (Nivre and et al., 2017) , test set results are reported by Plank and Agić (2018) showing that DSDS provides a viable alternative. Annotation projection To build the taggers for new languages, we resort to annotation projection following #TARGET_REF . In particular, they employ the approach by Agić et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences. The wide-coverage Watchtower corpus (WTC) by Agić et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following Plank and Agić (2018) . Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (#REF) and retrofitting initialization.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Annotation projection To build the taggers for new languages, we resort to annotation projection following #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Annotation projection To build the taggers for new languages, we resort to annotation projection following Plank and Agić (2018) .",
                "In particular, they employ the approach by Agić et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences.",
                "The wide-coverage Watchtower corpus (WTC) by Agić et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following #TARGET_REF .",
                "Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (#REF) and retrofitting initialization.",
                "Hyperparameters We use the same setup as Plank and Agić (2018) , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Annotation projection To build the taggers for new languages, we resort to annotation projection following Plank and Agić (2018) . In particular, they employ the approach by Agić et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences. The wide-coverage Watchtower corpus (WTC) by Agić et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following #TARGET_REF . Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (#REF) and retrofitting initialization. Hyperparameters We use the same setup as Plank and Agić (2018) , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The wide-coverage Watchtower corpus (WTC) by Agi\\u0107 et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Annotation projection To build the taggers for new languages, we resort to annotation projection following Plank and Agić (2018) .",
                "In particular, they employ the approach by Agić et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences.",
                "The wide-coverage Watchtower corpus (WTC) by Agić et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following Plank and Agić (2018) .",
                "Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (#REF) and retrofitting initialization.",
                "Hyperparameters We use the same setup as #TARGET_REF , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Annotation projection To build the taggers for new languages, we resort to annotation projection following Plank and Agić (2018) . In particular, they employ the approach by Agić et al. (2016) , where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences. The wide-coverage Watchtower corpus (WTC) by Agić et al. (2016) is used, where 5k instances are selected via data selection by alignment coverage following Plank and Agić (2018) . Baselines We compare to the following alternatives: type-constraint Wiktionary supervision (#REF) and retrofitting initialization. Hyperparameters We use the same setup as #TARGET_REF , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Hyperparameters We use the same setup as #TARGET_REF , i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions.\"]}"
    },
    {
        "gold": {
            "text": [
                "The lexicons we use so far are of different sizes (shown in Table 1 of #TARGET_REF ), spanning from 1,000 entries to considerable dictionaries of several hundred thousands entries.",
                "In a low-resource setup, large dictionaries might not be available.",
                "It is thus interesting to examine how tagging accuracy is affected by dictionary size.",
                "We examine two cases: randomly sampling dictionary entries and sampling by word frequency, over increasing dictionary sizes: 50, 100, 200, 400, 800, 1600 word types.",
                "The latter is motivated by the fact that an informed dictionary creation (under limited resources) might be more beneficial."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The lexicons we use so far are of different sizes (shown in Table 1 of #TARGET_REF ), spanning from 1,000 entries to considerable dictionaries of several hundred thousands entries. In a low-resource setup, large dictionaries might not be available. It is thus interesting to examine how tagging accuracy is affected by dictionary size. We examine two cases: randomly sampling dictionary entries and sampling by word frequency, over increasing dictionary sizes: 50, 100, 200, 400, 800, 1600 word types. The latter is motivated by the fact that an informed dictionary creation (under limited resources) might be more beneficial.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The lexicons we use so far are of different sizes (shown in Table 1 of #TARGET_REF ), spanning from 1,000 entries to considerable dictionaries of several hundred thousands entries.\"]}"
    },
    {
        "gold": {
            "text": [
                "The majority of recent work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are obsolete for deep learning methods.",
                "They rely on end-to-end training without resorting to additional linguistic resources.",
                "Our study contributes to the increasing literature to show the utility of linguistic resources for deep learning models by providing a deep analysis of a recently proposed model #TARGET_REF .",
                "Most prior work in this direction can be found on machine translation (#REF; #REF; #REF; #REF) , work on named entity recognition (#REF) and PoS tagging (Sagot and Martínez #REF) who use lexicons, but as n-hot features and without examining the crosslingual aspect.",
                "Somewhat complementary to evaluating the utility of linguistic resources empirically is the increasing body of work that uses linguistic insights to try to understand what properties neural-based representations capture (Kádár et al., 2017; #REF; #REF; #REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The majority of recent work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are obsolete for deep learning methods. They rely on end-to-end training without resorting to additional linguistic resources. Our study contributes to the increasing literature to show the utility of linguistic resources for deep learning models by providing a deep analysis of a recently proposed model #TARGET_REF . Most prior work in this direction can be found on machine translation (#REF; #REF; #REF; #REF) , work on named entity recognition (#REF) and PoS tagging (Sagot and Martínez #REF) who use lexicons, but as n-hot features and without examining the crosslingual aspect. Somewhat complementary to evaluating the utility of linguistic resources empirically is the increasing body of work that uses linguistic insights to try to understand what properties neural-based representations capture (Kádár et al., 2017; #REF; #REF; #REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Our study contributes to the increasing literature to show the utility of linguistic resources for deep learning models by providing a deep analysis of a recently proposed model #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We analyze DSDS, a recently-proposed lowresource tagger that symbiotically leverages neural representations and symbolic linguistic knowledge by integrating them in a soft manner.",
                "We replicated the results of #TARGET_REF , showing that the more implicit use of embedding user-generated dictionaries turns out to be more beneficial than approaches that rely more explicitly on symbolic knowledge, such a type constraints or retrofitting.",
                "By analyzing the reliance of DSDS on the linguistic knowledge, we found that the composition of the lexicon is more important than its size.",
                "Moreover, the tagger benefits from small dictionaries, as long as they do not contain tag set information contradictory to the evaluation data.",
                "Our quantitative analysis also sheds light on the internal representations, showing that they get more sensitive to the task."
            ],
            "label": [
                "USE",
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We analyze DSDS, a recently-proposed lowresource tagger that symbiotically leverages neural representations and symbolic linguistic knowledge by integrating them in a soft manner. We replicated the results of #TARGET_REF , showing that the more implicit use of embedding user-generated dictionaries turns out to be more beneficial than approaches that rely more explicitly on symbolic knowledge, such a type constraints or retrofitting. By analyzing the reliance of DSDS on the linguistic knowledge, we found that the composition of the lexicon is more important than its size. Moreover, the tagger benefits from small dictionaries, as long as they do not contain tag set information contradictory to the evaluation data. Our quantitative analysis also sheds light on the internal representations, showing that they get more sensitive to the task.",
        "output": "{\"label\": [\"USE\", \"EXT\"], \"context\": [\"We replicated the results of #TARGET_REF , showing that the more implicit use of embedding user-generated dictionaries turns out to be more beneficial than approaches that rely more explicitly on symbolic knowledge, such a type constraints or retrofitting.\"]}"
    },
    {
        "gold": {
            "text": [
                "Combining the best of two worlds results in the overall best tagging accuracy, confirming #TARGET_REF : Embedding lexical information into a neural tagger improves tagging accuracy from 83.4 to 84.1 (means over 21 languages).",
                "On 15 out of 21 languages, DSDS is the best performing model.",
                "On two languages, type constraints work the best (English and Greek).",
                "Retrofitting performs best only on one language (Persian); this is the language with the overall lowest performance.",
                "On three languages, Czech, French and Hungarian, the baseline remains the best model, none of the lexicon-enriching approaches works."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Combining the best of two worlds results in the overall best tagging accuracy, confirming #TARGET_REF : Embedding lexical information into a neural tagger improves tagging accuracy from 83.4 to 84.1 (means over 21 languages). On 15 out of 21 languages, DSDS is the best performing model. On two languages, type constraints work the best (English and Greek). Retrofitting performs best only on one language (Persian); this is the language with the overall lowest performance. On three languages, Czech, French and Hungarian, the baseline remains the best model, none of the lexicon-enriching approaches works.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Combining the best of two worlds results in the overall best tagging accuracy, confirming #TARGET_REF : Embedding lexical information into a neural tagger improves tagging accuracy from 83.4 to 84.1 (means over 21 languages).\"]}"
    },
    {
        "gold": {
            "text": [
                "However, since we found out that NPs mostly form trees rather than graphs in the AMR Bank, we can assume that AMR substructures corresponding to NPs are trees.",
                "Thus, we define our task as predicting the AMR tree structure, given a sequence of words in an NP.",
                "The previous method for AMR parsing takes a Train Dev #REF 463 398 Table 1 : Statistics of the extracted NP data two-step approach: first identifying distinct concepts (nodes) in the AMR graph, then defining the dependency relations between those concepts #TARGET_REF .",
                "In the concept identification step, unlike POS tagging, one word is sometimes assigned with more than one concept, and the number of possible concepts is far more than the number of possible parts-of-speech.",
                "As the concept identification accuracy remains low, such a pipeline method suffers from error propagation, thus resulting in a suboptimal AMR parsing performance."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "However, since we found out that NPs mostly form trees rather than graphs in the AMR Bank, we can assume that AMR substructures corresponding to NPs are trees. Thus, we define our task as predicting the AMR tree structure, given a sequence of words in an NP. The previous method for AMR parsing takes a Train Dev #REF 463 398 Table 1 : Statistics of the extracted NP data two-step approach: first identifying distinct concepts (nodes) in the AMR graph, then defining the dependency relations between those concepts #TARGET_REF . In the concept identification step, unlike POS tagging, one word is sometimes assigned with more than one concept, and the number of possible concepts is far more than the number of possible parts-of-speech. As the concept identification accuracy remains low, such a pipeline method suffers from error propagation, thus resulting in a suboptimal AMR parsing performance.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The previous method for AMR parsing takes a Train Dev #REF 463 398 Table 1 : Statistics of the extracted NP data two-step approach: first identifying distinct concepts (nodes) in the AMR graph, then defining the dependency relations between those concepts #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We extract substructures (subtrees) corresponding to NPs from the AMR Bank (LDC2014T12).",
                "In the AMR Bank, there is no alignment between the words and the concepts (nodes) in the AMR graphs.",
                "We obtain this alignment by using the rule-based alignment tool by #TARGET_REF .",
                "Then, we use the Stanford Parser (#REF) to obtain constituency trees, and extract NPs that contain more than one noun and are not included by another NP.",
                "We exclude NPs that contain named entities, because they would require various kinds of manually crafted rules for each type of named entity."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We extract substructures (subtrees) corresponding to NPs from the AMR Bank (LDC2014T12). In the AMR Bank, there is no alignment between the words and the concepts (nodes) in the AMR graphs. We obtain this alignment by using the rule-based alignment tool by #TARGET_REF . Then, we use the Stanford Parser (#REF) to obtain constituency trees, and extract NPs that contain more than one noun and are not included by another NP. We exclude NPs that contain named entities, because they would require various kinds of manually crafted rules for each type of named entity.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We obtain this alignment by using the rule-based alignment tool by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We adopt the method proposed by #TARGET_REF as our baseline, which is a two-step pipeline method of concept identification step and #TARGET_REF for a retired plant worker.",
                "∅ denotes an empty concept.",
                "relation identification step.",
                "Their method is designed for parsing sentences into AMR, but here, we use this method for parsing NPs.",
                "In their method, concept identification is formulated as a sequence labeling problem (#REF) and solved by the Viterbi algorithm."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We adopt the method proposed by #TARGET_REF as our baseline, which is a two-step pipeline method of concept identification step and #TARGET_REF for a retired plant worker. ∅ denotes an empty concept. relation identification step. Their method is designed for parsing sentences into AMR, but here, we use this method for parsing NPs. In their method, concept identification is formulated as a sequence labeling problem (#REF) and solved by the Viterbi algorithm.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We adopt the method proposed by #TARGET_REF as our baseline, which is a two-step pipeline method of concept identification step and #TARGET_REF for a retired plant worker.\"]}"
    },
    {
        "gold": {
            "text": [
                "We conduct an experiment using our NP data set (Table 1) .",
                "We use the implementation 2 of #TARGET_REF as our baseline.",
                "For the baseline, we use the features of the default settings.",
                "The method by #REF can only generate the concepts that appear in the training data.",
                "On the other hand, our method can generate concepts that do not appear in the training data using the concept generation rules LEMMA, DICT PRED , and DICT NOUN in Table 3 ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We conduct an experiment using our NP data set (Table 1) . We use the implementation 2 of #TARGET_REF as our baseline. For the baseline, we use the features of the default settings. The method by #REF can only generate the concepts that appear in the training data. On the other hand, our method can generate concepts that do not appear in the training data using the concept generation rules LEMMA, DICT PRED , and DICT NOUN in Table 3 .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use the implementation 2 of #TARGET_REF as our baseline.\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the implementation 2 of (#REF) as our baseline.",
                "For the baseline, we use the features of the default settings.",
                "The method by #TARGET_REF can only generate the concepts that appear in the training data.",
                "On the other hand, our method can generate concepts that do not appear in the training data using the concept generation rules LEMMA, DICT PRED , and DICT NOUN in Table 3 .",
                "For a fair comparison, first, we only use the rules EMPTY and KNOWN."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "We use the implementation 2 of (#REF) as our baseline. For the baseline, we use the features of the default settings. The method by #TARGET_REF can only generate the concepts that appear in the training data. On the other hand, our method can generate concepts that do not appear in the training data using the concept generation rules LEMMA, DICT PRED , and DICT NOUN in Table 3 . For a fair comparison, first, we only use the rules EMPTY and KNOWN.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The method by #TARGET_REF can only generate the concepts that appear in the training data.\", \"On the other hand, our method can generate concepts that do not appear in the training data using the concept generation rules LEMMA, DICT PRED , and DICT NOUN in Table 3 .\"]}"
    },
    {
        "gold": {
            "text": [
                "Domain adaptation is very important in the field of natural language processing (NLP) as it can reduce the expensive manual annotation effort in the target domain.",
                "Various NLP tasks have benefited from domain adaptation techniques, including part-ofspeech tagging (#REF; #REFa) , chunking (Daumé III, 2007; #TARGET_REF , named entity recognition (#REF; #REF) , dependency parsing (#REF; #REF) and semantic role labeling (#REF; #REFb) .",
                "In a typical domain adaptation scenario of NLP, the source and target domains contain text data of different genres (e.g., newswire vs biomedical (#REF) ).",
                "Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabularies and produce cross-domain feature distribution divergence and feature sparsity issue.",
                "A number of techniques have been developed in the literature to tackle the problem of cross-domain feature divergence and feature sparsity, including clustering based word representation learning methods (#REF; #REF) , word embedding based representation learning methods (#REF; #REF) and some other representation learning methods (#REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Domain adaptation is very important in the field of natural language processing (NLP) as it can reduce the expensive manual annotation effort in the target domain. Various NLP tasks have benefited from domain adaptation techniques, including part-ofspeech tagging (#REF; #REFa) , chunking (Daumé III, 2007; #TARGET_REF , named entity recognition (#REF; #REF) , dependency parsing (#REF; #REF) and semantic role labeling (#REF; #REFb) . In a typical domain adaptation scenario of NLP, the source and target domains contain text data of different genres (e.g., newswire vs biomedical (#REF) ). Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabularies and produce cross-domain feature distribution divergence and feature sparsity issue. A number of techniques have been developed in the literature to tackle the problem of cross-domain feature divergence and feature sparsity, including clustering based word representation learning methods (#REF; #REF) , word embedding based representation learning methods (#REF; #REF) and some other representation learning methods (#REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Various NLP tasks have benefited from domain adaptation techniques, including part-ofspeech tagging (#REF; #REFa) , chunking (Daum\\u00e9 III, 2007; #TARGET_REF , named entity recognition (#REF; #REF) , dependency parsing (#REF; #REF) and semantic role labeling (#REF; #REFb) .\"]}"
    },
    {
        "gold": {
            "text": [
                "A number of techniques have been developed in the literature to tackle the problem of cross-domain feature divergence and feature sparsity, including clustering based word representation learning methods #TARGET_REF; #REF) , word embedding based representation learning methods (#REF; #REF) and some other representation learning methods (#REF) .",
                "In this paper, we extend the standard hidden Markov models (HMMs) to perform distributed state representation learning and induce contextaware distributed word representations for domain adaptation.",
                "Instead of learning a single discrete latent state for each observation in a given sentence, we learn a distributed representation vector.",
                "We define a state embedding matrix to map each latent state value to a low-dimensional distributed vector and reformulate the three local distributions of HMMs based on the distributed state representations.",
                "We then simultaneously learn the state embedding matrix and the model parameters using an expectation-maximization (EM) algorithm."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "A number of techniques have been developed in the literature to tackle the problem of cross-domain feature divergence and feature sparsity, including clustering based word representation learning methods #TARGET_REF; #REF) , word embedding based representation learning methods (#REF; #REF) and some other representation learning methods (#REF) . In this paper, we extend the standard hidden Markov models (HMMs) to perform distributed state representation learning and induce contextaware distributed word representations for domain adaptation. Instead of learning a single discrete latent state for each observation in a given sentence, we learn a distributed representation vector. We define a state embedding matrix to map each latent state value to a low-dimensional distributed vector and reformulate the three local distributions of HMMs based on the distributed state representations. We then simultaneously learn the state embedding matrix and the model parameters using an expectation-maximization (EM) algorithm.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"A number of techniques have been developed in the literature to tackle the problem of cross-domain feature divergence and feature sparsity, including clustering based word representation learning methods #TARGET_REF; #REF) , word embedding based representation learning methods (#REF; #REF) and some other representation learning methods (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "A variety of representation learning approaches have been developed in the literature to address NLP domain adaptation problems.",
                "The clustering based word representation learning methods perform word clustering within the sentence structure and use word cluster indicators as generalizable features to address domain adaptation problems.",
                "For example, #TARGET_REF used the discrete hidden state of a word under HMMs as augmenting features for cross-domain POS tagging and NP chunking.",
                "Brown clusters (#REF) , which was used as latent features for simple in-domain dependency parsing (#REF) , has recently been exploited for out-ofdomain statistical parsing (#REF) .",
                "The word embedding based representation learning methods learn a dense real-valued representation vector for each word as latent features for domain adaptation."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "A variety of representation learning approaches have been developed in the literature to address NLP domain adaptation problems. The clustering based word representation learning methods perform word clustering within the sentence structure and use word cluster indicators as generalizable features to address domain adaptation problems. For example, #TARGET_REF used the discrete hidden state of a word under HMMs as augmenting features for cross-domain POS tagging and NP chunking. Brown clusters (#REF) , which was used as latent features for simple in-domain dependency parsing (#REF) , has recently been exploited for out-ofdomain statistical parsing (#REF) . The word embedding based representation learning methods learn a dense real-valued representation vector for each word as latent features for domain adaptation.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"For example, #TARGET_REF used the discrete hidden state of a word under HMMs as augmenting features for cross-domain POS tagging and NP chunking.\"]}"
    },
    {
        "gold": {
            "text": [
                "Let S = {s 1 , s 2 , . .",
                ". , s T } be the sequence of T hidden states, where each hidden state s t has a discrete state value from a total H hidden states H = {1, 2, . . . , H}. Besides, we assume that there is a low-dimensional distributed representation vector associated with each hidden state.",
                "Let M ∈ R H×m be the state embedding matrix where the i-th row M i: denotes the m-dimensional representation vector for the i-th state.",
                "Previous works have demonstrated the usefulness of discrete hidden states induced from a HMM on addressing feature sparsity in domain adaptation #TARGET_REF ).",
                "However, expressing a semantic word by a single discrete state value is too restrictive, as it has been shown in the literature that words have many different features in a multidimensional space where they could be separately characterized as number, POS tag, gender, tense, voice and other aspects (#REF; #REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Let S = {s 1 , s 2 , . . . , s T } be the sequence of T hidden states, where each hidden state s t has a discrete state value from a total H hidden states H = {1, 2, . . . , H}. Besides, we assume that there is a low-dimensional distributed representation vector associated with each hidden state. Let M ∈ R H×m be the state embedding matrix where the i-th row M i: denotes the m-dimensional representation vector for the i-th state. Previous works have demonstrated the usefulness of discrete hidden states induced from a HMM on addressing feature sparsity in domain adaptation #TARGET_REF ). However, expressing a semantic word by a single discrete state value is too restrictive, as it has been shown in the literature that words have many different features in a multidimensional space where they could be separately characterized as number, POS tag, gender, tense, voice and other aspects (#REF; #REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Previous works have demonstrated the usefulness of discrete hidden states induced from a HMM on addressing feature sparsity in domain adaptation #TARGET_REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "The hidden states of each word in a sentence can be decoded using the standard Viterbi decoding procedure of HMMs, and its distributed representation can be obtained by a simple mapping with the state embedding matrix.",
                "We then use the context-aware distributed representations of the words as their augmenting features to perform cross-domain part-of-speech (POS) tagging and noun-phrase (NP) chunking.",
                "The proposed approach is closely related to the clustering based method #TARGET_REF ) as we both use latent state representations as generalizable features.",
                "However, they use standard HMMs to produce discrete hidden state features for each observation word, while we induce distributed state representation vectors.",
                "Our distributed HMMs share similarities with the word embedding based method (#REF) , and can be more space-efficient than the standard HMMs."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The hidden states of each word in a sentence can be decoded using the standard Viterbi decoding procedure of HMMs, and its distributed representation can be obtained by a simple mapping with the state embedding matrix. We then use the context-aware distributed representations of the words as their augmenting features to perform cross-domain part-of-speech (POS) tagging and noun-phrase (NP) chunking. The proposed approach is closely related to the clustering based method #TARGET_REF ) as we both use latent state representations as generalizable features. However, they use standard HMMs to produce discrete hidden state features for each observation word, while we induce distributed state representation vectors. Our distributed HMMs share similarities with the word embedding based method (#REF) , and can be more space-efficient than the standard HMMs.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"The proposed approach is closely related to the clustering based method #TARGET_REF ) as we both use latent state representations as generalizable features.\"]}"
    },
    {
        "gold": {
            "text": [
                "We conducted experiments on cross-domain partof-speech (POS) tagging and noun-phrase (NP) chunking tasks.",
                "We used the same experimental datasets as in #TARGET_REF ) for cross-domain POS tagging from Wall Street Journal (WSJ) domain (#REF) to MED-LINE domain (PennBioIE, 2005) and for crossdomain NP chunking from CoNLL shared task dataset (#REF) to Open American National Corpus (OANC) (#REF) ."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "We conducted experiments on cross-domain partof-speech (POS) tagging and noun-phrase (NP) chunking tasks. We used the same experimental datasets as in #TARGET_REF ) for cross-domain POS tagging from Wall Street Journal (WSJ) domain (#REF) to MED-LINE domain (PennBioIE, 2005) and for crossdomain NP chunking from CoNLL shared task dataset (#REF) to Open American National Corpus (OANC) (#REF) .",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"We used the same experimental datasets as in #TARGET_REF ) for cross-domain POS tagging from Wall Street Journal (WSJ) domain (#REF) to MED-LINE domain (PennBioIE, 2005) and for crossdomain NP chunking from CoNLL shared task dataset (#REF) to Open American National Corpus (OANC) (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Evaluation in QA-SRL involves aligning predicted and ground truth argument spans and evaluating role label equivalence.",
                "Since detecting question paraphrases is still an open challenge, we propose both unlabeled and labeled evaluation metrics.",
                "Unlabeled Argument Detection (UA) Inspired by the method presented in ( #TARGET_REF) , arguments are matched using a span matching criterion of intersection over union ≥ 0.5 .",
                "To credit each argument only once, we employ maximal bipartite matching 4 between the two sets of arguments, drawing an edge for each pair that passes the above mentioned criterion.",
                "The resulting maximal matching determines the true-positive set, while remaining non-aligned arguments become false-positives or false-negatives."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Evaluation in QA-SRL involves aligning predicted and ground truth argument spans and evaluating role label equivalence. Since detecting question paraphrases is still an open challenge, we propose both unlabeled and labeled evaluation metrics. Unlabeled Argument Detection (UA) Inspired by the method presented in ( #TARGET_REF) , arguments are matched using a span matching criterion of intersection over union ≥ 0.5 . To credit each argument only once, we employ maximal bipartite matching 4 between the two sets of arguments, drawing an edge for each pair that passes the above mentioned criterion. The resulting maximal matching determines the true-positive set, while remaining non-aligned arguments become false-positives or false-negatives.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"Unlabeled Argument Detection (UA) Inspired by the method presented in ( #TARGET_REF) , arguments are matched using a span matching criterion of intersection over union \\u2265 0.5 .\"]}"
    },
    {
        "gold": {
            "text": [
                "We extend our metric for evaluating manual or automatic redundant annotations, like the Dense dataset or #TARGET_REF in ( #TARGET_REF) , which predicts argument spans independently of each other.",
                "To that end, we ignore predicted arguments that match ground-truth but are not selected by the bipartite matching due to redundancy.",
                "After con-necting unmatched predicted arguments that overlap, we count one false positive for every connected component to avoid penalizing precision too harshly when predictions are redundant.",
                "6"
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "We extend our metric for evaluating manual or automatic redundant annotations, like the Dense dataset or #TARGET_REF in ( #TARGET_REF) , which predicts argument spans independently of each other. To that end, we ignore predicted arguments that match ground-truth but are not selected by the bipartite matching due to redundancy. After con-necting unmatched predicted arguments that overlap, we count one false positive for every connected component to avoid penalizing precision too harshly when predictions are redundant. 6",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We extend our metric for evaluating manual or automatic redundant annotations, like the Dense dataset or #TARGET_REF in ( #TARGET_REF) , which predicts argument spans independently of each other.\"]}"
    },
    {
        "gold": {
            "text": [
                "Current statistical machine translation systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes.",
                "To alleviate this problem, we extract hierarchical rules from weighted alignment matrix #TARGET_REF .",
                "Since the sub-phrase pairs would change the inside and outside areas in the weighted alignment matrix of the hierarchical rules, we propose a new algorithm to calculate the relative frequencies and lexical weights of hierarchical rules.",
                "To achieve a balance between rule table size and performance, we construct a scoring measure that incorporates both frequency and lexical weight to select the best target phrase for each source phrase.",
                "Experiments show that our approach improves BLEU score by ranging from 1.4 to 1.9 points over baseline for hierarchical phrase-based, and 1.4 to 1.5 points for tree-to-string model."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Current statistical machine translation systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. To alleviate this problem, we extract hierarchical rules from weighted alignment matrix #TARGET_REF . Since the sub-phrase pairs would change the inside and outside areas in the weighted alignment matrix of the hierarchical rules, we propose a new algorithm to calculate the relative frequencies and lexical weights of hierarchical rules. To achieve a balance between rule table size and performance, we construct a scoring measure that incorporates both frequency and lexical weight to select the best target phrase for each source phrase. Experiments show that our approach improves BLEU score by ranging from 1.4 to 1.9 points over baseline for hierarchical phrase-based, and 1.4 to 1.5 points for tree-to-string model.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Current statistical machine translation systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes.\", \"To alleviate this problem, we extract hierarchical rules from weighted alignment matrix #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Another challenge is how to achieve a balance between performance and rule table size.",
                "Note that given a source phrase, there would be plenty of \"potential\" candidate target phrases in weighted matrices #TARGET_REF ).",
                "If we retain all of them, these phrase pairs would produce even more hierarchical rules.",
                "For computational tractability, we need to design a measure to score the phrase pairs and wipe out the low-quality ones.",
                "We propose a new algorithm to calculate the relative frequencies of rules, and construct a measure that incorporates both frequency and lexical weight to score target phrases."
            ],
            "label": [
                "MOT",
                "DIFFER"
            ],
            "context": [
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "Another challenge is how to achieve a balance between performance and rule table size. Note that given a source phrase, there would be plenty of \"potential\" candidate target phrases in weighted matrices #TARGET_REF ). If we retain all of them, these phrase pairs would produce even more hierarchical rules. For computational tractability, we need to design a measure to score the phrase pairs and wipe out the low-quality ones. We propose a new algorithm to calculate the relative frequencies of rules, and construct a measure that incorporates both frequency and lexical weight to score target phrases.",
        "output": "{\"label\": [\"MOT\", \"DIFFER\"], \"context\": [\"Another challenge is how to achieve a balance between performance and rule table size.\", \"Note that given a source phrase, there would be plenty of \\\"potential\\\" candidate target phrases in weighted matrices #TARGET_REF ).\", \"If we retain all of them, these phrase pairs would produce even more hierarchical rules.\", \"For computational tractability, we need to design a measure to score the phrase pairs and wipe out the low-quality ones.\", \"We propose a new algorithm to calculate the relative frequencies of rules, and construct a measure that incorporates both frequency and lexical weight to score target phrases.\"]}"
    },
    {
        "gold": {
            "text": [
                "Since #TARGET_REF show that weighted alignment matrix provides an elegant solution to these two drawbacks, we apply it to the hierarchical phrase-based model (#REF) and the tree-to-string model #REF) .",
                "While such an idea seems intuitive, it is non-trivial to extract hierarchical rules from weighted alignment matrices.",
                "Our work faces two major challenges.",
                "The first is how to calculate the relative frequencies and lex- ical weights of the rules with non-terminals (NTs).",
                "The sub-phrase pairs that are replaced with NTs in a rule, would change the inside and outside areas in the weighted alignment matrix of the rule."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Since #TARGET_REF show that weighted alignment matrix provides an elegant solution to these two drawbacks, we apply it to the hierarchical phrase-based model (#REF) and the tree-to-string model #REF) . While such an idea seems intuitive, it is non-trivial to extract hierarchical rules from weighted alignment matrices. Our work faces two major challenges. The first is how to calculate the relative frequencies and lex- ical weights of the rules with non-terminals (NTs). The sub-phrase pairs that are replaced with NTs in a rule, would change the inside and outside areas in the weighted alignment matrix of the rule.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Since #TARGET_REF show that weighted alignment matrix provides an elegant solution to these two drawbacks, we apply it to the hierarchical phrase-based model (#REF) and the tree-to-string model #REF) .\", \"While such an idea seems intuitive, it is non-trivial to extract hierarchical rules from weighted alignment matrices.\"]}"
    },
    {
        "gold": {
            "text": [
                "Since the tree structure of source side has no effect on the calculations of relative frequencies and lexical weights, we can represent both tree-to-string and hierarchical rules as below:",
                "where X is a nonterminal, γ and α are source and target strings (consist of terminals and NTs), and ∼ represents word alignments between NTs in γ and α.",
                "The bulk of syntax grammars consists of two parts: phrase pairs and variable rules.",
                "The difference between them is containing NTs or not.",
                "Since we can calculate relative frequencies and lexical weights of phrase pairs as in #TARGET_REF , we only focus on the calculation of variable rules."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Since the tree structure of source side has no effect on the calculations of relative frequencies and lexical weights, we can represent both tree-to-string and hierarchical rules as below: where X is a nonterminal, γ and α are source and target strings (consist of terminals and NTs), and ∼ represents word alignments between NTs in γ and α. The bulk of syntax grammars consists of two parts: phrase pairs and variable rules. The difference between them is containing NTs or not. Since we can calculate relative frequencies and lexical weights of phrase pairs as in #TARGET_REF , we only focus on the calculation of variable rules.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Since we can calculate relative frequencies and lexical weights of phrase pairs as in #TARGET_REF , we only focus on the calculation of variable rules.\"]}"
    },
    {
        "gold": {
            "text": [
                "We follow #TARGET_REF to calculate relative frequencies using the product of inside and outside probabilities.",
                "We now extend the definitions of inside and outside probabilities to hierarchical rules that contain NTs.",
                "Table 2 : Some hierarchical rules generated from the phrase pair (zhongguo de jingji, China's economy) in Figure 3 (suppose the structure of zhongguo de jingji is a complete sub-tree).",
                "Here α is inside probability, β is outside probability, and count is fractional count.",
                "Given a variable rule (f ′ , e ′ ), which is generated from the phrase pair (f"
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We follow #TARGET_REF to calculate relative frequencies using the product of inside and outside probabilities. We now extend the definitions of inside and outside probabilities to hierarchical rules that contain NTs. Table 2 : Some hierarchical rules generated from the phrase pair (zhongguo de jingji, China's economy) in Figure 3 (suppose the structure of zhongguo de jingji is a complete sub-tree). Here α is inside probability, β is outside probability, and count is fractional count. Given a variable rule (f ′ , e ′ ), which is generated from the phrase pair (f",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We follow #TARGET_REF to calculate relative frequencies using the product of inside and outside probabilities.\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, the inside probability of (X 1 de jingji, X 1 's economy) in Figure 5 is 1.0, and its outside probability is 0.4.",
                "We also use Equation 5 to calculate the fractional counts of hierarchical rules.",
                "We follow #TARGET_REF to prune rule table using a threshold of frequency.",
                "Table 2 lists some hierarchical rules generated from the phrase pair (zhongguo de jingji, China's economy) in Figure 3 .",
                "If the threshold is 0.2, we retain all the rules in Table 2 ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "For example, the inside probability of (X 1 de jingji, X 1 's economy) in Figure 5 is 1.0, and its outside probability is 0.4. We also use Equation 5 to calculate the fractional counts of hierarchical rules. We follow #TARGET_REF to prune rule table using a threshold of frequency. Table 2 lists some hierarchical rules generated from the phrase pair (zhongguo de jingji, China's economy) in Figure 3 . If the threshold is 0.2, we retain all the rules in Table 2 .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We follow #TARGET_REF to prune rule table using a threshold of frequency.\"]}"
    },
    {
        "gold": {
            "text": [
                "We train a 4-gram language model on the Xinhua portion of GIGA-WORD corpus using the SRI Language Modeling Toolkit (#REF) with modified KneserNey smoothing (#REF To obtain weighted alignment matrices, we follow #REF to produce n-best lists via GIZA++.",
                "We produce 20-best lists in two translation directions, then used \"grow-diag-finaland\" (#REF) to all 20 × 20 bidirectional alignment pairs.",
                "We follow #TARGET_REF to use p s2t × p t2s as the probabilities of an alignment pair.",
                "Analogously, we abandon duplicate alignments that are produced from different alignment pairs.",
                "After these steps, there are 110 candidate alignments on average for each sentence pair."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "We train a 4-gram language model on the Xinhua portion of GIGA-WORD corpus using the SRI Language Modeling Toolkit (#REF) with modified KneserNey smoothing (#REF To obtain weighted alignment matrices, we follow #REF to produce n-best lists via GIZA++. We produce 20-best lists in two translation directions, then used \"grow-diag-finaland\" (#REF) to all 20 × 20 bidirectional alignment pairs. We follow #TARGET_REF to use p s2t × p t2s as the probabilities of an alignment pair. Analogously, we abandon duplicate alignments that are produced from different alignment pairs. After these steps, there are 110 candidate alignments on average for each sentence pair.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We follow #TARGET_REF to use p s2t \\u00d7 p t2s as the probabilities of an alignment pair.\", \"Analogously, we abandon duplicate alignments that are produced from different alignment pairs.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, these models take more time to instantiate in comparison to weighting of a co-occurrence matrix, bring more parameters to explore and produce vector spaces with uninterpretable dimensions (vector space dimension interpretation is used by some lexical mod-els, for example, #REF , and the passage from formal semantics to tensor models relies on it (#REF) ).",
                "In this work we focus on vector spaces that directly weight a co-occurrence matrix and report results for SVD, GloVe and SGNS from the study of #TARGET_REF for comparison.",
                "The mismatch of recent experiments with nondense models in vector dimensionality between lexical and compositional tasks gives rise to a number of questions:",
                "• To what extent does model performance depend on vector dimensionality?",
                "• Do parameters influence 200K and 1K dimensional models similarly?"
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "However, these models take more time to instantiate in comparison to weighting of a co-occurrence matrix, bring more parameters to explore and produce vector spaces with uninterpretable dimensions (vector space dimension interpretation is used by some lexical mod-els, for example, #REF , and the passage from formal semantics to tensor models relies on it (#REF) ). In this work we focus on vector spaces that directly weight a co-occurrence matrix and report results for SVD, GloVe and SGNS from the study of #TARGET_REF for comparison. The mismatch of recent experiments with nondense models in vector dimensionality between lexical and compositional tasks gives rise to a number of questions: • To what extent does model performance depend on vector dimensionality? • Do parameters influence 200K and 1K dimensional models similarly?",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In this work we focus on vector spaces that directly weight a co-occurrence matrix and report results for SVD, GloVe and SGNS from the study of #TARGET_REF for comparison.\"]}"
    },
    {
        "gold": {
            "text": [
                "Many approaches use only positive PMI values, as negative PMI values may not positively contribute to model performance and sparser matrices are more computationally tractable (#REF) .",
                "This can be generalised to an additional cutoff parameter k (neg) following #TARGET_REF , giving our third PMI variant (abbreviated as SPMI): 2",
                "When k = 1 SPMI is equivalent to positive PMI.",
                "k > 1 increases the underlying matrix sparsity by keeping only highly associated co-occurrence pairs.",
                "k < 1 decreases the underlying matrix sparsity by including some unassociated cooccurrence pairs, which are usually excluded due to unreliability of probability estimates (#REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Many approaches use only positive PMI values, as negative PMI values may not positively contribute to model performance and sparser matrices are more computationally tractable (#REF) . This can be generalised to an additional cutoff parameter k (neg) following #TARGET_REF , giving our third PMI variant (abbreviated as SPMI): 2 When k = 1 SPMI is equivalent to positive PMI. k > 1 increases the underlying matrix sparsity by keeping only highly associated co-occurrence pairs. k < 1 decreases the underlying matrix sparsity by including some unassociated cooccurrence pairs, which are usually excluded due to unreliability of probability estimates (#REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"This can be generalised to an additional cutoff parameter k (neg) following #TARGET_REF , giving our third PMI variant (abbreviated as SPMI): 2\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate these heuristics by comparing the performance they give on #REF9 against that obtained using the best possible parameter selections (determined via an exhaustive search at each dimensionality setting).",
                "We also compare them to the best scores reported by #TARGET_REF for their model (PMI and SVD), word2vec-SGNS (#REF) and GloVe (#REF )-see Figure 3a , where only the betterperforming SPMI and SCPMI are shown.",
                "For lognPMI and lognCPMI, our heuristics pick the best possible models.",
                "For lognSPMI, where performance variance is low, the heuristics do well, giving a performance of no more than 0.01 points below the best configuration.",
                "For 1SPMI and nSPMI the difference is higher."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We evaluate these heuristics by comparing the performance they give on #REF9 against that obtained using the best possible parameter selections (determined via an exhaustive search at each dimensionality setting). We also compare them to the best scores reported by #TARGET_REF for their model (PMI and SVD), word2vec-SGNS (#REF) and GloVe (#REF )-see Figure 3a , where only the betterperforming SPMI and SCPMI are shown. For lognPMI and lognCPMI, our heuristics pick the best possible models. For lognSPMI, where performance variance is low, the heuristics do well, giving a performance of no more than 0.01 points below the best configuration. For 1SPMI and nSPMI the difference is higher.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We also compare them to the best scores reported by #TARGET_REF for their model (PMI and SVD), word2vec-SGNS (#REF) and GloVe (#REF )-see Figure 3a , where only the betterperforming SPMI and SCPMI are shown.\"]}"
    },
    {
        "gold": {
            "text": [
                "For lognSPMI, where performance variance is low, the heuristics do well, giving a performance of no more than 0.01 points below the best configuration.",
                "For 1SPMI and nSPMI the difference is higher.",
                "With lognSCPMI and 1SCPMI, the heuristics follow #TARGET_REF .",
                "We also give our best score, SVD, SGNS and GloVe numbers from that study for comparison.",
                "On the right, our heuristic in comparison to the best and average results together with the models selected using the recommendations presented in #REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "For lognSPMI, where performance variance is low, the heuristics do well, giving a performance of no more than 0.01 points below the best configuration. For 1SPMI and nSPMI the difference is higher. With lognSCPMI and 1SCPMI, the heuristics follow #TARGET_REF . We also give our best score, SVD, SGNS and GloVe numbers from that study for comparison. On the right, our heuristic in comparison to the best and average results together with the models selected using the recommendations presented in #REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"With lognSCPMI and 1SCPMI, the heuristics follow #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We also give our best score, SVD, SGNS and GloVe numbers from that study for comparison.",
                "On the right, our heuristic in comparison to the best and average results together with the models selected using the recommendations presented in #TARGET_REF .",
                "the best selection, but with a wider gap than the SPMI models.",
                "In general n-weighted models do not perform as well as others.",
                "Overall, log n weighting should be used with PMI, CPMI and SCPMI."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We also give our best score, SVD, SGNS and GloVe numbers from that study for comparison. On the right, our heuristic in comparison to the best and average results together with the models selected using the recommendations presented in #TARGET_REF . the best selection, but with a wider gap than the SPMI models. In general n-weighted models do not perform as well as others. Overall, log n weighting should be used with PMI, CPMI and SCPMI.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"On the right, our heuristic in comparison to the best and average results together with the models selected using the recommendations presented in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper presents a systematic study of cooccurrence quantification focusing on the selection of parameters presented in #TARGET_REF .",
                "We replicate their recommendation for high-dimensional vector spaces, and show that with appropriate parameter selection it is possible to achieve comparable performance with spaces of dimensionality of 1K to 50K, and propose a set of model selection heuristics that maximizes performance.",
                "We foresee the results of the paper are generalisable to other experiments, since model selection was performed on a similarity dataset, and was additionally tested on a relatedness dataset.",
                "In general, model performance depends on vector dimensionality (the best setup with 50K dimensions is better than the best setup with 1K dimensions by 0.03 on #REF9).",
                "Spaces with a few thousand dimensions benefit from being dense and unsmoothed (k < 1, global context probability); while high-dimensional spaces are better sparse and smooth (k > 1, α = 0.75)."
            ],
            "label": [
                "USE",
                "EXT"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "This paper presents a systematic study of cooccurrence quantification focusing on the selection of parameters presented in #TARGET_REF . We replicate their recommendation for high-dimensional vector spaces, and show that with appropriate parameter selection it is possible to achieve comparable performance with spaces of dimensionality of 1K to 50K, and propose a set of model selection heuristics that maximizes performance. We foresee the results of the paper are generalisable to other experiments, since model selection was performed on a similarity dataset, and was additionally tested on a relatedness dataset. In general, model performance depends on vector dimensionality (the best setup with 50K dimensions is better than the best setup with 1K dimensions by 0.03 on #REF9). Spaces with a few thousand dimensions benefit from being dense and unsmoothed (k < 1, global context probability); while high-dimensional spaces are better sparse and smooth (k > 1, α = 0.75).",
        "output": "{\"label\": [\"USE\", \"EXT\"], \"context\": [\"This paper presents a systematic study of cooccurrence quantification focusing on the selection of parameters presented in #TARGET_REF .\", \"We replicate their recommendation for high-dimensional vector spaces, and show that with appropriate parameter selection it is possible to achieve comparable performance with spaces of dimensionality of 1K to 50K, and propose a set of model selection heuristics that maximizes performance.\"]}"
    },
    {
        "gold": {
            "text": [
                "On the similarity dataset our model is 0.008 points behind a PPMI model, however on the relatedness dataset 0.020 points above.",
                "Note the difference in dimensionality, source corpora and window size.",
                "SVD, SGNS and GloVe numbers are given for comparison.",
                "* Results reported by #TARGET_REF .",
                "of the high variance of the corresponding scores."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "On the similarity dataset our model is 0.008 points behind a PPMI model, however on the relatedness dataset 0.020 points above. Note the difference in dimensionality, source corpora and window size. SVD, SGNS and GloVe numbers are given for comparison. * Results reported by #TARGET_REF . of the high variance of the corresponding scores.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"* Results reported by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The mismatch of recent experiments with nondense models in vector dimensionality between lexical and compositional tasks gives rise to a number of questions:",
                "• To what extent does model performance depend on vector dimensionality?",
                "• Do parameters influence 200K and 1K dimensional models similarly?",
                "Can the findings of #TARGET_REF be directly applied to models with a few thousand dimensions?",
                "• If not, can we derive suitable parameter selection heuristics which take account of dimensionality?"
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The mismatch of recent experiments with nondense models in vector dimensionality between lexical and compositional tasks gives rise to a number of questions: • To what extent does model performance depend on vector dimensionality? • Do parameters influence 200K and 1K dimensional models similarly? Can the findings of #TARGET_REF be directly applied to models with a few thousand dimensions? • If not, can we derive suitable parameter selection heuristics which take account of dimensionality?",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"The mismatch of recent experiments with nondense models in vector dimensionality between lexical and compositional tasks gives rise to a number of questions:\", \"Can the findings of #TARGET_REF be directly applied to models with a few thousand dimensions?\"]}"
    },
    {
        "gold": {
            "text": [
                "Another issue with PMI is its bias towards rare events #TARGET_REF ; one way of solving this issue is to weight the value by the co-occurrence frequency (#REF) :",
                "where n(x, y) is the number of times x was seen together with y. For clarity, we refer to n-weighted PMIs as nPMI, nSPMI, etc.",
                "When this weighting component is set to 1, it has no effect; we can explicitly label it as 1PMI, 1SPMI, etc.",
                "In addition to the extreme 1 and n weightings, we also experiment with a log n weighting.",
                "#REF show that performance is affected by smoothing the context distribution P (x):"
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Another issue with PMI is its bias towards rare events #TARGET_REF ; one way of solving this issue is to weight the value by the co-occurrence frequency (#REF) : where n(x, y) is the number of times x was seen together with y. For clarity, we refer to n-weighted PMIs as nPMI, nSPMI, etc. When this weighting component is set to 1, it has no effect; we can explicitly label it as 1PMI, 1SPMI, etc. In addition to the extreme 1 and n weightings, we also experiment with a log n weighting. #REF show that performance is affected by smoothing the context distribution P (x):",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Another issue with PMI is its bias towards rare events #TARGET_REF ; one way of solving this issue is to weight the value by the co-occurrence frequency (#REF) :\"]}"
    },
    {
        "gold": {
            "text": [
                "High-dimensional SPMI models show the same behaviour, but if D < 10K, no weighting should be applied.",
                "SPMI and SCPMI should be preferred over CPMI and PMI.",
                "As Figure 3b shows, our heuristics give performance close to the optimum for any dimensionality, with a large improvement over both an average parameter setting and the parameters suggested by #TARGET_REF in a high-dimensional setting.",
                "4 Finally, to see whether the heuristics transfer robustly, we repeat this comparison on the MEN dataset (see Figures 3c, 3d) .",
                "Again, PMI and CPMI follow the best possible setup, with SPMI and SCPMI showing only a slight drop below ideal performance; and again, the heuristic settings give performance close to the optimum, and significantly higher than average or standard parameters."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "High-dimensional SPMI models show the same behaviour, but if D < 10K, no weighting should be applied. SPMI and SCPMI should be preferred over CPMI and PMI. As Figure 3b shows, our heuristics give performance close to the optimum for any dimensionality, with a large improvement over both an average parameter setting and the parameters suggested by #TARGET_REF in a high-dimensional setting. 4 Finally, to see whether the heuristics transfer robustly, we repeat this comparison on the MEN dataset (see Figures 3c, 3d) . Again, PMI and CPMI follow the best possible setup, with SPMI and SCPMI showing only a slight drop below ideal performance; and again, the heuristic settings give performance close to the optimum, and significantly higher than average or standard parameters.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"As Figure 3b shows, our heuristics give performance close to the optimum for any dimensionality, with a large improvement over both an average parameter setting and the parameters suggested by #TARGET_REF in a high-dimensional setting.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, studies of Amazon's product reviews also show that the per- Meta-data MET the overall ratings of papers assigned by reviewers, and the absolute difference between the rating and the average score given by all reviewers.",
                "Table 1 : Generic features motivated by related work of product reviews (#REF) .",
                "ceived helpfulness of a review depends not only on its review content, but also on social effects such as product qualities, and individual bias in the presence of mixed opinion distribution (#REF ).",
                "Nonetheless, several properties distinguish our corpus of peer reviews from other types of reviews: 1) The helpfulness of our peer reviews is directly rated using a discrete scale from one to five instead of being defined as a function of binary votes (e.g. the percentage of \"helpful\" votes #TARGET_REF ); 2) Peer reviews frequently refer to the related students' papers, thus review analysis needs to take into account paper topics; 3) Within the context of education, peer-review helpfulness often has a writing specific semantics, e.g. improving revision likelihood; 4) In general, peer-review corpora collected from classrooms are of a much smaller size compared to online product reviews.",
                "To tailor existing techniques to peer reviews, we will thus propose new specialized features to address these issues."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "However, studies of Amazon's product reviews also show that the per- Meta-data MET the overall ratings of papers assigned by reviewers, and the absolute difference between the rating and the average score given by all reviewers. Table 1 : Generic features motivated by related work of product reviews (#REF) . ceived helpfulness of a review depends not only on its review content, but also on social effects such as product qualities, and individual bias in the presence of mixed opinion distribution (#REF ). Nonetheless, several properties distinguish our corpus of peer reviews from other types of reviews: 1) The helpfulness of our peer reviews is directly rated using a discrete scale from one to five instead of being defined as a function of binary votes (e.g. the percentage of \"helpful\" votes #TARGET_REF ); 2) Peer reviews frequently refer to the related students' papers, thus review analysis needs to take into account paper topics; 3) Within the context of education, peer-review helpfulness often has a writing specific semantics, e.g. improving revision likelihood; 4) In general, peer-review corpora collected from classrooms are of a much smaller size compared to online product reviews. To tailor existing techniques to peer reviews, we will thus propose new specialized features to address these issues.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Nonetheless, several properties distinguish our corpus of peer reviews from other types of reviews: 1) The helpfulness of our peer reviews is directly rated using a discrete scale from one to five instead of being defined as a function of binary votes (e.g. the percentage of \\\"helpful\\\" votes #TARGET_REF ); 2) Peer reviews frequently refer to the related students' papers, thus review analysis needs to take into account paper topics; 3) Within the context of education, peer-review helpfulness often has a writing specific semantics, e.g. improving revision likelihood; 4) In general, peer-review corpora collected from classrooms are of a much smaller size compared to online product reviews.\"]}"
    },
    {
        "gold": {
            "text": [
                "Evaluation of the generic features is presented in Table 2 , showing that all classes except syntactic (SYN) and meta-data (MET) features are sig-nificantly correlated with both helpfulness rating (r) and helpfulness ranking (r s ).",
                "Structural features (bolded) achieve the highest Pearson (0.60) and Spearman correlation coefficients (0.59) (although within the significant correlations, the difference among coefficients are insignificant).",
                "Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews #TARGET_REF where product scores are significantly correlated with product-review helpfulness.",
                "However, when combined with other features, MET does appear to add value (last row).",
                "When comparing the performance between predicting helpfulness ratings versus ranking, we observe r ≈ r s consistently for our peer reviews, while #REF reported r < r s for product reviews."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Evaluation of the generic features is presented in Table 2 , showing that all classes except syntactic (SYN) and meta-data (MET) features are sig-nificantly correlated with both helpfulness rating (r) and helpfulness ranking (r s ). Structural features (bolded) achieve the highest Pearson (0.60) and Spearman correlation coefficients (0.59) (although within the significant correlations, the difference among coefficients are insignificant). Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews #TARGET_REF where product scores are significantly correlated with product-review helpfulness. However, when combined with other features, MET does appear to add value (last row). When comparing the performance between predicting helpfulness ratings versus ranking, we observe r ≈ r s consistently for our peer reviews, while #REF reported r < r s for product reviews.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews #TARGET_REF where product scores are significantly correlated with product-review helpfulness.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, when combined with other features, MET does appear to add value (last row).",
                "When comparing the performance between predicting helpfulness ratings versus ranking, we observe r ≈ r s consistently for our peer reviews, while #TARGET_REF reported r < r s for product reviews.",
                "4 Finally, we observed a similar feature redundancy effect as #REF did, in that simply combining all features does not improve the model's performance.",
                "Interestingly, our best feature combination (last row) is the same as theirs.",
                "In sum our results verify our hypothesis that the effectiveness of generic features can be transferred to our peerreview domain for predicting review helpfulness."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "However, when combined with other features, MET does appear to add value (last row). When comparing the performance between predicting helpfulness ratings versus ranking, we observe r ≈ r s consistently for our peer reviews, while #TARGET_REF reported r < r s for product reviews. 4 Finally, we observed a similar feature redundancy effect as #REF did, in that simply combining all features does not improve the model's performance. Interestingly, our best feature combination (last row) is the same as theirs. In sum our results verify our hypothesis that the effectiveness of generic features can be transferred to our peerreview domain for predicting review helpfulness.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"When comparing the performance between predicting helpfulness ratings versus ranking, we observe r \\u2248 r s consistently for our peer reviews, while #TARGET_REF reported r < r s for product reviews.\"]}"
    },
    {
        "gold": {
            "text": [
                "The model's performance can be alter- natively achieved and further improved by adding auxiliary features tailored to peer reviews.",
                "These specialized features not only introduce domain expertise, but also capture linguistic information at an abstracted level, which can help avoid the risk of over-fitting.",
                "Given only 267 peer reviews in our case compared to more than ten thousand product reviews #TARGET_REF , this is an important consideration.",
                "Though our absolute quantitative results are not directly comparable to the results of #REF , we indirectly compared them by analyzing the utility of features in isolation and combined.",
                "While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews)."
            ],
            "label": [
                "DIFFER",
                "MOT"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "The model's performance can be alter- natively achieved and further improved by adding auxiliary features tailored to peer reviews. These specialized features not only introduce domain expertise, but also capture linguistic information at an abstracted level, which can help avoid the risk of over-fitting. Given only 267 peer reviews in our case compared to more than ten thousand product reviews #TARGET_REF , this is an important consideration. Though our absolute quantitative results are not directly comparable to the results of #REF , we indirectly compared them by analyzing the utility of features in isolation and combined. While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews).",
        "output": "{\"label\": [\"DIFFER\", \"MOT\"], \"context\": [\"These specialized features not only introduce domain expertise, but also capture linguistic information at an abstracted level, which can help avoid the risk of over-fitting.\", \"Given only 267 peer reviews in our case compared to more than ten thousand product reviews #TARGET_REF , this is an important consideration.\"]}"
    },
    {
        "gold": {
            "text": [
                "Though our absolute quantitative results are not directly comparable to the results of #REF , we indirectly compared them by analyzing the utility of features in isolation and combined.",
                "While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews).",
                "More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews (#REF; #REF) , have no predictive power for peer reviews.",
                "Perhaps because the paper grades and other helpfulness ratings are not visible to the reviewers, we have less of a social dimension for predicting the helpfulness of peer reviews.",
                "We also found that SVM regression does not favor ranking over predicting helpfulness as in #TARGET_REF ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Though our absolute quantitative results are not directly comparable to the results of #REF , we indirectly compared them by analyzing the utility of features in isolation and combined. While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews). More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews (#REF; #REF) , have no predictive power for peer reviews. Perhaps because the paper grades and other helpfulness ratings are not visible to the reviewers, we have less of a social dimension for predicting the helpfulness of peer reviews. We also found that SVM regression does not favor ranking over predicting helpfulness as in #TARGET_REF .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We also found that SVM regression does not favor ranking over predicting helpfulness as in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Though our absolute quantitative results are not directly comparable to the results of #REF , we indirectly compared them by analyzing the utility of features in isolation and combined.",
                "While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews).",
                "More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews #TARGET_REF; #REF) , have no predictive power for peer reviews.",
                "Perhaps because the paper grades and other helpfulness ratings are not visible to the reviewers, we have less of a social dimension for predicting the helpfulness of peer reviews.",
                "We also found that SVM regression does not favor ranking over predicting helpfulness as in (#REF) ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Though our absolute quantitative results are not directly comparable to the results of #REF , we indirectly compared them by analyzing the utility of features in isolation and combined. While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews). More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews #TARGET_REF; #REF) , have no predictive power for peer reviews. Perhaps because the paper grades and other helpfulness ratings are not visible to the reviewers, we have less of a social dimension for predicting the helpfulness of peer reviews. We also found that SVM regression does not favor ranking over predicting helpfulness as in (#REF) .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"More importantly, meta-data, which are found to significantly affect the perceived helpfulness of product reviews #TARGET_REF; #REF) , have no predictive power for peer reviews.\"]}"
    },
    {
        "gold": {
            "text": [
                "An unhelpful peer review of average-rating 1:",
                "Your paper and its main points are easy to find and to follow.",
                "As shown in Table 1 , we first mine generic linguistic features from reviews and papers based on the results of syntactic analysis of the texts, aiming to replicate the feature sets used by #TARGET_REF .",
                "While structural, lexical and syntactic features are created in the same way as suggested in their paper, we adapt the semantic and meta-data features to peer reviews by converting the mentions of product properties to mentions of the history topics and by using paper ratings assigned by peers instead of product scores.",
                "1"
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "An unhelpful peer review of average-rating 1: Your paper and its main points are easy to find and to follow. As shown in Table 1 , we first mine generic linguistic features from reviews and papers based on the results of syntactic analysis of the texts, aiming to replicate the feature sets used by #TARGET_REF . While structural, lexical and syntactic features are created in the same way as suggested in their paper, we adapt the semantic and meta-data features to peer reviews by converting the mentions of product properties to mentions of the history topics and by using paper ratings assigned by peers instead of product scores. 1",
        "output": "{\"label\": [\"USE\"], \"context\": [\"As shown in Table 1 , we first mine generic linguistic features from reviews and papers based on the results of syntactic analysis of the texts, aiming to replicate the feature sets used by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , we train our helpfulness model using SVM regression with a radial basis function kernel provided by SVM light (#REF) .",
                "We first evaluate each feature type in isolation to investigate its predictive power of peerreview helpfulness; we then examine them together in various combinations to find the most useful feature set for modeling peer-review helpfulness.",
                "Performance is evaluated in 10-fold cross validation of our 267 peer reviews by predicting the absolute helpfulness scores (with Pearson correlation coefficient r) as well as by predicting helpfulness ranking (with Spearman rank correlation coefficient r s ).",
                "Although predicted helpfulness ranking could be directly used to compare the helpfulness of a given set of reviews, predicting helpfulness rating is desirable in practice to compare helpfulness between existing reviews and new written ones without reranking all previously ranked reviews.",
                "Results are presented regarding the generic features and the specialized features respectively, with 95% confidence bounds."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Following #TARGET_REF , we train our helpfulness model using SVM regression with a radial basis function kernel provided by SVM light (#REF) . We first evaluate each feature type in isolation to investigate its predictive power of peerreview helpfulness; we then examine them together in various combinations to find the most useful feature set for modeling peer-review helpfulness. Performance is evaluated in 10-fold cross validation of our 267 peer reviews by predicting the absolute helpfulness scores (with Pearson correlation coefficient r) as well as by predicting helpfulness ranking (with Spearman rank correlation coefficient r s ). Although predicted helpfulness ranking could be directly used to compare the helpfulness of a given set of reviews, predicting helpfulness rating is desirable in practice to compare helpfulness between existing reviews and new written ones without reranking all previously ranked reviews. Results are presented regarding the generic features and the specialized features respectively, with 95% confidence bounds.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following #TARGET_REF , we train our helpfulness model using SVM regression with a radial basis function kernel provided by SVM light (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The model's performance can be alter- natively achieved and further improved by adding auxiliary features tailored to peer reviews.",
                "These specialized features not only introduce domain expertise, but also capture linguistic information at an abstracted level, which can help avoid the risk of over-fitting.",
                "Given only 267 peer reviews in our case compared to more than ten thousand product reviews (#REF) , this is an important consideration.",
                "Though our absolute quantitative results are not directly comparable to the results of #TARGET_REF , we indirectly compared them by analyzing the utility of features in isolation and combined.",
                "While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The model's performance can be alter- natively achieved and further improved by adding auxiliary features tailored to peer reviews. These specialized features not only introduce domain expertise, but also capture linguistic information at an abstracted level, which can help avoid the risk of over-fitting. Given only 267 peer reviews in our case compared to more than ten thousand product reviews (#REF) , this is an important consideration. Though our absolute quantitative results are not directly comparable to the results of #TARGET_REF , we indirectly compared them by analyzing the utility of features in isolation and combined. While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is different (review unigrams work best for product reviews; structural features work best for peer reviews).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Though our absolute quantitative results are not directly comparable to the results of #TARGET_REF , we indirectly compared them by analyzing the utility of features in isolation and combined.\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews (#REF) where product scores are significantly correlated with product-review helpfulness.",
                "However, when combined with other features, MET does appear to add value (last row).",
                "When comparing the performance between predicting helpfulness ratings versus ranking, we observe r ≈ r s consistently for our peer reviews, while #REF reported r < r s for product reviews.",
                "4 Finally, we observed a similar feature redundancy effect as #TARGET_REF did, in that simply combining all features does not improve the model's performance.",
                "Interestingly, our best feature combination (last row) is the same as theirs."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Note that in isolation, MET (paper ratings) are not significantly correlated with peer-review helpfulness, which is different from prior findings of product reviews (#REF) where product scores are significantly correlated with product-review helpfulness. However, when combined with other features, MET does appear to add value (last row). When comparing the performance between predicting helpfulness ratings versus ranking, we observe r ≈ r s consistently for our peer reviews, while #REF reported r < r s for product reviews. 4 Finally, we observed a similar feature redundancy effect as #TARGET_REF did, in that simply combining all features does not improve the model's performance. Interestingly, our best feature combination (last row) is the same as theirs.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"4 Finally, we observed a similar feature redundancy effect as #TARGET_REF did, in that simply combining all features does not improve the model's performance.\"]}"
    },
    {
        "gold": {
            "text": [
                "According to (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #TARGET_REF , the approaches of Chinese input methods (i.e. Chinese input systems) can be classified into two types: (1) keyboard based approach: including phonetic and pinyin based (#REF; #REF; #REF; #REF; #REF; #REF) , arbitrary codes based (#REF) and structure scheme based (#REF) ; and (2) non-keyboard based approach: including optical character recognition (OCR) (#REF) , online handwriting and speech recognition (#REF; #REF) .",
                "Currently, the most popular Chinese input system is phonetic and pinyin based approach, because Chinese people are taught to write phonetic and pinyin syllables of each Chinese character in primary school.",
                "In Chinese, each Chinese word can be a mono-syllabic word, such as \"鼠(mouse)\", a bisyllabic word, such as \"袋鼠(kangaroo)\", or a multi-syllabic word, such as \"米老鼠(Mickey mouse).\" The corresponding phonetic and pinyin syllables of each Chinese word is called syllable-words, such as \"dai4 shu3\" is the pinyin syllable-word of \"袋鼠(kangaroo).\" According to our computation, the {minimum, maximum, average} words per each distinct mono-syllableword and poly-syllable-word (including bisyllable-word and multi-syllable-word) in the CKIP dictionary (Chinese Knowledge Information Processing #REF) are {1, 28, 2.8} and {1, 7, 1.1}, respectively.",
                "The CKIP dictionary is one of most commonly-used Chinese dictionaries in the research field of Chinese natural language processing (NLP).",
                "Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (#REF; #REF; #REF; #REF; Microsoft Research Center in Beijing; #REF) are addressed on STW conversion."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "According to (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #TARGET_REF , the approaches of Chinese input methods (i.e. Chinese input systems) can be classified into two types: (1) keyboard based approach: including phonetic and pinyin based (#REF; #REF; #REF; #REF; #REF; #REF) , arbitrary codes based (#REF) and structure scheme based (#REF) ; and (2) non-keyboard based approach: including optical character recognition (OCR) (#REF) , online handwriting and speech recognition (#REF; #REF) . Currently, the most popular Chinese input system is phonetic and pinyin based approach, because Chinese people are taught to write phonetic and pinyin syllables of each Chinese character in primary school. In Chinese, each Chinese word can be a mono-syllabic word, such as \"鼠(mouse)\", a bisyllabic word, such as \"袋鼠(kangaroo)\", or a multi-syllabic word, such as \"米老鼠(Mickey mouse).\" The corresponding phonetic and pinyin syllables of each Chinese word is called syllable-words, such as \"dai4 shu3\" is the pinyin syllable-word of \"袋鼠(kangaroo).\" According to our computation, the {minimum, maximum, average} words per each distinct mono-syllableword and poly-syllable-word (including bisyllable-word and multi-syllable-word) in the CKIP dictionary (Chinese Knowledge Information Processing #REF) are {1, 28, 2.8} and {1, 7, 1.1}, respectively. The CKIP dictionary is one of most commonly-used Chinese dictionaries in the research field of Chinese natural language processing (NLP). Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (#REF; #REF; #REF; #REF; Microsoft Research Center in Beijing; #REF) are addressed on STW conversion.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"According to (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #TARGET_REF , the approaches of Chinese input methods (i.e. Chinese input systems) can be classified into two types: (1) keyboard based approach: including phonetic and pinyin based (#REF; #REF; #REF; #REF; #REF; #REF) , arbitrary codes based (#REF) and structure scheme based (#REF) ; and (2) non-keyboard based approach: including optical character recognition (OCR) (#REF) , online handwriting and speech recognition (#REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "In Chinese, each Chinese word can be a mono-syllabic word, such as \"鼠(mouse)\", a bisyllabic word, such as \"袋鼠(kangaroo)\", or a multi-syllabic word, such as \"米老鼠(Mickey mouse).\" The corresponding phonetic and pinyin syllables of each Chinese word is called syllable-words, such as \"dai4 shu3\" is the pinyin syllable-word of \"袋鼠(kangaroo).\" According to our computation, the {minimum, maximum, average} words per each distinct mono-syllableword and poly-syllable-word (including bisyllable-word and multi-syllable-word) in the CKIP dictionary (Chinese Knowledge Information Processing #REF) are {1, 28, 2.8} and {1, 7, 1.1}, respectively.",
                "The CKIP dictionary is one of most commonly-used Chinese dictionaries in the research field of Chinese natural language processing (NLP).",
                "Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (#REF; #REF; #REF; #REF; Microsoft Research Center in Beijing; #TARGET_REF are addressed on STW conversion.",
                "On the other hand, STW conversion is the main task of Chinese Language Processing in typical Chinese speech recognition systems (#REF; #REF; #REF; #REF) .",
                "As per (#REF; #REF; #REF; #REF; #REF; #REF) , homophone selection and syllableword segmentation are two critical problems in developing a Chinese input system."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In Chinese, each Chinese word can be a mono-syllabic word, such as \"鼠(mouse)\", a bisyllabic word, such as \"袋鼠(kangaroo)\", or a multi-syllabic word, such as \"米老鼠(Mickey mouse).\" The corresponding phonetic and pinyin syllables of each Chinese word is called syllable-words, such as \"dai4 shu3\" is the pinyin syllable-word of \"袋鼠(kangaroo).\" According to our computation, the {minimum, maximum, average} words per each distinct mono-syllableword and poly-syllable-word (including bisyllable-word and multi-syllable-word) in the CKIP dictionary (Chinese Knowledge Information Processing #REF) are {1, 28, 2.8} and {1, 7, 1.1}, respectively. The CKIP dictionary is one of most commonly-used Chinese dictionaries in the research field of Chinese natural language processing (NLP). Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (#REF; #REF; #REF; #REF; Microsoft Research Center in Beijing; #TARGET_REF are addressed on STW conversion. On the other hand, STW conversion is the main task of Chinese Language Processing in typical Chinese speech recognition systems (#REF; #REF; #REF; #REF) . As per (#REF; #REF; #REF; #REF; #REF; #REF) , homophone selection and syllableword segmentation are two critical problems in developing a Chinese input system.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Since the size of problem space for syllable-to-word (STW) conversion is much less than that of syllable-tocharacter (STC) conversion, the most pinyinbased Chinese input systems (#REF; #REF; #REF; #REF; Microsoft Research Center in Beijing; #TARGET_REF are addressed on STW conversion.\"]}"
    },
    {
        "gold": {
            "text": [
                "On the other hand, STW conversion is the main task of Chinese Language Processing in typical Chinese speech recognition systems (#REF; #REF; #REF; #REF) .",
                "As per (#REF; #REF; #REF; #REF; #REF; #TARGET_REF , homophone selection and syllableword segmentation are two critical problems in developing a Chinese input system.",
                "Incorrect homophone selection and syllable-word seg-mentation will directly influence the STW conversion accuracy.",
                "Conventionally, there are two approaches to resolve the two critical problems: (1) linguistic approach: based on syntax parsing, semantic template matching and contextual information (#REF; #REF; #REF; #REF; #REF) ; and (2) statistical approach: based on the n-gram models where n is usually 2, i.e. bigram model (#REF; #REF; #REF; #REF; #REF; #REF; #REF) .",
                "From the studies (#REF; #REF; #REF; #REF; #REF) , the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "On the other hand, STW conversion is the main task of Chinese Language Processing in typical Chinese speech recognition systems (#REF; #REF; #REF; #REF) . As per (#REF; #REF; #REF; #REF; #REF; #TARGET_REF , homophone selection and syllableword segmentation are two critical problems in developing a Chinese input system. Incorrect homophone selection and syllable-word seg-mentation will directly influence the STW conversion accuracy. Conventionally, there are two approaches to resolve the two critical problems: (1) linguistic approach: based on syntax parsing, semantic template matching and contextual information (#REF; #REF; #REF; #REF; #REF) ; and (2) statistical approach: based on the n-gram models where n is usually 2, i.e. bigram model (#REF; #REF; #REF; #REF; #REF; #REF; #REF) . From the studies (#REF; #REF; #REF; #REF; #REF) , the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"As per (#REF; #REF; #REF; #REF; #REF; #TARGET_REF , homophone selection and syllableword segmentation are two critical problems in developing a Chinese input system.\"]}"
    },
    {
        "gold": {
            "text": [
                "From the studies (#REF; #REF; #REF; #REF; #TARGET_REF , the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake.",
                "The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems.",
                "In our previous work (#REF) , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively.",
                "In (#REF) , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems.",
                "As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "From the studies (#REF; #REF; #REF; #REF; #TARGET_REF , the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake. The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems. In our previous work (#REF) , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively. In (#REF) , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems. As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"From the studies (#REF; #REF; #REF; #REF; #TARGET_REF , the linguistic approach requires considerable effort in designing effective syntax rules, semantic templates or contextual information, thus, it is more user-friendly than the statistical approach on understanding why such a system makes a mistake.\"]}"
    },
    {
        "gold": {
            "text": [
                "The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems.",
                "In our previous work #TARGET_REF , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively.",
                "In (#REF) , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems.",
                "As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences.",
                "Since the identified character ratio of the WP identifier (#REF ) is about 55%, there are still about 15% improving room left."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems. In our previous work #TARGET_REF , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively. In (#REF) , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems. As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences. Since the identified character ratio of the WP identifier (#REF ) is about 55%, there are still about 15% improving room left.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In our previous work #TARGET_REF , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively.\"]}"
    },
    {
        "gold": {
            "text": [
                "The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems.",
                "In our previous work (#REF) , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively.",
                "In #TARGET_REF , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems.",
                "As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences.",
                "Since the identified character ratio of the WP identifier (#REF ) is about 55%, there are still about 15% improving room left."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The statistical language model (SLM) used in the statistical approach requires less effort and has been widely adopted in commercial Chinese input systems. In our previous work (#REF) , a wordpair (WP) identifier was proposed and shown a simple and effective way to improve Chinese input systems by providing tonal and toneless STW accuracies of 98.5% and 90.7% on the identified poly-syllabic words, respectively. In #TARGET_REF , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems. As per our computation, poly-syllabic words cover about 70% characters of Chinese sentences. Since the identified character ratio of the WP identifier (#REF ) is about 55%, there are still about 15% improving room left.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In #TARGET_REF , we have shown that the WP identifier can be used to reduce the over weighting and corpus sparseness problems of bigram models and achieve better STW accuracy to improve Chinese input systems.\"]}"
    },
    {
        "gold": {
            "text": [
                "From Table 3a , the tonal and toneless STW improvements of the MSIME by using the WP identifier and the WSM are (18.9%, 10.1%) and (25.6%, 16.6%), respectively.",
                "From Table 3b , the tonal and toneless STW improvements of the BiGram by using the WP identifier and the WSM are (8.6%, 11.9%) and (17.1%, 22.0%), respectively.",
                "(Note that, as per #TARGET_REF , the differences between the tonal and toneless STW accuracies of the BiGram and the TriGram are less than 0.3%).",
                "Table 3c is the results of the MSIME and the BiGram by using the WSM as an adaptation processing with both system and user WP database.",
                "From Table 3c , we get the average tonal and toneless STW improvements of the MSIME and the BiGram by using the WSM as an adaptation processing are 37.2% and 34.6%, respectively."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "From Table 3a , the tonal and toneless STW improvements of the MSIME by using the WP identifier and the WSM are (18.9%, 10.1%) and (25.6%, 16.6%), respectively. From Table 3b , the tonal and toneless STW improvements of the BiGram by using the WP identifier and the WSM are (8.6%, 11.9%) and (17.1%, 22.0%), respectively. (Note that, as per #TARGET_REF , the differences between the tonal and toneless STW accuracies of the BiGram and the TriGram are less than 0.3%). Table 3c is the results of the MSIME and the BiGram by using the WSM as an adaptation processing with both system and user WP database. From Table 3c , we get the average tonal and toneless STW improvements of the MSIME and the BiGram by using the WSM as an adaptation processing are 37.2% and 34.6%, respectively.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"(Note that, as per #TARGET_REF , the differences between the tonal and toneless STW accuracies of the BiGram and the TriGram are less than 0.3%).\"]}"
    },
    {
        "gold": {
            "text": [
                "Since the identified character ratio of the WP identifier #TARGET_REF ) is about 55%, there are still about 15% improving room left.",
                "The objective of this study is to illustrate a word support model (WSM) that is able to improve our WP-identifier by achieving better identified character ratio and STW accuracy on the identified poly-syllabic words with the same word-pair database.",
                "We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method #REF, MSIME) , and an optimized bigram model, BiGram (#REF) , can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier.",
                "The remainder of this paper is arranged as follows.",
                "In Section 2, we present an auto wordpair (AUTO-WP) generation used to generate the WP database."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Since the identified character ratio of the WP identifier #TARGET_REF ) is about 55%, there are still about 15% improving room left. The objective of this study is to illustrate a word support model (WSM) that is able to improve our WP-identifier by achieving better identified character ratio and STW accuracy on the identified poly-syllabic words with the same word-pair database. We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method #REF, MSIME) , and an optimized bigram model, BiGram (#REF) , can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier. The remainder of this paper is arranged as follows. In Section 2, we present an auto wordpair (AUTO-WP) generation used to generate the WP database.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Since the identified character ratio of the WP identifier #TARGET_REF ) is about 55%, there are still about 15% improving room left.\", \"The objective of this study is to illustrate a word support model (WSM) that is able to improve our WP-identifier by achieving better identified character ratio and STW accuracy on the identified poly-syllabic words with the same word-pair database.\"]}"
    },
    {
        "gold": {
            "text": [
                "Since the identified character ratio of the WP identifier (#REF ) is about 55%, there are still about 15% improving room left.",
                "The objective of this study is to illustrate a word support model (WSM) that is able to improve our WP-identifier by achieving better identified character ratio and STW accuracy on the identified poly-syllabic words with the same word-pair database.",
                "We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method #REF, MSIME) , and an optimized bigram model, BiGram #TARGET_REF , can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier.",
                "The remainder of this paper is arranged as follows.",
                "In Section 2, we present an auto wordpair (AUTO-WP) generation used to generate the WP database."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Since the identified character ratio of the WP identifier (#REF ) is about 55%, there are still about 15% improving room left. The objective of this study is to illustrate a word support model (WSM) that is able to improve our WP-identifier by achieving better identified character ratio and STW accuracy on the identified poly-syllabic words with the same word-pair database. We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method #REF, MSIME) , and an optimized bigram model, BiGram #TARGET_REF , can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier. The remainder of this paper is arranged as follows. In Section 2, we present an auto wordpair (AUTO-WP) generation used to generate the WP database.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We conduct STW experiments to show the tonal and toneless STW accuracies of a commercial input product (Microsoft Input Method #REF, MSIME) , and an optimized bigram model, BiGram #TARGET_REF , can both be improved by our WSM and achieve better STW improvements than that of these systems with the WP identifier.\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , the three steps of autogenerating word-pairs (AUTO-WP) for a given Chinese sentence are as below: (the details of AUTO-WP can be found in #TARGET_REF Step 1. Get forward and backward word segmentations: Generate two types of word segmentations for a given Chinese sentence by forward maximum matching (FMM) and backward maximum matching (BMM) techniques (#REF; #REF) with the system dictionary.",
                "Step 2. Get initial WP set: Extract all the combinations of word-pairs from the FMM and the BMM segmentations of Step 1 to be the initial WP set.",
                "Step 3. Get finial WP set: Select out the wordpairs comprised of two poly-syllabic words from the initial WP set into the finial WP set.",
                "For the final WP set, if the word-pair is not found in the WP data-base, insert it into the WP database and set its frequency to 1; otherwise, increase its frequency by 1."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "Following #TARGET_REF , the three steps of autogenerating word-pairs (AUTO-WP) for a given Chinese sentence are as below: (the details of AUTO-WP can be found in #TARGET_REF Step 1. Get forward and backward word segmentations: Generate two types of word segmentations for a given Chinese sentence by forward maximum matching (FMM) and backward maximum matching (BMM) techniques (#REF; #REF) with the system dictionary. Step 2. Get initial WP set: Extract all the combinations of word-pairs from the FMM and the BMM segmentations of Step 1 to be the initial WP set. Step 3. Get finial WP set: Select out the wordpairs comprised of two poly-syllabic words from the initial WP set into the finial WP set. For the final WP set, if the word-pair is not found in the WP data-base, insert it into the WP database and set its frequency to 1; otherwise, increase its frequency by 1.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following #TARGET_REF , the three steps of autogenerating word-pairs (AUTO-WP) for a given Chinese sentence are as below: (the details of AUTO-WP can be found in #TARGET_REF Step 1. Get forward and backward word segmentations: Generate two types of word segmentations for a given Chinese sentence by forward maximum matching (FMM) and backward maximum matching (BMM) techniques (#REF; #REF) with the system dictionary.\"]}"
    },
    {
        "gold": {
            "text": [
                "The purpose of this experiment is to demonstrate the tonal and toneless STW accuracies among the identified words by using the WSM with the system WP database.",
                "The comparative system is the WP identifier #TARGET_REF .",
                "Table  2 is the experimental results.",
                "The WP database and system dictionary of the WP identifier is same with that of the WSM.",
                "From Table 2"
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The purpose of this experiment is to demonstrate the tonal and toneless STW accuracies among the identified words by using the WSM with the system WP database. The comparative system is the WP identifier #TARGET_REF . Table  2 is the experimental results. The WP database and system dictionary of the WP identifier is same with that of the WSM. From Table 2",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The purpose of this experiment is to demonstrate the tonal and toneless STW accuracies among the identified words by using the WSM with the system WP database.\", \"The comparative system is the WP identifier #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We selected Microsoft Input Method #REF for Traditional Chinese (MSIME) as our experimental commercial Chinese input system.",
                "In addition, following #TARGET_REF , an optimized bigram model called BiGram was developed.",
                "The BiGram STW system is a bigrambased model developing by SRILM (#REF) with Good-Turing back-off smoothing (#REF) , as well as forward and backward longest syllable-word first strategies (#REF; #REF) .",
                "The system dictionary of the BiGram is same with that of the WP identifier and the WSM.",
                "Table 3a compares the results of the MSIME, the MSIME with the WP identifier and the MSIME with the WSM on the closed and open test sentences."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We selected Microsoft Input Method #REF for Traditional Chinese (MSIME) as our experimental commercial Chinese input system. In addition, following #TARGET_REF , an optimized bigram model called BiGram was developed. The BiGram STW system is a bigrambased model developing by SRILM (#REF) with Good-Turing back-off smoothing (#REF) , as well as forward and backward longest syllable-word first strategies (#REF; #REF) . The system dictionary of the BiGram is same with that of the WP identifier and the WSM. Table 3a compares the results of the MSIME, the MSIME with the WP identifier and the MSIME with the WSM on the closed and open test sentences.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In addition, following #TARGET_REF , an optimized bigram model called BiGram was developed.\"]}"
    },
    {
        "gold": {
            "text": [
                "In most Chinese input systems, unknown word extraction is not specifically a STW problem, therefore, it is usually taken care of through online and offline manual editing processing (Hsu et al, 1999) .",
                "The results of Table 4 show that the most STW errors should be caused by ISWS and HS problems, not UW problem.",
                "This observation is similarly with that of our previous work #TARGET_REF .",
                "(2) The major problem of error conversions in tonal and toneless STW systems is different.",
                "This observation is similarly with that of (#REF) ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In most Chinese input systems, unknown word extraction is not specifically a STW problem, therefore, it is usually taken care of through online and offline manual editing processing (Hsu et al, 1999) . The results of Table 4 show that the most STW errors should be caused by ISWS and HS problems, not UW problem. This observation is similarly with that of our previous work #TARGET_REF . (2) The major problem of error conversions in tonal and toneless STW systems is different. This observation is similarly with that of (#REF) .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"This observation is similarly with that of our previous work #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The results of Table 4 show that the most STW errors should be caused by ISWS and HS problems, not UW problem.",
                "This observation is similarly with that of our previous work (#REF) .",
                "(2) The major problem of error conversions in tonal and toneless STW systems is different.",
                "This observation is similarly with that of #TARGET_REF .",
                "From Table 4 , the major improving targets of tonal STW performance are the HS errors because more than 50% tonal STW errors caused by HS problem."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The results of Table 4 show that the most STW errors should be caused by ISWS and HS problems, not UW problem. This observation is similarly with that of our previous work (#REF) . (2) The major problem of error conversions in tonal and toneless STW systems is different. This observation is similarly with that of #TARGET_REF . From Table 4 , the major improving targets of tonal STW performance are the HS errors because more than 50% tonal STW errors caused by HS problem.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"This observation is similarly with that of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we present a word support model (WSM) to improve the WP identifier #TARGET_REF and support the Chinese Language Processing on the STW conversion problem.",
                "All of the WP data can be generated fully automatically by applying the AUTO-WP on the given corpus.",
                "We are encouraged by the fact that the WSM with WP knowledge is able to achieve state-of-the-art tonal and toneless STW accuracies of 99% and 92%, respectively, for the identified poly-syllabic words.",
                "The WSM can be easily integrated into existing Chinese input systems by identifying words as a post processing.",
                "Our experimental results show that, by applying the WSM as an adaptation processing together with the MSIME (a trigram-like model) and the BiGram (an optimized bigram model), the average tonal and toneless STW improvements of the two Chinese input systems are 37% and 35%, respectively."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In this paper, we present a word support model (WSM) to improve the WP identifier #TARGET_REF and support the Chinese Language Processing on the STW conversion problem. All of the WP data can be generated fully automatically by applying the AUTO-WP on the given corpus. We are encouraged by the fact that the WSM with WP knowledge is able to achieve state-of-the-art tonal and toneless STW accuracies of 99% and 92%, respectively, for the identified poly-syllabic words. The WSM can be easily integrated into existing Chinese input systems by identifying words as a post processing. Our experimental results show that, by applying the WSM as an adaptation processing together with the MSIME (a trigram-like model) and the BiGram (an optimized bigram model), the average tonal and toneless STW improvements of the two Chinese input systems are 37% and 35%, respectively.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"In this paper, we present a word support model (WSM) to improve the WP identifier #TARGET_REF and support the Chinese Language Processing on the STW conversion problem.\"]}"
    },
    {
        "gold": {
            "text": [
                "The main part of the training corpora (approximately 75%) was part of Pangeanic's own repository harvested through web crawling and also OpenSubtitles (#REF) .",
                "The rest of the corpus was automatically validated synthetic material using general data from Leipzig (#REF Engine customization The data was cleaned using the Bicleaner tool #TARGET_REF .",
                "The data was lowercased and extra embeddings were added in order to keep the case information.",
                "The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE (#REF) approach.",
                "The models were trained with multi-domain data and we improved performance following a domainmixing approach (#REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The main part of the training corpora (approximately 75%) was part of Pangeanic's own repository harvested through web crawling and also OpenSubtitles (#REF) . The rest of the corpus was automatically validated synthetic material using general data from Leipzig (#REF Engine customization The data was cleaned using the Bicleaner tool #TARGET_REF . The data was lowercased and extra embeddings were added in order to keep the case information. The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE (#REF) approach. The models were trained with multi-domain data and we improved performance following a domainmixing approach (#REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The rest of the corpus was automatically validated synthetic material using general data from Leipzig (#REF Engine customization The data was cleaned using the Bicleaner tool #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The rest of the corpus was automatically validated synthetic material using general data from Leipzig (#REF) .",
                "Engine customization The data was cleaned using the Bicleaner tool #TARGET_REF .",
                "The data was lowercased and extra embeddings were added in order to keep the case information.",
                "The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE (#REF) approach.",
                "The models were trained with multi-domain data and we improved performance following a domainmixing approach (#REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The rest of the corpus was automatically validated synthetic material using general data from Leipzig (#REF) . Engine customization The data was cleaned using the Bicleaner tool #TARGET_REF . The data was lowercased and extra embeddings were added in order to keep the case information. The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE (#REF) approach. The models were trained with multi-domain data and we improved performance following a domainmixing approach (#REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Engine customization The data was cleaned using the Bicleaner tool #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Audio and Word Embeddings.",
                "Multiple works in the recent past (#REF; #REF; Lopopolo and #REF; #TARGET_REF; #REF) have explored using perceptual modalities like vision and sound to learn language embeddings.",
                "While Lopopolo and #REF show preliminary results on using sound to learn distributional representations, #REF build on ideas from #REF to learn word embeddings that respect both linguistic and auditory relationships by optimizing a joint objective.",
                "Further, they propose various fusion strategies to combine knowledge from both the modalities.",
                "Instead, we \"specialize\" embeddings to exclusively respect relationships defined by sounds, while initializing with word2vec embeddings for smoothness."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Audio and Word Embeddings. Multiple works in the recent past (#REF; #REF; Lopopolo and #REF; #TARGET_REF; #REF) have explored using perceptual modalities like vision and sound to learn language embeddings. While Lopopolo and #REF show preliminary results on using sound to learn distributional representations, #REF build on ideas from #REF to learn word embeddings that respect both linguistic and auditory relationships by optimizing a joint objective. Further, they propose various fusion strategies to combine knowledge from both the modalities. Instead, we \"specialize\" embeddings to exclusively respect relationships defined by sounds, while initializing with word2vec embeddings for smoothness.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Multiple works in the recent past (#REF; #REF; Lopopolo and #REF; #TARGET_REF; #REF) have explored using perceptual modalities like vision and sound to learn language embeddings.\"]}"
    },
    {
        "gold": {
            "text": [
                "Audio and Word Embeddings.",
                "Multiple works in the recent past (#REF; #REF; Lopopolo and #REF; #REF; #REF) have explored using perceptual modalities like vision and sound to learn language embeddings.",
                "While Lopopolo and #REF show preliminary results on using sound to learn distributional representations, #TARGET_REF build on ideas from #REF to learn word embeddings that respect both linguistic and auditory relationships by optimizing a joint objective.",
                "Further, they propose various fusion strategies to combine knowledge from both the modalities.",
                "Instead, we \"specialize\" embeddings to exclusively respect relationships defined by sounds, while initializing with word2vec embeddings for smoothness."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Audio and Word Embeddings. Multiple works in the recent past (#REF; #REF; Lopopolo and #REF; #REF; #REF) have explored using perceptual modalities like vision and sound to learn language embeddings. While Lopopolo and #REF show preliminary results on using sound to learn distributional representations, #TARGET_REF build on ideas from #REF to learn word embeddings that respect both linguistic and auditory relationships by optimizing a joint objective. Further, they propose various fusion strategies to combine knowledge from both the modalities. Instead, we \"specialize\" embeddings to exclusively respect relationships defined by sounds, while initializing with word2vec embeddings for smoothness.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"While Lopopolo and #REF show preliminary results on using sound to learn distributional representations, #TARGET_REF build on ideas from #REF to learn word embeddings that respect both linguistic and auditory relationships by optimizing a joint objective.\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, the description to produce a foley \"driving on gravel\" sound is to record the \"crunching sound of plastic or polyethene bags\".",
                "AMEN and ASLex.",
                "AMEN and ASLex #TARGET_REF are subsets of the standard MEN (#REF) and SimLex (#REF) word similarity datasets consisting of word-pairs that \"can be associated with a distinctive associated sound\".",
                "We evaluate on this dataset for completeness to benchmark our approach against previous work.",
                "However, we are primarily interested in the slightly different problem of relating words with similar auditory instantions that may or may not be semantically related as opposed to relating semantically similar words that can be associated with some common auditory signal."
            ],
            "label": [
                "BACK",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "For example, the description to produce a foley \"driving on gravel\" sound is to record the \"crunching sound of plastic or polyethene bags\". AMEN and ASLex. AMEN and ASLex #TARGET_REF are subsets of the standard MEN (#REF) and SimLex (#REF) word similarity datasets consisting of word-pairs that \"can be associated with a distinctive associated sound\". We evaluate on this dataset for completeness to benchmark our approach against previous work. However, we are primarily interested in the slightly different problem of relating words with similar auditory instantions that may or may not be semantically related as opposed to relating semantically similar words that can be associated with some common auditory signal.",
        "output": "{\"label\": [\"BACK\", \"USE\"], \"context\": [\"AMEN and ASLex #TARGET_REF are subsets of the standard MEN (#REF) and SimLex (#REF) word similarity datasets consisting of word-pairs that \\\"can be associated with a distinctive associated sound\\\".\", \"We evaluate on this dataset for completeness to benchmark our approach against previous work.\"]}"
    },
    {
        "gold": {
            "text": [
                "AMEN and ASLex #TARGET_REF are subsets of the MEN and #REF9 datasets for word relatedness grounded in sound.",
                "From Table 2, we can see that our embeddings outperform (#REF) on both AMEN and ASLex.",
                "These datasets were curated by annotating concepts related by sound; however we observe that relatedness is often confounded.",
                "For example, (river, water), (automobile, car) are marked as aurally related however they do not stand out as aurally-related examples as they are already semantically related.",
                "In contrast, we are interested in how onomatopoeic words relate to regular words (Table 3) , which we study by explicit grounding in sound."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "AMEN and ASLex #TARGET_REF are subsets of the MEN and #REF9 datasets for word relatedness grounded in sound. From Table 2, we can see that our embeddings outperform (#REF) on both AMEN and ASLex. These datasets were curated by annotating concepts related by sound; however we observe that relatedness is often confounded. For example, (river, water), (automobile, car) are marked as aurally related however they do not stand out as aurally-related examples as they are already semantically related. In contrast, we are interested in how onomatopoeic words relate to regular words (Table 3) , which we study by explicit grounding in sound.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"AMEN and ASLex #TARGET_REF are subsets of the MEN and #REF9 datasets for word relatedness grounded in sound.\"]}"
    },
    {
        "gold": {
            "text": [
                "Freesound.",
                "We use the freesound database (#REF) , also used in prior work #TARGET_REF; Lopopolo and #REF) to learn the proposed sound-word2vec embeddings.",
                "Freesound is a freely available, collaborative dataset consisting of user uploaded sounds permitting reuse.",
                "All uploaded sounds have human descriptions in the form of tags and captions in natural language.",
                "The tags contain a broad set of relevant topics for a sound (e.g., ambience, electronic, birds, city, reverb) and captions describing the content of the sound, in addition to details pertaining to audio quality."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Freesound. We use the freesound database (#REF) , also used in prior work #TARGET_REF; Lopopolo and #REF) to learn the proposed sound-word2vec embeddings. Freesound is a freely available, collaborative dataset consisting of user uploaded sounds permitting reuse. All uploaded sounds have human descriptions in the form of tags and captions in natural language. The tags contain a broad set of relevant topics for a sound (e.g., ambience, electronic, birds, city, reverb) and captions describing the content of the sound, in addition to details pertaining to audio quality.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use the freesound database (#REF) , also used in prior work #TARGET_REF; Lopopolo and #REF) to learn the proposed sound-word2vec embeddings.\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare against previous works Lopopolo and #REF and #REF .",
                "While the former uses a standard bag of words and SVD pipeline to arrive at distributional representations for words, the latter trains under a joint objective that respects both linguistic and auditory similarity.",
                "We use the openly available implementation for Lopopolo and #REF and re-implement #TARGET_REF and train them on our dataset for a fair comparison of the methods.",
                "In addition, we show a comparison to word-vectors released by (#REF) in the supplementary material.",
                "All approaches use an embedding size of 300 for consistency."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We compare against previous works Lopopolo and #REF and #REF . While the former uses a standard bag of words and SVD pipeline to arrive at distributional representations for words, the latter trains under a joint objective that respects both linguistic and auditory similarity. We use the openly available implementation for Lopopolo and #REF and re-implement #TARGET_REF and train them on our dataset for a fair comparison of the methods. In addition, we show a comparison to word-vectors released by (#REF) in the supplementary material. All approaches use an embedding size of 300 for consistency.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use the openly available implementation for Lopopolo and #REF and re-implement #TARGET_REF and train them on our dataset for a fair comparison of the methods.\"]}"
    },
    {
        "gold": {
            "text": [
                "We fine-tune on a subset of 9578 tags which are present in both Freesound as well as Google news corpus datasets, which is 55.68% of the original tags in the Freesound dataset.",
                "This helps us remove noisy tags unrelated to the content of the sound.",
                "In addition to enlarging the vocabulary, the pretraining helps induce smoothness in the soundword2vec embeddings -allowing us to transfer semantics learnt from sounds to words that were not present as tags in the Freesound database.",
                "Indeed, we find that word2vec pre-training helps improve performance (Sec. 5.3).",
                "Our use of language embeddings as an initialization to fine-tune (specialize) from, as opposed to formulating a joint objective with language and audio context #TARGET_REF is driven by the fact that we are interested in embeddings for words grounded in sounds, and not better generic word similarity."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "We fine-tune on a subset of 9578 tags which are present in both Freesound as well as Google news corpus datasets, which is 55.68% of the original tags in the Freesound dataset. This helps us remove noisy tags unrelated to the content of the sound. In addition to enlarging the vocabulary, the pretraining helps induce smoothness in the soundword2vec embeddings -allowing us to transfer semantics learnt from sounds to words that were not present as tags in the Freesound database. Indeed, we find that word2vec pre-training helps improve performance (Sec. 5.3). Our use of language embeddings as an initialization to fine-tune (specialize) from, as opposed to formulating a joint objective with language and audio context #TARGET_REF is driven by the fact that we are interested in embeddings for words grounded in sounds, and not better generic word similarity.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Our use of language embeddings as an initialization to fine-tune (specialize) from, as opposed to formulating a joint objective with language and audio context #TARGET_REF is driven by the fact that we are interested in embeddings for words grounded in sounds, and not better generic word similarity.\"]}"
    },
    {
        "gold": {
            "text": [
                "1 shows that our sound-word2vec embeddings outperform the baselines.",
                "We see that specializing the embeddings for sound using our two-stage training outperforms prior work #TARGET_REF and Lopopolo and #REF ), which did not do specialization.",
                "Among our approaches, tag-word2vec performs second best -this is intuitive since the tag distributions implicitly capture auditory relatedness (a sound may have tags cat and meow), while word2vec and sound-word2vec(r) have the lowest performance.",
                "(#REF) (higher is better).",
                "Our approach performs better than #REF ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "1 shows that our sound-word2vec embeddings outperform the baselines. We see that specializing the embeddings for sound using our two-stage training outperforms prior work #TARGET_REF and Lopopolo and #REF ), which did not do specialization. Among our approaches, tag-word2vec performs second best -this is intuitive since the tag distributions implicitly capture auditory relatedness (a sound may have tags cat and meow), while word2vec and sound-word2vec(r) have the lowest performance. (#REF) (higher is better). Our approach performs better than #REF .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We see that specializing the embeddings for sound using our two-stage training outperforms prior work #TARGET_REF and Lopopolo and #REF ), which did not do specialization.\"]}"
    },
    {
        "gold": {
            "text": [
                "1 shows that our sound-word2vec embeddings outperform the baselines.",
                "We see that specializing the embeddings for sound using our two-stage training outperforms prior work (#REF and Lopopolo and #REF ), which did not do specialization.",
                "Among our approaches, tag-word2vec performs second best -this is intuitive since the tag distributions implicitly capture auditory relatedness (a sound may have tags cat and meow), while word2vec and sound-word2vec(r) have the lowest performance.",
                "(#REF) (higher is better).",
                "Our approach performs better than #TARGET_REF ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "1 shows that our sound-word2vec embeddings outperform the baselines. We see that specializing the embeddings for sound using our two-stage training outperforms prior work (#REF and Lopopolo and #REF ), which did not do specialization. Among our approaches, tag-word2vec performs second best -this is intuitive since the tag distributions implicitly capture auditory relatedness (a sound may have tags cat and meow), while word2vec and sound-word2vec(r) have the lowest performance. (#REF) (higher is better). Our approach performs better than #TARGET_REF .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Our approach performs better than #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We find that Sound-word2vec performs the best with a mean rank of 34.6 compared to other baselines tag-word2vec (38.9), soundword2vec(r) (114.3) and word2vec (189.45).",
                "As observed previously, the second best performing approach is tag-word2vec.",
                "Lopopolo and #REF and #TARGET_REF perform worse than tag-word2vec with a mean rank of 48.4 and 42.1 respectively.",
                "Note that random chance gets a rank of (|V| + 1)/2 = 4789.5."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                1,
                1,
                1,
                0
            ]
        },
        "input": "We find that Sound-word2vec performs the best with a mean rank of 34.6 compared to other baselines tag-word2vec (38.9), soundword2vec(r) (114.3) and word2vec (189.45). As observed previously, the second best performing approach is tag-word2vec. Lopopolo and #REF and #TARGET_REF perform worse than tag-word2vec with a mean rank of 48.4 and 42.1 respectively. Note that random chance gets a rank of (|V| + 1)/2 = 4789.5.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We find that Sound-word2vec performs the best with a mean rank of 34.6 compared to other baselines tag-word2vec (38.9), soundword2vec(r) (114.3) and word2vec (189.45).\", \"As observed previously, the second best performing approach is tag-word2vec.\", \"Lopopolo and #REF and #TARGET_REF perform worse than tag-word2vec with a mean rank of 48.4 and 42.1 respectively.\"]}"
    },
    {
        "gold": {
            "text": [
                "From Table 2, we can see that our embeddings outperform #TARGET_REF on both AMEN and ASLex.",
                "These datasets were curated by annotating concepts related by sound; however we observe that relatedness is often confounded.",
                "For example, (river, water), (automobile, car) are marked as aurally related however they do not stand out as aurally-related examples as they are already semantically related.",
                "In contrast, we are interested in how onomatopoeic words relate to regular words (Table 3) , which we study by explicit grounding in sound.",
                "Thus while we show competitive performance on this dataset, it might not be best suited for studying the benefits of our approach."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "From Table 2, we can see that our embeddings outperform #TARGET_REF on both AMEN and ASLex. These datasets were curated by annotating concepts related by sound; however we observe that relatedness is often confounded. For example, (river, water), (automobile, car) are marked as aurally related however they do not stand out as aurally-related examples as they are already semantically related. In contrast, we are interested in how onomatopoeic words relate to regular words (Table 3) , which we study by explicit grounding in sound. Thus while we show competitive performance on this dataset, it might not be best suited for studying the benefits of our approach.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"From Table 2, we can see that our embeddings outperform #TARGET_REF on both AMEN and ASLex.\"]}"
    },
    {
        "gold": {
            "text": [
                "Representations based on the Fourier transform have proven effective at this task as the transform forms a complete basis for signal reconstruction.",
                "Deep learning's recent success in speech recognition is based on learning feature hierarchies atop these representations [1, #TARGET_REF .",
                "There has been increasing focus on extending this end-toend learning approach down to the level of the raw waveform.",
                "A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8] .",
                "While some studies find inferior performance for convolutional filters learned in this way, deeper networks have recently matched the performance of hand-engineered features on large vocabulary speech recognition tasks [4] ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Representations based on the Fourier transform have proven effective at this task as the transform forms a complete basis for signal reconstruction. Deep learning's recent success in speech recognition is based on learning feature hierarchies atop these representations [1, #TARGET_REF . There has been increasing focus on extending this end-toend learning approach down to the level of the raw waveform. A popular approach is pass the waveform through strided convolutions, or networks connected to local temporal frames, often followed by a pooling step to create invariance to phase shifts and further downsample the signal [3, 4, 5, 6, 7, 8] . While some studies find inferior performance for convolutional filters learned in this way, deeper networks have recently matched the performance of hand-engineered features on large vocabulary speech recognition tasks [4] .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Deep learning's recent success in speech recognition is based on learning feature hierarchies atop these representations [1, #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The experimental design of this study is modelled after our previous work on end-to-end speech recognition [12, #TARGET_REF .",
                "However, to decrease the experimental latency, we train on a reduced version of the model and a subset of the training data.",
                "The basic architecture is shown in Table 1 .",
                "While we vary the front end processing, the backend remains the same: a convolutional (through time) layer, followed by three bidirectional simple recurrent layers, and a fully connected layer.",
                "Batch normalization [13] , is employed between each layer, but not between individual timesteps [2] ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The experimental design of this study is modelled after our previous work on end-to-end speech recognition [12, #TARGET_REF . However, to decrease the experimental latency, we train on a reduced version of the model and a subset of the training data. The basic architecture is shown in Table 1 . While we vary the front end processing, the backend remains the same: a convolutional (through time) layer, followed by three bidirectional simple recurrent layers, and a fully connected layer. Batch normalization [13] , is employed between each layer, but not between individual timesteps [2] .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The experimental design of this study is modelled after our previous work on end-to-end speech recognition [12, #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "However, to decrease the experimental latency, we train on a reduced version of the model and a subset of the training data.",
                "The basic architecture is shown in Table 1 .",
                "While we vary the front end processing, the backend remains the same: a convolutional (through time) layer, followed by three bidirectional simple recurrent layers, and a fully connected layer.",
                "Batch normalization [13] , is employed between each layer, but not between individual timesteps #TARGET_REF .",
                "Rectified linear unit (ReLU) activation functions are used for all layers, including between timesteps."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "However, to decrease the experimental latency, we train on a reduced version of the model and a subset of the training data. The basic architecture is shown in Table 1 . While we vary the front end processing, the backend remains the same: a convolutional (through time) layer, followed by three bidirectional simple recurrent layers, and a fully connected layer. Batch normalization [13] , is employed between each layer, but not between individual timesteps #TARGET_REF . Rectified linear unit (ReLU) activation functions are used for all layers, including between timesteps.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Batch normalization [13] , is employed between each layer, but not between individual timesteps #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Rectified linear unit (ReLU) activation functions are used for all layers, including between timesteps.",
                "We use the Connectionist Temporal Classification (CTC) cost function to integrate over all possible alignments between the network outputs and characters of the English alphabet [14] .",
                "Training is conducted on 2,400 hours of audio randomly sampled from 12,000 hours of data.",
                "The training data is drawn from a diverse collection of sources including read, conversational, accented, and noisy speech #TARGET_REF .",
                "At each epoch, 40% of the utterances are randomly selected to have background noise Table 2 : Single scale waveform convolution outperforms the spectrogram baseline at low strides."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Rectified linear unit (ReLU) activation functions are used for all layers, including between timesteps. We use the Connectionist Temporal Classification (CTC) cost function to integrate over all possible alignments between the network outputs and characters of the English alphabet [14] . Training is conducted on 2,400 hours of audio randomly sampled from 12,000 hours of data. The training data is drawn from a diverse collection of sources including read, conversational, accented, and noisy speech #TARGET_REF . At each epoch, 40% of the utterances are randomly selected to have background noise Table 2 : Single scale waveform convolution outperforms the spectrogram baseline at low strides.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The training data is drawn from a diverse collection of sources including read, conversational, accented, and noisy speech #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Hyperparameters are tuned for each model by optimizing a hold-out set.",
                "Typical values are a learning rate of 3e-4 and momentum of 0.99, and training converges after 20 epochs.",
                "Following #TARGET_REF , we sort the first epoch by utterance length (SortaGrad), to promote stability of training on long utterances.",
                "While the CTC-trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement it with a Kneser-Ney smoothed 5-gram model that is trained using the KenLM toolkit [15] on cleaned text from the Common Crawl Repository.",
                "Decoding is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Hyperparameters are tuned for each model by optimizing a hold-out set. Typical values are a learning rate of 3e-4 and momentum of 0.99, and training converges after 20 epochs. Following #TARGET_REF , we sort the first epoch by utterance length (SortaGrad), to promote stability of training on long utterances. While the CTC-trained acoustic model learns a rudimentary language model itself from the training data, for testing, we supplement it with a Kneser-Ney smoothed 5-gram model that is trained using the KenLM toolkit [15] on cleaned text from the Common Crawl Repository. Decoding is done via beam search, where a weighted combination of the acoustic model and language model with an added word insert penalty is used as the value function.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following #TARGET_REF , we sort the first epoch by utterance length (SortaGrad), to promote stability of training on long utterances.\"]}"
    },
    {
        "gold": {
            "text": [
                "Test set word error rates (WER) are reported on a difficult in-house test set of 2048 utterances, diversely composed of noisy, conversational, voice-command, and accented speech.",
                "The test set is collected internally and from industry partners and is not represented in the training data.",
                "As previously observed #TARGET_REF , deep neural networks trained on sufficient data perform better as the model size grows.",
                "In order to make fair comparisons, the number of parameters of the models used in our experiments is held constant at 35M.",
                "We are aware that the results are not directly comparable to literature due to the use of proprietary datasets."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Test set word error rates (WER) are reported on a difficult in-house test set of 2048 utterances, diversely composed of noisy, conversational, voice-command, and accented speech. The test set is collected internally and from industry partners and is not represented in the training data. As previously observed #TARGET_REF , deep neural networks trained on sufficient data perform better as the model size grows. In order to make fair comparisons, the number of parameters of the models used in our experiments is held constant at 35M. We are aware that the results are not directly comparable to literature due to the use of proprietary datasets.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"As previously observed #TARGET_REF , deep neural networks trained on sufficient data perform better as the model size grows.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF found noticeable improvements from supplementing log-mel filterbanks in such a manner.",
                "While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs #TARGET_REF 16] .",
                "Our learned features underperform in this context, which is understandable as they are not spectrally ordered, and lack spatial structure.",
                "Regularization techniques such as [17] could perhaps be key to learning ordered filter maps with useful structure.",
                "In our experiments, we made sure to downsample each scale equally with appropriate stride such that the signals can be concatenated for the later recurrent layers."
            ],
            "label": [
                "SIM",
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "#REF found noticeable improvements from supplementing log-mel filterbanks in such a manner. While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs #TARGET_REF 16] . Our learned features underperform in this context, which is understandable as they are not spectrally ordered, and lack spatial structure. Regularization techniques such as [17] could perhaps be key to learning ordered filter maps with useful structure. In our experiments, we made sure to downsample each scale equally with appropriate stride such that the signals can be concatenated for the later recurrent layers.",
        "output": "{\"label\": [\"SIM\", \"DIFFER\"], \"context\": [\"While learned features outperformed spectrograms feeding into temporal convolution in this study, many state of the art systems apply two-dimensional convolutions to their inputs #TARGET_REF 16] .\", \"Our learned features underperform in this context, which is understandable as they are not spectrally ordered, and lack spatial structure.\"]}"
    },
    {
        "gold": {
            "text": [
                "This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (#REF; #REF) to document summarization (#REF ), text classification (#REF; #REF) , text segmentation (#REF) , information extraction (#REF) , image caption generation (#REF; #REF) , sentence generation ( #TARGET_REF) , dialogue control (#REF; #REFa) , sentiment classification, recommendation system, question answering (#REF) and machine translation , to name a few.",
                "Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model.",
                "The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.",
                "The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.",
                "This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #REF) , memory-augmented neural network (#REF; #REF) , stochastic neural network #REF) , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (#REF; #REF) to document summarization (#REF ), text classification (#REF; #REF) , text segmentation (#REF) , information extraction (#REF) , image caption generation (#REF; #REF) , sentence generation ( #TARGET_REF) , dialogue control (#REF; #REFa) , sentiment classification, recommendation system, question answering (#REF) and machine translation , to name a few. Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model. The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs. The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference. This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #REF) , memory-augmented neural network (#REF; #REF) , stochastic neural network #REF) , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (#REF; #REF) to document summarization (#REF ), text classification (#REF; #REF) , text segmentation (#REF) , information extraction (#REF) , image caption generation (#REF; #REF) , sentence generation ( #TARGET_REF) , dialogue control (#REF; #REFa) , sentiment classification, recommendation system, question answering (#REF) and machine translation , to name a few.\"]}"
    },
    {
        "gold": {
            "text": [
                "dos #REF conducts experiments on NER for Portuguese and Spanish.",
                "Most work uses the development set to select hyperparameters (#REF; #REF) , while others add development set into training set (#REF; #REF) .",
                "Reimers and Gurevych (2017b) use a smaller dataset (13862 vs 14987 sentences).",
                "Different from #REF and #TARGET_REF , choose a different data split on the POS dataset.",
                "#REF and #REF use different development sets for chunking."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "dos #REF conducts experiments on NER for Portuguese and Spanish. Most work uses the development set to select hyperparameters (#REF; #REF) , while others add development set into training set (#REF; #REF) . Reimers and Gurevych (2017b) use a smaller dataset (13862 vs 14987 sentences). Different from #REF and #TARGET_REF , choose a different data split on the POS dataset. #REF and #REF use different development sets for chunking.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"Different from #REF and #TARGET_REF , choose a different data split on the POS dataset.\"]}"
    },
    {
        "gold": {
            "text": [
                "The NER dataset has been standardly split in Tjong Kim Sang and De Meulder (2003 (#REF; #REF; #REF; #TARGET_REF , we adopt the standard splits by using sections 0-18 as training set, sections 19-21 as development set and sections 22-24 as test set.",
                "No preprocessing is performed on either dataset except for normalizing digits.",
                "The dataset statistics are listed in Table 2 .",
                "Hyperparameters.",
                "Table 3 shows the hyperparameters used in our experiments, which mostly follow #REF , including the learning rate η = 0.015 for word LSTM models."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The NER dataset has been standardly split in Tjong Kim Sang and De Meulder (2003 (#REF; #REF; #REF; #TARGET_REF , we adopt the standard splits by using sections 0-18 as training set, sections 19-21 as development set and sections 22-24 as test set. No preprocessing is performed on either dataset except for normalizing digits. The dataset statistics are listed in Table 2 . Hyperparameters. Table 3 shows the hyperparameters used in our experiments, which mostly follow #REF , including the learning rate η = 0.015 for word LSTM models.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The NER dataset has been standardly split in Tjong Kim Sang and De Meulder (2003 (#REF; #REF; #REF; #TARGET_REF , we adopt the standard splits by using sections 0-18 as training set, sections 19-21 as development set and sections 22-24 as test set.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF search for the hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters.",
                "However, existing models use different parameter settings, which affects the fair comparison.",
                "• Evaluation.",
                "Some literature reports results using mean and standard deviation under different random seeds (#REF; #REF; #TARGET_REF .",
                "Others report the best result among different trials (#REF) , which cannot be compared directly."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "#REF search for the hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters. However, existing models use different parameter settings, which affects the fair comparison. • Evaluation. Some literature reports results using mean and standard deviation under different random seeds (#REF; #REF; #TARGET_REF . Others report the best result among different trials (#REF) , which cannot be compared directly.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Some literature reports results using mean and standard deviation under different random seeds (#REF; #REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF was the first to exploit LSTM for sequence labeling.",
                "built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (#REF; #TARGET_REF , GRU (#REF) , and CNN (#REF; #REF) features.",
                "Yang et al. (2017a) proposed a neural reranking model to improve NER models.",
                "These models achieve state-of-the-art results in the literature.",
                "Reimers and Gurevych (2017b) compared several word-based LSTM models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "#REF was the first to exploit LSTM for sequence labeling. built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (#REF; #TARGET_REF , GRU (#REF) , and CNN (#REF; #REF) features. Yang et al. (2017a) proposed a neural reranking model to improve NER models. These models achieve state-of-the-art results in the literature. Reimers and Gurevych (2017b) compared several word-based LSTM models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#REF was the first to exploit LSTM for sequence labeling.\", \"built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (#REF; #TARGET_REF , GRU (#REF) , and CNN (#REF; #REF) features.\"]}"
    },
    {
        "gold": {
            "text": [
                "Similar to character sequences in words, we can model word sequence information through LSTM or CNN structures.",
                "LSTM has been widely used in sequence labeling (#REF; #REF; #REF; #TARGET_REF .",
                "CNN can be much faster than LSTM due to the fact that convolution calculation can be parallel on the input sequence (#REF; dos #REF; #REF) .",
                "Word CNN.",
                "Figure 3(a) shows the multi-layer CNN on word sequence, where words are represented by embeddings."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Similar to character sequences in words, we can model word sequence information through LSTM or CNN structures. LSTM has been widely used in sequence labeling (#REF; #REF; #REF; #TARGET_REF . CNN can be much faster than LSTM due to the fact that convolution calculation can be parallel on the input sequence (#REF; dos #REF; #REF) . Word CNN. Figure 3(a) shows the multi-layer CNN on word sequence, where words are represented by embeddings.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"LSTM has been widely used in sequence labeling (#REF; #REF; #REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "This temporal tagger then contributes towards high performance at matching event mentions with the month and year in which they occurred based on the complete posting history of users.",
                "It does so with high accuracy on informal event mentions in social media by learning to integrate the likelihood of multiple candidate dates extracted from event mentions in timerich sentences with temporal constraints extracted from event-related sentences.",
                "Despite considerable prior work in temporal information extraction, to date state-of-the-art resources are designed for extracting temporally scoped facts about public figures/organizations from newswire or Wikipedia articles #TARGET_REF; Garrido et [11/15/2008] I have noticed some pulling recently and I won't start rads until March.",
                "[11/20/2008] It is sloowwwly healing, so slowly, in fact, that she said she HOPES it will be healed by March, when I am supposed to start rads.",
                "al., 2012)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "This temporal tagger then contributes towards high performance at matching event mentions with the month and year in which they occurred based on the complete posting history of users. It does so with high accuracy on informal event mentions in social media by learning to integrate the likelihood of multiple candidate dates extracted from event mentions in timerich sentences with temporal constraints extracted from event-related sentences. Despite considerable prior work in temporal information extraction, to date state-of-the-art resources are designed for extracting temporally scoped facts about public figures/organizations from newswire or Wikipedia articles #TARGET_REF; Garrido et [11/15/2008] I have noticed some pulling recently and I won't start rads until March. [11/20/2008] It is sloowwwly healing, so slowly, in fact, that she said she HOPES it will be healed by March, when I am supposed to start rads. al., 2012).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Despite considerable prior work in temporal information extraction, to date state-of-the-art resources are designed for extracting temporally scoped facts about public figures/organizations from newswire or Wikipedia articles #TARGET_REF; Garrido et [11/15/2008] I have noticed some pulling recently and I won't start rads until March.\"]}"
    },
    {
        "gold": {
            "text": [
                "Besides a standard event-time classifier for within-sentence event-time anchoring, we leverage a new source of temporal information to train a constraint-based event-time classifier.",
                "Previous work only retrieves time-rich sentences that include both the query and some TEs #TARGET_REF; #REF) .",
                "However, sentences that contain only the event mention but no explicit TE can also be informative.",
                "For example, the post time (usually referred to as document creation time or DCT) of the sentence \"metastasis was found in my bone\" might be labeled as being after the \"metastasis\" event date.",
                "These DCTs impose constraints on the possible event dates, which can be integrated with the event-time classifier, as a variant on related work (#REF) ."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Besides a standard event-time classifier for within-sentence event-time anchoring, we leverage a new source of temporal information to train a constraint-based event-time classifier. Previous work only retrieves time-rich sentences that include both the query and some TEs #TARGET_REF; #REF) . However, sentences that contain only the event mention but no explicit TE can also be informative. For example, the post time (usually referred to as document creation time or DCT) of the sentence \"metastasis was found in my bone\" might be labeled as being after the \"metastasis\" event date. These DCTs impose constraints on the possible event dates, which can be integrated with the event-time classifier, as a variant on related work (#REF) .",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Previous work only retrieves time-rich sentences that include both the query and some TEs #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous work only retrieves time-rich sentences (i.e., date sentences) (#REF; #TARGET_REF; #REF) .",
                "However, keyword sentences can inform temporal constraints for events and therefore should not be ignored.",
                "For example, \"Well, I'm officially a Radiation grad!\" indicates the user has done radiation by the time of the post (DCT).",
                "\"Radiation is not a choice for me.\" indicates the user probably never had radiation.",
                "The topic of the sentence can also indicate the temporal relation."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Previous work only retrieves time-rich sentences (i.e., date sentences) (#REF; #TARGET_REF; #REF) . However, keyword sentences can inform temporal constraints for events and therefore should not be ignored. For example, \"Well, I'm officially a Radiation grad!\" indicates the user has done radiation by the time of the post (DCT). \"Radiation is not a choice for me.\" indicates the user probably never had radiation. The topic of the sentence can also indicate the temporal relation.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Previous work only retrieves time-rich sentences (i.e., date sentences) (#REF; #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The extracted date is only considered correct if it completely matches the gold date.",
                "For less than 4% of users, we have multiple dates for the same event (e.g., a user had a mastectomy twice).",
                "Similar to the evaluation metric in a previous study , in these cases, we give the system the benefit of the doubt and the extracted date is considered correct if it matches one of the gold dates.",
                "In previous work #TARGET_REF; , the evaluation metric score is defined as 1/((1 + |d|)) where d is the difference between the values in years.",
                "We choose a much stricter evaluation metric because we need a precise event date to study user behavior changes."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "The extracted date is only considered correct if it completely matches the gold date. For less than 4% of users, we have multiple dates for the same event (e.g., a user had a mastectomy twice). Similar to the evaluation metric in a previous study , in these cases, we give the system the benefit of the doubt and the extracted date is considered correct if it matches one of the gold dates. In previous work #TARGET_REF; , the evaluation metric score is defined as 1/((1 + |d|)) where d is the difference between the values in years. We choose a much stricter evaluation metric because we need a precise event date to study user behavior changes.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"In previous work #TARGET_REF; , the evaluation metric score is defined as 1/((1 + |d|)) where d is the difference between the values in years.\", \"We choose a much stricter evaluation metric because we need a precise event date to study user behavior changes.\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous work on TE extraction has focused mainly on newswire text (#REF; #REF) .",
                "This paper presents a rule-based TE extractor that identifies and resolves a higher percentage of nonstandard TEs than earlier state-of-art temporal taggers.",
                "Our task is closest to the temporal slot filling track in the TAC-KBP 2011 shared task and timelining task #TARGET_REF .",
                "Their goal was to extract the temporal bounds of event relations.",
                "Our task has two key differences."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Previous work on TE extraction has focused mainly on newswire text (#REF; #REF) . This paper presents a rule-based TE extractor that identifies and resolves a higher percentage of nonstandard TEs than earlier state-of-art temporal taggers. Our task is closest to the temporal slot filling track in the TAC-KBP 2011 shared task and timelining task #TARGET_REF . Their goal was to extract the temporal bounds of event relations. Our task has two key differences.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Our task is closest to the temporal slot filling track in the TAC-KBP 2011 shared task and timelining task #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We train a MaxEnt classifier to predict the temporal relationship between the retrieved TE and the event date as overlap or no-overlap, similar to the within-sentence event-time anchoring task in TempEval-2 (#REF).",
                "Features for the classifier include many of those in #TARGET_REF; #REF ): namely, event keyword and its dominant verb, verb and preposition that dominate TE, dependency path between TE and keyword and its length, unigram and bigram word and POS features.",
                "New features include the Event-Subject, Negative and Modality features.",
                "In online support groups, users not only tell stories about themselves, they also share other patients' stories (as shown in Figure 1 ).",
                "So we add subject features to remove this kind of noise, which includes the governing subject of the event keyword and its POS tag."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We train a MaxEnt classifier to predict the temporal relationship between the retrieved TE and the event date as overlap or no-overlap, similar to the within-sentence event-time anchoring task in TempEval-2 (#REF). Features for the classifier include many of those in #TARGET_REF; #REF ): namely, event keyword and its dominant verb, verb and preposition that dominate TE, dependency path between TE and keyword and its length, unigram and bigram word and POS features. New features include the Event-Subject, Negative and Modality features. In online support groups, users not only tell stories about themselves, they also share other patients' stories (as shown in Figure 1 ). So we add subject features to remove this kind of noise, which includes the governing subject of the event keyword and its POS tag.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Features for the classifier include many of those in #TARGET_REF; #REF ): namely, event keyword and its dominant verb, verb and preposition that dominate TE, dependency path between TE and keyword and its length, unigram and bigram word and POS features.\"]}"
    },
    {
        "gold": {
            "text": [
                "The goal of an EL approach is as follows: Given a piece of text, a reference knowledge base K and a set of entity mentions in that text, map each entity mention to the corresponding resource in K #TARGET_REF .",
                "A large number of challenges has to be addressed while performing a disambiguation.",
                "For instance, a given resource can be referred to using different labels due to phenomena such as synonymy, acronyms or typos.",
                "For example, New York City, NY and Big Apple are all labels for the same entity.",
                "Also, multiple entities can share the same name due to homonymy and ambiguity."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The goal of an EL approach is as follows: Given a piece of text, a reference knowledge base K and a set of entity mentions in that text, map each entity mention to the corresponding resource in K #TARGET_REF . A large number of challenges has to be addressed while performing a disambiguation. For instance, a given resource can be referred to using different labels due to phenomena such as synonymy, acronyms or typos. For example, New York City, NY and Big Apple are all labels for the same entity. Also, multiple entities can share the same name due to homonymy and ambiguity.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The goal of an EL approach is as follows: Given a piece of text, a reference knowledge base K and a set of entity mentions in that text, map each entity mention to the corresponding resource in K #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "During the online phase, the EL is carried out in two steps: 1) candidate generation and 2) disambiguation.",
                "The goal of the candidate generation step is to retrieve a tractable number of candidates for each mention.",
                "These candidates are later inserted into the disambiguation graph, which is used to determine the mapping between entities and mentions.",
                "MAG implements two graph-based algorithms to disambiguate entities, i.e., PageRank and HITS.",
                "Independently of the chosen graph algorithm, the highest candidate score among the set of candidates is chosen as correct disambiguation for a given mention #TARGET_REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "During the online phase, the EL is carried out in two steps: 1) candidate generation and 2) disambiguation. The goal of the candidate generation step is to retrieve a tractable number of candidates for each mention. These candidates are later inserted into the disambiguation graph, which is used to determine the mapping between entities and mentions. MAG implements two graph-based algorithms to disambiguate entities, i.e., PageRank and HITS. Independently of the chosen graph algorithm, the highest candidate score among the set of candidates is chosen as correct disambiguation for a given mention #TARGET_REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Independently of the chosen graph algorithm, the highest candidate score among the set of candidates is chosen as correct disambiguation for a given mention #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The current implementation offers HITS and PageRank as algorithms, algorithm=hits or algorithm =pagerank.",
                "-Search by Context -This boolean parameter provides a search of candidates using a context index #TARGET_REF .",
                "-Acronyms -This parameter enables a search by acronyms.",
                "In this case, MAG uses an additional index to filter the acronyms by expanding their labels and assigns them a high probability.",
                "For example, PSG equals Paris Saint-Germain."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The current implementation offers HITS and PageRank as algorithms, algorithm=hits or algorithm =pagerank. -Search by Context -This boolean parameter provides a search of candidates using a context index #TARGET_REF . -Acronyms -This parameter enables a search by acronyms. In this case, MAG uses an additional index to filter the acronyms by expanding their labels and assigns them a high probability. For example, PSG equals Paris Saint-Germain.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"-Search by Context -This boolean parameter provides a search of candidates using a context index #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "However, MAG (Multilingual AGDISTIS) #TARGET_REF showed that the underlying models being trained on English corpora make them prone to failure when migrated to a different language.",
                "Additionally, these approaches hardly make their models or data available on more than three languages [6] .",
                "The new version of MAG (which is the quintessence of this demo) provides support for 40 different languages using sophisticated indices 4 .",
                "For the sake of server space, we deployed MAG-based web services for 9 languages and offer the other 31 languages for download.",
                "Additionally, we provide an English index using Wikidata to show the knowledge-base agnosticism of MAG."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "However, MAG (Multilingual AGDISTIS) #TARGET_REF showed that the underlying models being trained on English corpora make them prone to failure when migrated to a different language. Additionally, these approaches hardly make their models or data available on more than three languages [6] . The new version of MAG (which is the quintessence of this demo) provides support for 40 different languages using sophisticated indices 4 . For the sake of server space, we deployed MAG-based web services for 9 languages and offer the other 31 languages for download. Additionally, we provide an English index using Wikidata to show the knowledge-base agnosticism of MAG.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"However, MAG (Multilingual AGDISTIS) #TARGET_REF showed that the underlying models being trained on English corpora make them prone to failure when migrated to a different language.\"]}"
    },
    {
        "gold": {
            "text": [
                "The ultimate goal of \"grounded\" language learning is to develop computational systems that can acquire language more like a human child.",
                "Given only supervision in the form of sentences paired with relevant but ambiguous perceptual contexts, a system should learn to interpret and/or generate language describing situations and events in the world.",
                "For example, systems have learned to commentate simulated robot soccer games by learning from sample sportscasts (#REF; #REF; #TARGET_REF , or understand navigation instructions by learning from action traces produced when following the directions (#REF; #REF) .",
                "Börschinger et al. (2011) recently introduced an approach to grounded language learning using unsupervised induction of probabilistic context free grammars (PCFGs) to learn from ambiguous contextual supervision.",
                "Their approach first constructs a large set of production rules from sentences paired with descriptions of their ambiguous context, and then trains the parameters of this grammar using EM."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The ultimate goal of \"grounded\" language learning is to develop computational systems that can acquire language more like a human child. Given only supervision in the form of sentences paired with relevant but ambiguous perceptual contexts, a system should learn to interpret and/or generate language describing situations and events in the world. For example, systems have learned to commentate simulated robot soccer games by learning from sample sportscasts (#REF; #REF; #TARGET_REF , or understand navigation instructions by learning from action traces produced when following the directions (#REF; #REF) . Börschinger et al. (2011) recently introduced an approach to grounded language learning using unsupervised induction of probabilistic context free grammars (PCFGs) to learn from ambiguous contextual supervision. Their approach first constructs a large set of production rules from sentences paired with descriptions of their ambiguous context, and then trains the parameters of this grammar using EM.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"For example, systems have learned to commentate simulated robot soccer games by learning from sample sportscasts (#REF; #REF; #TARGET_REF , or understand navigation instructions by learning from action traces produced when following the directions (#REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "A number of approaches (#REF; #REF; #REF; #TARGET_REF assume training data consisting of a set of sentences each associated with a small set of MRs, one of which is usually the correct meaning of the sentence.",
                "Many of these approaches (#REF; #REF; #REF) disambiguate the data and match NL sentences to their correct MR by iteratively retraining a supervised semantic parser.",
                "proposed a generative semantic parsing model that first chooses which MRs to describe and then generates a hybrid tree structure (#REF) containing both the MR and NL sentence.",
                "They train this model on ambiguous data using EM.",
                "As previously discussed, Börschinger et al. (2011) use a PCFG generative model and also train it on ambiguous data using EM."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "A number of approaches (#REF; #REF; #REF; #TARGET_REF assume training data consisting of a set of sentences each associated with a small set of MRs, one of which is usually the correct meaning of the sentence. Many of these approaches (#REF; #REF; #REF) disambiguate the data and match NL sentences to their correct MR by iteratively retraining a supervised semantic parser. proposed a generative semantic parsing model that first chooses which MRs to describe and then generates a hybrid tree structure (#REF) containing both the MR and NL sentence. They train this model on ambiguous data using EM. As previously discussed, Börschinger et al. (2011) use a PCFG generative model and also train it on ambiguous data using EM.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"A number of approaches (#REF; #REF; #REF; #TARGET_REF assume training data consisting of a set of sentences each associated with a small set of MRs, one of which is usually the correct meaning of the sentence.\"]}"
    },
    {
        "gold": {
            "text": [
                "They train this model on ambiguous data using EM.",
                "As previously discussed, #TARGET_REF use a PCFG generative model and also train it on ambiguous data using EM.",
                "#REF assume each sentence maps to one or more semantic records (i.e. MRs) and trains a hierarchical semi-Markov generative model using EM, and then finds a Viterbi alignment between NL words and records and their constituents.",
                "Several recent projects (#REF; #REF) use NL instructions to guide reinforcement learning from independent exploration with delayed rewards.",
                "These systems do not even need the ambiguous supervision obtained from observing humans follow instructions; however, they do not learn semantic parsers that map sentences to complex, structural representations of their meaning."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "They train this model on ambiguous data using EM. As previously discussed, #TARGET_REF use a PCFG generative model and also train it on ambiguous data using EM. #REF assume each sentence maps to one or more semantic records (i.e. MRs) and trains a hierarchical semi-Markov generative model using EM, and then finds a Viterbi alignment between NL words and records and their constituents. Several recent projects (#REF; #REF) use NL instructions to guide reinforcement learning from independent exploration with delayed rewards. These systems do not even need the ambiguous supervision obtained from observing humans follow instructions; however, they do not learn semantic parsers that map sentences to complex, structural representations of their meaning.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"As previously discussed, #TARGET_REF use a PCFG generative model and also train it on ambiguous data using EM.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our approach extends that of #TARGET_REF , which in turn was inspired by a series of previous techniques (#REF; #REF; following the idea of constructing correspondences between NL and MR in a single probabilistic generative framework.",
                "Particularly, their approach automatically constructs a PCFG that generates NL sentences from MRs, which indicates how atomic MR constituents are probabilistically related to NL words.",
                "The nonterminals in the grammar correspond to complete MRs, MR constituents, and NL phrases.",
                "The nonterminal for a composite MR generates each of its MR constituents, and each atomic MR, x, generates an NL phrase, P hrase x .",
                "Each P hrase x then generates a sequence of W ord x 's for describing x, and each W ord x can generate each possible word in the natural language."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Our approach extends that of #TARGET_REF , which in turn was inspired by a series of previous techniques (#REF; #REF; following the idea of constructing correspondences between NL and MR in a single probabilistic generative framework. Particularly, their approach automatically constructs a PCFG that generates NL sentences from MRs, which indicates how atomic MR constituents are probabilistically related to NL words. The nonterminals in the grammar correspond to complete MRs, MR constituents, and NL phrases. The nonterminal for a composite MR generates each of its MR constituents, and each atomic MR, x, generates an NL phrase, P hrase x . Each P hrase x then generates a sequence of W ord x 's for describing x, and each W ord x can generate each possible word in the natural language.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"Our approach extends that of #TARGET_REF , which in turn was inspired by a series of previous techniques (#REF; #REF; following the idea of constructing correspondences between NL and MR in a single probabilistic generative framework.\"]}"
    },
    {
        "gold": {
            "text": [
                "The next step composes PCFG rules from the LHGs and is summarized in Figure 6 .",
                "We basically follow the scheme of #TARGET_REF , but instead of generating NL words from each atomic MR, words are generated from each lexeme MR, Figure 6 : Summary of the rule generation process.",
                "NLs refer to the set of NL words in the corpus.",
                "Lexeme rules come from the schemata of Börschinger et al. (2011) , and allow every lexeme MR to generate one or more NL words.",
                "Note that pseudo-lexeme nodes do not produce NL words."
            ],
            "label": [
                "EXT",
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The next step composes PCFG rules from the LHGs and is summarized in Figure 6 . We basically follow the scheme of #TARGET_REF , but instead of generating NL words from each atomic MR, words are generated from each lexeme MR, Figure 6 : Summary of the rule generation process. NLs refer to the set of NL words in the corpus. Lexeme rules come from the schemata of Börschinger et al. (2011) , and allow every lexeme MR to generate one or more NL words. Note that pseudo-lexeme nodes do not produce NL words.",
        "output": "{\"label\": [\"EXT\", \"DIFFER\"], \"context\": [\"We basically follow the scheme of #TARGET_REF , but instead of generating NL words from each atomic MR, words are generated from each lexeme MR, Figure 6 : Summary of the rule generation process.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our approach improves on #TARGET_REF 's method in the following ways:",
                "• The building blocks for associating NL and MR are semantic lexemes instead of atomic MR constituents.",
                "This prevents the number of constructed PCFG rules from becoming intractably large as happens with Börschinger et al.'s approach.",
                "As previously mentioned, lexeme MRs are intuitively analogous to syntactic categories in that complex lexeme MRs represent complicated semantic concepts whereas higher-level syntactic categories such as S, VP, or NP represent complex syntactic structures.",
                "• Our approach has the ability to produce previously unseen MRs, whereas Börschinger et al. can only generate an MR if it is explicitly included in the PCFG rules constructed from the training data."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Our approach improves on #TARGET_REF 's method in the following ways: • The building blocks for associating NL and MR are semantic lexemes instead of atomic MR constituents. This prevents the number of constructed PCFG rules from becoming intractably large as happens with Börschinger et al.'s approach. As previously mentioned, lexeme MRs are intuitively analogous to syntactic categories in that complex lexeme MRs represent complicated semantic concepts whereas higher-level syntactic categories such as S, VP, or NP represent complex syntactic structures. • Our approach has the ability to produce previously unseen MRs, whereas Börschinger et al. can only generate an MR if it is explicitly included in the PCFG rules constructed from the training data.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"Our approach improves on #TARGET_REF 's method in the following ways:\"]}"
    },
    {
        "gold": {
            "text": [
                "We have presented a novel method for learning a semantic parser given only highly ambiguous supervision.",
                "Our model enhances #TARGET_REF 's approach to reducing the problem of grounded learning of semantic parsers to PCFG induction.",
                "We use a learned semantic lexicon to aid the construction of a smaller and more focused set of PCFG productions.",
                "This allows the approach to scale to complex MR languages that define a large (potentially infinite) space of representations for capturing the meaning of sentences.",
                "By contrast, the previous PCFG approach requires a finite MR language and its grammar grows intractably large for even moderately complex MR languages."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We have presented a novel method for learning a semantic parser given only highly ambiguous supervision. Our model enhances #TARGET_REF 's approach to reducing the problem of grounded learning of semantic parsers to PCFG induction. We use a learned semantic lexicon to aid the construction of a smaller and more focused set of PCFG productions. This allows the approach to scale to complex MR languages that define a large (potentially infinite) space of representations for capturing the meaning of sentences. By contrast, the previous PCFG approach requires a finite MR language and its grammar grows intractably large for even moderately complex MR languages.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"Our model enhances #TARGET_REF 's approach to reducing the problem of grounded learning of semantic parsers to PCFG induction.\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the landmarks plans and the learned lexicon produced by #REF as inputs to our system.",
                "2 Like #TARGET_REF , our approach learns a semantic parser directly from ambiguous supervision, specifically NL instructions paired with their complete landmarks plans as context.",
                "Our method incorporates the semantic lexemes as building blocks to find correspondences between NL words and semantic concepts represented by the lexeme MRs, instead of building connections between NL words and every possible MR constituent as in Börschinger et al.'s approach.",
                "Particularly, we utilize the hierarchical subgraph relationships between the MRs in the learned semantic lexicon to produce a smaller, more focused set of PCFG rules.",
                "3 The intuition behind our approach is analogous to the hierarchical relations between nonterminals in syntactic parsing, where higher-level categories such as S, VP, or NP are further divided into smaller categories such as V, N, or Det, thereby forming a hierarchical structure."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We use the landmarks plans and the learned lexicon produced by #REF as inputs to our system. 2 Like #TARGET_REF , our approach learns a semantic parser directly from ambiguous supervision, specifically NL instructions paired with their complete landmarks plans as context. Our method incorporates the semantic lexemes as building blocks to find correspondences between NL words and semantic concepts represented by the lexeme MRs, instead of building connections between NL words and every possible MR constituent as in Börschinger et al.'s approach. Particularly, we utilize the hierarchical subgraph relationships between the MRs in the learned semantic lexicon to produce a smaller, more focused set of PCFG rules. 3 The intuition behind our approach is analogous to the hierarchical relations between nonterminals in syntactic parsing, where higher-level categories such as S, VP, or NP are further divided into smaller categories such as V, N, or Det, thereby forming a hierarchical structure.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"2 Like #TARGET_REF , our approach learns a semantic parser directly from ambiguous supervision, specifically NL instructions paired with their complete landmarks plans as context.\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 7 graphically depicts a sample trace of this algorithm.",
                "The algorithm recursively traverses the parse tree.",
                "When a leaf-node is reached, it marks all of the nodes in its MR.",
                "After traversing all of its children, 5 We used the implementation available at http://web.",
                "science.mq.edu.au/˜mjohnson/Software.htm which was also used by #TARGET_REF ."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "Figure 7 graphically depicts a sample trace of this algorithm. The algorithm recursively traverses the parse tree. When a leaf-node is reached, it marks all of the nodes in its MR. After traversing all of its children, 5 We used the implementation available at http://web. science.mq.edu.au/˜mjohnson/Software.htm which was also used by #TARGET_REF .",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"After traversing all of its children, 5 We used the implementation available at http://web.\", \"science.mq.edu.au/\\u02dcmjohnson/Software.htm which was also used by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We basically follow the scheme of Börschinger et al. (2011) , but instead of generating NL words from each atomic MR, words are generated from each lexeme MR, Figure 6 : Summary of the rule generation process.",
                "NLs refer to the set of NL words in the corpus.",
                "Lexeme rules come from the schemata of #TARGET_REF , and allow every lexeme MR to generate one or more NL words.",
                "Note that pseudo-lexeme nodes do not produce NL words.",
                "and smaller lexeme MRs are generated from more complex ones as given by the LHGs."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We basically follow the scheme of Börschinger et al. (2011) , but instead of generating NL words from each atomic MR, words are generated from each lexeme MR, Figure 6 : Summary of the rule generation process. NLs refer to the set of NL words in the corpus. Lexeme rules come from the schemata of #TARGET_REF , and allow every lexeme MR to generate one or more NL words. Note that pseudo-lexeme nodes do not produce NL words. and smaller lexeme MRs are generated from more complex ones as given by the LHGs.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Lexeme rules come from the schemata of #TARGET_REF , and allow every lexeme MR to generate one or more NL words.\"]}"
    },
    {
        "gold": {
            "text": [
                "For the last ten years, many methods have been proposed for the segmentation of texts in topically related units on the basis of lexical cohesion.",
                "The major distinction between these methods is in the contrast between the approaches based exclusively on the information contained in the text to be segmented, such as lexical repetition (e.g., #TARGET_REF; #REF; #REF; Kehagias, Pavlina, and #REF; #REF) , and those approaches that rest on complementary semantic knowledge extracted from dictionaries and thesauruses (e.g., #REF; #REF; #REF) , or from collocations collected in large corpora (#REF; Brants, Chen, and #REF; #REF; #REF; #REF; #REF) .",
                "According to their authors, methods that use additional knowledge allow for a solution to problems encountered when sentences belonging to a unique topic do not share common words due to the use of hyperonyms or synonyms and allow words that are semantically related to be taken as positive evidence for topic continuity.",
                "Empirical arguments in favor of these methods have been provided recently by #REF in a study using Latent Semantic Analysis (Latent Semantic Indexing, #REF ) to extract a semantic space from a corpus allowing determination of the similarity of meanings of words, sentences, or paragraphs.",
                "By comparing the accuracy of the very same algorithm according to whether or not it takes into account complementary semantic knowledge, they were able to show the benefit derived from such knowledge."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "For the last ten years, many methods have been proposed for the segmentation of texts in topically related units on the basis of lexical cohesion. The major distinction between these methods is in the contrast between the approaches based exclusively on the information contained in the text to be segmented, such as lexical repetition (e.g., #TARGET_REF; #REF; #REF; Kehagias, Pavlina, and #REF; #REF) , and those approaches that rest on complementary semantic knowledge extracted from dictionaries and thesauruses (e.g., #REF; #REF; #REF) , or from collocations collected in large corpora (#REF; Brants, Chen, and #REF; #REF; #REF; #REF; #REF) . According to their authors, methods that use additional knowledge allow for a solution to problems encountered when sentences belonging to a unique topic do not share common words due to the use of hyperonyms or synonyms and allow words that are semantically related to be taken as positive evidence for topic continuity. Empirical arguments in favor of these methods have been provided recently by #REF in a study using Latent Semantic Analysis (Latent Semantic Indexing, #REF ) to extract a semantic space from a corpus allowing determination of the similarity of meanings of words, sentences, or paragraphs. By comparing the accuracy of the very same algorithm according to whether or not it takes into account complementary semantic knowledge, they were able to show the benefit derived from such knowledge.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"The major distinction between these methods is in the contrast between the approaches based exclusively on the information contained in the text to be segmented, such as lexical repetition (e.g., #TARGET_REF; #REF; #REF; Kehagias, Pavlina, and #REF; #REF) , and those approaches that rest on complementary semantic knowledge extracted from dictionaries and thesauruses (e.g., #REF; #REF; #REF) , or from collocations collected in large corpora (#REF; Brants, Chen, and #REF; #REF; #REF; #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The present study shows, however, that the presence of the test materials in the LSA corpus has an important effect, but also that the generic semantic knowledge derived from large corpora clearly improves the segmentation accuracy.",
                "This conclusion is drawn from two experiments in which the presence or absence of the test materials in the LSA corpus is manipulated.",
                "The first experiment is based on the original materials from Choi et al., which consisted of a small corpus (1,000,000 words).",
                "The second experiment is based on a much larger corpus (25,000,000 words).",
                "Before reporting these experiments, #TARGET_REF and the use of LSA within this framework are described."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The present study shows, however, that the presence of the test materials in the LSA corpus has an important effect, but also that the generic semantic knowledge derived from large corpora clearly improves the segmentation accuracy. This conclusion is drawn from two experiments in which the presence or absence of the test materials in the LSA corpus is manipulated. The first experiment is based on the original materials from Choi et al., which consisted of a small corpus (1,000,000 words). The second experiment is based on a much larger corpus (25,000,000 words). Before reporting these experiments, #TARGET_REF and the use of LSA within this framework are described.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Before reporting these experiments, #TARGET_REF and the use of LSA within this framework are described.\"]}"
    },
    {
        "gold": {
            "text": [
                "The segmentation algorithm proposed by #TARGET_REF is made up of the three steps usually found in any segmentation procedure based on lexical cohesion.",
                "Firstly, the document to be segmented is divided into minimal textual units, usually sentences.",
                "Then, a similarity index between every pair of adjacent units is calculated.",
                "Each raw similarity value is cast on an ordinal scale by taking the proportion of neighboring values that are smaller than it.",
                "Lastly, the document is segmented recursively according to the boundaries between the units that maximize the sum of the average similarities inside the segments thus comprised (divisive clustering)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The segmentation algorithm proposed by #TARGET_REF is made up of the three steps usually found in any segmentation procedure based on lexical cohesion. Firstly, the document to be segmented is divided into minimal textual units, usually sentences. Then, a similarity index between every pair of adjacent units is calculated. Each raw similarity value is cast on an ordinal scale by taking the proportion of neighboring values that are smaller than it. Lastly, the document is segmented recursively according to the boundaries between the units that maximize the sum of the average similarities inside the segments thus comprised (divisive clustering).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The segmentation algorithm proposed by #TARGET_REF is made up of the three steps usually found in any segmentation procedure based on lexical cohesion.\"]}"
    },
    {
        "gold": {
            "text": [
                "Then, a similarity index between every pair of adjacent units is calculated.",
                "Each raw similarity value is cast on an ordinal scale by taking the proportion of neighboring values that are smaller than it.",
                "Lastly, the document is segmented recursively according to the boundaries between the units that maximize the sum of the average similarities inside the segments thus comprised (divisive clustering).",
                "The step of greatest interest here is the one that calculates the inter-sentence similarities.",
                "The procedure initially proposed by #TARGET_REF , C99, rests exclusively on the information contained in the text to be segmented."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Then, a similarity index between every pair of adjacent units is calculated. Each raw similarity value is cast on an ordinal scale by taking the proportion of neighboring values that are smaller than it. Lastly, the document is segmented recursively according to the boundaries between the units that maximize the sum of the average similarities inside the segments thus comprised (divisive clustering). The step of greatest interest here is the one that calculates the inter-sentence similarities. The procedure initially proposed by #TARGET_REF , C99, rests exclusively on the information contained in the text to be segmented.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The procedure initially proposed by #TARGET_REF , C99, rests exclusively on the information contained in the text to be segmented.\"]}"
    },
    {
        "gold": {
            "text": [
                "The step of greatest interest here is the one that calculates the inter-sentence similarities.",
                "The procedure initially proposed by #REF , C99, rests exclusively on the information contained in the text to be segmented.",
                "According to the vector space model, each sentence is represented by a vector of word frequency count, and the similarity between two sentences is calculated by means of the cosine measure between the corresponding vectors.",
                "In a first evaluation based on the procedure described below, #TARGET_REF showed that its algorithm outperforms several other approaches such as TextTiling (#REF) and Segmenter (Kan, Klavans, and #REF) .",
                "#REF claimed that it was possible to improve the inter-sentence similarities index by taking into account the semantic proximities between words estimated on the basis of Latent Semantic Analysis (LSA)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The step of greatest interest here is the one that calculates the inter-sentence similarities. The procedure initially proposed by #REF , C99, rests exclusively on the information contained in the text to be segmented. According to the vector space model, each sentence is represented by a vector of word frequency count, and the similarity between two sentences is calculated by means of the cosine measure between the corresponding vectors. In a first evaluation based on the procedure described below, #TARGET_REF showed that its algorithm outperforms several other approaches such as TextTiling (#REF) and Segmenter (Kan, Klavans, and #REF) . #REF claimed that it was possible to improve the inter-sentence similarities index by taking into account the semantic proximities between words estimated on the basis of Latent Semantic Analysis (LSA).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In a first evaluation based on the procedure described below, #TARGET_REF showed that its algorithm outperforms several other approaches such as TextTiling (#REF) and Segmenter (Kan, Klavans, and #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "This experiment was based on the procedure and test materials designed by #TARGET_REF , which was also used by several authors as a benchmark for comparing segmentation systems (#REF; #REF; #REF; #REF) .",
                "The task consists in finding the boundaries between concatenated texts.",
                "Each test sample is a concatenation of ten text segments.",
                "Each segment consisted in the first n sentences of a randomly selected text from two sub-sections of the Brown corpus.",
                "For the present experiment, I used the most general test materials built by #REF , in which the size of the segments within each sample varies randomly from 3 to 11 sentences."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "This experiment was based on the procedure and test materials designed by #TARGET_REF , which was also used by several authors as a benchmark for comparing segmentation systems (#REF; #REF; #REF; #REF) . The task consists in finding the boundaries between concatenated texts. Each test sample is a concatenation of ten text segments. Each segment consisted in the first n sentences of a randomly selected text from two sub-sections of the Brown corpus. For the present experiment, I used the most general test materials built by #REF , in which the size of the segments within each sample varies randomly from 3 to 11 sentences.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"This experiment was based on the procedure and test materials designed by #TARGET_REF , which was also used by several authors as a benchmark for comparing segmentation systems (#REF; #REF; #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The task consists in finding the boundaries between concatenated texts.",
                "Each test sample is a concatenation of ten text segments.",
                "Each segment consisted in the first n sentences of a randomly selected text from two sub-sections of the Brown corpus.",
                "For the present experiment, I used the most general test materials built by #TARGET_REF , in which the size of the segments within each sample varies randomly from 3 to 11 sentences.",
                "It is composed of 400 samples."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The task consists in finding the boundaries between concatenated texts. Each test sample is a concatenation of ten text segments. Each segment consisted in the first n sentences of a randomly selected text from two sub-sections of the Brown corpus. For the present experiment, I used the most general test materials built by #TARGET_REF , in which the size of the segments within each sample varies randomly from 3 to 11 sentences. It is composed of 400 samples.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For the present experiment, I used the most general test materials built by #TARGET_REF , in which the size of the segments within each sample varies randomly from 3 to 11 sentences.\"]}"
    },
    {
        "gold": {
            "text": [
                "The words on Choi's stoplist were removed, as were those that appeared only once in the whole corpus.",
                "Words were not stemmed, as in #REF .",
                "To build the LSA space, the singular value decomposition was realized using the program SVDPACKC (#REF; #REF) , and the first 300 singular vectors were retained.",
                "Concerning the segmentation algorithm, I used the version in which the number of boundaries to be found is imposed, and thus fixed at nine.",
                "An 11 × 11 rank mask was used for the ordinal transformation, as recommended by #TARGET_REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The words on Choi's stoplist were removed, as were those that appeared only once in the whole corpus. Words were not stemmed, as in #REF . To build the LSA space, the singular value decomposition was realized using the program SVDPACKC (#REF; #REF) , and the first 300 singular vectors were retained. Concerning the segmentation algorithm, I used the version in which the number of boundaries to be found is imposed, and thus fixed at nine. An 11 × 11 rank mask was used for the ordinal transformation, as recommended by #TARGET_REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"An 11 \\u00d7 11 rank mask was used for the ordinal transformation, as recommended by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The test materials were extracted from the 1997-1998 corpus following the guidelines given in #TARGET_REF .",
                "It is composed of 400 samples of ten segments, of which the length varies randomly from 3 to 11 sentences.",
                "Three types of LSA space were composed.",
                "The Within space is based on the whole 1997-1998 corpus.",
                "Four hundred different Without spaces were built as described in Experiment 1."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The test materials were extracted from the 1997-1998 corpus following the guidelines given in #TARGET_REF . It is composed of 400 samples of ten segments, of which the length varies randomly from 3 to 11 sentences. Three types of LSA space were composed. The Within space is based on the whole 1997-1998 corpus. Four hundred different Without spaces were built as described in Experiment 1.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The test materials were extracted from the 1997-1998 corpus following the guidelines given in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Several research work have been reported since 2010 in this research field of hate speech detection (#REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; Gambäck and #REF; #REF; #REF) .",
                "#REF & #REF reviewed the approaches used for hate speech detection.",
                "#REF used bag of words and bi-gram features with machine learning approach to classify the tweets as \"racist\" or \"nonracist\".",
                "#REF developed a supervised algorithm for hateful and antagonistic content in Twitter using voted ensemble meta-classifier.",
                "#REF learnt distributed low-dimensional representations of social media comments using neural language models for hate speech detection."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Several research work have been reported since 2010 in this research field of hate speech detection (#REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; Gambäck and #REF; #REF; #REF) . #REF & #REF reviewed the approaches used for hate speech detection. #REF used bag of words and bi-gram features with machine learning approach to classify the tweets as \"racist\" or \"nonracist\". #REF developed a supervised algorithm for hateful and antagonistic content in Twitter using voted ensemble meta-classifier. #REF learnt distributed low-dimensional representations of social media comments using neural language models for hate speech detection.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Several research work have been reported since 2010 in this research field of hate speech detection (#REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; Gamb\\u00e4ck and #REF; #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We have employed 2 layered bi-directional LSTM with Scaled Luong and Normed Bahdanau attention mechanisms to build the model for all the three sub tasks.",
                "The instances are vectorized using TF-IDF score for traditional machine learning models with minimum count two.",
                "The classifiers namely Multinomial Naive Bayes and Support Vector Machine with Stochastic Gradient Descent optimizer were employed to build the models for sub tasks B and C. Deep learning with Scaled Luong attention, deep learning with Normed Bahdanau attention, traditional machine learning with SVM give better results for Task A, Task B and Task C respectively.",
                "Our models outperform the base line for all the three tasks.",
                "The performance may be improved further by incorporating external datasets (#REFa; #TARGET_REF"
            ],
            "label": [
                "FUT"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "We have employed 2 layered bi-directional LSTM with Scaled Luong and Normed Bahdanau attention mechanisms to build the model for all the three sub tasks. The instances are vectorized using TF-IDF score for traditional machine learning models with minimum count two. The classifiers namely Multinomial Naive Bayes and Support Vector Machine with Stochastic Gradient Descent optimizer were employed to build the models for sub tasks B and C. Deep learning with Scaled Luong attention, deep learning with Normed Bahdanau attention, traditional machine learning with SVM give better results for Task A, Task B and Task C respectively. Our models outperform the base line for all the three tasks. The performance may be improved further by incorporating external datasets (#REFa; #TARGET_REF",
        "output": "{\"label\": [\"FUT\"], \"context\": [\"The performance may be improved further by incorporating external datasets (#REFa; #TARGET_REF\"]}"
    },
    {
        "gold": {
            "text": [
                "The goal is to make the text easier to understand for children, language learners, people with cognitive disabilities and even machines.",
                "Approaches to lexical simplification generally follow a standard pipeline consisting of two main steps: generation and ranking.",
                "In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (#REF) , learning substitution rules from sentence-aligned parallel corpora of complex-simple texts #TARGET_REF; #REF) , and learning word embeddings from a large corpora to obtain similar words of the complex word (Glavaš andŠtajner, 2015; #REF; #REFa, 2017) .",
                "In the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness.",
                "* This research was conducted while the first author was a Post Doctoral Fellow at the City University of Hong Kong."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The goal is to make the text easier to understand for children, language learners, people with cognitive disabilities and even machines. Approaches to lexical simplification generally follow a standard pipeline consisting of two main steps: generation and ranking. In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (#REF) , learning substitution rules from sentence-aligned parallel corpora of complex-simple texts #TARGET_REF; #REF) , and learning word embeddings from a large corpora to obtain similar words of the complex word (Glavaš andŠtajner, 2015; #REF; #REFa, 2017) . In the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness. * This research was conducted while the first author was a Post Doctoral Fellow at the City University of Hong Kong.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (#REF) , learning substitution rules from sentence-aligned parallel corpora of complex-simple texts #TARGET_REF; #REF) , and learning word embeddings from a large corpora to obtain similar words of the complex word (Glava\\u0161 and\\u0160tajner, 2015; #REF; #REFa, 2017) .\"]}"
    },
    {
        "gold": {
            "text": [
                "State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model #TARGET_REF; Bingel and Søgaard, 2016; #REFa, 2017) .",
                "Moreover, deep architectures are not explored in these models.",
                "Surface features and shallow architectures might not be able to explore the features at different levels of abstractions that capture nuances that discriminate the candidates.",
                "In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (#REF) to rank substitution candidates.",
                "The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sentences."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                1,
                1,
                0,
                1,
                0
            ]
        },
        "input": "State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model #TARGET_REF; Bingel and Søgaard, 2016; #REFa, 2017) . Moreover, deep architectures are not explored in these models. Surface features and shallow architectures might not be able to explore the features at different levels of abstractions that capture nuances that discriminate the candidates. In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (#REF) to rank substitution candidates. The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sentences.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model #TARGET_REF; Bingel and S\\u00f8gaard, 2016; #REFa, 2017) .\", \"Moreover, deep architectures are not explored in these models.\", \"In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (#REF) to rank substitution candidates.\"]}"
    },
    {
        "gold": {
            "text": [
                "Following previous works that used supervised machine learning for ranking in lexical simplification #TARGET_REF; #REF) , we train the DSSM using the LexMTurk dataset #TARGET_REF , which contains 500 instances composed of a sentence, a target word and substitution candidates ranked by simplicity (#REF) .",
                "In order to learn the parameters W t and W s (Figure 1 ) of the DSSM, we use the standard backpropagation algorithm (#REF) .",
                "The objective used in this paper follows the pair-wise learning-to-rank paradigm outlined in (#REF) .",
                "Given a target word and its sentential context T , we obtain a list of candidates L. We set different positive values to the candidates based on their simplicity rankings.",
                "E.g., if the list of the candidates is ordered by simplificity as, L = {A + > B + > C + }, the labels are first constructed as L = {y A + = 3, y B + = 2, y C + = 1}. The values are then normalized by dividing by the maximum value in the list: L = {y A + = 1, y B + = 0.667, y C + = 0.333}. If the target word was not originally in L, we add it with label 0."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Following previous works that used supervised machine learning for ranking in lexical simplification #TARGET_REF; #REF) , we train the DSSM using the LexMTurk dataset #TARGET_REF , which contains 500 instances composed of a sentence, a target word and substitution candidates ranked by simplicity (#REF) . In order to learn the parameters W t and W s (Figure 1 ) of the DSSM, we use the standard backpropagation algorithm (#REF) . The objective used in this paper follows the pair-wise learning-to-rank paradigm outlined in (#REF) . Given a target word and its sentential context T , we obtain a list of candidates L. We set different positive values to the candidates based on their simplicity rankings. E.g., if the list of the candidates is ordered by simplificity as, L = {A + > B + > C + }, the labels are first constructed as L = {y A + = 3, y B + = 2, y C + = 1}. The values are then normalized by dividing by the maximum value in the list: L = {y A + = 1, y B + = 0.667, y C + = 0.333}. If the target word was not originally in L, we add it with label 0.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following previous works that used supervised machine learning for ranking in lexical simplification #TARGET_REF; #REF) , we train the DSSM using the LexMTurk dataset #TARGET_REF , which contains 500 instances composed of a sentence, a target word and substitution candidates ranked by simplicity (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "To evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification: BenchLS (#REFb) , which contains 929 instances, and NNSEval (#REFa) , which contains 239 instances.",
                "Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity (#REF) .",
                "Since both datasets contain instances from the LexMturk dataset #TARGET_REF , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 .",
                "We finally obtain 429 remaining instances in the BenchLS dataset, and 78 instances in the NNEval dataset, which are used in our evaluation.",
                "We adopt the same evaluation metrics featured in Glavaš andŠtajner (2015) and #REF : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "To evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification: BenchLS (#REFb) , which contains 929 instances, and NNSEval (#REFa) , which contains 239 instances. Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity (#REF) . Since both datasets contain instances from the LexMturk dataset #TARGET_REF , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 . We finally obtain 429 remaining instances in the BenchLS dataset, and 78 instances in the NNEval dataset, which are used in our evaluation. We adopt the same evaluation metrics featured in Glavaš andŠtajner (2015) and #REF : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Since both datasets contain instances from the LexMturk dataset #TARGET_REF , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 .\"]}"
    },
    {
        "gold": {
            "text": [
                "To evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification: BenchLS (#REFb) , which contains 929 instances, and NNSEval (#REFa) , which contains 239 instances.",
                "Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity (#REF) .",
                "Since both datasets contain instances from the LexMturk dataset (#REF) , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 .",
                "We finally obtain 429 remaining instances in the BenchLS dataset, and 78 instances in the NNEval dataset, which are used in our evaluation.",
                "We adopt the same evaluation metrics featured in Glavaš andŠtajner (2015) and #TARGET_REF : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "To evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification: BenchLS (#REFb) , which contains 929 instances, and NNSEval (#REFa) , which contains 239 instances. Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity (#REF) . Since both datasets contain instances from the LexMturk dataset (#REF) , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 . We finally obtain 429 remaining instances in the BenchLS dataset, and 78 instances in the NNEval dataset, which are used in our evaluation. We adopt the same evaluation metrics featured in Glavaš andŠtajner (2015) and #TARGET_REF : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We adopt the same evaluation metrics featured in Glava\\u0161 and\\u0160tajner (2015) and #TARGET_REF : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system.\"]}"
    },
    {
        "gold": {
            "text": [
                "n-gram probs.",
                "denotes the n-gram probability features described in #REF , and all denotes all features described in Section 2.3.",
                "All values marked in bold are significantly higher compared to the best baseline, SVM rank , measured by t-test at p-value of 0.05.",
                "with default parameters) for ranking substitution candidates, similar to the method described in #TARGET_REF .",
                "All the three models employ the n-gram probability features extracted from the SubIMDB corpus (#REF) , as described in (#REF) , and are trained using the LexMTurk dataset."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "n-gram probs. denotes the n-gram probability features described in #REF , and all denotes all features described in Section 2.3. All values marked in bold are significantly higher compared to the best baseline, SVM rank , measured by t-test at p-value of 0.05. with default parameters) for ranking substitution candidates, similar to the method described in #TARGET_REF . All the three models employ the n-gram probability features extracted from the SubIMDB corpus (#REF) , as described in (#REF) , and are trained using the LexMTurk dataset.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"with default parameters) for ranking substitution candidates, similar to the method described in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "An aspect term is a word or a phrase that describes an entity's attribute or feature that is the target of an opinion.",
                "An opinion term is a word or a phrase that shows subjective emotion toward an attribute or feature of an entity.",
                "For example, in a hotel review \"Tempat tidur di hotel ini tidak bersih\" (The bed in this hotel is not clean), extraction process returns \"Tempat tidur\" (bed) as aspect term and \"tidak bersih\" (not clean) as opinion term.",
                "Aspect and/or opinion terms extraction research has been conducted by #REF and Xu et al. #TARGET_REF that outperformed the best systems in the aspect-based sentiment analysis task on the International Workshop on Semantic Evaluation (SemEval) for aspect and opinion terms extraction.",
                "#REF proposed a deep learning model for aspect and opinion terms extraction, named Coupled MultiLayer Attentions (CMLA), with word embedding as its feature."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "An aspect term is a word or a phrase that describes an entity's attribute or feature that is the target of an opinion. An opinion term is a word or a phrase that shows subjective emotion toward an attribute or feature of an entity. For example, in a hotel review \"Tempat tidur di hotel ini tidak bersih\" (The bed in this hotel is not clean), extraction process returns \"Tempat tidur\" (bed) as aspect term and \"tidak bersih\" (not clean) as opinion term. Aspect and/or opinion terms extraction research has been conducted by #REF and Xu et al. #TARGET_REF that outperformed the best systems in the aspect-based sentiment analysis task on the International Workshop on Semantic Evaluation (SemEval) for aspect and opinion terms extraction. #REF proposed a deep learning model for aspect and opinion terms extraction, named Coupled MultiLayer Attentions (CMLA), with word embedding as its feature.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Aspect and/or opinion terms extraction research has been conducted by #REF and Xu et al. #TARGET_REF that outperformed the best systems in the aspect-based sentiment analysis task on the International Workshop on Semantic Evaluation (SemEval) for aspect and opinion terms extraction.\"]}"
    },
    {
        "gold": {
            "text": [
                "The model is a multi-layer attention network, where each layer consists of a couple of attentions with tensor xoperators, one attention for aspect term extraction and the other for opinion term extraction.",
                "The model achieved F1-measure of 0.7073 and 0.7368 for aspect and opinion term extraction respectively using #REF task 12 subtask 1 restaurant dataset [4] .",
                "Xu et al. #TARGET_REF proposed a Convolutional Neural Network (CNN) model employing two types of pre-trained word embeddings, general-purpose embeddings and domainspecific embeddings, for aspect term extraction.",
                "The two embeddings are concatenated into one word embedding called double embeddings.",
                "The model achieved F1-measure of 0.7437 for aspect term extraction using #REF task 5 subtask 1 restaurant dataset [5] ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The model is a multi-layer attention network, where each layer consists of a couple of attentions with tensor xoperators, one attention for aspect term extraction and the other for opinion term extraction. The model achieved F1-measure of 0.7073 and 0.7368 for aspect and opinion term extraction respectively using #REF task 12 subtask 1 restaurant dataset [4] . Xu et al. #TARGET_REF proposed a Convolutional Neural Network (CNN) model employing two types of pre-trained word embeddings, general-purpose embeddings and domainspecific embeddings, for aspect term extraction. The two embeddings are concatenated into one word embedding called double embeddings. The model achieved F1-measure of 0.7437 for aspect term extraction using #REF task 5 subtask 1 restaurant dataset [5] .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Xu et al. #TARGET_REF proposed a Convolutional Neural Network (CNN) model employing two types of pre-trained word embeddings, general-purpose embeddings and domainspecific embeddings, for aspect term extraction.\"]}"
    },
    {
        "gold": {
            "text": [
                "The two embeddings are concatenated into one word embedding called double embeddings.",
                "The model achieved F1-measure of 0.7437 for aspect term extraction using #REF task 5 subtask 1 restaurant dataset [5] .",
                "#REF and Xu et al. #TARGET_REF approaches have not been applied for Indonesian reviews.",
                "This paper aims to perform aspect and opinion terms extraction in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism [3] .",
                "The adaption in this paper is conducted by changing the English resources used in word embedding into Indonesian version."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The two embeddings are concatenated into one word embedding called double embeddings. The model achieved F1-measure of 0.7437 for aspect term extraction using #REF task 5 subtask 1 restaurant dataset [5] . #REF and Xu et al. #TARGET_REF approaches have not been applied for Indonesian reviews. This paper aims to perform aspect and opinion terms extraction in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism [3] . The adaption in this paper is conducted by changing the English resources used in word embedding into Indonesian version.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#REF and Xu et al. #TARGET_REF approaches have not been applied for Indonesian reviews.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF used attention mechanism [12] to identify the possibility of each token being an aspect or opinion term.",
                "The coupled multilayer attentions that was proposed by [2] models the relations among tokens automatically without any syntactic/dependency parsing or linguistic resources as additional information for the input and achieves good performance for aspect and opinion terms extraction.",
                "The coupled attentions are used to exploit the correlations between aspect and opinion terms using tensor operators [2] .",
                "Xu et al. #TARGET_REF use double embeddings that leverage both general embeddings and domain embeddings as a feature for a CNN model and let the CNN model decide which embeddings have more useful information.",
                "The experiment conducted in [3] demonstrated that double embedding mechanism achieved better performance for aspect terms extraction compared to the use of general embeddings or domain embeddings alone."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "#REF used attention mechanism [12] to identify the possibility of each token being an aspect or opinion term. The coupled multilayer attentions that was proposed by [2] models the relations among tokens automatically without any syntactic/dependency parsing or linguistic resources as additional information for the input and achieves good performance for aspect and opinion terms extraction. The coupled attentions are used to exploit the correlations between aspect and opinion terms using tensor operators [2] . Xu et al. #TARGET_REF use double embeddings that leverage both general embeddings and domain embeddings as a feature for a CNN model and let the CNN model decide which embeddings have more useful information. The experiment conducted in [3] demonstrated that double embedding mechanism achieved better performance for aspect terms extraction compared to the use of general embeddings or domain embeddings alone.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Xu et al. #TARGET_REF use double embeddings that leverage both general embeddings and domain embeddings as a feature for a CNN model and let the CNN model decide which embeddings have more useful information.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF used attention mechanism [12] to identify the possibility of each token being an aspect or opinion term.",
                "The coupled multilayer attentions that was proposed by [2] models the relations among tokens automatically without any syntactic/dependency parsing or linguistic resources as additional information for the input and achieves good performance for aspect and opinion terms extraction.",
                "The coupled attentions are used to exploit the correlations between aspect and opinion terms using tensor operators [2] .",
                "#REF use double embeddings that leverage both general embeddings and domain embeddings as a feature for a CNN model and let the CNN model decide which embeddings have more useful information.",
                "The experiment conducted in #TARGET_REF demonstrated that double embedding mechanism achieved better performance for aspect terms extraction compared to the use of general embeddings or domain embeddings alone."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "#REF used attention mechanism [12] to identify the possibility of each token being an aspect or opinion term. The coupled multilayer attentions that was proposed by [2] models the relations among tokens automatically without any syntactic/dependency parsing or linguistic resources as additional information for the input and achieves good performance for aspect and opinion terms extraction. The coupled attentions are used to exploit the correlations between aspect and opinion terms using tensor operators [2] . #REF use double embeddings that leverage both general embeddings and domain embeddings as a feature for a CNN model and let the CNN model decide which embeddings have more useful information. The experiment conducted in #TARGET_REF demonstrated that double embedding mechanism achieved better performance for aspect terms extraction compared to the use of general embeddings or domain embeddings alone.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The experiment conducted in #TARGET_REF demonstrated that double embedding mechanism achieved better performance for aspect terms extraction compared to the use of general embeddings or domain embeddings alone.\"]}"
    },
    {
        "gold": {
            "text": [
                "The two embeddings are concatenated into one word embedding called double embeddings.",
                "The model achieved F1-measure of 0.7437 for aspect term extraction using #REF task 5 subtask 1 restaurant dataset [5] .",
                "#REF and #REF approaches have not been applied for Indonesian reviews.",
                "This paper aims to perform aspect and opinion terms extraction in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism #TARGET_REF .",
                "The adaption in this paper is conducted by changing the English resources used in word embedding into Indonesian version."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The two embeddings are concatenated into one word embedding called double embeddings. The model achieved F1-measure of 0.7437 for aspect term extraction using #REF task 5 subtask 1 restaurant dataset [5] . #REF and #REF approaches have not been applied for Indonesian reviews. This paper aims to perform aspect and opinion terms extraction in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism #TARGET_REF . The adaption in this paper is conducted by changing the English resources used in word embedding into Indonesian version.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"This paper aims to perform aspect and opinion terms extraction in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "As stated previously, the goal of this work is to extract aspect and opinion terms in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism #TARGET_REF .",
                "The architecture of the model used in this work can be seen in Fig. 1 .",
                "Review texts are preprocessed to be used in training extraction model by using InaNLP [13] .",
                "The preprocess consists of sentence normalization, casefolding, and tokenization.",
                "Normalization is done because there are many informal words, abbreviations, and typos in the reviews."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "As stated previously, the goal of this work is to extract aspect and opinion terms in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism #TARGET_REF . The architecture of the model used in this work can be seen in Fig. 1 . Review texts are preprocessed to be used in training extraction model by using InaNLP [13] . The preprocess consists of sentence normalization, casefolding, and tokenization. Normalization is done because there are many informal words, abbreviations, and typos in the reviews.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"As stated previously, the goal of this work is to extract aspect and opinion terms in Indonesian hotel reviews by adapting CMLA architecture [2] and double embeddings mechanism #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We use 142810 reviews to train the domain embeddings.",
                "The hybrid embeddings use the combined corpus between Indonesian Wikipedia articles and Indonesian hotel reviews.",
                "All of the word embeddings are trained using fast#REF .",
                "For the general embeddings and domain embeddings, we use the same dimension and number of iterations as in #TARGET_REF .",
                "The embedding dimensions and number of iterations used to train the word embeddings can be seen in Table III ."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We use 142810 reviews to train the domain embeddings. The hybrid embeddings use the combined corpus between Indonesian Wikipedia articles and Indonesian hotel reviews. All of the word embeddings are trained using fast#REF . For the general embeddings and domain embeddings, we use the same dimension and number of iterations as in #TARGET_REF . The embedding dimensions and number of iterations used to train the word embeddings can be seen in Table III .",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"For the general embeddings and domain embeddings, we use the same dimension and number of iterations as in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In the experiment, we will try other variations of Recurrent Neural Network (RNN) to replace the Gated Recurrent Unit (GRU) used in CMLA.",
                "Specifically, we conduct experiment using GRU, LSTM, B-GRU, and B-LSTM and choose the one that gives the best performance based on the experiment as the final model.",
                "We implement and train the model using #REF .",
                "We use various types of word embeddings adapted from #TARGET_REF .",
                "Specifically, we conduct experiment using double embeddings, general embeddings, domain embeddings, and hybrid embeddings as the feature used by the model and choose the word embedding that gives the best performance as the feature used by the final model."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "In the experiment, we will try other variations of Recurrent Neural Network (RNN) to replace the Gated Recurrent Unit (GRU) used in CMLA. Specifically, we conduct experiment using GRU, LSTM, B-GRU, and B-LSTM and choose the one that gives the best performance based on the experiment as the final model. We implement and train the model using #REF . We use various types of word embeddings adapted from #TARGET_REF . Specifically, we conduct experiment using double embeddings, general embeddings, domain embeddings, and hybrid embeddings as the feature used by the model and choose the word embedding that gives the best performance as the feature used by the final model.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use various types of word embeddings adapted from #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "LID refers to the process of automatically identifying the language class for given speech segment or text document, while DID classifies between dialects within the same language class, making it a more challenging task than LID.",
                "A good DID system used as a front-end to an automatic speech recognition system, can help improve the recognition performance by providing dialectal data for acoustic and language model adaptation to the specific dialect being spoken [1] .",
                "In this work, we focus on Arabic DID which can can be posed as a five class classification problem, given that the Arabic language can be divided into five major dialects; Egyptian (EGY), Gulf (GLF), Lavantine (LAV), Modern Standard Arabic (MSA) and North African (NOR) #TARGET_REF .",
                "Over the past decade, great advances have been made in the field of automatic language identification (LID).",
                "Research effort has focused on coming up with mathematical representations of speech utterances, that encodes the information about the language being spoken."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "LID refers to the process of automatically identifying the language class for given speech segment or text document, while DID classifies between dialects within the same language class, making it a more challenging task than LID. A good DID system used as a front-end to an automatic speech recognition system, can help improve the recognition performance by providing dialectal data for acoustic and language model adaptation to the specific dialect being spoken [1] . In this work, we focus on Arabic DID which can can be posed as a five class classification problem, given that the Arabic language can be divided into five major dialects; Egyptian (EGY), Gulf (GLF), Lavantine (LAV), Modern Standard Arabic (MSA) and North African (NOR) #TARGET_REF . Over the past decade, great advances have been made in the field of automatic language identification (LID). Research effort has focused on coming up with mathematical representations of speech utterances, that encodes the information about the language being spoken.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"In this work, we focus on Arabic DID which can can be posed as a five class classification problem, given that the Arabic language can be divided into five major dialects; Egyptian (EGY), Gulf (GLF), Lavantine (LAV), Modern Standard Arabic (MSA) and North African (NOR) #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the same Deep Neural Network (DNN) based ASR system to extract the BNF as in our previous works #TARGET_REF 13] .",
                "Two DNNs are used with 5 hidden layers and 1 Bottleneck Layer, all having sigmoidal neurons.",
                "Tied-phone states are used as the target to the DNNs.",
                "The target labels of dimension 3040 are provided by a GMM-HMM baseline system trained on 60 hours of Arabic Broadcast speech [14] .",
                "Input to the DNN consists of 11 consecutive frames stacked together, where for each frame 23 fbank features along with pitch and voicing probability are extracted."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We use the same Deep Neural Network (DNN) based ASR system to extract the BNF as in our previous works #TARGET_REF 13] . Two DNNs are used with 5 hidden layers and 1 Bottleneck Layer, all having sigmoidal neurons. Tied-phone states are used as the target to the DNNs. The target labels of dimension 3040 are provided by a GMM-HMM baseline system trained on 60 hours of Arabic Broadcast speech [14] . Input to the DNN consists of 11 consecutive frames stacked together, where for each frame 23 fbank features along with pitch and voicing probability are extracted.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"We use the same Deep Neural Network (DNN) based ASR system to extract the BNF as in our previous works #TARGET_REF 13] .\"]}"
    },
    {
        "gold": {
            "text": [
                "where u is GMM-UBM mean supervector.",
                "v is the latent vector, known as the i − V ector and T is the lower dimensional Vector Subspace.",
                "The parameters of the model are estimated using Maximum Likelihood training criterion.",
                "For a detailed explanation of i-Vector modeling framework, reader is directed to excellent work in [15, 11] .",
                "In this work, GMM-UBM model has 2048 gaussian components, MFCC features are extracted using a 25 ms window and the i-Vectors are 400 dimensional #TARGET_REF ."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "where u is GMM-UBM mean supervector. v is the latent vector, known as the i − V ector and T is the lower dimensional Vector Subspace. The parameters of the model are estimated using Maximum Likelihood training criterion. For a detailed explanation of i-Vector modeling framework, reader is directed to excellent work in [15, 11] . In this work, GMM-UBM model has 2048 gaussian components, MFCC features are extracted using a 25 ms window and the i-Vectors are 400 dimensional #TARGET_REF .",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"In this work, GMM-UBM model has 2048 gaussian components, MFCC features are extracted using a 25 ms window and the i-Vectors are 400 dimensional #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Training and test data used in this work is the same as used in #TARGET_REF .",
                "Table 1 gives the number of hours of data available for each dialect for training and testing.",
                "Train 13  10  11  9  10  Test  2  2  2  2  2   Table 1 . Number of hours of training and testing data for each dialect Table 2 shows the number of speech utterances that are available for training and testing the DID system.",
                "#REF 1907 1059 1934 1820  Test  315  348  238  355  265   Table 2 . Number of training and test utterances for DID system development"
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "Training and test data used in this work is the same as used in #TARGET_REF . Table 1 gives the number of hours of data available for each dialect for training and testing. Train 13  10  11  9  10  Test  2  2  2  2  2   Table 1 . Number of hours of training and testing data for each dialect Table 2 shows the number of speech utterances that are available for training and testing the DID system. #REF 1907 1059 1934 1820  Test  315  348  238  355  265   Table 2 . Number of training and test utterances for DID system development",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"Training and test data used in this work is the same as used in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In other cases, a phone n-gram language model is used to model the phone statistics instead of a VSM [6, 7, 8] .",
                "On the other hand, Acoustic approaches attempt to extract dialect discriminative information from speech using low level acoustic features, such as pitch, prosody, shifted delta ceptral coefficients, bottleneck features [9, 10] .",
                "One of the most successful acoustic approaches is, the use of i-Vector framework for LID, where i-Vectors are extracted for each speech utterance, using an i-Vector extractor that consists of a GMM-UBM trained on top of BNF, followed by a Total Variability Subspace Model #TARGET_REF 11] .",
                "The extracted i-Vectors give an Acoustic VSM (Section 2.2).",
                "These methods are also used for DID."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In other cases, a phone n-gram language model is used to model the phone statistics instead of a VSM [6, 7, 8] . On the other hand, Acoustic approaches attempt to extract dialect discriminative information from speech using low level acoustic features, such as pitch, prosody, shifted delta ceptral coefficients, bottleneck features [9, 10] . One of the most successful acoustic approaches is, the use of i-Vector framework for LID, where i-Vectors are extracted for each speech utterance, using an i-Vector extractor that consists of a GMM-UBM trained on top of BNF, followed by a Total Variability Subspace Model #TARGET_REF 11] . The extracted i-Vectors give an Acoustic VSM (Section 2.2). These methods are also used for DID.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"One of the most successful acoustic approaches is, the use of i-Vector framework for LID, where i-Vectors are extracted for each speech utterance, using an i-Vector extractor that consists of a GMM-UBM trained on top of BNF, followed by a Total Variability Subspace Model #TARGET_REF 11] .\"]}"
    },
    {
        "gold": {
            "text": [
                "This gives us two DID systems built using the Acoustic and Phonotactic VSMs.",
                "At prediction time, output scores from the two DID systems are combined to give a final score, on the basis of which classification decision is made.",
                "This model combination approach has been shown to give performace improvements on the DID task #TARGET_REF .",
                "This also shows that the two systems are complementary to each other, which leads us to investigate a feature space combination approach i.e. to construct a single VSM by combining Phonotactic and Acoustic VSMs, in an attempt to encode useful discriminative information in that single VSM.",
                "In this work, we present a feature space combination approach."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "This gives us two DID systems built using the Acoustic and Phonotactic VSMs. At prediction time, output scores from the two DID systems are combined to give a final score, on the basis of which classification decision is made. This model combination approach has been shown to give performace improvements on the DID task #TARGET_REF . This also shows that the two systems are complementary to each other, which leads us to investigate a feature space combination approach i.e. to construct a single VSM by combining Phonotactic and Acoustic VSMs, in an attempt to encode useful discriminative information in that single VSM. In this work, we present a feature space combination approach.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"This model combination approach has been shown to give performace improvements on the DID task #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Details about the phone recognizer can be found in #TARGET_REF .",
                "VSM is constructed in two steps; 1) Construct a term-document matrix, X ∈ R N ×d (See Fig 1) , where each speech utterance in represented by a Phonotactic feature vector,",
                ", where N is the number of speech utterances and f (p, s) is the number of times a phone n-gram (term) s appears in the utterance (document) p and 2) Perform Truncated Singular Value Decomposition (SVD) (Equation 2) on X to learn a lower dimensional linear manifold, Π ∈ R d×k , where k << d. SVD attempts to discover the latent structure in the high dimensional feature space.",
                "Note that, k is the number of largest singular values.",
                "X is projected down to Π to get the Phonotactic VSM, X P (Equation 2)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Details about the phone recognizer can be found in #TARGET_REF . VSM is constructed in two steps; 1) Construct a term-document matrix, X ∈ R N ×d (See Fig 1) , where each speech utterance in represented by a Phonotactic feature vector, , where N is the number of speech utterances and f (p, s) is the number of times a phone n-gram (term) s appears in the utterance (document) p and 2) Perform Truncated Singular Value Decomposition (SVD) (Equation 2) on X to learn a lower dimensional linear manifold, Π ∈ R d×k , where k << d. SVD attempts to discover the latent structure in the high dimensional feature space. Note that, k is the number of largest singular values. X is projected down to Π to get the Phonotactic VSM, X P (Equation 2).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Details about the phone recognizer can be found in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally, we construct the acoustic VSM, X A ∈ R N ×400 , where the i th row is the 400 dimensional i-Vector representation corresponding to the speech utterance, a i .",
                "We also perform Linear Discriminant Analysis (LDA) and Within Class Co-variance Normalization (WCCN) on the Acoustic Vector Space, to increase the discriminative strength of the VSM.",
                "This method has been shown to improve DID (LID) performance #TARGET_REF 11] .",
                "Here, we give a brief overview of the mathematical foundations of the CCA .",
                "Fig 2 gives a probabilistic graphical model of CCA."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Finally, we construct the acoustic VSM, X A ∈ R N ×400 , where the i th row is the 400 dimensional i-Vector representation corresponding to the speech utterance, a i . We also perform Linear Discriminant Analysis (LDA) and Within Class Co-variance Normalization (WCCN) on the Acoustic Vector Space, to increase the discriminative strength of the VSM. This method has been shown to improve DID (LID) performance #TARGET_REF 11] . Here, we give a brief overview of the mathematical foundations of the CCA . Fig 2 gives a probabilistic graphical model of CCA.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"This method has been shown to improve DID (LID) performance #TARGET_REF 11] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Training data consist of recording from the Arabic Broadcast domain and contains utterances spoken in all the five dialects; EGY, GLF, LAV, MSA and NOR.",
                "The test set is from the same broadcast domain but is collected from Al-Jazeera and hence, unlike training data set, the recording are of high quality.",
                "The test set is labeled using CrowdFlower, a crowd source platform, by QCRI and is publicly available on their web portal 1 .",
                "More details about the train and test data can be found in #TARGET_REF 18] .",
                "Fig 3 gives an overview of our DID system, which can be seen as a combination of two broad components; 1) Vector Space Modeling Component and 2) Back-end classifier."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Training data consist of recording from the Arabic Broadcast domain and contains utterances spoken in all the five dialects; EGY, GLF, LAV, MSA and NOR. The test set is from the same broadcast domain but is collected from Al-Jazeera and hence, unlike training data set, the recording are of high quality. The test set is labeled using CrowdFlower, a crowd source platform, by QCRI and is publicly available on their web portal 1 . More details about the train and test data can be found in #TARGET_REF 18] . Fig 3 gives an overview of our DID system, which can be seen as a combination of two broad components; 1) Vector Space Modeling Component and 2) Back-end classifier.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"More details about the train and test data can be found in #TARGET_REF 18] .\"]}"
    },
    {
        "gold": {
            "text": [
                "The Transformer neural sequence model #TARGET_REF has emerged as a popular alternative to recurrent sequence models.",
                "Transformer relies on attention layers to communicate information between and across sequences.",
                "One major challenge with Transformer is the speed of incremental inference.",
                "As we will discuss, the speed of incremental Transformer inference on modern computing hardware is limited by the memory bandwidth necessary to reload the large \"keys\" and \"values\" tensors which encode the state of the attention layers.",
                "In the following sections, we will review the multi-head-attention layers used by Transformer, provide a performance analysis, and propose an architectural variation (multi-query attention) which greatly improves inference speed with only minor quality degradation."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The Transformer neural sequence model #TARGET_REF has emerged as a popular alternative to recurrent sequence models. Transformer relies on attention layers to communicate information between and across sequences. One major challenge with Transformer is the speed of incremental inference. As we will discuss, the speed of incremental Transformer inference on modern computing hardware is limited by the memory bandwidth necessary to reload the large \"keys\" and \"values\" tensors which encode the state of the attention layers. In the following sections, we will review the multi-head-attention layers used by Transformer, provide a performance analysis, and propose an architectural variation (multi-query attention) which greatly improves inference speed with only minor quality degradation.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The Transformer neural sequence model #TARGET_REF has emerged as a popular alternative to recurrent sequence models.\"]}"
    },
    {
        "gold": {
            "text": [
                "The \"Transformer\" seuqence-to-sequence model #TARGET_REF uses h different attention layers (heads) in parallel, which the authors refer to as \"Multi-head attention\".",
                "The query vectors for the h different layers are derived from h different learned linear projections P q of an input vector x. Similarly, the keys and values are derived from h different learned linear projections P k , P v of a collection M of m different input vectors.",
                "The outputs of the h layers are themselves passed through different learned linear projections P o , then summed.",
                "For simplicity, we give the input and output vectors identical dimensionality d. The The computation can be expressed as follows: d e f M u l t i h e a d A t t e n t i o n (",
                "x , M, P_q, P_k, P_v, P_o ) : \" \" \" Multi−head A t t e n t i o n on one quer y ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The \"Transformer\" seuqence-to-sequence model #TARGET_REF uses h different attention layers (heads) in parallel, which the authors refer to as \"Multi-head attention\". The query vectors for the h different layers are derived from h different learned linear projections P q of an input vector x. Similarly, the keys and values are derived from h different learned linear projections P k , P v of a collection M of m different input vectors. The outputs of the h layers are themselves passed through different learned linear projections P o , then summed. For simplicity, we give the input and output vectors identical dimensionality d. The The computation can be expressed as follows: d e f M u l t i h e a d A t t e n t i o n ( x , M, P_q, P_k, P_v, P_o ) : \" \" \" Multi−head A t t e n t i o n on one quer y .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The \\\"Transformer\\\" seuqence-to-sequence model #TARGET_REF uses h different attention layers (heads) in parallel, which the authors refer to as \\\"Multi-head attention\\\".\"]}"
    },
    {
        "gold": {
            "text": [
                "einsum ( \" hk , hmk−>hm\" , q , K) w e i g h t s = t f .",
                "so ftma x ( l o g i t s ) o = t f .",
                "einsum ( \"hm, hmv−>hv \" , weig hts , V) y = t f .",
                "einsum ( \" hv , hdv−>d \" , o , P_o) r e t u r n y Note: #TARGET_REF include a constant scaling factor on the logits.",
                "We omit this in our code, as it can be folded into the linear projections P q or P k ."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "einsum ( \" hk , hmk−>hm\" , q , K) w e i g h t s = t f . so ftma x ( l o g i t s ) o = t f . einsum ( \"hm, hmv−>hv \" , weig hts , V) y = t f . einsum ( \" hv , hdv−>d \" , o , P_o) r e t u r n y Note: #TARGET_REF include a constant scaling factor on the logits. We omit this in our code, as it can be folded into the linear projections P q or P k .",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"einsum ( \\\" hv , hdv\\u2212>d \\\" , o , P_o) r e t u r n y Note: #TARGET_REF include a constant scaling factor on the logits.\", \"We omit this in our code, as it can be folded into the linear projections P q or P k .\"]}"
    },
    {
        "gold": {
            "text": [
                "The code below adds two types of batching.",
                "First, we generate queries from n different positions in a sequence.",
                "These queries all interact with the same keys and values.",
                "In addition, we process a batch of b different non-interacting sequences at once.",
                "Following #TARGET_REF , in an autoregressive model, we can prevent backward-information-flow by adding a \"mask\" to the logits containing the value −∞ in the illegal positions."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The code below adds two types of batching. First, we generate queries from n different positions in a sequence. These queries all interact with the same keys and values. In addition, we process a batch of b different non-interacting sequences at once. Following #TARGET_REF , in an autoregressive model, we can prevent backward-information-flow by adding a \"mask\" to the logits containing the value −∞ in the illegal positions.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following #TARGET_REF , in an autoregressive model, we can prevent backward-information-flow by adding a \\\"mask\\\" to the logits containing the value \\u2212\\u221e in the illegal positions.\"]}"
    },
    {
        "gold": {
            "text": [
                "To simplify the performance analysis, we will make several simplifying assumptions:",
                "h , as suggested by #TARGET_REF",
                "The total number of arithmetic operations is Θ(bnd 2 ).",
                "(Since the complexity of each of the tf.einsum operations above is O(bnd 2 ) given the simplifying assumptions.",
                "The total size of memory to be accessed is equal to the sum of the sizes of all the tensors involved: O(bnd + bhn 2 + d 2 )."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "To simplify the performance analysis, we will make several simplifying assumptions: h , as suggested by #TARGET_REF The total number of arithmetic operations is Θ(bnd 2 ). (Since the complexity of each of the tf.einsum operations above is O(bnd 2 ) given the simplifying assumptions. The total size of memory to be accessed is equal to the sum of the sizes of all the tensors involved: O(bnd + bhn 2 + d 2 ).",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"To simplify the performance analysis, we will make several simplifying assumptions:\", \"h , as suggested by #TARGET_REF\"]}"
    },
    {
        "gold": {
            "text": [
                "We introduce multi-query Attention as a variation of multi-head attention as described in #TARGET_REF .",
                "Multi-head attention consists of multiple attention layers (heads) in parallel with different linear transformations on the queries, keys, values and outputs.",
                "Multi-query attention is identical except that the different heads share a single set of keys and values.",
                "The code for (incremental) multi-query (self) attention is identical to the code listed above for multi-head attention, except that we remove the letter \"h\" from the tf.einsum equations where it represents the \"heads\" dimension of K, V , P k , or P v ."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "We introduce multi-query Attention as a variation of multi-head attention as described in #TARGET_REF . Multi-head attention consists of multiple attention layers (heads) in parallel with different linear transformations on the queries, keys, values and outputs. Multi-query attention is identical except that the different heads share a single set of keys and values. The code for (incremental) multi-query (self) attention is identical to the code listed above for multi-head attention, except that we remove the letter \"h\" from the tf.einsum equations where it represents the \"heads\" dimension of K, V , P k , or P v .",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"We introduce multi-query Attention as a variation of multi-head attention as described in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , we evaluate on the WMT 2014 English-German translation task.",
                "As a baseline, we use an encoder-decoder Transformer model with 6 layers, using d model = 1024 d f f = 4096, h = 8, d k = d v = 128, learned positional embeddings, and weight-sharing between the token-embedding and output layers.",
                "The baseline model and all variations have 211 million parameters.",
                "All models were trained for 100,000 steps ( 20 epochs).",
                "Each training batch consisted of 128 examples, each of which consisted of a 256-token input sequence and a 256-token target sequence (multiple training sentences were concatenated together to reach this length)."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Following #TARGET_REF , we evaluate on the WMT 2014 English-German translation task. As a baseline, we use an encoder-decoder Transformer model with 6 layers, using d model = 1024 d f f = 4096, h = 8, d k = d v = 128, learned positional embeddings, and weight-sharing between the token-embedding and output layers. The baseline model and all variations have 211 million parameters. All models were trained for 100,000 steps ( 20 epochs). Each training batch consisted of 128 examples, each of which consisted of a 256-token input sequence and a 256-token target sequence (multiple training sentences were concatenated together to reach this length).",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"Following #TARGET_REF , we evaluate on the WMT 2014 English-German translation task.\"]}"
    },
    {
        "gold": {
            "text": [
                "In some settings, data dependencies make it is impossible to process queries from multiple positions in parallel.",
                "An example is a self-attention layer in an autoregressive language model such as Transformer #TARGET_REF .",
                "The queries produced at each position attend to key-value pairs produced at all positions up to and including that position.",
                "During training, the ground-truth target sequence is known, and we can use an efficient parallel implementation similar to that in section 2.3.",
                "However, when generating from the trained model, the output of the self-attention layer at a particular position affects the token that is generated at the next position, which in turn affects the input to that layer at the next position."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In some settings, data dependencies make it is impossible to process queries from multiple positions in parallel. An example is a self-attention layer in an autoregressive language model such as Transformer #TARGET_REF . The queries produced at each position attend to key-value pairs produced at all positions up to and including that position. During training, the ground-truth target sequence is known, and we can use an efficient parallel implementation similar to that in section 2.3. However, when generating from the trained model, the output of the self-attention layer at a particular position affects the token that is generated at the next position, which in turn affects the input to that layer at the next position.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"An example is a self-attention layer in an autoregressive language model such as Transformer #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "While most of them assign grammatical functions on top of constituency trees (#REF; Jijkoun and de #REF; Chrupała and #REF; #REF; #REF) , less work has tried to predict GF labels for unlabelled dependency trees.",
                "One of them is #REF who first generate the unlabelled trees using a graph-based parser, and then model the assignment of dependency labels as a sequence labelling task.",
                "Another approach has been proposed by #TARGET_REF who present a simple, yet efficient and accurate parsing model that generates unlabelled trees by identifying the most probable head for each token in the input.",
                "Then, in a post-processing step, they assign labels to each head-dependent pair, using a two-layer rectifier network.",
                "Dependency Parsing as Head Selection Our labelling model is an extension of the parsing model of #REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "While most of them assign grammatical functions on top of constituency trees (#REF; Jijkoun and de #REF; Chrupała and #REF; #REF; #REF) , less work has tried to predict GF labels for unlabelled dependency trees. One of them is #REF who first generate the unlabelled trees using a graph-based parser, and then model the assignment of dependency labels as a sequence labelling task. Another approach has been proposed by #TARGET_REF who present a simple, yet efficient and accurate parsing model that generates unlabelled trees by identifying the most probable head for each token in the input. Then, in a post-processing step, they assign labels to each head-dependent pair, using a two-layer rectifier network. Dependency Parsing as Head Selection Our labelling model is an extension of the parsing model of #REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Another approach has been proposed by #TARGET_REF who present a simple, yet efficient and accurate parsing model that generates unlabelled trees by identifying the most probable head for each token in the input.\"]}"
    },
    {
        "gold": {
            "text": [
                "The unlabelled tree is then built by selecting the most probable head for each word.",
                "The score of word w j being the head of word w i is computed by a single hidden layer neural network on their representations a j and a i .",
                "An additional classifier with two rectified hidden layers is used to predict dependency labels, and is trained separately from the unlabeled parsing component, in a pipeline architecture.",
                "The classifier predictions are based on the representations of the head and the dependent, b j and b i , which are the concatenation of the input and the bidirectional LSTM-based representations:",
                "Despite its simplicity and the lack of global optimisation, #TARGET_REF report competitive results for English, Czech, and German."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The unlabelled tree is then built by selecting the most probable head for each word. The score of word w j being the head of word w i is computed by a single hidden layer neural network on their representations a j and a i . An additional classifier with two rectified hidden layers is used to predict dependency labels, and is trained separately from the unlabeled parsing component, in a pipeline architecture. The classifier predictions are based on the representations of the head and the dependent, b j and b i , which are the concatenation of the input and the bidirectional LSTM-based representations: Despite its simplicity and the lack of global optimisation, #TARGET_REF report competitive results for English, Czech, and German.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Despite its simplicity and the lack of global optimisation, #TARGET_REF report competitive results for English, Czech, and German.\"]}"
    },
    {
        "gold": {
            "text": [
                "Although the labelling approach in #TARGET_REF is simple and efficient, looking at head and dependent only when assigning the labels comes with some disadvantages.",
                "First, some labels are easier to predict when we also take context into account, e.g. the parent and grandparent nodes or the siblings of the head or dependent.",
                "Consider, for example, the following sentence: Is this the future of chamber music? and its syntactic structure (figure 1).",
                "If we only consider the nodes this and future, there is a chance that the edge between them is labelled as det (determiner).",
                "However, if we also look at the local context, we know that node the to the left of future is more likely to be the determiner, and thus this should be assigned a different label."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Although the labelling approach in #TARGET_REF is simple and efficient, looking at head and dependent only when assigning the labels comes with some disadvantages. First, some labels are easier to predict when we also take context into account, e.g. the parent and grandparent nodes or the siblings of the head or dependent. Consider, for example, the following sentence: Is this the future of chamber music? and its syntactic structure (figure 1). If we only consider the nodes this and future, there is a chance that the edge between them is labelled as det (determiner). However, if we also look at the local context, we know that node the to the left of future is more likely to be the determiner, and thus this should be assigned a different label.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Although the labelling approach in #TARGET_REF is simple and efficient, looking at head and dependent only when assigning the labels comes with some disadvantages.\"]}"
    },
    {
        "gold": {
            "text": [
                "Then, in a post-processing step, they assign labels to each head-dependent pair, using a two-layer rectifier network.",
                "Dependency Parsing as Head Selection Our labelling model is an extension of the parsing model of #TARGET_REF .",
                "We use our own implementation of the head-selection parser and focus on the grammatical function labelling part.",
                "The parser uses a bidirectional LSTM to extract a dense, positional representation a i of the word w i at position i in a sentence:",
                "x i is the input at position i, which is the concatenation of the word embeddings and the tag embeddings of word w i ."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Then, in a post-processing step, they assign labels to each head-dependent pair, using a two-layer rectifier network. Dependency Parsing as Head Selection Our labelling model is an extension of the parsing model of #TARGET_REF . We use our own implementation of the head-selection parser and focus on the grammatical function labelling part. The parser uses a bidirectional LSTM to extract a dense, positional representation a i of the word w i at position i in a sentence: x i is the input at position i, which is the concatenation of the word embeddings and the tag embeddings of word w i .",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"Dependency Parsing as Head Selection Our labelling model is an extension of the parsing model of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Our interest is focussed on German, but to put our work in context, we follow #TARGET_REF and report results also for English, which has a configurational word order, and for Czech, which has a free word order, rich morphology, and less ambiguity in the case paradigm than German.",
                "For English, we use the Penn Treebank (PTB) (#REF) with standard training/dev/test splits.",
                "The POS tags are assigned using the Stanford POS tagger (#REF) with ten-way jackknifing, and constituency trees are converted to Stanford basic dependencies (De #REF) .",
                "The German and Czech data come from the CoNLL-X shared task (#REF) and our data split follows #REF .",
                "As the CoNLL-X testsets are rather small (∼ 360 sentences), we also"
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Our interest is focussed on German, but to put our work in context, we follow #TARGET_REF and report results also for English, which has a configurational word order, and for Czech, which has a free word order, rich morphology, and less ambiguity in the case paradigm than German. For English, we use the Penn Treebank (PTB) (#REF) with standard training/dev/test splits. The POS tags are assigned using the Stanford POS tagger (#REF) with ten-way jackknifing, and constituency trees are converted to Stanford basic dependencies (De #REF) . The German and Czech data come from the CoNLL-X shared task (#REF) and our data split follows #REF . As the CoNLL-X testsets are rather small (∼ 360 sentences), we also",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Our interest is focussed on German, but to put our work in context, we follow #TARGET_REF and report results also for English, which has a configurational word order, and for Czech, which has a free word order, rich morphology, and less ambiguity in the case paradigm than German.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our interest is focussed on German, but to put our work in context, we follow #REF and report results also for English, which has a configurational word order, and for Czech, which has a free word order, rich morphology, and less ambiguity in the case paradigm than German.",
                "For English, we use the Penn Treebank (PTB) (#REF) with standard training/dev/test splits.",
                "The POS tags are assigned using the Stanford POS tagger (#REF) with ten-way jackknifing, and constituency trees are converted to Stanford basic dependencies (De #REF) .",
                "The German and Czech data come from the CoNLL-X shared task (#REF) and our data split follows #TARGET_REF .",
                "As the CoNLL-X testsets are rather small (∼ 360 sentences), we also"
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Our interest is focussed on German, but to put our work in context, we follow #REF and report results also for English, which has a configurational word order, and for Czech, which has a free word order, rich morphology, and less ambiguity in the case paradigm than German. For English, we use the Penn Treebank (PTB) (#REF) with standard training/dev/test splits. The POS tags are assigned using the Stanford POS tagger (#REF) with ten-way jackknifing, and constituency trees are converted to Stanford basic dependencies (De #REF) . The German and Czech data come from the CoNLL-X shared task (#REF) and our data split follows #TARGET_REF . As the CoNLL-X testsets are rather small (∼ 360 sentences), we also",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The German and Czech data come from the CoNLL-X shared task (#REF) and our data split follows #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We test different labelling models on top of the unlabelled trees produced by our re-implementation of the parsing as head selection model ( §2).",
                "We first train the unlabelled parsing models for the three languages.",
                "Unless stated otherwise, all parameters are set according to #TARGET_REF , and tag embedding size was set to 40 for all languages.",
                "Please note that we do not use pre-trained embeddings in our experiments.",
                "In the next step, we train four different labelling models: the labeller of #REF that uses a rectifier neural network with two hidden layers (baseline), two bidirectional LSTM models (BILSTM(L) and BILSTM(B)), and one tree LSTM model (TREELSTM) ( §3)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We test different labelling models on top of the unlabelled trees produced by our re-implementation of the parsing as head selection model ( §2). We first train the unlabelled parsing models for the three languages. Unless stated otherwise, all parameters are set according to #TARGET_REF , and tag embedding size was set to 40 for all languages. Please note that we do not use pre-trained embeddings in our experiments. In the next step, we train four different labelling models: the labeller of #REF that uses a rectifier neural network with two hidden layers (baseline), two bidirectional LSTM models (BILSTM(L) and BILSTM(B)), and one tree LSTM model (TREELSTM) ( §3).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Unless stated otherwise, all parameters are set according to #TARGET_REF , and tag embedding size was set to 40 for all languages.\"]}"
    },
    {
        "gold": {
            "text": [
                "Unless stated otherwise, all parameters are set according to #REF , and tag embedding size was set to 40 for all languages.",
                "Please note that we do not use pre-trained embeddings in our experiments.",
                "In the next step, we train four different labelling models: the labeller of #TARGET_REF that uses a rectifier neural network with two hidden layers (baseline), two bidirectional LSTM models (BILSTM(L) and BILSTM(B)), and one tree LSTM model (TREELSTM) ( §3).",
                "The hidden layer dimension in all LSTM models was set to 200.",
                "The models were trained for 10 epochs, and were optimized using Adam ( #REF) with default parameters (initial learning rate 0.001, first momentum coefficient 0.9, second momentum coefficient 0.999)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Unless stated otherwise, all parameters are set according to #REF , and tag embedding size was set to 40 for all languages. Please note that we do not use pre-trained embeddings in our experiments. In the next step, we train four different labelling models: the labeller of #TARGET_REF that uses a rectifier neural network with two hidden layers (baseline), two bidirectional LSTM models (BILSTM(L) and BILSTM(B)), and one tree LSTM model (TREELSTM) ( §3). The hidden layer dimension in all LSTM models was set to 200. The models were trained for 10 epochs, and were optimized using Adam ( #REF) with default parameters (initial learning rate 0.001, first momentum coefficient 0.9, second momentum coefficient 0.999).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In the next step, we train four different labelling models: the labeller of #TARGET_REF that uses a rectifier neural network with two hidden layers (baseline), two bidirectional LSTM models (BILSTM(L) and BILSTM(B)), and one tree LSTM model (TREELSTM) ( \\u00a73).\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 1 shows the unlabelled attachment score (UAS) for the unlabelled trees and the labelled attachment scores (LAS) for the different labellers (excluding punctuation).",
                "All history-based labelling models perform significantly better than the local baseline model, 1 but for English the improvements are smaller (0.3%) than for the nonconfigurational languages (∼0.7%).",
                "While we tried to reimplement the model of #TARGET_REF following the details in the paper, our reimplemented model yields higher scores for German, compared to the results in the paper.",
                "The scores for English are slightly lower since, in contrast to #REF , we do not use pre-trained embeddings.",
                "When using our historybased labellers, we get similar results for English (91.9%) and higher results for both Czech (84.1% vs. 81.7%) and German (91.0% vs. 89.6%) on the same data without using pre-trained embeddings or post-processing."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Table 1 shows the unlabelled attachment score (UAS) for the unlabelled trees and the labelled attachment scores (LAS) for the different labellers (excluding punctuation). All history-based labelling models perform significantly better than the local baseline model, 1 but for English the improvements are smaller (0.3%) than for the nonconfigurational languages (∼0.7%). While we tried to reimplement the model of #TARGET_REF following the details in the paper, our reimplemented model yields higher scores for German, compared to the results in the paper. The scores for English are slightly lower since, in contrast to #REF , we do not use pre-trained embeddings. When using our historybased labellers, we get similar results for English (91.9%) and higher results for both Czech (84.1% vs. 81.7%) and German (91.0% vs. 89.6%) on the same data without using pre-trained embeddings or post-processing.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"While we tried to reimplement the model of #TARGET_REF following the details in the paper, our reimplemented model yields higher scores for German, compared to the results in the paper.\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 1 shows the unlabelled attachment score (UAS) for the unlabelled trees and the labelled attachment scores (LAS) for the different labellers (excluding punctuation).",
                "All history-based labelling models perform significantly better than the local baseline model, 1 but for English the improvements are smaller (0.3%) than for the nonconfigurational languages (∼0.7%).",
                "While we tried to reimplement the model of #REF following the details in the paper, our reimplemented model yields higher scores for German, compared to the results in the paper.",
                "The scores for English are slightly lower since, in contrast to #TARGET_REF , we do not use pre-trained embeddings.",
                "When using our historybased labellers, we get similar results for English (91.9%) and higher results for both Czech (84.1% vs. 81.7%) and German (91.0% vs. 89.6%) on the same data without using pre-trained embeddings or post-processing."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Table 1 shows the unlabelled attachment score (UAS) for the unlabelled trees and the labelled attachment scores (LAS) for the different labellers (excluding punctuation). All history-based labelling models perform significantly better than the local baseline model, 1 but for English the improvements are smaller (0.3%) than for the nonconfigurational languages (∼0.7%). While we tried to reimplement the model of #REF following the details in the paper, our reimplemented model yields higher scores for German, compared to the results in the paper. The scores for English are slightly lower since, in contrast to #TARGET_REF , we do not use pre-trained embeddings. When using our historybased labellers, we get similar results for English (91.9%) and higher results for both Czech (84.1% vs. 81.7%) and German (91.0% vs. 89.6%) on the same data without using pre-trained embeddings or post-processing.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The scores for English are slightly lower since, in contrast to #TARGET_REF , we do not use pre-trained embeddings.\"]}"
    },
    {
        "gold": {
            "text": [
                "On the SPMRL 2014 shared task data, our results are only 0.3% lower than the ones of the winning system (Björkelund et al., 2014) fectiveness of our models, we also ran our labeller on the unlabelled output of the SPMRL 2014 winning system and on unlabelled gold trees.",
                "On the output of the blended system LAS slightly improves from 88.62% to 88.76% (TREELSTM).",
                "3 When applied to unlabelled gold trees, the distance between our models and the baseline becomes larger and the best of our history-based models (BILSTM(B), 97.38%) outperforms the original labeller of #TARGET_REF (96.15%) by more than 1%.",
                "We would like to emphasize that our historybased LSTM labeller is practically simple and computationally inexpensive (as compared to global training or inference), so our model manages to preserve simplicity while significantly improving labelling performance."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0
            ]
        },
        "input": "On the SPMRL 2014 shared task data, our results are only 0.3% lower than the ones of the winning system (Björkelund et al., 2014) fectiveness of our models, we also ran our labeller on the unlabelled output of the SPMRL 2014 winning system and on unlabelled gold trees. On the output of the blended system LAS slightly improves from 88.62% to 88.76% (TREELSTM). 3 When applied to unlabelled gold trees, the distance between our models and the baseline becomes larger and the best of our history-based models (BILSTM(B), 97.38%) outperforms the original labeller of #TARGET_REF (96.15%) by more than 1%. We would like to emphasize that our historybased LSTM labeller is practically simple and computationally inexpensive (as compared to global training or inference), so our model manages to preserve simplicity while significantly improving labelling performance.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"3 When applied to unlabelled gold trees, the distance between our models and the baseline becomes larger and the best of our history-based models (BILSTM(B), 97.38%) outperforms the original labeller of #TARGET_REF (96.15%) by more than 1%.\"]}"
    },
    {
        "gold": {
            "text": [
                "We have shown that GF labelling, which is of crucial importance for languages like German, can be improved by combining LSTM models with a decision history.",
                "All our models outperform the original labeller of #TARGET_REF and give results in the same range as the best system from the SPMRL-2014 shared task (without the reranker), but with a much simpler model.",
                "Our results show that the history is especially important for languages that show more word order variation.",
                "Here, presenting the input in a structured BFS order not only significantly outperforms the baseline, but also yields improvements over the other LSTM models on core grammatical functions."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0
            ]
        },
        "input": "We have shown that GF labelling, which is of crucial importance for languages like German, can be improved by combining LSTM models with a decision history. All our models outperform the original labeller of #TARGET_REF and give results in the same range as the best system from the SPMRL-2014 shared task (without the reranker), but with a much simpler model. Our results show that the history is especially important for languages that show more word order variation. Here, presenting the input in a structured BFS order not only significantly outperforms the baseline, but also yields improvements over the other LSTM models on core grammatical functions.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"All our models outperform the original labeller of #TARGET_REF and give results in the same range as the best system from the SPMRL-2014 shared task (without the reranker), but with a much simpler model.\"]}"
    },
    {
        "gold": {
            "text": [
                "Fully supervised maximum entropy Markov models have been used for cascaded prediction of POS tags followed by supertags (#REF) .",
                "Here, we learn supertaggers given only a POS tag dictionary and supertag dictionary or a small amount of material labeled with both types of information.",
                "Previous work has used Bayesian HMMs to learn taggers for both POS tagging and supertagging #TARGET_REF separately.",
                "Modeling them jointly has the potential to produce more robust and accurate supertaggers trained with less supervision and thereby potentially help in the creation of useful models for new languages and domains.",
                "Our results show that joint inference improves supervised supertag prediction (compared to HMMs), especially when labeled training data is scarce."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Fully supervised maximum entropy Markov models have been used for cascaded prediction of POS tags followed by supertags (#REF) . Here, we learn supertaggers given only a POS tag dictionary and supertag dictionary or a small amount of material labeled with both types of information. Previous work has used Bayesian HMMs to learn taggers for both POS tagging and supertagging #TARGET_REF separately. Modeling them jointly has the potential to produce more robust and accurate supertaggers trained with less supervision and thereby potentially help in the creation of useful models for new languages and domains. Our results show that joint inference improves supervised supertag prediction (compared to HMMs), especially when labeled training data is scarce.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Previous work has used Bayesian HMMs to learn taggers for both POS tagging and supertagging #TARGET_REF separately.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of #REF .",
                "It is however, quite short of the 56.1% accuracy achieved by the model of #TARGET_REF that uses grammar informed initialization (combination of category based initialization along with category transition rules).",
                "Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM.",
                "The HMM performs much better when there is a high level of frequency based filtering of the categories.",
                "However, recall that frequency based filtering of categories is a strong form of supervision that we use here only as an oracle and which one could not expect to have in real world tag dictionaries."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of #REF . It is however, quite short of the 56.1% accuracy achieved by the model of #TARGET_REF that uses grammar informed initialization (combination of category based initialization along with category transition rules). Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM. The HMM performs much better when there is a high level of frequency based filtering of the categories. However, recall that frequency based filtering of categories is a strong form of supervision that we use here only as an oracle and which one could not expect to have in real world tag dictionaries.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"It is however, quite short of the 56.1% accuracy achieved by the model of #TARGET_REF that uses grammar informed initialization (combination of category based initialization along with category transition rules).\"]}"
    },
    {
        "gold": {
            "text": [
                "In this experiment, we use the training and test sets used by #TARGET_REF from CCGbank.",
                "We vary the amount of training material by using 100, 1000, 10,000 and all 38015 training set sentences.",
                "We also vary the transition prior α choosing α = 1.0 and α = 0.05 on the CCG tags.",
                "The emission prior β was held constant at 1.0.",
                "The results of these experiments for α = 0.05 are tabulated in Table 3 (a)."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In this experiment, we use the training and test sets used by #TARGET_REF from CCGbank. We vary the amount of training material by using 100, 1000, 10,000 and all 38015 training set sentences. We also vary the transition prior α choosing α = 1.0 and α = 0.05 on the CCG tags. The emission prior β was held constant at 1.0. The results of these experiments for α = 0.05 are tabulated in Table 3 (a).",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"In this experiment, we use the training and test sets used by #TARGET_REF from CCGbank.\"]}"
    },
    {
        "gold": {
            "text": [
                "Since annotation is costly, we are interested in automatic annotation of unlabeled sentences with minimal supervision.",
                "In the weakly supervised learning setting, we are provided with a lexicon that lists possible POS tags and supertags for many, though not all, words.",
                "We draw the initial sample of CCG tag sequences corresponding to the observation sequence, using probabilities based on grammar informed initialization #TARGET_REF .",
                "We consider the prior probability of occurrence of categories based on their complexity: given a lexicon L, the probability of a category c i is inversely proportional to its complexity:",
                "where complexity(c i ) is defined as the number of sub-categories contained in category c i ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Since annotation is costly, we are interested in automatic annotation of unlabeled sentences with minimal supervision. In the weakly supervised learning setting, we are provided with a lexicon that lists possible POS tags and supertags for many, though not all, words. We draw the initial sample of CCG tag sequences corresponding to the observation sequence, using probabilities based on grammar informed initialization #TARGET_REF . We consider the prior probability of occurrence of categories based on their complexity: given a lexicon L, the probability of a category c i is inversely proportional to its complexity: where complexity(c i ) is defined as the number of sub-categories contained in category c i .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We draw the initial sample of CCG tag sequences corresponding to the observation sequence, using probabilities based on grammar informed initialization #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We also report the CCG accuracy values inclusive of unambiguous types in Table 5 (b) for α = 1.0 and Table 6 (b) respectively.",
                "The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of #TARGET_REF that uses variational Bayes EM (33%).",
                "Our complexity based initialization is not directly comparable to the results in #REF because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism.",
                "However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of #REF .",
                "It is however, quite short of the 56.1% accuracy achieved by the model of #REF that uses grammar informed initialization (combination of category based initialization along with category transition rules)."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We also report the CCG accuracy values inclusive of unambiguous types in Table 5 (b) for α = 1.0 and Table 6 (b) respectively. The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of #TARGET_REF that uses variational Bayes EM (33%). Our complexity based initialization is not directly comparable to the results in #REF because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism. However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of #REF . It is however, quite short of the 56.1% accuracy achieved by the model of #REF that uses grammar informed initialization (combination of category based initialization along with category transition rules).",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of #TARGET_REF that uses variational Bayes EM (33%).\"]}"
    },
    {
        "gold": {
            "text": [
                "There is plenty of scope for further improvements.",
                "Overall, the discriminative C&C supertagger outperforms the FHMMs in all supervised settings.",
                "Despite this, the FHMMs are suited for estimating models with less supervision, such as from tag dictionaries alone and incorporating more informative prior distributions such as those in #TARGET_REF .",
                "This may make them more appropriate for developing CCGbanks for other languages and domains.",
                "Furthermore, Bayesian inference is modular and extensible, so our models could be supplemented by finding optimal values of the hyperparameters α (for POS tags) and β."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "There is plenty of scope for further improvements. Overall, the discriminative C&C supertagger outperforms the FHMMs in all supervised settings. Despite this, the FHMMs are suited for estimating models with less supervision, such as from tag dictionaries alone and incorporating more informative prior distributions such as those in #TARGET_REF . This may make them more appropriate for developing CCGbanks for other languages and domains. Furthermore, Bayesian inference is modular and extensible, so our models could be supplemented by finding optimal values of the hyperparameters α (for POS tags) and β.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Despite this, the FHMMs are suited for estimating models with less supervision, such as from tag dictionaries alone and incorporating more informative prior distributions such as those in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "State-of-the-art POS taggers report accuracies in the range of 96−97%; our model FHMMB was comparable (95.35% for α = 0.05 and 94.41 for α = 1.0).",
                "The FHMMA model and the HMM model achieved 91% and 92.5% accuracy on POS tags, respectively.",
                "The accuracy of our HMM is lower than the performance of #TARGET_REF for supertags.",
                "We attribute this to better tag-specific smoothing in his model for emissions, compared to our use of a symmetric parameter for all tags.",
                "We stress that our interest here is in evaluating the advantage of joint inference over POS tags and supertags rather than direct supertag prediction while holding all other modeling considerations equal."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "State-of-the-art POS taggers report accuracies in the range of 96−97%; our model FHMMB was comparable (95.35% for α = 0.05 and 94.41 for α = 1.0). The FHMMA model and the HMM model achieved 91% and 92.5% accuracy on POS tags, respectively. The accuracy of our HMM is lower than the performance of #TARGET_REF for supertags. We attribute this to better tag-specific smoothing in his model for emissions, compared to our use of a symmetric parameter for all tags. We stress that our interest here is in evaluating the advantage of joint inference over POS tags and supertags rather than direct supertag prediction while holding all other modeling considerations equal.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The accuracy of our HMM is lower than the performance of #TARGET_REF for supertags.\"]}"
    },
    {
        "gold": {
            "text": [
                "The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of #REF that uses variational Bayes EM (33%).",
                "Our complexity based initialization is not directly comparable to the results in #TARGET_REF because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism.",
                "However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of #REF .",
                "It is however, quite short of the 56.1% accuracy achieved by the model of #REF that uses grammar informed initialization (combination of category based initialization along with category transition rules).",
                "Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of #REF that uses variational Bayes EM (33%). Our complexity based initialization is not directly comparable to the results in #TARGET_REF because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism. However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of #REF . It is however, quite short of the 56.1% accuracy achieved by the model of #REF that uses grammar informed initialization (combination of category based initialization along with category transition rules). Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Our complexity based initialization is not directly comparable to the results in #TARGET_REF because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism.\"]}"
    },
    {
        "gold": {
            "text": [
                "The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of #REF that uses variational Bayes EM (33%).",
                "Our complexity based initialization is not directly comparable to the results in #REF because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism.",
                "However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of #TARGET_REF .",
                "It is however, quite short of the 56.1% accuracy achieved by the model of #REF that uses grammar informed initialization (combination of category based initialization along with category transition rules).",
                "Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The performance of the HMM model (31%) in Table 5 (a) without any frequency cut-off on the CCG categories, is comparable to the bitag HMM of #REF that uses variational Bayes EM (33%). Our complexity based initialization is not directly comparable to the results in #REF because the values there are based on a weighted combination of complexity based initialization and modified transition priors based on the CCG formalism. However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of #TARGET_REF . It is however, quite short of the 56.1% accuracy achieved by the model of #REF that uses grammar informed initialization (combination of category based initialization along with category transition rules). Without any frequency cut-off on CCG categories, FHMMB achieves over 17% improvement in the prediction accuracy of ambiguous CCG categories, in comparison with the HMM.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"However, it is encouraging to see that when there is no cut-off based filtering of the categories, FHMMB (47.98%) greatly outperforms the HMM-EM model of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "However, recall that frequency based filtering of categories is a strong form of supervision that we use here only as an oracle and which one could not expect to have in real world tag dictionaries.",
                "The POS accuracies in these experiments were 83.5-85%, 84.5-86.2% and 78.3-78.4% for models FHMMB, FHMMA and HMM respectively (without any frequency cut-off).",
                "In the weakly supervised setting, the choice of the transition prior α of 0.05 lead to severe degradation in the prediction accuracy of CCG tags.",
                "Unlike POS tagging, where a symmetric transition prior of α = 0.05 captured the sparsity of the tag transition distribution , in supertagging the transition priors are asymmetric.",
                "We expect that CCG transition rules #TARGET_REF when encoded as category specific transition priors, will lead to better performance with the FHMMs."
            ],
            "label": [
                "FUT"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "However, recall that frequency based filtering of categories is a strong form of supervision that we use here only as an oracle and which one could not expect to have in real world tag dictionaries. The POS accuracies in these experiments were 83.5-85%, 84.5-86.2% and 78.3-78.4% for models FHMMB, FHMMA and HMM respectively (without any frequency cut-off). In the weakly supervised setting, the choice of the transition prior α of 0.05 lead to severe degradation in the prediction accuracy of CCG tags. Unlike POS tagging, where a symmetric transition prior of α = 0.05 captured the sparsity of the tag transition distribution , in supertagging the transition priors are asymmetric. We expect that CCG transition rules #TARGET_REF when encoded as category specific transition priors, will lead to better performance with the FHMMs.",
        "output": "{\"label\": [\"FUT\"], \"context\": [\"We expect that CCG transition rules #TARGET_REF when encoded as category specific transition priors, will lead to better performance with the FHMMs.\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper follows the work of #REF , #TARGET_REF and #REF .",
                "#REF uses FHMMs for jointly labeling the POS and NP chunk tags for the CoNLL2000 dataset (#REF) .",
                "His is a fully supervised model for a simpler task.",
                "We address the harder problem of supertagging in this paper and especially in the weakly supervised setting, with FHMMs.",
                "#REF uses a Bayesian tritag HMM (BHMM) for POS tagging and considers three different scenarios: (1) a weakly supervised setting with fixed hyperparameters α and β, (2) hyper parameter inference (learning the optimal values for α and β) and (3) hyper parameter inference with varying corpus size and dictionary knowledge."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "This paper follows the work of #REF , #TARGET_REF and #REF . #REF uses FHMMs for jointly labeling the POS and NP chunk tags for the CoNLL2000 dataset (#REF) . His is a fully supervised model for a simpler task. We address the harder problem of supertagging in this paper and especially in the weakly supervised setting, with FHMMs. #REF uses a Bayesian tritag HMM (BHMM) for POS tagging and considers three different scenarios: (1) a weakly supervised setting with fixed hyperparameters α and β, (2) hyper parameter inference (learning the optimal values for α and β) and (3) hyper parameter inference with varying corpus size and dictionary knowledge.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"This paper follows the work of #REF , #TARGET_REF and #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Although this annotation was performed by the first author only, we know from previous work that similar styles of annotation can achieve acceptable interannotator agreement (#REF ).",
                "An example annotation for #REF is given in Figure  2 , where the first column shows the line number and the second one shows the class label.",
                "To compare our work with #TARGET_REF , we also applied a three-class annotation scheme.",
                "In this method of annotation, we merge the citation context into a single sentence.",
                "Since the context introduces more than one sentiment per citation, we marked the citation sentiment with the last sentiment mentioned in the context window as this is pragmatically most likely to be the real intention (#REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Although this annotation was performed by the first author only, we know from previous work that similar styles of annotation can achieve acceptable interannotator agreement (#REF ). An example annotation for #REF is given in Figure  2 , where the first column shows the line number and the second one shows the class label. To compare our work with #TARGET_REF , we also applied a three-class annotation scheme. In this method of annotation, we merge the citation context into a single sentence. Since the context introduces more than one sentiment per citation, we marked the citation sentiment with the last sentiment mentioned in the context window as this is pragmatically most likely to be the real intention (#REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"To compare our work with #TARGET_REF , we also applied a three-class annotation scheme.\"]}"
    },
    {
        "gold": {
            "text": [
                "The dependency triplets are constructed by merging the relation, governor and dependent in a single string, for instance, the relation nsubj(failed, method) is represented as nsubj failed method .",
                "This setup has been shown to produce good results earlier as well (#REF; #TARGET_REF) .",
                "The first set of experiments focuses on simultaneous detection of sentiment and context sentences.",
                "For this purpose, we use the four-class annotated corpus described earlier.",
                "While the original annotations were performed for a window of length 4, we also experiment with asymmetrical windows of l sentences preceding the citation and r sentences succeeding it."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The dependency triplets are constructed by merging the relation, governor and dependent in a single string, for instance, the relation nsubj(failed, method) is represented as nsubj failed method . This setup has been shown to produce good results earlier as well (#REF; #TARGET_REF) . The first set of experiments focuses on simultaneous detection of sentiment and context sentences. For this purpose, we use the four-class annotated corpus described earlier. While the original annotations were performed for a window of length 4, we also experiment with asymmetrical windows of l sentences preceding the citation and r sentences succeeding it.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"This setup has been shown to produce good results earlier as well (#REF; #TARGET_REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "While different schemes have been proposed for annotating citations according to their function (#REF; #REF; #REF) , the only recent work on citation sentiment detection using a relatively large corpus is by #TARGET_REF .",
                "However, this work does not handle citation context.",
                "#REF proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources.",
                "A common approach for sentiment detection is to use a labelled lexicon to score sentences (#REF; #REF; #REF) .",
                "However, such approaches have been found to be highly topic dependent (Engström, 2004; #REF; #REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "While different schemes have been proposed for annotating citations according to their function (#REF; #REF; #REF) , the only recent work on citation sentiment detection using a relatively large corpus is by #TARGET_REF . However, this work does not handle citation context. #REF proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (#REF; #REF; #REF) . However, such approaches have been found to be highly topic dependent (Engström, 2004; #REF; #REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"While different schemes have been proposed for annotating citations according to their function (#REF; #REF; #REF) , the only recent work on citation sentiment detection using a relatively large corpus is by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The baseline score, shown in bold, is obtained with no context window and is comparable to the results reported by #TARGET_REF .",
                "However, we can observe that the F scores decrease as more context is introduced.",
                "This may be attributed to the increase in the vocabulary size of the n-grams and a consequent reduction in the discriminating power of the decision boundaries.",
                "These results show that the task of jointly detecting sentiment and context is a hard problem.",
                "For our second set of experiments, we use the three-class annotation scheme."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The baseline score, shown in bold, is obtained with no context window and is comparable to the results reported by #TARGET_REF . However, we can observe that the F scores decrease as more context is introduced. This may be attributed to the increase in the vocabulary size of the n-grams and a consequent reduction in the discriminating power of the decision boundaries. These results show that the task of jointly detecting sentiment and context is a hard problem. For our second set of experiments, we use the three-class annotation scheme.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"The baseline score, shown in bold, is obtained with no context window and is comparable to the results reported by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "These results show that the task of jointly detecting sentiment and context is a hard problem.",
                "For our second set of experiments, we use the three-class annotation scheme.",
                "We merge the text of the sentences in the context windows as well as their dependency triplets to obtain the features.",
                "The results are reported in Table 3 with best results in bold.",
                "Although these results are not better than the context-less baseline, the reason might be data sparsity since existing work on citation sentiment analysis uses more data #TARGET_REF ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "These results show that the task of jointly detecting sentiment and context is a hard problem. For our second set of experiments, we use the three-class annotation scheme. We merge the text of the sentences in the context windows as well as their dependency triplets to obtain the features. The results are reported in Table 3 with best results in bold. Although these results are not better than the context-less baseline, the reason might be data sparsity since existing work on citation sentiment analysis uses more data #TARGET_REF .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Although these results are not better than the context-less baseline, the reason might be data sparsity since existing work on citation sentiment analysis uses more data #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "I can't help it, she said, pulling a long face, It's them pills I took, to bring it off, she said [158] [159] Her chatty tone and colloquial grammar and lexis distinguish her voice from many others in the poem, such as the formal and traditionally poetic voice of a narrator that recurs many times in the poem:",
                "Above the antique mantel was displayed As though a window gave upon the sylvan scene The change of #REF [98] [99] Although the stylistic contrasts between these and other voices are clear to many readers, Eliot does not explicitly mark the transitions, nor is it obvious when a voice has reappeared.",
                "Our previous work focused on only the segmentation part of the voice identification task #TARGET_REF .",
                "Here, we instead assume an initial segmentation and then try to create clusters corresponding to segments of the The Waste Land which are spoken by the same voice.",
                "Of particular interest is the influence of the initial segmentation on the success of this downstream task."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                1
            ]
        },
        "input": "I can't help it, she said, pulling a long face, It's them pills I took, to bring it off, she said [158] [159] Her chatty tone and colloquial grammar and lexis distinguish her voice from many others in the poem, such as the formal and traditionally poetic voice of a narrator that recurs many times in the poem: Above the antique mantel was displayed As though a window gave upon the sylvan scene The change of #REF [98] [99] Although the stylistic contrasts between these and other voices are clear to many readers, Eliot does not explicitly mark the transitions, nor is it obvious when a voice has reappeared. Our previous work focused on only the segmentation part of the voice identification task #TARGET_REF . Here, we instead assume an initial segmentation and then try to create clusters corresponding to segments of the The Waste Land which are spoken by the same voice. Of particular interest is the influence of the initial segmentation on the success of this downstream task.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Our previous work focused on only the segmentation part of the voice identification task #TARGET_REF .\", \"Here, we instead assume an initial segmentation and then try to create clusters corresponding to segments of the The Waste Land which are spoken by the same voice.\", \"Of particular interest is the influence of the initial segmentation on the success of this downstream task.\"]}"
    },
    {
        "gold": {
            "text": [
                "Clustering techniques have been applied to literature in general; for instance, #REF clustered novels according to style, and recent work in distinguishing two authors of sections of the Bible (#REF) relies crucially on an initial clustering which is bootstrapped into a supervised classifier which is applied to segments.",
                "Beyond literature, the tasks of stylistic inconsistency detection (#REF; #REF) and intrinsic (unsupervised) plagiarism detection (#REF) are very closely related to our interests here, though in such tasks usually only two authors are posited; more general kinds of authorship identification (#REF ) may include many more authors, though some form of supervision (i.e. training data) is usually assumed.",
                "Our work here is built on our earlier work #TARGET_REF .",
                "Our segmentation model for The Waste Land was based on a stylistic change curve whose values are the distance between stylistic feature vectors derived from 50 token spans on either side of each point (spaces between tokens) in the text; the local maxima of this curve represent likely voice switches.",
                "Performance on The Waste Land was far from perfect, but evaluation using standard text segmentation metrics (#REF) indicated that it was well above various baselines."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Clustering techniques have been applied to literature in general; for instance, #REF clustered novels according to style, and recent work in distinguishing two authors of sections of the Bible (#REF) relies crucially on an initial clustering which is bootstrapped into a supervised classifier which is applied to segments. Beyond literature, the tasks of stylistic inconsistency detection (#REF; #REF) and intrinsic (unsupervised) plagiarism detection (#REF) are very closely related to our interests here, though in such tasks usually only two authors are posited; more general kinds of authorship identification (#REF ) may include many more authors, though some form of supervision (i.e. training data) is usually assumed. Our work here is built on our earlier work #TARGET_REF . Our segmentation model for The Waste Land was based on a stylistic change curve whose values are the distance between stylistic feature vectors derived from 50 token spans on either side of each point (spaces between tokens) in the text; the local maxima of this curve represent likely voice switches. Performance on The Waste Land was far from perfect, but evaluation using standard text segmentation metrics (#REF) indicated that it was well above various baselines.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Our work here is built on our earlier work #TARGET_REF .\", \"Our segmentation model for The Waste Land was based on a stylistic change curve whose values are the distance between stylistic feature vectors derived from 50 token spans on either side of each point (spaces between tokens) in the text; the local maxima of this curve represent likely voice switches.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our approach to voice identification in The Waste Land consists first of identifying the boundaries of voice spans #TARGET_REF .",
                "Given a segmentation of the text, we consider each span as a data point in a clustering problem.",
                "The elements of the vector correspond to the best feature set from the segmentation task, with the rationale that features which were useful for detecting changes in style should also be useful for identifying stylistic similarities.",
                "Our features therefore include: a collection of readability metrics (including word length), frequency of punctuation, line breaks, and various parts-ofspeech, lexical density, average frequency in a large external corpus (#REF) , lexiconbased sentiment metrics using SentiWordNet (#REF) , formality score (#REF) , and, perhaps most notably, the centroid of 20-dimensional distributional vectors built using latent semantic analysis (#REF), reflecting the use of words in a large web corpus (#REF) ; in previous work (#REF) , we established that such vectors contain useful stylistic information about the English lexicon (including rare words that appear only occasionally in such a corpus), and indeed LSA vectors were the single most promising feature type for segmentation.",
                "For a more detailed discussion of the feature set, see #REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                1,
                1,
                1,
                0
            ]
        },
        "input": "Our approach to voice identification in The Waste Land consists first of identifying the boundaries of voice spans #TARGET_REF . Given a segmentation of the text, we consider each span as a data point in a clustering problem. The elements of the vector correspond to the best feature set from the segmentation task, with the rationale that features which were useful for detecting changes in style should also be useful for identifying stylistic similarities. Our features therefore include: a collection of readability metrics (including word length), frequency of punctuation, line breaks, and various parts-ofspeech, lexical density, average frequency in a large external corpus (#REF) , lexiconbased sentiment metrics using SentiWordNet (#REF) , formality score (#REF) , and, perhaps most notably, the centroid of 20-dimensional distributional vectors built using latent semantic analysis (#REF), reflecting the use of words in a large web corpus (#REF) ; in previous work (#REF) , we established that such vectors contain useful stylistic information about the English lexicon (including rare words that appear only occasionally in such a corpus), and indeed LSA vectors were the single most promising feature type for segmentation. For a more detailed discussion of the feature set, see #REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Our approach to voice identification in The Waste Land consists first of identifying the boundaries of voice spans #TARGET_REF .\", \"Given a segmentation of the text, we consider each span as a data point in a clustering problem.\", \"The elements of the vector correspond to the best feature set from the segmentation task, with the rationale that features which were useful for detecting changes in style should also be useful for identifying stylistic similarities.\", \"Our features therefore include: a collection of readability metrics (including word length), frequency of punctuation, line breaks, and various parts-ofspeech, lexical density, average frequency in a large external corpus (#REF) , lexiconbased sentiment metrics using SentiWordNet (#REF) , formality score (#REF) , and, perhaps most notably, the centroid of 20-dimensional distributional vectors built using latent semantic analysis (#REF), reflecting the use of words in a large web corpus (#REF) ; in previous work (#REF) , we established that such vectors contain useful stylistic information about the English lexicon (including rare words that appear only occasionally in such a corpus), and indeed LSA vectors were the single most promising feature type for segmentation.\"]}"
    },
    {
        "gold": {
            "text": [
                "The elements of the vector correspond to the best feature set from the segmentation task, with the rationale that features which were useful for detecting changes in style should also be useful for identifying stylistic similarities.",
                "Our features therefore include: a collection of readability metrics (including word length), frequency of punctuation, line breaks, and various parts-ofspeech, lexical density, average frequency in a large external corpus (#REF) , lexiconbased sentiment metrics using SentiWordNet (#REF) , formality score (#REF) , and, perhaps most notably, the centroid of 20-dimensional distributional vectors built using latent semantic analysis (#REF), reflecting the use of words in a large web corpus (#REF) ; in previous work (#REF) , we established that such vectors contain useful stylistic information about the English lexicon (including rare words that appear only occasionally in such a corpus), and indeed LSA vectors were the single most promising feature type for segmentation.",
                "For a more detailed discussion of the feature set, see #TARGET_REF .",
                "All the features are normalized to a mean of zero and a standard deviation of 1.",
                "For clustering, we use a slightly modified version of the popular k-means algorithm (#REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The elements of the vector correspond to the best feature set from the segmentation task, with the rationale that features which were useful for detecting changes in style should also be useful for identifying stylistic similarities. Our features therefore include: a collection of readability metrics (including word length), frequency of punctuation, line breaks, and various parts-ofspeech, lexical density, average frequency in a large external corpus (#REF) , lexiconbased sentiment metrics using SentiWordNet (#REF) , formality score (#REF) , and, perhaps most notably, the centroid of 20-dimensional distributional vectors built using latent semantic analysis (#REF), reflecting the use of words in a large web corpus (#REF) ; in previous work (#REF) , we established that such vectors contain useful stylistic information about the English lexicon (including rare words that appear only occasionally in such a corpus), and indeed LSA vectors were the single most promising feature type for segmentation. For a more detailed discussion of the feature set, see #TARGET_REF . All the features are normalized to a mean of zero and a standard deviation of 1. For clustering, we use a slightly modified version of the popular k-means algorithm (#REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"For a more detailed discussion of the feature set, see #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We consider three segmentations: the segmentation of our gold standard (Gold), the segmentation predicted by our segmentation model (Automatic), and a segmentation which consists of equallength spans (Even), with the same number of spans as in the gold standard.",
                "The Even segmentation should be viewed as the baseline for segmentation, and the Gold segmentation an \"oracle\" representing an upper bound on segmentation performance.",
                "For the automatic segmentation model, we use the settings from #TARGET_REF .",
                "We also compare three possible clusterings for each segmentation: no clustering at all (Initial), that is, we assume that each segment is a new voice; k-means clustering (k-means), as outlined above; and random clustering (Random), in which we randomly assign each voice to a cluster.",
                "For these latter two methods, which both have a random component, we averaged our metrics over 50 runs."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We consider three segmentations: the segmentation of our gold standard (Gold), the segmentation predicted by our segmentation model (Automatic), and a segmentation which consists of equallength spans (Even), with the same number of spans as in the gold standard. The Even segmentation should be viewed as the baseline for segmentation, and the Gold segmentation an \"oracle\" representing an upper bound on segmentation performance. For the automatic segmentation model, we use the settings from #TARGET_REF . We also compare three possible clusterings for each segmentation: no clustering at all (Initial), that is, we assume that each segment is a new voice; k-means clustering (k-means), as outlined above; and random clustering (Random), in which we randomly assign each voice to a cluster. For these latter two methods, which both have a random component, we averaged our metrics over 50 runs.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For the automatic segmentation model, we use the settings from #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Non-parametric Bayesian methods have been successfully applied to directly learn phrase pairs from a bilingual corpus with little or no dependence on word alignments (#REF; #REF) .",
                "Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (#REF) or the Hidden Markov Model (HMM) (#REF) .",
                "We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (#REF; #REF; #TARGET_REF; #REF) , as opposed to formally syntactic systems such as Hiero (#REF) .",
                "The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given.",
                "Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Non-parametric Bayesian methods have been successfully applied to directly learn phrase pairs from a bilingual corpus with little or no dependence on word alignments (#REF; #REF) . Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (#REF) or the Hidden Markov Model (HMM) (#REF) . We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (#REF; #REF; #TARGET_REF; #REF) , as opposed to formally syntactic systems such as Hiero (#REF) . The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given. Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (#REF; #REF; #TARGET_REF; #REF) , as opposed to formally syntactic systems such as Hiero (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "LHSN, as shown by #TARGET_REF , cannot accurately restore the true conditional probabilities of the target sentences given the source sentences in the training corpus.",
                "This indicates that LHSN is not good at predicting unseen sentences or at translating new sentences.",
                "But this deficiency does not affect its ability to estimate the expected counts of the TTS templates, because the posteriors of the TTS templates only depend on the comparative probabilities of the different derivations of a training instance (a pair of tree and string).",
                "In fact, as we show in Section 4, LHSN is better than ROOTN in liberating smaller TTS templates out of the big templates, since it is less biased to the big templates in the EM training.",
                "4 Because the two normalization methods have their 4 Based on LHSN, the difference between the probability of a big Template and the product of the probabilities of E-step: for all pair of syntax tree T and target string S do for all TTS Template t do EC(t)+ = (#REF ) is is used in our system to speed up the training process, similar to #REF ."
            ],
            "label": [
                "USE",
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                1
            ]
        },
        "input": "LHSN, as shown by #TARGET_REF , cannot accurately restore the true conditional probabilities of the target sentences given the source sentences in the training corpus. This indicates that LHSN is not good at predicting unseen sentences or at translating new sentences. But this deficiency does not affect its ability to estimate the expected counts of the TTS templates, because the posteriors of the TTS templates only depend on the comparative probabilities of the different derivations of a training instance (a pair of tree and string). In fact, as we show in Section 4, LHSN is better than ROOTN in liberating smaller TTS templates out of the big templates, since it is less biased to the big templates in the EM training. 4 Because the two normalization methods have their 4 Based on LHSN, the difference between the probability of a big Template and the product of the probabilities of E-step: for all pair of syntax tree T and target string S do for all TTS Template t do EC(t)+ = (#REF ) is is used in our system to speed up the training process, similar to #REF .",
        "output": "{\"label\": [\"USE\", \"BACK\"], \"context\": [\"LHSN, as shown by #TARGET_REF , cannot accurately restore the true conditional probabilities of the target sentences given the source sentences in the training corpus.\", \"4 Because the two normalization methods have their 4 Based on LHSN, the difference between the probability of a big Template and the product of the probabilities of E-step: for all pair of syntax tree T and target string S do for all TTS Template t do EC(t)+ = (#REF ) is is used in our system to speed up the training process, similar to #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "There are two ways that TTS templates are commonly used in machine translation.",
                "The first is synchronous parsing #TARGET_REF; #REF) , where TTS templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up.",
                "The other way is the TTS transducer (#REF; #REF) , where TTS templates are used just as their name indicates: to transform a source parse tree (or forest) into the proper target string.",
                "Since synchronous parsing considers all possible synchronous parse trees of the source sentence, it is less constrained than TTS transducers and hence requires more computational power.",
                "In this paper, we use a TTS transducer to test the performance of different TTS templates, but our techniques could also be applied to SSMT systems based on synchronous parsing."
            ],
            "label": [
                "BACK",
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                1
            ]
        },
        "input": "There are two ways that TTS templates are commonly used in machine translation. The first is synchronous parsing #TARGET_REF; #REF) , where TTS templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up. The other way is the TTS transducer (#REF; #REF) , where TTS templates are used just as their name indicates: to transform a source parse tree (or forest) into the proper target string. Since synchronous parsing considers all possible synchronous parse trees of the source sentence, it is less constrained than TTS transducers and hence requires more computational power. In this paper, we use a TTS transducer to test the performance of different TTS templates, but our techniques could also be applied to SSMT systems based on synchronous parsing.",
        "output": "{\"label\": [\"BACK\", \"SIM\"], \"context\": [\"The first is synchronous parsing #TARGET_REF; #REF) , where TTS templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up.\", \"In this paper, we use a TTS transducer to test the performance of different TTS templates, but our techniques could also be applied to SSMT systems based on synchronous parsing.\"]}"
    },
    {
        "gold": {
            "text": [
                "The Expectation-Maximization (EM) algorithm (#REF) can be used to estimate the TTS templates' probabilities, given a generative model addressing how a pair of source syntax tree and target string is generated.",
                "There are two commonly used generative models for syntaxbased MT systems, each of which corresponds to a normalization method for the TTS templates.",
                "The LHS-based normalization (LHSN) (#REF; #REF) , corresponds to the generative process where the source syntax subtree is first generated, and then the target string is generated given the source syntax subtree.",
                "The other one is normalization based on the root of the LHS (ROOTN) #TARGET_REF , corresponding to the generative process where, given the root of the syntax subtree, the LHS syntax subtree and the RHS string are generated simultaneously.",
                "By omitting the decomposition probability in the LHS-based generative model, the two generative models share the same formula for computing the probability of a training instance:"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The Expectation-Maximization (EM) algorithm (#REF) can be used to estimate the TTS templates' probabilities, given a generative model addressing how a pair of source syntax tree and target string is generated. There are two commonly used generative models for syntaxbased MT systems, each of which corresponds to a normalization method for the TTS templates. The LHS-based normalization (LHSN) (#REF; #REF) , corresponds to the generative process where the source syntax subtree is first generated, and then the target string is generated given the source syntax subtree. The other one is normalization based on the root of the LHS (ROOTN) #TARGET_REF , corresponding to the generative process where, given the root of the syntax subtree, the LHS syntax subtree and the RHS string are generated simultaneously. By omitting the decomposition probability in the LHS-based generative model, the two generative models share the same formula for computing the probability of a training instance:",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The other one is normalization based on the root of the LHS (ROOTN) #TARGET_REF , corresponding to the generative process where, given the root of the syntax subtree, the LHS syntax subtree and the RHS string are generated simultaneously.\"]}"
    },
    {
        "gold": {
            "text": [
                "TTS templates are commonly generated by decomposing a pair of aligned source syntax tree and target string into smaller pairs of tree fragments and target string (i.e., the TTS templates).",
                "To keep the number of TTS templates to a manageable scale, only the non-decomposable TTS templates are generated.",
                "This algorithm is referred to as GHKM (#REF) and is widely used in SSMT systems #TARGET_REF; #REF; #REF) .",
                "The word alignment used in GHKM is usually computed independent of the syntactic structure, and as #REF and #REF have noted,",
                "Ch-En En-Ch Union Heuristic 28.6% 33.0% 45.9% 20.1% Table 2 : In the selected big templates, the distribution of words in the templates of different sizes, which are measured based on the number of symbols in their RHSs is not the best for SSMT systems."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "TTS templates are commonly generated by decomposing a pair of aligned source syntax tree and target string into smaller pairs of tree fragments and target string (i.e., the TTS templates). To keep the number of TTS templates to a manageable scale, only the non-decomposable TTS templates are generated. This algorithm is referred to as GHKM (#REF) and is widely used in SSMT systems #TARGET_REF; #REF; #REF) . The word alignment used in GHKM is usually computed independent of the syntactic structure, and as #REF and #REF have noted, Ch-En En-Ch Union Heuristic 28.6% 33.0% 45.9% 20.1% Table 2 : In the selected big templates, the distribution of words in the templates of different sizes, which are measured based on the number of symbols in their RHSs is not the best for SSMT systems.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"This algorithm is referred to as GHKM (#REF) and is widely used in SSMT systems #TARGET_REF; #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Another task similar to the WSI is the unsupervised author name disambiguation (UAND) task (#REF) , where it aims to automatically find different authors, instead of words, with the same name.",
                "In this paper, we consider a latent variable modeling approach to WSI problem as it is proven to be more effective than other approaches (Chang, Pei, and #REF; #REF) .",
                "Specifically, we look into methods based on Latent Dirichlet Allocation (LDA) (Blei, Ng, and #REF) , a topic modeling method that automatically discovers the topics underlying a set of documents using Dirichlet priors to infer the multinomial distribution over words and topics.",
                "LDA naturally answers two of the three main problems mentioned above, i.e. (C1) and (C2), of the WSI task (#REF) .",
                "However, it is not flexible with regards to (C3), or the sense granularity problem, as it requires the users to specify the number of senses: Current systems ( #TARGET_REF; Chang, Pei, and #REF) required to set the number of senses to a small number (set to 3 or 5 in the literature) to get a good accuracy, however many words may have a large number of senses, e.g. play in Figure 1 ."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Another task similar to the WSI is the unsupervised author name disambiguation (UAND) task (#REF) , where it aims to automatically find different authors, instead of words, with the same name. In this paper, we consider a latent variable modeling approach to WSI problem as it is proven to be more effective than other approaches (Chang, Pei, and #REF; #REF) . Specifically, we look into methods based on Latent Dirichlet Allocation (LDA) (Blei, Ng, and #REF) , a topic modeling method that automatically discovers the topics underlying a set of documents using Dirichlet priors to infer the multinomial distribution over words and topics. LDA naturally answers two of the three main problems mentioned above, i.e. (C1) and (C2), of the WSI task (#REF) . However, it is not flexible with regards to (C3), or the sense granularity problem, as it requires the users to specify the number of senses: Current systems ( #TARGET_REF; Chang, Pei, and #REF) required to set the number of senses to a small number (set to 3 or 5 in the literature) to get a good accuracy, however many words may have a large number of senses, e.g. play in Figure 1 .",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"However, it is not flexible with regards to (C3), or the sense granularity problem, as it requires the users to specify the number of senses: Current systems ( #TARGET_REF; Chang, Pei, and #REF) required to set the number of senses to a small number (set to 3 or 5 in the literature) to get a good accuracy, however many words may have a large number of senses, e.g. play in Figure 1 .\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, when the number of senses S is set to 10, the model tries to assign all the senses to all instances even when the original number of senses of a target word is 3.",
                "LDA extensions ( #TARGET_REF; Chang, Pei, and #REF) mitigated this problem by setting S to a small number (e.g. 3 or 5).",
                "However, this is not a good solution because there are many words with more than five senses.",
                "Second, LDA and its extensions do not consider the existence of fine-grained senses.",
                "For example, the cold: absence of heat and the cold: sensation from low temperature senses are fine-grained senses because they are similarly related to temperature yet have different usage."
            ],
            "label": [
                "MOT",
                "BACK"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "For example, when the number of senses S is set to 10, the model tries to assign all the senses to all instances even when the original number of senses of a target word is 3. LDA extensions ( #TARGET_REF; Chang, Pei, and #REF) mitigated this problem by setting S to a small number (e.g. 3 or 5). However, this is not a good solution because there are many words with more than five senses. Second, LDA and its extensions do not consider the existence of fine-grained senses. For example, the cold: absence of heat and the cold: sensation from low temperature senses are fine-grained senses because they are similarly related to temperature yet have different usage.",
        "output": "{\"label\": [\"MOT\", \"BACK\"], \"context\": [\"LDA extensions ( #TARGET_REF; Chang, Pei, and #REF) mitigated this problem by setting S to a small number (e.g. 3 or 5).\", \"However, this is not a good solution because there are many words with more than five senses.\"]}"
    },
    {
        "gold": {
            "text": [
                "The main weakness of LDA when used on WSI task is the sense granularity problem.",
                "Recent models such as HC (Chang, Pei, and #REF) and STM (#REF) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error.",
                "However, such tuning, often empirically set to a small number such as S = 3 ( #TARGET_REF) , fails to infer varying number of senses of words, especially for words with a higher number of senses.",
                "Nonparametric models such as HDP and BNP-HC Chang, Pei, and #REF) claim to automatically induce different S for each word.",
                "However, as shown in the results in Table 2 , the estimated S is far from the actual number of senses and both models are ineffective."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The main weakness of LDA when used on WSI task is the sense granularity problem. Recent models such as HC (Chang, Pei, and #REF) and STM (#REF) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error. However, such tuning, often empirically set to a small number such as S = 3 ( #TARGET_REF) , fails to infer varying number of senses of words, especially for words with a higher number of senses. Nonparametric models such as HDP and BNP-HC Chang, Pei, and #REF) claim to automatically induce different S for each word. However, as shown in the results in Table 2 , the estimated S is far from the actual number of senses and both models are ineffective.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"However, such tuning, often empirically set to a small number such as S = 3 ( #TARGET_REF) , fails to infer varying number of senses of words, especially for words with a higher number of senses.\"]}"
    },
    {
        "gold": {
            "text": [
                "In the most recent shared task on WSI (#REF) , top models used lexical substitution method (AI-KU) (#REF) and Hierarchical Dirichlet Process trained with additional instances (Unimelb) .",
                "Latent variable models such as LDA (Blei, Ng, and #REF) are used to induce the word sense of a target word after rigorous preprocessing and feature extraction (LDA, Spectral) (#REF).",
                "More recent models introduced a latent variable for the sense of a word, with the assumption that a sense has multiple concepts (HC, HC+Zipf) (Chang, Pei, and #REF) and that topics and senses should be inferred jointly (STM) ( #TARGET_REF) .",
                "In this paper, we also use a separate sense latent variable, however we show boost in performance by representing it with more versatility and by incorporating the use of targetneighbor pairs.",
                "HC was also extended to a nonparametric model (BNP-HC) (#REF ) in order to automatically set the number of senses of a word, providing flexibility to the sense granularity (Yao and Van #REF; #REF; ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In the most recent shared task on WSI (#REF) , top models used lexical substitution method (AI-KU) (#REF) and Hierarchical Dirichlet Process trained with additional instances (Unimelb) . Latent variable models such as LDA (Blei, Ng, and #REF) are used to induce the word sense of a target word after rigorous preprocessing and feature extraction (LDA, Spectral) (#REF). More recent models introduced a latent variable for the sense of a word, with the assumption that a sense has multiple concepts (HC, HC+Zipf) (Chang, Pei, and #REF) and that topics and senses should be inferred jointly (STM) ( #TARGET_REF) . In this paper, we also use a separate sense latent variable, however we show boost in performance by representing it with more versatility and by incorporating the use of targetneighbor pairs. HC was also extended to a nonparametric model (BNP-HC) (#REF ) in order to automatically set the number of senses of a word, providing flexibility to the sense granularity (Yao and Van #REF; #REF; .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"More recent models introduced a latent variable for the sense of a word, with the assumption that a sense has multiple concepts (HC, HC+Zipf) (Chang, Pei, and #REF) and that topics and senses should be inferred jointly (STM) ( #TARGET_REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Recent inclusions to the WSI models are neural-based dense distributional representation models.",
                "STM also used word embeddings (#REF) to assign similarity weights during inference (STM+w2v) ( #TARGET_REF) .",
                "Existing sense embeddings are also used to perform word sense induction (CRP-PPMI, SE-WSI-fix, WG, DIVE) (#REF; #REF; #REF ).",
                "These models, on their own, do not perform well on the WSI task until recently when embeddings of words and their dependencies are used to construct a probabilistic model (MCC) (#REF) .",
                "We show that neuralbased embeddings are still ineffective for this task and that our model performs better than these models as well."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Recent inclusions to the WSI models are neural-based dense distributional representation models. STM also used word embeddings (#REF) to assign similarity weights during inference (STM+w2v) ( #TARGET_REF) . Existing sense embeddings are also used to perform word sense induction (CRP-PPMI, SE-WSI-fix, WG, DIVE) (#REF; #REF; #REF ). These models, on their own, do not perform well on the WSI task until recently when embeddings of words and their dependencies are used to construct a probabilistic model (MCC) (#REF) . We show that neuralbased embeddings are still ineffective for this task and that our model performs better than these models as well.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"STM also used word embeddings (#REF) to assign similarity weights during inference (STM+w2v) ( #TARGET_REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "To solve the problems above, we propose to extend LDA in two parts.",
                "First, we introduce a new latent variable, apart from the topic latent variable, to represent word senses.",
                "Previous works also attempted to introduce a separate sense latent variable to generate all the words (Chang, Pei, and #REF) , or to generate only the neighboring words within a local context, decided by a strict user-specified window ( #TARGET_REF) .",
                "We improve by softening the strict local context assumption by introducing a switch variable which decides whether a word not in a local context should be generated by conditioning also on the sense latent variable.",
                "Our experiments show that our sense representation provides superior improvements from previous models."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "To solve the problems above, we propose to extend LDA in two parts. First, we introduce a new latent variable, apart from the topic latent variable, to represent word senses. Previous works also attempted to introduce a separate sense latent variable to generate all the words (Chang, Pei, and #REF) , or to generate only the neighboring words within a local context, decided by a strict user-specified window ( #TARGET_REF) . We improve by softening the strict local context assumption by introducing a switch variable which decides whether a word not in a local context should be generated by conditioning also on the sense latent variable. Our experiments show that our sense representation provides superior improvements from previous models.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Previous works also attempted to introduce a separate sense latent variable to generate all the words (Chang, Pei, and #REF) , or to generate only the neighboring words within a local context, decided by a strict user-specified window ( #TARGET_REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The main weakness of LDA when used on WSI task is the sense granularity problem.",
                "Recent models such as HC (Chang, Pei, and #REF) and STM ( #TARGET_REF) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error.",
                "However, such tuning, often empirically set to a small number such as S = 3 (#REF) , fails to infer varying number of senses of words, especially for words with a higher number of senses.",
                "Nonparametric models such as HDP and BNP-HC Chang, Pei, and #REF) claim to automatically induce different S for each word.",
                "However, as shown in the results in Table 2 , the estimated S is far from the actual number of senses and both models are ineffective."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The main weakness of LDA when used on WSI task is the sense granularity problem. Recent models such as HC (Chang, Pei, and #REF) and STM ( #TARGET_REF) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error. However, such tuning, often empirically set to a small number such as S = 3 (#REF) , fails to infer varying number of senses of words, especially for words with a higher number of senses. Nonparametric models such as HDP and BNP-HC Chang, Pei, and #REF) claim to automatically induce different S for each word. However, as shown in the results in Table 2 , the estimated S is far from the actual number of senses and both models are ineffective.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Recent models such as HC (Chang, Pei, and #REF) and STM ( #TARGET_REF) mitigated this problem by tuning the number of senses hyperparameter S to minimize the error.\"]}"
    },
    {
        "gold": {
            "text": [
                "Datasets and preprocessing We use two publicly available datasets: #REF Task 14 (#REF) For preprocessing, we do tokenization, lemmatization, and removing of symbols to build the word lists using Stanford CoreNLP (#REF) .",
                "We divide the word lists into two contexts: the local and global context.",
                "Following ( #TARGET_REF), we set the local context window to 10, with a maximum number of words of 21 (i.e. 10 words before and 10 words after).",
                "Other words are put into the global context.",
                "Note however that AutoSense has a less strict global/local context assumption as it treats some words in the global context as local depending on the switch variable."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Datasets and preprocessing We use two publicly available datasets: #REF Task 14 (#REF) For preprocessing, we do tokenization, lemmatization, and removing of symbols to build the word lists using Stanford CoreNLP (#REF) . We divide the word lists into two contexts: the local and global context. Following ( #TARGET_REF), we set the local context window to 10, with a maximum number of words of 21 (i.e. 10 words before and 10 words after). Other words are put into the global context. Note however that AutoSense has a less strict global/local context assumption as it treats some words in the global context as local depending on the switch variable.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following ( #TARGET_REF), we set the local context window to 10, with a maximum number of words of 21 (i.e. 10 words before and 10 words after).\"]}"
    },
    {
        "gold": {
            "text": [
                "Note however that AutoSense has a less strict global/local context assumption as it treats some words in the global context as local depending on the switch variable.",
                "Parameter setting We set the hyperparameters to α = 0.1, β = 0.01, γ = 0.3, following the conventional setup (#REF; Chemudugunta, Smyth, and #REF) .",
                "We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following ( #TARGET_REF) .",
                "We also include four other versions of our model: AutoSense −wp removes the target-neighbor pair constraint and transforms the local context to that of STM, AutoSense −sw removes the switch variable and transforms the global context to that of LDA, AutoSense s=X is a tuned and best version of the model, where the number of senses is tuned over a separate development set provided by the shared tasks and X is the tuned number of sense, different for each dataset, and AutoSense s=100 is the overestimated and worst version of the model, where we set the number of senses to an arbitrary large number, i.e. 100.",
                "We set the number of iterations to 2000 and run the Gibbs sampler."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Note however that AutoSense has a less strict global/local context assumption as it treats some words in the global context as local depending on the switch variable. Parameter setting We set the hyperparameters to α = 0.1, β = 0.01, γ = 0.3, following the conventional setup (#REF; Chemudugunta, Smyth, and #REF) . We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following ( #TARGET_REF) . We also include four other versions of our model: AutoSense −wp removes the target-neighbor pair constraint and transforms the local context to that of STM, AutoSense −sw removes the switch variable and transforms the global context to that of LDA, AutoSense s=X is a tuned and best version of the model, where the number of senses is tuned over a separate development set provided by the shared tasks and X is the tuned number of sense, different for each dataset, and AutoSense s=100 is the overestimated and worst version of the model, where we set the number of senses to an arbitrary large number, i.e. 100. We set the number of iterations to 2000 and run the Gibbs sampler.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following ( #TARGET_REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following (#REF) .",
                "We also include four other versions of our model: AutoSense −wp removes the target-neighbor pair constraint and transforms the local context to that of STM, AutoSense −sw removes the switch variable and transforms the global context to that of LDA, AutoSense s=X is a tuned and best version of the model, where the number of senses is tuned over a separate development set provided by the shared tasks and X is the tuned number of sense, different for each dataset, and AutoSense s=100 is the overestimated and worst version of the model, where we set the number of senses to an arbitrary large number, i.e. 100.",
                "We set the number of iterations to 2000 and run the Gibbs sampler.",
                "Following the convention of previous works (#REF; #REF; #TARGET_REF) , we assume convergence when the number of iterations is high.",
                "However, due to the randomized nature of Gibbs sampling, we report the average scores over 5 runs of Gibbs sampling."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following (#REF) . We also include four other versions of our model: AutoSense −wp removes the target-neighbor pair constraint and transforms the local context to that of STM, AutoSense −sw removes the switch variable and transforms the global context to that of LDA, AutoSense s=X is a tuned and best version of the model, where the number of senses is tuned over a separate development set provided by the shared tasks and X is the tuned number of sense, different for each dataset, and AutoSense s=100 is the overestimated and worst version of the model, where we set the number of senses to an arbitrary large number, i.e. 100. We set the number of iterations to 2000 and run the Gibbs sampler. Following the convention of previous works (#REF; #REF; #TARGET_REF) , we assume convergence when the number of iterations is high. However, due to the randomized nature of Gibbs sampling, we report the average scores over 5 runs of Gibbs sampling.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following the convention of previous works (#REF; #REF; #TARGET_REF) , we assume convergence when the number of iterations is high.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF For the #REF dataset, we compare models using two unsupervised metrics: V-measure (V-M) and paired F-score (F-S).",
                "V-M favors a high number of senses (e.g. assigning one cluster per instance), while F-S favors a small number of senses (e.g. all instances in one cluster) (#REF) .",
                "In order to get a common ground for comparison, we do a geometric average AVG of both metrics, following ( #TARGET_REF) .",
                "Finally, we also report the absolute difference between the actual (3.85) and induced number of senses as δ(#S).",
                "We compare with seven other models: a) LDA on cooccurrence graphs (LDA) and b) spectral clustering on cooccurrence graphs (Spectral) as reported in (Goyal and  Hovy Results are shown in Table 2a , where AutoSense outperforms other competing models on AVG."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "#REF For the #REF dataset, we compare models using two unsupervised metrics: V-measure (V-M) and paired F-score (F-S). V-M favors a high number of senses (e.g. assigning one cluster per instance), while F-S favors a small number of senses (e.g. all instances in one cluster) (#REF) . In order to get a common ground for comparison, we do a geometric average AVG of both metrics, following ( #TARGET_REF) . Finally, we also report the absolute difference between the actual (3.85) and induced number of senses as δ(#S). We compare with seven other models: a) LDA on cooccurrence graphs (LDA) and b) spectral clustering on cooccurrence graphs (Spectral) as reported in (Goyal and  Hovy Results are shown in Table 2a , where AutoSense outperforms other competing models on AVG.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In order to get a common ground for comparison, we do a geometric average AVG of both metrics, following ( #TARGET_REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "F-BC gives preference to labelling all instances with the same sense, while F-NMI gives preference to labelling all instances with distinct senses.",
                "Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in ( #TARGET_REF) .",
                "We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (#REF) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in (#REF) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (#REF) , and g) Multi Context Continuous model MCC as reported in (#REF) .",
                "Results are shown in Table 2b .",
                "Among the models, all versions of AutoSense perform better than other models on AVG."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "F-BC gives preference to labelling all instances with the same sense, while F-NMI gives preference to labelling all instances with distinct senses. Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in ( #TARGET_REF) . We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (#REF) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in (#REF) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (#REF) , and g) Multi Context Continuous model MCC as reported in (#REF) . Results are shown in Table 2b . Among the models, all versions of AutoSense perform better than other models on AVG.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in ( #TARGET_REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in (#REF) .",
                "We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (#REF) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in ( #TARGET_REF) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (#REF) , and g) Multi Context Continuous model MCC as reported in (#REF) .",
                "Results are shown in Table 2b .",
                "Among the models, all versions of AutoSense perform better than other models on AVG.",
                "The untuned AutoSense and AutoSense s=7 especially garner noticeable increase of 6.1% on fuzzy B-cubed metric from MCC, the previous best model."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in (#REF) . We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (#REF) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in ( #TARGET_REF) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (#REF) , and g) Multi Context Continuous model MCC as reported in (#REF) . Results are shown in Table 2b . Among the models, all versions of AutoSense perform better than other models on AVG. The untuned AutoSense and AutoSense s=7 especially garner noticeable increase of 6.1% on fuzzy B-cubed metric from MCC, the previous best model.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model (Unimelb) as reported in (#REF) , c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in ( #TARGET_REF) , e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (#REF) , and g) Multi Context Continuous model MCC as reported in (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "This means that introducing the target-neighbor pair is crucial to the improvement of the model.",
                "Finally, the overestimated AutoSense model performs as well as the other AutoSense models, even outperforming all previous models on AVG, which proves the effectiveness of AutoSense even when s is set to a large value.",
                "For completeness, we also report STM with additional contexts, STM+actual and STM+ukWac ( #TARGET_REF) , where they used the actual additional contexts from the original data and semantically similar contexts from ukWac, respectively, as additional global context.",
                "With the performance gain we achieved, AutoSense without additional context can perform comparably to models with additional contexts: Our model greatly outperforms these models on the Sense Word distribution   #Docs  1  hotel tour tourist summer flight  22  2  month ticket available performance  3  3 guest office stateroom class suite 3 * advance overseas line popular japan 0 * email day buy unable tour 0 * sort basic tour time 0 Table 3 : Six of the 15 senses of the target verb book using AutoSense with S = 15.",
                "The word lists shown are preprocessed to remove stopwords and the target word."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "This means that introducing the target-neighbor pair is crucial to the improvement of the model. Finally, the overestimated AutoSense model performs as well as the other AutoSense models, even outperforming all previous models on AVG, which proves the effectiveness of AutoSense even when s is set to a large value. For completeness, we also report STM with additional contexts, STM+actual and STM+ukWac ( #TARGET_REF) , where they used the actual additional contexts from the original data and semantically similar contexts from ukWac, respectively, as additional global context. With the performance gain we achieved, AutoSense without additional context can perform comparably to models with additional contexts: Our model greatly outperforms these models on the Sense Word distribution   #Docs  1  hotel tour tourist summer flight  22  2  month ticket available performance  3  3 guest office stateroom class suite 3 * advance overseas line popular japan 0 * email day buy unable tour 0 * sort basic tour time 0 Table 3 : Six of the 15 senses of the target verb book using AutoSense with S = 15. The word lists shown are preprocessed to remove stopwords and the target word.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For completeness, we also report STM with additional contexts, STM+actual and STM+ukWac ( #TARGET_REF) , where they used the actual additional contexts from the original data and semantically similar contexts from ukWac, respectively, as additional global context.\"]}"
    },
    {
        "gold": {
            "text": [
                "Incorporating the target-neighbor pairs helps the model differentiates both into fine-grained senses book: arrange for and reserve in advance and book: engage for a performance.",
                "We compare the competing models quantitatively on how they correctly detect the actual number of sense clusters using cluster error, which is the mean absolute error between the detected number and the actual number of sense clusters.",
                "We compare the cluster errors of LDA (Blei, Ng, and #REF) , STM ( #TARGET_REF) , HC (Chang, Pei, and #REF) , and a nonparametric model HDP (#REF ), with AutoSense.",
                "We report the results in Figure 4 .",
                "Results show that the cluster error of LDA increases sharply as the number of senses exceeds the actual mean number of senses."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Incorporating the target-neighbor pairs helps the model differentiates both into fine-grained senses book: arrange for and reserve in advance and book: engage for a performance. We compare the competing models quantitatively on how they correctly detect the actual number of sense clusters using cluster error, which is the mean absolute error between the detected number and the actual number of sense clusters. We compare the cluster errors of LDA (Blei, Ng, and #REF) , STM ( #TARGET_REF) , HC (Chang, Pei, and #REF) , and a nonparametric model HDP (#REF ), with AutoSense. We report the results in Figure 4 . Results show that the cluster error of LDA increases sharply as the number of senses exceeds the actual mean number of senses.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We compare the cluster errors of LDA (Blei, Ng, and #REF) , STM ( #TARGET_REF) , HC (Chang, Pei, and #REF) , and a nonparametric model HDP (#REF ), with AutoSense.\"]}"
    },
    {
        "gold": {
            "text": [
                "It includes the PubMed ID of the papers authored by the given author name.",
                "We extract the title, author list, publication venue, and abstract of each PubMed ID from the PubMed website.",
                "We use LDA (Blei, Ng, and #REF) , HC (Chang, Pei, and #REF) and STM ( #TARGET_REF) as baselines.",
                "We do not compare with non-text feature-based models (#REF; #REF ) because our goal is to compare sense topic models on a task where the sense granularities are more varied.",
                "For STM and AutoSense, the title, publication venue and the author names are used as local contexts while the abstract is used as the global context."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "It includes the PubMed ID of the papers authored by the given author name. We extract the title, author list, publication venue, and abstract of each PubMed ID from the PubMed website. We use LDA (Blei, Ng, and #REF) , HC (Chang, Pei, and #REF) and STM ( #TARGET_REF) as baselines. We do not compare with non-text feature-based models (#REF; #REF ) because our goal is to compare sense topic models on a task where the sense granularities are more varied. For STM and AutoSense, the title, publication venue and the author names are used as local contexts while the abstract is used as the global context.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use LDA (Blei, Ng, and #REF) , HC (Chang, Pei, and #REF) and STM ( #TARGET_REF) as baselines.\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper evaluates two semi-supervised techniques for the adaptation of a parse selection model to Wikipedia domains.",
                "The techniques examined are Structural Correspondence Learning (SCL) #TARGET_REF and Self-training (#REF; #REF) .",
                "A preliminary evaluation favors the use of SCL over the simpler self-training techniques."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0
            ]
        },
        "input": "This paper evaluates two semi-supervised techniques for the adaptation of a parse selection model to Wikipedia domains. The techniques examined are Structural Correspondence Learning (SCL) #TARGET_REF and Self-training (#REF; #REF) . A preliminary evaluation favors the use of SCL over the simpler self-training techniques.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The techniques examined are Structural Correspondence Learning (SCL) #TARGET_REF and Self-training (#REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "So far, Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis #TARGET_REF; ).",
                "An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (#REF) .",
                "However, the system just ended up at rank 7 out of 8 teams.",
                "Based on annotation differences in the datasets and a bug in their system (#REF) , their results are inconclusive.",
                "A recent attempt (#REF) shows promising results on applying SCL to parse disambiguation."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "So far, Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis #TARGET_REF; ). An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (#REF) . However, the system just ended up at rank 7 out of 8 teams. Based on annotation differences in the datasets and a bug in their system (#REF) , their results are inconclusive. A recent attempt (#REF) shows promising results on applying SCL to parse disambiguation.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"So far, Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis #TARGET_REF; ).\"]}"
    },
    {
        "gold": {
            "text": [
                "Structural Correspondence Learning #TARGET_REF exploits unlabeled data from both source and target domain to find correspondences among features from different domains.",
                "These correspondences are then integrated as new features in the labeled data of the source domain.",
                "The outline of SCL is given in Algorithm 1.",
                "The key to SCL is to exploit pivot features to automatically identify feature correspondences.",
                "Pivots are features occurring frequently and behaving similarly in both domains (#REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Structural Correspondence Learning #TARGET_REF exploits unlabeled data from both source and target domain to find correspondences among features from different domains. These correspondences are then integrated as new features in the labeled data of the source domain. The outline of SCL is given in Algorithm 1. The key to SCL is to exploit pivot features to automatically identify feature correspondences. Pivots are features occurring frequently and behaving similarly in both domains (#REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Structural Correspondence Learning #TARGET_REF exploits unlabeled data from both source and target domain to find correspondences among features from different domains.\"]}"
    },
    {
        "gold": {
            "text": [
                "The outline of SCL is given in Algorithm 1.",
                "The key to SCL is to exploit pivot features to automatically identify feature correspondences.",
                "Pivots are features occurring frequently and behaving similarly in both domains #TARGET_REF .",
                "They correspond to auxiliary problems in #REF .",
                "For every such pivot feature, a binary classifier is trained (step 2 of Algorithm 1) by masking the pivot feature in the data and trying to predict it with the remaining non-pivot features."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The outline of SCL is given in Algorithm 1. The key to SCL is to exploit pivot features to automatically identify feature correspondences. Pivots are features occurring frequently and behaving similarly in both domains #TARGET_REF . They correspond to auxiliary problems in #REF . For every such pivot feature, a binary classifier is trained (step 2 of Algorithm 1) by masking the pivot feature in the data and trying to predict it with the remaining non-pivot features.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Pivots are features occurring frequently and behaving similarly in both domains #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "So far, pivot features on the word level were used #TARGET_REF; .",
                "However, for parse disambiguation based on a conditional model they are irrelevant.",
                "Hence, we follow #REF and actually first parse the unlabeled data.",
                "This allows a possibly noisy, but more abstract representation of the underlying data.",
                "Features thus correspond to properties of parses: application of grammar rules (r1,r2 features), dependency relations (dep), PoS tags (f1,f2), syntactic features (s1), precedence (mf ), bilexical preferences (z), apposition (appos) and further features for unknown words, temporal phrases, coordination (h,in year and p1, respectively)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "So far, pivot features on the word level were used #TARGET_REF; . However, for parse disambiguation based on a conditional model they are irrelevant. Hence, we follow #REF and actually first parse the unlabeled data. This allows a possibly noisy, but more abstract representation of the underlying data. Features thus correspond to properties of parses: application of grammar rules (r1,r2 features), dependency relations (dep), PoS tags (f1,f2), syntactic features (s1), precedence (mf ), bilexical preferences (z), apposition (appos) and further features for unknown words, temporal phrases, coordination (h,in year and p1, respectively).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"So far, pivot features on the word level were used #TARGET_REF; .\"]}"
    },
    {
        "gold": {
            "text": [
                "In contrast, in semisupervised domain adaptation one has only unlabeled target data.",
                "It is a more realistic situation, but at the same time also considerably more difficult.",
                "In this paper we evaluate two semi-supervised approaches to domain adaptation of a discriminative parse selection model.",
                "We examine Structural Correspondence Learning (SCL) #TARGET_REF for this task, and compare it to several variants of Self-training (#REF; #REF) .",
                "For empirical evaluation (section 4) we use the Alpino parsing system for Dutch (#REF) ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "In contrast, in semisupervised domain adaptation one has only unlabeled target data. It is a more realistic situation, but at the same time also considerably more difficult. In this paper we evaluate two semi-supervised approaches to domain adaptation of a discriminative parse selection model. We examine Structural Correspondence Learning (SCL) #TARGET_REF for this task, and compare it to several variants of Self-training (#REF; #REF) . For empirical evaluation (section 4) we use the Alpino parsing system for Dutch (#REF) .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We examine Structural Correspondence Learning (SCL) #TARGET_REF for this task, and compare it to several variants of Self-training (#REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "They correspond to auxiliary problems in #REF .",
                "For every such pivot feature, a binary classifier is trained (step 2 of Algorithm 1) by masking the pivot feature in the data and trying to predict it with the remaining non-pivot features.",
                "Non-pivots that correlate with many of the same pivots are assumed to correspond.",
                "These pivot predictor weight vectors thus implicitly align non-pivot features from source and target domain.",
                "Intuitively, if we are able to find good correspondences through 'linking' pivots, then the augmented source data should transfer better to a target domain #TARGET_REF ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "They correspond to auxiliary problems in #REF . For every such pivot feature, a binary classifier is trained (step 2 of Algorithm 1) by masking the pivot feature in the data and trying to predict it with the remaining non-pivot features. Non-pivots that correlate with many of the same pivots are assumed to correspond. These pivot predictor weight vectors thus implicitly align non-pivot features from source and target domain. Intuitively, if we are able to find good correspondences through 'linking' pivots, then the augmented source data should transfer better to a target domain #TARGET_REF .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Intuitively, if we are able to find good correspondences through 'linking' pivots, then the augmented source data should transfer better to a target domain #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus, we exploit Wikipedia's category system to gather domain-specific target data.",
                "In our empirical setup, we follow #TARGET_REF and balance the size of source and target data.",
                "Thus, depending on the size of the resulting target domain dataset, and the \"broadness\" of the categories involved in creating it, we might wish to filter out certain pages.",
                "We implemented a filter mechanism that excludes pages of a certain category (e.g. a supercategory that is hypothesized to be \"too broad\").",
                "Further details about the dataset construction are given in (#REF The size of the target domain testsets is given in Table 2 ."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Thus, we exploit Wikipedia's category system to gather domain-specific target data. In our empirical setup, we follow #TARGET_REF and balance the size of source and target data. Thus, depending on the size of the resulting target domain dataset, and the \"broadness\" of the categories involved in creating it, we might wish to filter out certain pages. We implemented a filter mechanism that excludes pages of a certain category (e.g. a supercategory that is hypothesized to be \"too broad\"). Further details about the dataset construction are given in (#REF The size of the target domain testsets is given in Table 2 .",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"In our empirical setup, we follow #TARGET_REF and balance the size of source and target data.\"]}"
    },
    {
        "gold": {
            "text": [
                "The paper compares Structural Correspondence Learning #TARGET_REF with (various instances of) self-training (#REF; #REF) for the adaptation of a parse selection model to Wikipedia domains.",
                "The empirical findings show that none of the evaluated self-training variants (delible/indelible, single versus multiple iterations, various selection techniques) achieves a significant improvement over the baseline.",
                "The more 'indirect' exploitation of unlabeled data through SCL is more fruitful than pure self-training.",
                "Thus, favoring the use of the more complex method, although the findings are not confirmed on all testsets.",
                "Of course, our results are preliminary and, rather than warranting yet many definite conclusions, encourage further investigation of SCL (varying size of target data, pivots selection, bigger testsets as well as other domains etc.) as well as related semisupervised adaptation techniques."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The paper compares Structural Correspondence Learning #TARGET_REF with (various instances of) self-training (#REF; #REF) for the adaptation of a parse selection model to Wikipedia domains. The empirical findings show that none of the evaluated self-training variants (delible/indelible, single versus multiple iterations, various selection techniques) achieves a significant improvement over the baseline. The more 'indirect' exploitation of unlabeled data through SCL is more fruitful than pure self-training. Thus, favoring the use of the more complex method, although the findings are not confirmed on all testsets. Of course, our results are preliminary and, rather than warranting yet many definite conclusions, encourage further investigation of SCL (varying size of target data, pivots selection, bigger testsets as well as other domains etc.) as well as related semisupervised adaptation techniques.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"The paper compares Structural Correspondence Learning #TARGET_REF with (various instances of) self-training (#REF; #REF) for the adaptation of a parse selection model to Wikipedia domains.\"]}"
    },
    {
        "gold": {
            "text": [
                "This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (#REF; #REF) to document summarization (#REF ), text classification (#REF; #REF) , text segmentation (#REF) , information extraction ( #TARGET_REF) , image caption generation (#REF; #REF) , sentence generation (#REFb) , dialogue control (#REF; #REFa) , sentiment classification, recommendation system, question answering (#REF) and machine translation , to name a few.",
                "Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model.",
                "The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.",
                "The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.",
                "This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #REF) , memory-augmented neural network (#REF; #REF) , stochastic neural network #REF) , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) ."
            ],
            "label": [
                "USE",
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (#REF; #REF) to document summarization (#REF ), text classification (#REF; #REF) , text segmentation (#REF) , information extraction ( #TARGET_REF) , image caption generation (#REF; #REF) , sentence generation (#REFb) , dialogue control (#REF; #REFa) , sentiment classification, recommendation system, question answering (#REF) and machine translation , to name a few. Traditionally, \"deep learning\" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model. The \"semantic structure\" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs. The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference. This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #REF) , memory-augmented neural network (#REF; #REF) , stochastic neural network #REF) , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) .",
        "output": "{\"label\": [\"USE\", \"BACK\"], \"context\": [\"This tutorial introduces the advances in deep Bayesian learning with abundant applications for natural language understanding ranging from speech recognition (#REF; #REF) to document summarization (#REF ), text classification (#REF; #REF) , text segmentation (#REF) , information extraction ( #TARGET_REF) , image caption generation (#REF; #REF) , sentence generation (#REFb) , dialogue control (#REF; #REFa) , sentiment classification, recommendation system, question answering (#REF) and machine translation , to name a few.\"]}"
    },
    {
        "gold": {
            "text": [
                "We propose smoothed max pooling loss and its application to keyword spotting systems.",
                "The proposed approach jointly trains an encoder (to detect keyword parts) and a decoder (to detect whole keyword) in a semi-supervised manner.",
                "The proposed new loss function allows training a model to detect parts and whole of a keyword, without strictly depending on frame-level labeling from LVCSR (Large vocabulary continuous speech recognition), making further optimization possible.",
                "The proposed system outperforms the baseline keyword spotting model in [ #TARGET_REF] due to increased optimizability.",
                "Further, it can be more easily adapted for on-device learning applications due to reduced dependency on LVCSR."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We propose smoothed max pooling loss and its application to keyword spotting systems. The proposed approach jointly trains an encoder (to detect keyword parts) and a decoder (to detect whole keyword) in a semi-supervised manner. The proposed new loss function allows training a model to detect parts and whole of a keyword, without strictly depending on frame-level labeling from LVCSR (Large vocabulary continuous speech recognition), making further optimization possible. The proposed system outperforms the baseline keyword spotting model in [ #TARGET_REF] due to increased optimizability. Further, it can be more easily adapted for on-device learning applications due to reduced dependency on LVCSR.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The proposed system outperforms the baseline keyword spotting model in [ #TARGET_REF] due to increased optimizability.\"]}"
    },
    {
        "gold": {
            "text": [
                "The proposed model uses the same encoder/decoder structure as [ #TARGET_REF] ( Fig.1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss.",
                "#REF , both encoder and decoder models are trained with cross entropy (CE) loss using frame level labels.",
                "In the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously.",
                "The proposed smoothed max pooling loss doesnt strictly depend on phoneme-level alignment, allowing better optimization than the baseline."
            ],
            "label": [
                "DIFFER",
                "USE",
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "The proposed model uses the same encoder/decoder structure as [ #TARGET_REF] ( Fig.1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss. #REF , both encoder and decoder models are trained with cross entropy (CE) loss using frame level labels. In the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously. The proposed smoothed max pooling loss doesnt strictly depend on phoneme-level alignment, allowing better optimization than the baseline.",
        "output": "{\"label\": [\"DIFFER\", \"USE\", \"SIM\"], \"context\": [\"The proposed model uses the same encoder/decoder structure as [ #TARGET_REF] ( Fig.1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss.\"]}"
    },
    {
        "gold": {
            "text": [
                "The proposed model uses the same encoder/decoder structure as [1] ( Fig.1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss.",
                "#REF , both encoder and decoder models are trained with cross entropy (CE) loss using frame level labels.",
                "In the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously.",
                "The proposed smoothed max pooling loss doesnt strictly depend on phoneme-level alignment, allowing better optimization than the #TARGET_REF."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1
            ]
        },
        "input": "The proposed model uses the same encoder/decoder structure as [1] ( Fig.1 ), but it differs in that encoder and decoder models are trained simultaneously using smoothed max pooling loss. #REF , both encoder and decoder models are trained with cross entropy (CE) loss using frame level labels. In the proposed approach, we define losses for encoder and decoder using smoothed max pooling loss, and optimize the combination of two losses simultaneously. The proposed smoothed max pooling loss doesnt strictly depend on phoneme-level alignment, allowing better optimization than the #TARGET_REF.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The proposed smoothed max pooling loss doesnt strictly depend on phoneme-level alignment, allowing better optimization than the #TARGET_REF.\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare the model trained with the new smoothed max pooling loss on encoder/decoder architecture with the baseline in [ #TARGET_REF] .",
                "Both the baseline and the proposed model have the same architecture.",
                "Only the training losses are different.",
                "Details of the setup are discussed below."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "We compare the model trained with the new smoothed max pooling loss on encoder/decoder architecture with the baseline in [ #TARGET_REF] . Both the baseline and the proposed model have the same architecture. Only the training losses are different. Details of the setup are discussed below.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We compare the model trained with the new smoothed max pooling loss on encoder/decoder architecture with the baseline in [ #TARGET_REF] .\"]}"
    },
    {
        "gold": {
            "text": [
                "As shown in Fig. 1 , the model has 7 SVDF layers and 3 linear bottleneck dense layers.",
                "For detailed architectural parameters, please refer to [1] .",
                "We call the #TARGET_REF as Baseline CE CE where encoder and decoder submodels are trained with CE loss.",
                "We call the proposed model as Max4 SMP SMP where both encoder and decoder submodels are trained by SMP (smoothed max pooling) loss.",
                "We also performed ablation study by testing other models that use different losses."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "As shown in Fig. 1 , the model has 7 SVDF layers and 3 linear bottleneck dense layers. For detailed architectural parameters, please refer to [1] . We call the #TARGET_REF as Baseline CE CE where encoder and decoder submodels are trained with CE loss. We call the proposed model as Max4 SMP SMP where both encoder and decoder submodels are trained by SMP (smoothed max pooling) loss. We also performed ablation study by testing other models that use different losses.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We call the #TARGET_REF as Baseline CE CE where encoder and decoder submodels are trained with CE loss.\", \"We call the proposed model as Max4 SMP SMP where both encoder and decoder submodels are trained by SMP (smoothed max pooling) loss.\"]}"
    },
    {
        "gold": {
            "text": [
                "Fig.3 shows the ROC curves of various models (baseline, Max1-Max4) across different conditions.",
                "Figure 4 shows the ROC curves of Max4-Max7 models across different conditions.",
                "Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve.",
                "Max3 CE MP model also performs better than the #TARGET_REF but not as good as Max4.",
                "Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than baseline."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Fig.3 shows the ROC curves of various models (baseline, Max1-Max4) across different conditions. Figure 4 shows the ROC curves of Max4-Max7 models across different conditions. Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve. Max3 CE MP model also performs better than the #TARGET_REF but not as good as Max4. Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than baseline.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Max3 CE MP model also performs better than the #TARGET_REF but not as good as Max4.\"]}"
    },
    {
        "gold": {
            "text": [
                "Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve.",
                "Max3 CE MP model also performs better than the baseline but not as good as Max4.",
                "Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than #TARGET_REF",
                "Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss).",
                "Especially the proposed Max4 model reduces FR rate to nearly half of the baseline in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve. Max3 CE MP model also performs better than the baseline but not as good as Max4. Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than #TARGET_REF Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss). Especially the proposed Max4 model reduces FR rate to nearly half of the baseline in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than #TARGET_REF\"]}"
    },
    {
        "gold": {
            "text": [
                "Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve.",
                "Max3 CE MP model also performs better than the baseline but not as good as Max4.",
                "Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than baseline.",
                "Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss).",
                "Especially the proposed Max4 model reduces FR rate to nearly half of the #TARGET_REF in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Across model types and evaluation conditions Max4 SMP SMP shows the best accuracy and ROC curve. Max3 CE MP model also performs better than the baseline but not as good as Max4. Other variations Max2 (has only decoder loss) and Max1 (has CTC encoder loss) performed worse than baseline. Comparison among models with max pooling and different smoothing options (Fig.4) shows that Max4 SMP SMP (smoothed max poling on both encoder and decoder) performs the best and outperforms Max7(no smoothing on encoder and decoder max pooling loss). Especially the proposed Max4 model reduces FR rate to nearly half of the #TARGET_REF in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Especially the proposed Max4 model reduces FR rate to nearly half of the #TARGET_REF in clean accented and noisy inside-vehicle conditions, where it's more difficult to obtain training data with accurate alignments.\"]}"
    },
    {
        "gold": {
            "text": [
                "We presented smoothed max pooling loss for training keyword spotting model with improved optimizability.",
                "Experiments show that the proposed approach outperforms the #TARGET_REF with CE loss by relative 22%-54% across a variety of conditions.",
                "Further, we show that applying smoothing before max pooling is highly important for achieving accuracy better than the baseline.",
                "The proposed approach provides further benefits of reducing dependence on LVCSR to provide phoneme level alignments, which is desirable for embedded learning scenarios, like on-device learning [20] [21] ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0
            ]
        },
        "input": "We presented smoothed max pooling loss for training keyword spotting model with improved optimizability. Experiments show that the proposed approach outperforms the #TARGET_REF with CE loss by relative 22%-54% across a variety of conditions. Further, we show that applying smoothing before max pooling is highly important for achieving accuracy better than the baseline. The proposed approach provides further benefits of reducing dependence on LVCSR to provide phoneme level alignments, which is desirable for embedded learning scenarios, like on-device learning [20] [21] .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Experiments show that the proposed approach outperforms the #TARGET_REF with CE loss by relative 22%-54% across a variety of conditions.\"]}"
    },
    {
        "gold": {
            "text": [
                "We presented smoothed max pooling loss for training keyword spotting model with improved optimizability.",
                "Experiments show that the proposed approach outperforms the baseline model with CE loss by relative 22%-54% across a variety of conditions.",
                "Further, we show that applying smoothing before max pooling is highly important for achieving accuracy better than the #TARGET_REF.",
                "The proposed approach provides further benefits of reducing dependence on LVCSR to provide phoneme level alignments, which is desirable for embedded learning scenarios, like on-device learning [20] [21] ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0
            ]
        },
        "input": "We presented smoothed max pooling loss for training keyword spotting model with improved optimizability. Experiments show that the proposed approach outperforms the baseline model with CE loss by relative 22%-54% across a variety of conditions. Further, we show that applying smoothing before max pooling is highly important for achieving accuracy better than the #TARGET_REF. The proposed approach provides further benefits of reducing dependence on LVCSR to provide phoneme level alignments, which is desirable for embedded learning scenarios, like on-device learning [20] [21] .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Further, we show that applying smoothing before max pooling is highly important for achieving accuracy better than the #TARGET_REF.\"]}"
    },
    {
        "gold": {
            "text": [
                "As assistant technology spreads to more ubiquitous use-cases (mobile, IOT), reducing resource consumption (memory and computation) while improving accuracy has been the key success criteria of keyword spotting techniques.",
                "Following the successes in general ASR [2, 3] , the neural network based approach has been extensively explored in keyword spotting area with benefits of lowering resource requirements and improving accuracy [4, 5, 6, 7, 8, 9, 10, 11] .",
                "Such works include DNN + temporal integration [4, 5, 11, 12] , and HMM + DNN hybrid approaches [6, 7, 8, 9, 10] .",
                "Recently introduced end-to-end trainable DNN approaches [ #TARGET_REF 13] further improved accuracy and lowered resource requirements using highly optimizable system design.",
                "In general, training of such DNN based systems required framelevel labels generated by LVCSR systems [14, 1] ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "As assistant technology spreads to more ubiquitous use-cases (mobile, IOT), reducing resource consumption (memory and computation) while improving accuracy has been the key success criteria of keyword spotting techniques. Following the successes in general ASR [2, 3] , the neural network based approach has been extensively explored in keyword spotting area with benefits of lowering resource requirements and improving accuracy [4, 5, 6, 7, 8, 9, 10, 11] . Such works include DNN + temporal integration [4, 5, 11, 12] , and HMM + DNN hybrid approaches [6, 7, 8, 9, 10] . Recently introduced end-to-end trainable DNN approaches [ #TARGET_REF 13] further improved accuracy and lowered resource requirements using highly optimizable system design. In general, training of such DNN based systems required framelevel labels generated by LVCSR systems [14, 1] .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Recently introduced end-to-end trainable DNN approaches [ #TARGET_REF 13] further improved accuracy and lowered resource requirements using highly optimizable system design.\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently introduced end-to-end trainable DNN approaches [1, 13] further improved accuracy and lowered resource requirements using highly optimizable system design.",
                "In general, training of such DNN based systems required framelevel labels generated by LVCSR systems [14, #TARGET_REF] .",
                "These approaches make end-to-end optimizable keyword spotting system depend on labels generated from non-end-to-end system trained for a different task.",
                "However, for keyword-spotting, the exact position of the keyword is not as relevant as its presence.",
                "Therefore, such strict dependency on frame-level labels may limit further optimization promised by the end-to-end approach."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Recently introduced end-to-end trainable DNN approaches [1, 13] further improved accuracy and lowered resource requirements using highly optimizable system design. In general, training of such DNN based systems required framelevel labels generated by LVCSR systems [14, #TARGET_REF] . These approaches make end-to-end optimizable keyword spotting system depend on labels generated from non-end-to-end system trained for a different task. However, for keyword-spotting, the exact position of the keyword is not as relevant as its presence. Therefore, such strict dependency on frame-level labels may limit further optimization promised by the end-to-end approach.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In general, training of such DNN based systems required framelevel labels generated by LVCSR systems [14, #TARGET_REF] .\"]}"
    },
    {
        "gold": {
            "text": [
                "However, for keyword-spotting, the exact position of the keyword is not as relevant as its presence.",
                "Therefore, such strict dependency on frame-level labels may limit further optimization promised by the end-to-end approach.",
                "In [ #TARGET_REF] , the top level loss is derived by integrating frame-level losses, which are computed using frame-level labels from LVCSR.",
                "Integrating frame-level losses penalizes slightly mis-aligned correct predictions, which can limit detection accuracy, especially for difficult data (e.g. noisy or accented speech) where LVCSR labels may have higher-than-normal uncertainty.",
                "In such case, losses can be fully minimized only when the predicted value and position-in-time matches that of provided frame level labels, where exact position match is not highly relevant for high accuracy."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "However, for keyword-spotting, the exact position of the keyword is not as relevant as its presence. Therefore, such strict dependency on frame-level labels may limit further optimization promised by the end-to-end approach. In [ #TARGET_REF] , the top level loss is derived by integrating frame-level losses, which are computed using frame-level labels from LVCSR. Integrating frame-level losses penalizes slightly mis-aligned correct predictions, which can limit detection accuracy, especially for difficult data (e.g. noisy or accented speech) where LVCSR labels may have higher-than-normal uncertainty. In such case, losses can be fully minimized only when the predicted value and position-in-time matches that of provided frame level labels, where exact position match is not highly relevant for high accuracy.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In [ #TARGET_REF] , the top level loss is derived by integrating frame-level losses, which are computed using frame-level labels from LVCSR.\"]}"
    },
    {
        "gold": {
            "text": [
                "The decoder model takes the encoder output as input and generates binary output Y D that predicts existence of a keyword .",
                "The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner.",
                "In [ #TARGET_REF] , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen.",
                "#REF , the encoder model is trained to predict phonemelevel labels provided from LVCSR.",
                "Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt−C l , · · · , xt, · · · , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The decoder model takes the encoder output as input and generates binary output Y D that predicts existence of a keyword . The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner. In [ #TARGET_REF] , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen. #REF , the encoder model is trained to predict phonemelevel labels provided from LVCSR. Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt−C l , · · · , xt, · · · , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In [ #TARGET_REF] , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen.\"]}"
    },
    {
        "gold": {
            "text": [
                "The decoder model takes the encoder output as input and generates binary output Y D that predicts existence of a keyword .",
                "The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner.",
                "#REF , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen.",
                "In [ #TARGET_REF] , the encoder model is trained to predict phonemelevel labels provided from LVCSR.",
                "Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt−C l , · · · , xt, · · · , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The decoder model takes the encoder output as input and generates binary output Y D that predicts existence of a keyword . The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner. #REF , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen. In [ #TARGET_REF] , the encoder model is trained to predict phonemelevel labels provided from LVCSR. Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt−C l , · · · , xt, · · · , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In [ #TARGET_REF] , the encoder model is trained to predict phonemelevel labels provided from LVCSR.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF , the encoder model is trained to predict phonemelevel labels provided from LVCSR.",
                "Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt−C l , · · · , xt, · · · , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t.",
                "In [ #TARGET_REF] , target label sequence consists of intervals of repeated labels which we call runs.",
                "These label runs define clearly defined intervals where a model should learn to generate strong activation in label output dimension.",
                "While such model behavior can be trained end-to-end, the labels need to be provided from a LVCSR system which is typically non-end-to-end system [2] ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "#REF , the encoder model is trained to predict phonemelevel labels provided from LVCSR. Both encoder and decoder models use CE-loss defined in Eq (1) and (2), where Xt = [xt−C l , · · · , xt, · · · , xt+C r ], xt is spectral feature of d-dimension, yi(Xt, W ) stands for ith dimension of network's softmax output, W is network weight, and ct is a frame-level label at frame t. In [ #TARGET_REF] , target label sequence consists of intervals of repeated labels which we call runs. These label runs define clearly defined intervals where a model should learn to generate strong activation in label output dimension. While such model behavior can be trained end-to-end, the labels need to be provided from a LVCSR system which is typically non-end-to-end system [2] .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In [ #TARGET_REF] , target label sequence consists of intervals of repeated labels which we call runs.\"]}"
    },
    {
        "gold": {
            "text": [
                "We used the same frontend feature extract as the baseline [ #TARGET_REF] in our experiments.",
                "The front-end extracts and stacks a 40-d feature vector of log-mel filter-bank energies at each frame and stacks them to generate input feature vector Xt. Refer to [1] for further details."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                1,
                0
            ]
        },
        "input": "We used the same frontend feature extract as the baseline [ #TARGET_REF] in our experiments. The front-end extracts and stacks a 40-d feature vector of log-mel filter-bank energies at each frame and stacks them to generate input feature vector Xt. Refer to [1] for further details.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"We used the same frontend feature extract as the baseline [ #TARGET_REF] in our experiments.\"]}"
    },
    {
        "gold": {
            "text": [
                "We selected E2E 318K architecture in [ #TARGET_REF] as the baseline and use the same structure for testing all other models.",
                "As shown in Fig. 1 , the model has 7 SVDF layers and 3 linear bottleneck dense layers.",
                "For detailed architectural parameters, please refer to [1] .",
                "We call the baseline model as Baseline CE CE where encoder and decoder submodels are trained with CE loss.",
                "We call the proposed model as Max4 SMP SMP where both encoder and decoder submodels are trained by SMP (smoothed max pooling) loss."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We selected E2E 318K architecture in [ #TARGET_REF] as the baseline and use the same structure for testing all other models. As shown in Fig. 1 , the model has 7 SVDF layers and 3 linear bottleneck dense layers. For detailed architectural parameters, please refer to [1] . We call the baseline model as Baseline CE CE where encoder and decoder submodels are trained with CE loss. We call the proposed model as Max4 SMP SMP where both encoder and decoder submodels are trained by SMP (smoothed max pooling) loss.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"We selected E2E 318K architecture in [ #TARGET_REF] as the baseline and use the same structure for testing all other models.\"]}"
    },
    {
        "gold": {
            "text": [
                "CTC loss doesnt need alignments, but it learns peaky activations whose peak values are not highly stable.",
                "Max2 NA SMP has no encoder loss (i.e. α = 0), s.t.",
                "the entire network is trained by decoder loss only.",
                "Max3 CE SMP used #TARGET_REF CE loss for encoder.",
                "Model Max4-Max7 are tested to measure the importance of the smoothing operation."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "CTC loss doesnt need alignments, but it learns peaky activations whose peak values are not highly stable. Max2 NA SMP has no encoder loss (i.e. α = 0), s.t. the entire network is trained by decoder loss only. Max3 CE SMP used #TARGET_REF CE loss for encoder. Model Max4-Max7 are tested to measure the importance of the smoothing operation.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Max3 CE SMP used #TARGET_REF CE loss for encoder.\"]}"
    },
    {
        "gold": {
            "text": [
                "To show effectiveness of the proposed approach, we evaluated falsereject (FR) and false-accept (FA) tradeoff across various models described in Section 3.",
                "All models are converted to inference models using TensorFlow Lites quantization [19] .",
                "Table 2 summarizes FR rates of models in Fig.3 and 4 at selected FA rate (0.1 FA per hour measured on 64K re-recorded TV noise set).",
                "Fig.3 shows the ROC curves of various models ( #TARGET_REF, Max1-Max4) across different conditions.",
                "Figure 4 shows the ROC curves of Max4-Max7 models across different conditions."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "To show effectiveness of the proposed approach, we evaluated falsereject (FR) and false-accept (FA) tradeoff across various models described in Section 3. All models are converted to inference models using TensorFlow Lites quantization [19] . Table 2 summarizes FR rates of models in Fig.3 and 4 at selected FA rate (0.1 FA per hour measured on 64K re-recorded TV noise set). Fig.3 shows the ROC curves of various models ( #TARGET_REF, Max1-Max4) across different conditions. Figure 4 shows the ROC curves of Max4-Max7 models across different conditions.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Fig.3 shows the ROC curves of various models ( #TARGET_REF, Max1-Max4) across different conditions.\"]}"
    },
    {
        "gold": {
            "text": [
                "Both the #TARGET_REF and the proposed model have an encoder which takes spectral domain feature Xt as input and generate (K+1) outputs Y E corresponding to phoneme-like sound units (Fig.1 ).",
                "The decoder model takes the encoder output as input and generates binary output Y D that predicts existence of a keyword .",
                "The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner.",
                "#REF , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen.",
                "#REF , the encoder model is trained to predict phonemelevel labels provided from LVCSR."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Both the #TARGET_REF and the proposed model have an encoder which takes spectral domain feature Xt as input and generate (K+1) outputs Y E corresponding to phoneme-like sound units (Fig.1 ). The decoder model takes the encoder output as input and generates binary output Y D that predicts existence of a keyword . The model is fed with acoustic input features at each frame (generated every 10ms), and generates prediction labels at each frame in a streaming manner. #REF , the encoder model is trained first, and then the decoder model is trained while encoder model weights are frozen. #REF , the encoder model is trained to predict phonemelevel labels provided from LVCSR.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Both the #TARGET_REF and the proposed model have an encoder which takes spectral domain feature Xt as input and generate (K+1) outputs Y E corresponding to phoneme-like sound units (Fig.1 ).\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare the model trained with the new smoothed max pooling loss on encoder/decoder architecture with the baseline in [1] .",
                "Both the #TARGET_REF and the proposed model have the same architecture.",
                "Only the training losses are different.",
                "Details of the setup are discussed below."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0
            ]
        },
        "input": "We compare the model trained with the new smoothed max pooling loss on encoder/decoder architecture with the baseline in [1] . Both the #TARGET_REF and the proposed model have the same architecture. Only the training losses are different. Details of the setup are discussed below.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Both the #TARGET_REF and the proposed model have the same architecture.\"]}"
    },
    {
        "gold": {
            "text": [
                "The training data consists of 2.1 million anonymized utterances with the keywords Ok Google and Hey Google.",
                "Data augmentation similar to [ #TARGET_REF] has been used for better robustness.",
                "Evaluation is done on four data sets separate from training data, representing diverse environmental conditions -Clean non-accented set contains 170K non-accented English utterances of keywords in quiet condition.",
                "Clean accented has 138K English utterances of keyword with Australian, British, and Indian accents in quiet conditions.",
                "Query logs contains 58K utterances from anonymized voice search queries."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The training data consists of 2.1 million anonymized utterances with the keywords Ok Google and Hey Google. Data augmentation similar to [ #TARGET_REF] has been used for better robustness. Evaluation is done on four data sets separate from training data, representing diverse environmental conditions -Clean non-accented set contains 170K non-accented English utterances of keywords in quiet condition. Clean accented has 138K English utterances of keyword with Australian, British, and Indian accents in quiet conditions. Query logs contains 58K utterances from anonymized voice search queries.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Data augmentation similar to [ #TARGET_REF] has been used for better robustness.\"]}"
    },
    {
        "gold": {
            "text": [
                "The task of definition modeling, introduced by #TARGET_REF , consists in generating the dictionary definition of a specific word: for instance, given the word \"monotreme\" as input, the system would need to produce a definition such as \"any of an order (Monotremata) of egg-laying mammals comprising the platypuses and echidnas\".",
                "1 Following the tradition set by lexicographers, we call the word being defined a definiendum (pl. definienda), whereas a word occurring in its definition is called a definiens (pl. definientia).",
                "Definition modeling can prove useful in a variety of applications.",
                "Systems trained for the task may generate dictionaries for low resource languages, or extend the coverage of existing lexicographic resources where needed, e.g. of domainspecific vocabulary.",
                "Such systems may also be 1 Definition from Merriam-Webster."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The task of definition modeling, introduced by #TARGET_REF , consists in generating the dictionary definition of a specific word: for instance, given the word \"monotreme\" as input, the system would need to produce a definition such as \"any of an order (Monotremata) of egg-laying mammals comprising the platypuses and echidnas\". 1 Following the tradition set by lexicographers, we call the word being defined a definiendum (pl. definienda), whereas a word occurring in its definition is called a definiens (pl. definientia). Definition modeling can prove useful in a variety of applications. Systems trained for the task may generate dictionaries for low resource languages, or extend the coverage of existing lexicographic resources where needed, e.g. of domainspecific vocabulary. Such systems may also be 1 Definition from Merriam-Webster.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The task of definition modeling, introduced by #TARGET_REF , consists in generating the dictionary definition of a specific word: for instance, given the word \\\"monotreme\\\" as input, the system would need to produce a definition such as \\\"any of an order (Monotremata) of egg-laying mammals comprising the platypuses and echidnas\\\".\"]}"
    },
    {
        "gold": {
            "text": [
                "able to provide reading help by giving definitions for words in the text.",
                "A major intended application of definition modeling is the explication and evaluation of distributed lexical representations, also known as word embeddings #TARGET_REF .",
                "This evaluation procedure is based on the postulate that the meaning of a word, as is captured by its embedding, should be convertible into a human-readable dictionary definition.",
                "How well the meaning is captured must impact the ability of the model to reproduce the definition, and therefore embedding architectures can be compared according to their downstream performance on definition modeling.",
                "This intended usage motivates the requirement that definition modeling architectures take as input the embedding of the definiendum and not retrain it."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "able to provide reading help by giving definitions for words in the text. A major intended application of definition modeling is the explication and evaluation of distributed lexical representations, also known as word embeddings #TARGET_REF . This evaluation procedure is based on the postulate that the meaning of a word, as is captured by its embedding, should be convertible into a human-readable dictionary definition. How well the meaning is captured must impact the ability of the model to reproduce the definition, and therefore embedding architectures can be compared according to their downstream performance on definition modeling. This intended usage motivates the requirement that definition modeling architectures take as input the embedding of the definiendum and not retrain it.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"A major intended application of definition modeling is the explication and evaluation of distributed lexical representations, also known as word embeddings #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "sections 3 & 4) .",
                "We implement this approach in a Transformer-based sequence-to-sequence model that achieves state-of-the-art performances (sections 5 & 6).",
                "arXiv:1911.05715v1 [cs.CL] 13 #REF 2 Related Work",
                "In their seminal work on definition modeling, #TARGET_REF likened systems generating definitions to language models, which can naturally be used to generate arbitrary text.",
                "They built a sequential LSTM seeded with the embedding of the definiendum; its output at each time-step was mixed through a gating mechanism with a feature vector derived from the definiendum."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "sections 3 & 4) . We implement this approach in a Transformer-based sequence-to-sequence model that achieves state-of-the-art performances (sections 5 & 6). arXiv:1911.05715v1 [cs.CL] 13 #REF 2 Related Work In their seminal work on definition modeling, #TARGET_REF likened systems generating definitions to language models, which can naturally be used to generate arbitrary text. They built a sequential LSTM seeded with the embedding of the definiendum; its output at each time-step was mixed through a gating mechanism with a feature vector derived from the definiendum.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In their seminal work on definition modeling, #TARGET_REF likened systems generating definitions to language models, which can naturally be used to generate arbitrary text.\"]}"
    },
    {
        "gold": {
            "text": [
                "When applied to definition modeling, the hypothesis can be rephrased as follows: the correct definition of a word can only be given when knowing in what linguistic context(s) it occurs.",
                "Though different kinds of linguistic contexts have been suggested throughout the literature, we remark here that sentential context may sometimes suffice to guess the meaning of a word that we don't know (#REF) .",
                "Quoting from the example above, the context \"enough around-let's get back to work!\" sufficiently characterizes the meaning of the omitted verb to allow for an approximate definition for it even if the blank is not filled (#REF; #REF) .",
                "This reformulation can appear contrary to the original proposal by #TARGET_REF , which conceived definition modeling as a \"word-tosequence task\".",
                "They argued for an approach related to, though distinct from sequence-to-sequence architectures."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "When applied to definition modeling, the hypothesis can be rephrased as follows: the correct definition of a word can only be given when knowing in what linguistic context(s) it occurs. Though different kinds of linguistic contexts have been suggested throughout the literature, we remark here that sentential context may sometimes suffice to guess the meaning of a word that we don't know (#REF) . Quoting from the example above, the context \"enough around-let's get back to work!\" sufficiently characterizes the meaning of the omitted verb to allow for an approximate definition for it even if the blank is not filled (#REF; #REF) . This reformulation can appear contrary to the original proposal by #TARGET_REF , which conceived definition modeling as a \"word-tosequence task\". They argued for an approach related to, though distinct from sequence-to-sequence architectures.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"Though different kinds of linguistic contexts have been suggested throughout the literature, we remark here that sentential context may sometimes suffice to guess the meaning of a word that we don't know (#REF) .\", \"Quoting from the example above, the context \\\"enough around-let's get back to work!\\\" sufficiently characterizes the meaning of the omitted verb to allow for an approximate definition for it even if the blank is not filled (#REF; #REF) .\", \"This reformulation can appear contrary to the original proposal by #TARGET_REF , which conceived definition modeling as a \\\"word-tosequence task\\\".\"]}"
    },
    {
        "gold": {
            "text": [
                "Despite some key differences, all of the previously proposed architectures we are aware of (#REF; #REF; followed a pattern similar to sequence-to-sequence models.",
                "They all implicitly or explicitly used distinct submodules to encode the definiendum and to generate the definientia.",
                "In the case of #TARGET_REF , the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters derived from a characterlevel CNN, and its \"hypernym embedding\".",
                "#REF used a sigmoid-based gating module to tweak the definiendum embedding.",
                "The architecture proposed by is comprised of four modules, only one of which is used as a decoder: the remaining three are meant to convert the definiendum as a sparse embedding, select some of the sparse components of its meaning based on a provided context, and encode it into a representation adequate for the decoder."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Despite some key differences, all of the previously proposed architectures we are aware of (#REF; #REF; followed a pattern similar to sequence-to-sequence models. They all implicitly or explicitly used distinct submodules to encode the definiendum and to generate the definientia. In the case of #TARGET_REF , the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters derived from a characterlevel CNN, and its \"hypernym embedding\". #REF used a sigmoid-based gating module to tweak the definiendum embedding. The architecture proposed by is comprised of four modules, only one of which is used as a decoder: the remaining three are meant to convert the definiendum as a sparse embedding, select some of the sparse components of its meaning based on a provided context, and encode it into a representation adequate for the decoder.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In the case of #TARGET_REF , the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters derived from a characterlevel CNN, and its \\\"hypernym embedding\\\".\"]}"
    },
    {
        "gold": {
            "text": [
                "However consider for instance the word \"elation\": that it be defined either as \"mirth\" or \"joy\" should only influence our metric slightly, and not be discounted as a completely wrong prediction. , as they did not report the perplexity of their system and focused on a different dataset; likewise, consider only the Chinese variant of the task.",
                "Perplexity measures for #TARGET_REF and #REF are taken from the authors' respective publications.",
                "All our models perform better than previous proposals, by a margin of 4 to 10 points, for a relative improvement of 11-23%.",
                "Part of this improvement may be due to our use of Transformerbased architectures (#REF) , which is known to perform well on semantic tasks (#REF; #REF; #REF; #REF, eg.) .",
                "Like #REF , we conclude that disambiguating the definiendum, when done correctly, improves performances: our best performing contex-tual model outranks the non-contextual variant by 5 to 6 points."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "However consider for instance the word \"elation\": that it be defined either as \"mirth\" or \"joy\" should only influence our metric slightly, and not be discounted as a completely wrong prediction. , as they did not report the perplexity of their system and focused on a different dataset; likewise, consider only the Chinese variant of the task. Perplexity measures for #TARGET_REF and #REF are taken from the authors' respective publications. All our models perform better than previous proposals, by a margin of 4 to 10 points, for a relative improvement of 11-23%. Part of this improvement may be due to our use of Transformerbased architectures (#REF) , which is known to perform well on semantic tasks (#REF; #REF; #REF; #REF, eg.) . Like #REF , we conclude that disambiguating the definiendum, when done correctly, improves performances: our best performing contex-tual model outranks the non-contextual variant by 5 to 6 points.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"Perplexity measures for #TARGET_REF and #REF are taken from the authors' respective publications.\", \"All our models perform better than previous proposals, by a margin of 4 to 10 points, for a relative improvement of 11-23%.\"]}"
    },
    {
        "gold": {
            "text": [
                "We train our models on three distinct datasets, which are all borrowed or adapted from previous works on definition modeling.",
                "As a consequence, our experiments focus on the English language.",
                "The dataset of #TARGET_REF (henceforth D Nor ) maps definienda to their respective definientia, as well as additional information not used here.",
                "In the dataset of #REF (henceforth D Gad ), each example consists of a definiendum, the definientia for one of its meanings and a contextual cue sentence.",
                "D Nor contains on average shorter definitions than D Gad ."
            ],
            "label": [
                "DIFFER",
                "USE"
            ],
            "context": [
                1,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We train our models on three distinct datasets, which are all borrowed or adapted from previous works on definition modeling. As a consequence, our experiments focus on the English language. The dataset of #TARGET_REF (henceforth D Nor ) maps definienda to their respective definientia, as well as additional information not used here. In the dataset of #REF (henceforth D Gad ), each example consists of a definiendum, the definientia for one of its meanings and a contextual cue sentence. D Nor contains on average shorter definitions than D Gad .",
        "output": "{\"label\": [\"DIFFER\", \"USE\"], \"context\": [\"We train our models on three distinct datasets, which are all borrowed or adapted from previous works on definition modeling.\", \"The dataset of #TARGET_REF (henceforth D Nor ) maps definienda to their respective definientia, as well as additional information not used here.\"]}"
    },
    {
        "gold": {
            "text": [
                "Self-referring definitions highlight that our models equate the meaning of the definiendum to the composed meaning of its definientia.",
                "Simply masking the corresponding output embedding might suffice to prevent this specific problem; preliminary experiments in that direction suggest that this may also help decrease perplexity further.",
                "As for POS-mismatches, we do note that the work of #TARGET_REF had a much lower rate of 4.29%: we suggest that this may be due to the fact that they employ a learned character-level convolutional network, which arguably would be able to capture orthography and rudiments of morphology.",
                "Adding such a sub-module to our proposed architecture might diminish the number of mistagged definienda.",
                "Another possibility would be to pre-train the model, as was done by #REF : in our case in particular, the encoder could be trained for POS-tagging or lemmatization."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Self-referring definitions highlight that our models equate the meaning of the definiendum to the composed meaning of its definientia. Simply masking the corresponding output embedding might suffice to prevent this specific problem; preliminary experiments in that direction suggest that this may also help decrease perplexity further. As for POS-mismatches, we do note that the work of #TARGET_REF had a much lower rate of 4.29%: we suggest that this may be due to the fact that they employ a learned character-level convolutional network, which arguably would be able to capture orthography and rudiments of morphology. Adding such a sub-module to our proposed architecture might diminish the number of mistagged definienda. Another possibility would be to pre-train the model, as was done by #REF : in our case in particular, the encoder could be trained for POS-tagging or lemmatization.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"As for POS-mismatches, we do note that the work of #TARGET_REF had a much lower rate of 4.29%: we suggest that this may be due to the fact that they employ a learned character-level convolutional network, which arguably would be able to capture orthography and rudiments of morphology.\"]}"
    },
    {
        "gold": {
            "text": [
                "There is no guarantee that a single embedding, even if it be contextualized, would preserve this wealth of information-that is to say, that you can cram all the information pertaining to the syntactic context into a single vector.",
                "Despite some key differences, all of the previously proposed architectures we are aware of #TARGET_REF; #REF; followed a pattern similar to sequence-to-sequence models.",
                "They all implicitly or explicitly used distinct submodules to encode the definiendum and to generate the definientia.",
                "In the case of #REF , the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters derived from a characterlevel CNN, and its \"hypernym embedding\".",
                "#REF used a sigmoid-based gating module to tweak the definiendum embedding."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "There is no guarantee that a single embedding, even if it be contextualized, would preserve this wealth of information-that is to say, that you can cram all the information pertaining to the syntactic context into a single vector. Despite some key differences, all of the previously proposed architectures we are aware of #TARGET_REF; #REF; followed a pattern similar to sequence-to-sequence models. They all implicitly or explicitly used distinct submodules to encode the definiendum and to generate the definientia. In the case of #REF , the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters derived from a characterlevel CNN, and its \"hypernym embedding\". #REF used a sigmoid-based gating module to tweak the definiendum embedding.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Despite some key differences, all of the previously proposed architectures we are aware of #TARGET_REF; #REF; followed a pattern similar to sequence-to-sequence models.\"]}"
    },
    {
        "gold": {
            "text": [
                "The contextualized definiendum encoding bears the trace of its context, but detailed information is irreparably lost.",
                "Hence, we refer to such an integration mechanism as a SELECT marking of the definiendum.",
                "When to apply marking, as introduced by eq. 4, is crucial when using the multiplicative marking scheme SELECT.",
                "Should we mark the definiendum before encoding, then only the definiendum embedding is passed into the encoder: the resulting system provides out-of-context definitions, like in #TARGET_REF where the definition is not linked to the context of a word but to its definiendum only.",
                "For context to be taken into account under the multiplicative strategy, tokens w k must be encoded and contextualized before integration with the indicator i k ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The contextualized definiendum encoding bears the trace of its context, but detailed information is irreparably lost. Hence, we refer to such an integration mechanism as a SELECT marking of the definiendum. When to apply marking, as introduced by eq. 4, is crucial when using the multiplicative marking scheme SELECT. Should we mark the definiendum before encoding, then only the definiendum embedding is passed into the encoder: the resulting system provides out-of-context definitions, like in #TARGET_REF where the definition is not linked to the context of a word but to its definiendum only. For context to be taken into account under the multiplicative strategy, tokens w k must be encoded and contextualized before integration with the indicator i k .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Should we mark the definiendum before encoding, then only the definiendum embedding is passed into the encoder: the resulting system provides out-of-context definitions, like in #TARGET_REF where the definition is not linked to the context of a word but to its definiendum only.\"]}"
    },
    {
        "gold": {
            "text": [
                "The stability of the performance improvement over the noncontextual variant in both contextual datasets also highlights that our proposed additive marking is fairly robust, and functions equally well when confronted to somewhat artificial inputs, as in D Gad , or to linguistically coherent sequences, as in D Ctx .",
                "A manual analysis of definitions produced by our system reveals issues similar to those discussed by #TARGET_REF , namely selfreference, 7 POS-mismatches, over-and underspecificity, antonymy, and incoherence.",
                "Annotating distinct productions from the validation set, for the non-contextual model trained on D Nor , we counted 9.9% of self-references, 11.6% POSmismatches, and 1.3% of words defined as their antonyms.",
                "We counted POS-mismatches whenever the definition seemed to fit another part-of-speech than that of the definiendum, regardless of both of their meanings; cf.",
                "Table 2 for examples."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The stability of the performance improvement over the noncontextual variant in both contextual datasets also highlights that our proposed additive marking is fairly robust, and functions equally well when confronted to somewhat artificial inputs, as in D Gad , or to linguistically coherent sequences, as in D Ctx . A manual analysis of definitions produced by our system reveals issues similar to those discussed by #TARGET_REF , namely selfreference, 7 POS-mismatches, over-and underspecificity, antonymy, and incoherence. Annotating distinct productions from the validation set, for the non-contextual model trained on D Nor , we counted 9.9% of self-references, 11.6% POSmismatches, and 1.3% of words defined as their antonyms. We counted POS-mismatches whenever the definition seemed to fit another part-of-speech than that of the definiendum, regardless of both of their meanings; cf. Table 2 for examples.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"A manual analysis of definitions produced by our system reveals issues similar to those discussed by #TARGET_REF , namely selfreference, 7 POS-mismatches, over-and underspecificity, antonymy, and incoherence.\"]}"
    },
    {
        "gold": {
            "text": [
                "As the source corresponds only to the definiendum, we conjecture that few parameters are required for the encoder.",
                "We use 1 layer for the encoder, 6 for the decoder, 300 dimensions per hidden representations and 6 heads for multi-head attention.",
                "We do not share vocabularies between the encoder and the decoder: therefore output tokens can only correspond to words attested as definientia.",
                "4 The dropout rate and warmup steps number were set using a hyperparameter search on the dataset from #TARGET_REF , during which encoder and decoder vocabulary were merged for computational simplicity and models stopped after 12,000 steps.",
                "We first fixed dropout to 0.1 and tested warmup step values between 1000 and 10,000 by increments of 1000, then focused on the most promising span (1000-4000 steps) and exhaustively tested dropout rates from 0.2 to 0.8 by increments of 0.1."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "As the source corresponds only to the definiendum, we conjecture that few parameters are required for the encoder. We use 1 layer for the encoder, 6 for the decoder, 300 dimensions per hidden representations and 6 heads for multi-head attention. We do not share vocabularies between the encoder and the decoder: therefore output tokens can only correspond to words attested as definientia. 4 The dropout rate and warmup steps number were set using a hyperparameter search on the dataset from #TARGET_REF , during which encoder and decoder vocabulary were merged for computational simplicity and models stopped after 12,000 steps. We first fixed dropout to 0.1 and tested warmup step values between 1000 and 10,000 by increments of 1000, then focused on the most promising span (1000-4000 steps) and exhaustively tested dropout rates from 0.2 to 0.8 by increments of 0.1.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"4 The dropout rate and warmup steps number were set using a hyperparameter search on the dataset from #TARGET_REF , during which encoder and decoder vocabulary were merged for computational simplicity and models stopped after 12,000 steps.\"]}"
    },
    {
        "gold": {
            "text": [
                "Multi-task learning (MTL) in deep neural networks is typically a result of parameter sharing between two networks (of usually the same dimensions) #TARGET_REF .",
                "If you have two three-layered, recurrent neural networks, both with an embedding inner layer and each recurrent layer feeding the task-specific classifier function through a feed-forward neural network, we have 19 pairs of layers that could share parameters.",
                "With the option of having private spaces, this gives us 5 19 =19,073,486,328,125 possible MTL architectures.",
                "If we additionally consider soft sharing of parameters, the number of possible architectures grows infinite.",
                "It is obviously not feasible to search this space."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Multi-task learning (MTL) in deep neural networks is typically a result of parameter sharing between two networks (of usually the same dimensions) #TARGET_REF . If you have two three-layered, recurrent neural networks, both with an embedding inner layer and each recurrent layer feeding the task-specific classifier function through a feed-forward neural network, we have 19 pairs of layers that could share parameters. With the option of having private spaces, this gives us 5 19 =19,073,486,328,125 possible MTL architectures. If we additionally consider soft sharing of parameters, the number of possible architectures grows infinite. It is obviously not feasible to search this space.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Multi-task learning (MTL) in deep neural networks is typically a result of parameter sharing between two networks (of usually the same dimensions) #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "For simplicity, we ignore subspaces at first and assume only two tasks A and B. The outputs h A,k,t and h B,k,t of the k-th layer for time step t for task A and B respectively interact through the α parameters (see Figure 1 ).",
                "Omitting t for simplicity, the output of the α layers is:",
                "where h A,k is a linear combination of the outputs that is fed to the k+1-th layer of task A, and a , b designates the stacking of two vectors a, b ∈ R D to a matrix M ∈ R 2×D .",
                "Subspaces (Virtanen, Klami, and #REF; #TARGET_REF ) should allow the model to focus on task-specific and shared features in different parts of its parameter space.",
                "Extending the α-layers to include subspaces, for 2 tasks and 2 subspaces, we obtain an α matrix ∈ R 4×4 that not only controls the interaction between the layers of both tasks, but also between their subspaces:"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "For simplicity, we ignore subspaces at first and assume only two tasks A and B. The outputs h A,k,t and h B,k,t of the k-th layer for time step t for task A and B respectively interact through the α parameters (see Figure 1 ). Omitting t for simplicity, the output of the α layers is: where h A,k is a linear combination of the outputs that is fed to the k+1-th layer of task A, and a , b designates the stacking of two vectors a, b ∈ R D to a matrix M ∈ R 2×D . Subspaces (Virtanen, Klami, and #REF; #TARGET_REF ) should allow the model to focus on task-specific and shared features in different parts of its parameter space. Extending the α-layers to include subspaces, for 2 tasks and 2 subspaces, we obtain an α matrix ∈ R 4×4 that not only controls the interaction between the layers of both tasks, but also between their subspaces:",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Subspaces (Virtanen, Klami, and #REF; #TARGET_REF ) should allow the model to focus on task-specific and shared features in different parts of its parameter space.\"]}"
    },
    {
        "gold": {
            "text": [
                "Sluice networks outperform the comparison models for both tasks on in-domain test data and successfully generalize to out-of-domain test data on average.",
                "They yield the best performance on 5 out of 7 domains and 4 out of 7 domains for NER and semantic role labeling.",
                "Joint model Most work on MTL for NLP uses a single auxiliary task #TARGET_REF; Martínez #REF) .",
                "In this experiment, we use one sluice network to jointly learn our four tasks on the newswire domain and show results in Table 5 .",
                "Here, the low-level POS tagging and simplified SRL tasks are the only ones that benefit from hard parameter sharing highlighting that hard parameter sharing by itself is not sufficient for doing effective multi-task learning with semantic tasks."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Sluice networks outperform the comparison models for both tasks on in-domain test data and successfully generalize to out-of-domain test data on average. They yield the best performance on 5 out of 7 domains and 4 out of 7 domains for NER and semantic role labeling. Joint model Most work on MTL for NLP uses a single auxiliary task #TARGET_REF; Martínez #REF) . In this experiment, we use one sluice network to jointly learn our four tasks on the newswire domain and show results in Table 5 . Here, the low-level POS tagging and simplified SRL tasks are the only ones that benefit from hard parameter sharing highlighting that hard parameter sharing by itself is not sufficient for doing effective multi-task learning with semantic tasks.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"Joint model Most work on MTL for NLP uses a single auxiliary task #TARGET_REF; Mart\\u00ednez #REF) .\", \"In this experiment, we use one sluice network to jointly learn our four tasks on the newswire domain and show results in Table 5 .\"]}"
    },
    {
        "gold": {
            "text": [
                "To better understand the properties and behavior of our metaarchitecture, we conduct a series of analyses and ablations.",
                "Task Properties and Performance #TARGET_REF correlate meta-characteristics of task pairs and gains compared to hard parameter sharing across a large set of NLP task pairs.",
                "Similarly, we correlate various metacharacteristics with error reductions and α, β values.",
                "Most importantly, we find that a) multi-task learning gains, also in sluice networks, are higher when there is less training data; and b) sluice networks learn to share more when there is more variance in the training data (cross-task αs are higher, intra-task αs lower).",
                "Generally, α values at the inner layers correlate more strongly with meta-characteristics than α values at the outer layers."
            ],
            "label": [
                "BACK",
                "SIM"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "To better understand the properties and behavior of our metaarchitecture, we conduct a series of analyses and ablations. Task Properties and Performance #TARGET_REF correlate meta-characteristics of task pairs and gains compared to hard parameter sharing across a large set of NLP task pairs. Similarly, we correlate various metacharacteristics with error reductions and α, β values. Most importantly, we find that a) multi-task learning gains, also in sluice networks, are higher when there is less training data; and b) sluice networks learn to share more when there is more variance in the training data (cross-task αs are higher, intra-task αs lower). Generally, α values at the inner layers correlate more strongly with meta-characteristics than α values at the outer layers.",
        "output": "{\"label\": [\"BACK\", \"SIM\"], \"context\": [\"Task Properties and Performance #TARGET_REF correlate meta-characteristics of task pairs and gains compared to hard parameter sharing across a large set of NLP task pairs.\", \"Similarly, we correlate various metacharacteristics with error reductions and \\u03b1, \\u03b2 values.\"]}"
    },
    {
        "gold": {
            "text": [
                "Hard parameter sharing #TARGET_REF ) is easy to implement, reduces overfitting, but is only guaranteed to work for (certain types of) closely related tasks (#REF; #REF ).",
                "#REF apply a variation of hard parameter sharing to multi-domain multi-task sequence tagging with a shared CRF layer and domain-specific projection layers.",
                "Yang, Salakhutdinov, and #REF use hard parameter sharing to jointly learn different sequence-tagging tasks across languages.",
                "Martínez #REF explore a similar set-up, but sharing is limited to the initial layer.",
                "In all three papers, the amount of sharing between the networks is fixed in advance."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Hard parameter sharing #TARGET_REF ) is easy to implement, reduces overfitting, but is only guaranteed to work for (certain types of) closely related tasks (#REF; #REF ). #REF apply a variation of hard parameter sharing to multi-domain multi-task sequence tagging with a shared CRF layer and domain-specific projection layers. Yang, Salakhutdinov, and #REF use hard parameter sharing to jointly learn different sequence-tagging tasks across languages. Martínez #REF explore a similar set-up, but sharing is limited to the initial layer. In all three papers, the amount of sharing between the networks is fixed in advance.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Hard parameter sharing #TARGET_REF ) is easy to implement, reduces overfitting, but is only guaranteed to work for (certain types of) closely related tasks (#REF; #REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "Let W ∈ R M ×D be a matrix in which each row i corresponds to a model θ i with D parameters.",
                "The loss that sluice networks minimize, with a penalty term Ω, is then as follows:",
                "The loss functions L i are crossentropy functions of the form − y p(y) log q(y) where y i are the labels of task i. Note that sluice networks are not restricted to tasks with the same loss functions, but could also be applied to jointly learn regression and classification tasks.",
                "The weights λ i determine the importance of the different tasks during training.",
                "We explicitly add inductive bias to the model via the regularizer Ω below, but our model also implicitly learns regularization through multi-task learning #TARGET_REF ) mediated by the α parameters, while the β parameters are used to learn the mixture functions f (·), as detailed in the following."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Let W ∈ R M ×D be a matrix in which each row i corresponds to a model θ i with D parameters. The loss that sluice networks minimize, with a penalty term Ω, is then as follows: The loss functions L i are crossentropy functions of the form − y p(y) log q(y) where y i are the labels of task i. Note that sluice networks are not restricted to tasks with the same loss functions, but could also be applied to jointly learn regression and classification tasks. The weights λ i determine the importance of the different tasks during training. We explicitly add inductive bias to the model via the regularizer Ω below, but our model also implicitly learns regularization through multi-task learning #TARGET_REF ) mediated by the α parameters, while the β parameters are used to learn the mixture functions f (·), as detailed in the following.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We explicitly add inductive bias to the model via the regularizer \\u2126 below, but our model also implicitly learns regularization through multi-task learning #TARGET_REF ) mediated by the \\u03b1 parameters, while the \\u03b2 parameters are used to learn the mixture functions f (\\u00b7), as detailed in the following.\"]}"
    },
    {
        "gold": {
            "text": [
                "Adding Inductive Bias Naturally, we can also add explicit inductive bias to sluice networks by partially constraining the regularizer or adding to the learned penalty.",
                "Inspired by work on shared-space component analysis (#REF ), we add a penalty to enforce a division of labor and discourage redundancy between shared and task-specific subspaces.",
                "While the networks can theoretically learn such a separation, an explicit constraint empirically leads to better results and enables the sluice networks to take better advantage of subspace-specific α-values.",
                "We introduce an orthogonality constraint #TARGET_REF ) between the layer-wise subspaces of each model:",
                "F , where M is the number of tasks, K is the number of layers, · 2 F is the squared Frobenius norm, and G m,k,1 and G m,k,2 are the first and second subspace respectively in the k-th layer of the m-th task model."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Adding Inductive Bias Naturally, we can also add explicit inductive bias to sluice networks by partially constraining the regularizer or adding to the learned penalty. Inspired by work on shared-space component analysis (#REF ), we add a penalty to enforce a division of labor and discourage redundancy between shared and task-specific subspaces. While the networks can theoretically learn such a separation, an explicit constraint empirically leads to better results and enables the sluice networks to take better advantage of subspace-specific α-values. We introduce an orthogonality constraint #TARGET_REF ) between the layer-wise subspaces of each model: F , where M is the number of tasks, K is the number of layers, · 2 F is the squared Frobenius norm, and G m,k,1 and G m,k,2 are the first and second subspace respectively in the k-th layer of the m-th task model.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We introduce an orthogonality constraint #TARGET_REF ) between the layer-wise subspaces of each model:\"]}"
    },
    {
        "gold": {
            "text": [
                "We train our models on each domain and evaluate them both on the indomain test set (Table 3 , top) as well as on the test sets of all other domains (Table 3 , bottom) to evaluate their out-ofdomain generalization ability.",
                "Note that due to this set-up, our results are not directly comparable to the results reported in , who only train on the WSJ domain and use OntoNotes 4.0.",
                "Baseline Models As baselines, we compare against i) a single-task model only trained on chunking; ii) the low supervision model , which predicts the auxiliary task at the first layer; iii) an MTL model based on hard parameter sharing #TARGET_REF ; and iv) cross-stitch networks (#REF) .",
                "We compare these against our complete sluice network with subspace constraints and learned α and β parameters.",
                "We implement all models in DyNet (#REF ) and make our code available at https://github.com/ sebastianruder/sluice-networks."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We train our models on each domain and evaluate them both on the indomain test set (Table 3 , top) as well as on the test sets of all other domains (Table 3 , bottom) to evaluate their out-ofdomain generalization ability. Note that due to this set-up, our results are not directly comparable to the results reported in , who only train on the WSJ domain and use OntoNotes 4.0. Baseline Models As baselines, we compare against i) a single-task model only trained on chunking; ii) the low supervision model , which predicts the auxiliary task at the first layer; iii) an MTL model based on hard parameter sharing #TARGET_REF ; and iv) cross-stitch networks (#REF) . We compare these against our complete sluice network with subspace constraints and learned α and β parameters. We implement all models in DyNet (#REF ) and make our code available at https://github.com/ sebastianruder/sluice-networks.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Baseline Models As baselines, we compare against i) a single-task model only trained on chunking; ii) the low supervision model , which predicts the auxiliary task at the first layer; iii) an MTL model based on hard parameter sharing #TARGET_REF ; and iv) cross-stitch networks (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We review those in Section 3.",
                "For now just observe that if all α-values are set to 0.25 (or any other constant), we obtain hard parameter sharing #TARGET_REF , which is equivalent to a heavy L 0 matrix regularizer.",
                "Adding Inductive Bias Naturally, we can also add explicit inductive bias to sluice networks by partially constraining the regularizer or adding to the learned penalty.",
                "Inspired by work on shared-space component analysis (#REF ), we add a penalty to enforce a division of labor and discourage redundancy between shared and task-specific subspaces.",
                "While the networks can theoretically learn such a separation, an explicit constraint empirically leads to better results and enables the sluice networks to take better advantage of subspace-specific α-values."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We review those in Section 3. For now just observe that if all α-values are set to 0.25 (or any other constant), we obtain hard parameter sharing #TARGET_REF , which is equivalent to a heavy L 0 matrix regularizer. Adding Inductive Bias Naturally, we can also add explicit inductive bias to sluice networks by partially constraining the regularizer or adding to the learned penalty. Inspired by work on shared-space component analysis (#REF ), we add a penalty to enforce a division of labor and discourage redundancy between shared and task-specific subspaces. While the networks can theoretically learn such a separation, an explicit constraint empirically leads to better results and enables the sluice networks to take better advantage of subspace-specific α-values.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"For now just observe that if all \\u03b1-values are set to 0.25 (or any other constant), we obtain hard parameter sharing #TARGET_REF , which is equivalent to a heavy L 0 matrix regularizer.\"]}"
    },
    {
        "gold": {
            "text": [
                "Chunking results We show results on in-domain and outof-domain tests sets in Table 3 .",
                "On average, sluice networks significantly outperform all other model architectures on both in-domain and out-of-domain data and perform best for all domains, except for the telephone conversation (tc) domain, where they are outperformed by cross-stitch networks.",
                "The performance boost is particularly significant for the out-ofdomain setting, where sluice networks add more than 1 point in accuracy compared to hard parameter sharing and almost .5 compared to the strongest baseline on average, demonstrating that sluice networks are particularly useful to help a model generalize better.",
                "In contrast to previous studies on MTL (Martínez #REF; #TARGET_REF; Augenstein, Ruder, and Søgaard 2018) , our model also consistently outperforms single-task learning.",
                "Overall, this demonstrates that our meta-architecture for learning which parts of multi-task models to share, with a small set of additional parameters to learn, can achieve significant and consistent improvements over strong baseline methods."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Chunking results We show results on in-domain and outof-domain tests sets in Table 3 . On average, sluice networks significantly outperform all other model architectures on both in-domain and out-of-domain data and perform best for all domains, except for the telephone conversation (tc) domain, where they are outperformed by cross-stitch networks. The performance boost is particularly significant for the out-ofdomain setting, where sluice networks add more than 1 point in accuracy compared to hard parameter sharing and almost .5 compared to the strongest baseline on average, demonstrating that sluice networks are particularly useful to help a model generalize better. In contrast to previous studies on MTL (Martínez #REF; #TARGET_REF; Augenstein, Ruder, and Søgaard 2018) , our model also consistently outperforms single-task learning. Overall, this demonstrates that our meta-architecture for learning which parts of multi-task models to share, with a small set of additional parameters to learn, can achieve significant and consistent improvements over strong baseline methods.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"In contrast to previous studies on MTL (Mart\\u00ednez #REF; #TARGET_REF; Augenstein, Ruder, and S\\u00f8gaard 2018) , our model also consistently outperforms single-task learning.\"]}"
    },
    {
        "gold": {
            "text": [
                "As intelligent systems/robots are brought out of the laboratory and into the physical world, they must become capable of natural everyday conversation with their human users about their physical surroundings.",
                "Among other competencies, this involves the ability to learn and adapt mappings between words, phrases, and sentences in Natural Language (NL) and perceptual aspects of the external environment -this is widely known as the grounding problem.",
                "Our work is similar in spirit to e.g. (#REF; #REF) but advances it in several aspects #TARGET_REF .",
                "In this demo paper, we present a dialogue agent that learns visually grounded word meanings interactively from a human tutor, which we call: VOILA (Visually Optimised Interactive Learning Agent).",
                "Our goal is to enable this agent to learn to identify and describe objects/attributes (colour 1 http://www.furhatrobotics.com/ and shape in this case) in its immediate visual environment through interaction with human users, incrementally, over time."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "As intelligent systems/robots are brought out of the laboratory and into the physical world, they must become capable of natural everyday conversation with their human users about their physical surroundings. Among other competencies, this involves the ability to learn and adapt mappings between words, phrases, and sentences in Natural Language (NL) and perceptual aspects of the external environment -this is widely known as the grounding problem. Our work is similar in spirit to e.g. (#REF; #REF) but advances it in several aspects #TARGET_REF . In this demo paper, we present a dialogue agent that learns visually grounded word meanings interactively from a human tutor, which we call: VOILA (Visually Optimised Interactive Learning Agent). Our goal is to enable this agent to learn to identify and describe objects/attributes (colour 1 http://www.furhatrobotics.com/ and shape in this case) in its immediate visual environment through interaction with human users, incrementally, over time.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Our work is similar in spirit to e.g. (#REF; #REF) but advances it in several aspects #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We developed a multimodal framework in support of building an interactive learning system, which loosely follows that of #TARGET_REF .",
                "The framework consists of two core modules:",
                "Vision Module The vision module produces visual attribute predictions, using two base feature categories: the HSV colour space for colour attributes, and a 'bag of visual words' (i.e. PHOW descriptors) for the object shapes/class.",
                "It consists of a set of binary classifiers -Logistic Regression SVM classifiers with Stochastic Gradient Descent (SGD) (#REF) -to incrementally learn attribute predictions.",
                "The visual classifiers ground visual attribute words such as 'red', 'circle' etc."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We developed a multimodal framework in support of building an interactive learning system, which loosely follows that of #TARGET_REF . The framework consists of two core modules: Vision Module The vision module produces visual attribute predictions, using two base feature categories: the HSV colour space for colour attributes, and a 'bag of visual words' (i.e. PHOW descriptors) for the object shapes/class. It consists of a set of binary classifiers -Logistic Regression SVM classifiers with Stochastic Gradient Descent (SGD) (#REF) -to incrementally learn attribute predictions. The visual classifiers ground visual attribute words such as 'red', 'circle' etc.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"We developed a multimodal framework in support of building an interactive learning system, which loosely follows that of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The first MDP performs a kind of active learning: the learner/agent only acquires the feedback from humans about a visual attribute if it is not confident enough already about its own predictions.",
                "Following previous work #TARGET_REF , here we use a positive confidence threshold, which determines when the agent believes its own predictions.",
                "For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as #REF point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent.",
                "We therefore assign a separate but dependent component MDP for adjusting the threshold dynamically in order to optimise the trade-off between accuracy and cost.",
                "Note now that the adjusted confidence threshold will affect the agent's dialogue behaviour, modeled in the other MDP presented in the next section (natural interaction with humans)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The first MDP performs a kind of active learning: the learner/agent only acquires the feedback from humans about a visual attribute if it is not confident enough already about its own predictions. Following previous work #TARGET_REF , here we use a positive confidence threshold, which determines when the agent believes its own predictions. For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as #REF point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent. We therefore assign a separate but dependent component MDP for adjusting the threshold dynamically in order to optimise the trade-off between accuracy and cost. Note now that the adjusted confidence threshold will affect the agent's dialogue behaviour, modeled in the other MDP presented in the next section (natural interaction with humans).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following previous work #TARGET_REF , here we use a positive confidence threshold, which determines when the agent believes its own predictions.\"]}"
    },
    {
        "gold": {
            "text": [
                "Hence, the agent must learn from scratch: (1) the perceptual/visual categories themselves; and (2) how NL expressions map to these; and in addition, (3) as a standard conversational agent, the agent much also learn to conduct natural, spontaneous conversations with real humans.",
                "In this demonstration, VOILA plays the role of an interactive, concept learning agent that takes initiative in the dialogues and actively learns novel visual knowledge from the feedback from the human tutor.",
                "What sets VOILA apart from other work in this area is:",
                "• VOILA's dialogue strategy is optimised via Reinforcement Learning to achieve an optimal trade-off between the accuracy of the concepts it learns/has learnt from users, and the effort that the dialogues incur on the users: this is a form of active learning where the agent only asks about something if it doesn't already know the answer with some appropriate confidence (see #TARGET_REF for more detail).",
                "• VOILA is trained on a corpus of real HumanHuman conversations (#REF) , and is thus able to process natural human dialogue, which contains phenomena such as self-corrections, repetitions and restarts, pauses, fillers, and continuations VOILA is deployed onto Furhat, a humanlike robot head with a custom back-projected face, built-in stereo microphones, and a Microsoft"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Hence, the agent must learn from scratch: (1) the perceptual/visual categories themselves; and (2) how NL expressions map to these; and in addition, (3) as a standard conversational agent, the agent much also learn to conduct natural, spontaneous conversations with real humans. In this demonstration, VOILA plays the role of an interactive, concept learning agent that takes initiative in the dialogues and actively learns novel visual knowledge from the feedback from the human tutor. What sets VOILA apart from other work in this area is: • VOILA's dialogue strategy is optimised via Reinforcement Learning to achieve an optimal trade-off between the accuracy of the concepts it learns/has learnt from users, and the effort that the dialogues incur on the users: this is a form of active learning where the agent only asks about something if it doesn't already know the answer with some appropriate confidence (see #TARGET_REF for more detail). • VOILA is trained on a corpus of real HumanHuman conversations (#REF) , and is thus able to process natural human dialogue, which contains phenomena such as self-corrections, repetitions and restarts, pauses, fillers, and continuations VOILA is deployed onto Furhat, a humanlike robot head with a custom back-projected face, built-in stereo microphones, and a Microsoft",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"\\u2022 VOILA's dialogue strategy is optimised via Reinforcement Learning to achieve an optimal trade-off between the accuracy of the concepts it learns/has learnt from users, and the effort that the dialogues incur on the users: this is a form of active learning where the agent only asks about something if it doesn't already know the answer with some appropriate confidence (see #TARGET_REF for more detail).\"]}"
    },
    {
        "gold": {
            "text": [
                "The first MDP performs a kind of active learning: the learner/agent only acquires the feedback from humans about a visual attribute if it is not confident enough already about its own predictions.",
                "Following previous work (#REF) , here we use a positive confidence threshold, which determines when the agent believes its own predictions.",
                "For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as #TARGET_REF point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent.",
                "We therefore assign a separate but dependent component MDP for adjusting the threshold dynamically in order to optimise the trade-off between accuracy and cost.",
                "Note now that the adjusted confidence threshold will affect the agent's dialogue behaviour, modeled in the other MDP presented in the next section (natural interaction with humans)."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The first MDP performs a kind of active learning: the learner/agent only acquires the feedback from humans about a visual attribute if it is not confident enough already about its own predictions. Following previous work (#REF) , here we use a positive confidence threshold, which determines when the agent believes its own predictions. For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as #TARGET_REF point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent. We therefore assign a separate but dependent component MDP for adjusting the threshold dynamically in order to optimise the trade-off between accuracy and cost. Note now that the adjusted confidence threshold will affect the agent's dialogue behaviour, modeled in the other MDP presented in the next section (natural interaction with humans).",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"For instance, the learner can ask either polar or WH-questions about an attribute if its confidence score is higher than a certain threshold; otherwise, there should be no interaction about that attribute. But as #TARGET_REF point out the confidence score from a classifier is not reliable enough at the early stages of learning, so in order to find an optimum dialogue policy, a threshold should be able to dynamically adjust according to the previous learning performance of the agent.\"]}"
    },
    {
        "gold": {
            "text": [
                "When operating in multilingual settings, it is highly desirable to learn embeddings for words denoting similar concepts that are very close in the shared inter-lingual embedding space (e.g., the representations for the English word school and the Spanish word escuela should be very similar).",
                "These shared interlingual embedding spaces may then be used in a myriad of multilingual natural language processing tasks, such as fundamental tasks of computing cross-lingual and multilingual semantic word similarity and bilingual lexicon induction (BLI), etc.",
                "However, all these models critically require at least sentence-aligned parallel data and/or readilyavailable translation dictionaries to induce bilingual word embeddings (BWEs) that are consistent and closely aligned over languages in the same semantic space.",
                "Contributions In this work, we alleviate the requirements: (1) We present the first model that is able to induce bilingual word embeddings from non-parallel data without any other readily available translation resources such as pre-given bilingual lexicons; (2) We demonstrate the utility of BWEs induced by this simple yet effective model in the BLI task from comparable Wikipedia data on benchmarking datasets for three language pairs #TARGET_REF ).",
                "Our BLI model based on our novel BWEs significantly outperforms a series of strong baselines that reported previous best scores on these datasets in the same learning setting, as well as other BLI models based on recently proposed BWE induction models (#REF; #REF) ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "When operating in multilingual settings, it is highly desirable to learn embeddings for words denoting similar concepts that are very close in the shared inter-lingual embedding space (e.g., the representations for the English word school and the Spanish word escuela should be very similar). These shared interlingual embedding spaces may then be used in a myriad of multilingual natural language processing tasks, such as fundamental tasks of computing cross-lingual and multilingual semantic word similarity and bilingual lexicon induction (BLI), etc. However, all these models critically require at least sentence-aligned parallel data and/or readilyavailable translation dictionaries to induce bilingual word embeddings (BWEs) that are consistent and closely aligned over languages in the same semantic space. Contributions In this work, we alleviate the requirements: (1) We present the first model that is able to induce bilingual word embeddings from non-parallel data without any other readily available translation resources such as pre-given bilingual lexicons; (2) We demonstrate the utility of BWEs induced by this simple yet effective model in the BLI task from comparable Wikipedia data on benchmarking datasets for three language pairs #TARGET_REF ). Our BLI model based on our novel BWEs significantly outperforms a series of strong baselines that reported previous best scores on these datasets in the same learning setting, as well as other BLI models based on recently proposed BWE induction models (#REF; #REF) .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Contributions In this work, we alleviate the requirements: (1) We present the first model that is able to induce bilingual word embeddings from non-parallel data without any other readily available translation resources such as pre-given bilingual lexicons; (2) We demonstrate the utility of BWEs induced by this simple yet effective model in the BLI task from comparable Wikipedia data on benchmarking datasets for three language pairs #TARGET_REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "Training Data We use comparable Wikipedia data introduced in (Vulić and #REFa; #TARGET_REF ) available in three language pairs to induce bilingual word embeddings: (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (ES-EN), (ii) a collection of 18, 898 ItalianEnglish Wikipedia article pairs (IT-EN), and (iii) a collection of 7, 612 Dutch-English Wikipedia article pairs (NL-EN).",
                "All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general not direct translations.",
                "Following prior work (#REF; #REF; Vulić and #REFb) , we retain only nouns that occur at least 5 times in the corpus.",
                "Lemmatized word forms are recorded when available, and original forms otherwise.",
                "TreeTagger (#REF ) is used for POS tagging and lemmatization."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Training Data We use comparable Wikipedia data introduced in (Vulić and #REFa; #TARGET_REF ) available in three language pairs to induce bilingual word embeddings: (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (ES-EN), (ii) a collection of 18, 898 ItalianEnglish Wikipedia article pairs (IT-EN), and (iii) a collection of 7, 612 Dutch-English Wikipedia article pairs (NL-EN). All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general not direct translations. Following prior work (#REF; #REF; Vulić and #REFb) , we retain only nouns that occur at least 5 times in the corpus. Lemmatized word forms are recorded when available, and original forms otherwise. TreeTagger (#REF ) is used for POS tagging and lemmatization.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Training Data We use comparable Wikipedia data introduced in (Vuli\\u0107 and #REFa; #TARGET_REF ) available in three language pairs to induce bilingual word embeddings: (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (ES-EN), (ii) a collection of 18, 898 ItalianEnglish Wikipedia article pairs (IT-EN), and (iii) a collection of 7, 612 Dutch-English Wikipedia article pairs (NL-EN).\"]}"
    },
    {
        "gold": {
            "text": [
                "(1) BiLDA-BLI -A BLI model that relies on the induction of latent cross-lingual topics (#REF) by the bilingual LDA model and represents words as probability distributions over these topics (Vulić et al., 2011) .",
                "(2) Assoc-BLI -A BLI model that represents words as vectors of association norms (Roller and Schulte im #REF) over both vocabularies, where these norms are computed using a multilingual topic model (Vulić and #REFa) .",
                "(3) PPMI+cos -A standard distributional model for BLI relying on positive pointwise mutual information and cosine similarity (#REF) .",
                "The seed lexicon is bootstrapped using the method from (Peirsman and Padó, 2011; #TARGET_REF .",
                "All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (#REF; Vulić and #REFa; Vulić and #REFb; #REF) ."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "(1) BiLDA-BLI -A BLI model that relies on the induction of latent cross-lingual topics (#REF) by the bilingual LDA model and represents words as probability distributions over these topics (Vulić et al., 2011) . (2) Assoc-BLI -A BLI model that represents words as vectors of association norms (Roller and Schulte im #REF) over both vocabularies, where these norms are computed using a multilingual topic model (Vulić and #REFa) . (3) PPMI+cos -A standard distributional model for BLI relying on positive pointwise mutual information and cosine similarity (#REF) . The seed lexicon is bootstrapped using the method from (Peirsman and Padó, 2011; #TARGET_REF . All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (#REF; Vulić and #REFa; Vulić and #REFb; #REF) .",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"The seed lexicon is bootstrapped using the method from (Peirsman and Pad\\u00f3, 2011; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The seed lexicon is bootstrapped using the method from (Peirsman and Padó, 2011; Vulić and #REFb) .",
                "All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (#REF; Vulić and #REFa; #TARGET_REF; #REF) .",
                "Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Padó, 2011; #REF; Vulić and #REFa; Vulić and #REFb) .",
                "Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NL-EN) (Vulić and #REFa; Vulić and #REFb) .",
                "Translation direction is ES/IT/NL → EN."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The seed lexicon is bootstrapped using the method from (Peirsman and Padó, 2011; Vulić and #REFb) . All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (#REF; Vulić and #REFa; #TARGET_REF; #REF) . Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Padó, 2011; #REF; Vulić and #REFa; Vulić and #REFb) . Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NL-EN) (Vulić and #REFa; Vulić and #REFb) . Translation direction is ES/IT/NL → EN.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (#REF; Vuli\\u0107 and #REFa; #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The seed lexicon is bootstrapped using the method from (Peirsman and Padó, 2011; Vulić and #REFb) .",
                "All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (#REF; Vulić and #REFa; Vulić and #REFb; #REF) .",
                "Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Padó, 2011; #REF; Vulić and #REFa; Vulić and #REFb) .",
                "Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NL-EN) (Vulić and #REFa; #TARGET_REF .",
                "Translation direction is ES/IT/NL → EN."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The seed lexicon is bootstrapped using the method from (Peirsman and Padó, 2011; Vulić and #REFb) . All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (#REF; Vulić and #REFa; Vulić and #REFb; #REF) . Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Padó, 2011; #REF; Vulić and #REFa; Vulić and #REFb) . Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NL-EN) (Vulić and #REFa; #TARGET_REF . Translation direction is ES/IT/NL → EN.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NL-EN) (Vuli\\u0107 and #REFa; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general not direct translations.",
                "Following prior work (#REF; #REF; #TARGET_REF , we retain only nouns that occur at least 5 times in the corpus.",
                "Lemmatized word forms are recorded when available, and original forms otherwise.",
                "TreeTagger (#REF ) is used for POS tagging and lemmatization.",
                "After the preprocessing vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general not direct translations. Following prior work (#REF; #REF; #TARGET_REF , we retain only nouns that occur at least 5 times in the corpus. Lemmatized word forms are recorded when available, and original forms otherwise. TreeTagger (#REF ) is used for POS tagging and lemmatization. After the preprocessing vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"Following prior work (#REF; #REF; #TARGET_REF , we retain only nouns that occur at least 5 times in the corpus.\"]}"
    },
    {
        "gold": {
            "text": [
                "The seed lexicon is bootstrapped using the method from (Peirsman and Padó, 2011; Vulić and #REFb) .",
                "All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (#REF; Vulić and #REFa; Vulić and #REFb; #REF) .",
                "Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Padó, 2011; #REF; Vulić and #REFa; #TARGET_REF .",
                "Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NL-EN) (Vulić and #REFa; Vulić and #REFb) .",
                "Translation direction is ES/IT/NL → EN."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The seed lexicon is bootstrapped using the method from (Peirsman and Padó, 2011; Vulić and #REFb) . All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (#REF; Vulić and #REFa; Vulić and #REFb; #REF) . Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Padó, 2011; #REF; Vulić and #REFa; #TARGET_REF . Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NL-EN) (Vulić and #REFa; Vulić and #REFb) . Translation direction is ES/IT/NL → EN.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Pad\\u00f3, 2011; #REF; Vuli\\u0107 and #REFa; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Translation direction is ES/IT/NL → EN.",
                "Evaluation Metrics Since we can build a oneto-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflected in the Acc 1 score, that is, the number of source language (ES/IT/NL) words w S i from ground truth translation pairs for which the top ranked word cross-lingually is the correct trans- Table 1 : Example lists of top 10 semantically similar words for all 3 language pairs obtained using BWESG+cos; d = 200, cs = 48; (col 1.) only source language words (ES/IT/NL) are listed while target language words are skipped (monolingual similarity); (2) only target language words (EN) are listed (cross-lingual similarity); (3) words from both languages are listed (multilingual similarity).",
                "EN words are given in italic.",
                "The correct one-to-one translation for each source word is marked by (+).",
                "lation in the other language (EN) according to the ground truth over the total number of ground truth translation pairs (=1000) (#REF; #REF; #TARGET_REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Translation direction is ES/IT/NL → EN. Evaluation Metrics Since we can build a oneto-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflected in the Acc 1 score, that is, the number of source language (ES/IT/NL) words w S i from ground truth translation pairs for which the top ranked word cross-lingually is the correct trans- Table 1 : Example lists of top 10 semantically similar words for all 3 language pairs obtained using BWESG+cos; d = 200, cs = 48; (col 1.) only source language words (ES/IT/NL) are listed while target language words are skipped (monolingual similarity); (2) only target language words (EN) are listed (cross-lingual similarity); (3) words from both languages are listed (multilingual similarity). EN words are given in italic. The correct one-to-one translation for each source word is marked by (+). lation in the other language (EN) according to the ground truth over the total number of ground truth translation pairs (=1000) (#REF; #REF; #TARGET_REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"lation in the other language (EN) according to the ground truth over the total number of ground truth translation pairs (=1000) (#REF; #REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We have demonstrated its utility in the task of bilingual lexicon induction from such comparable data, where our new BWESG-based BLI model outperforms state-of-the-art models for BLI from document-aligned comparable data and related BWE induction models.",
                "The low-cost BWEs may be used in other (semantic) tasks besides the ones discussed here, and it would be interesting to experiment with other types of context aggregation and selection beyond random shuffling, and other objective functions.",
                "Preliminary studies also demonstrate the utility of the BWEs in monolingual and cross-lingual information retrieval (Vulić and #REF) .",
                "Finally, we may use the knowledge of BWEs obtained by BWESG from document-aligned data to learn bilingual correspondences (e.g., word translation pairs or lists of semantically similar words across languages) which may in turn be used for representation learning from large unaligned multilingual datasets as proposed in (#REF; #REFb; #TARGET_REF .",
                "In the long run, this idea may lead to large-scale fully data-driven representation learning models from huge amounts of multilingual data without any \"pre-requirement\" for parallel data or manually built lexicons."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We have demonstrated its utility in the task of bilingual lexicon induction from such comparable data, where our new BWESG-based BLI model outperforms state-of-the-art models for BLI from document-aligned comparable data and related BWE induction models. The low-cost BWEs may be used in other (semantic) tasks besides the ones discussed here, and it would be interesting to experiment with other types of context aggregation and selection beyond random shuffling, and other objective functions. Preliminary studies also demonstrate the utility of the BWEs in monolingual and cross-lingual information retrieval (Vulić and #REF) . Finally, we may use the knowledge of BWEs obtained by BWESG from document-aligned data to learn bilingual correspondences (e.g., word translation pairs or lists of semantically similar words across languages) which may in turn be used for representation learning from large unaligned multilingual datasets as proposed in (#REF; #REFb; #TARGET_REF . In the long run, this idea may lead to large-scale fully data-driven representation learning models from huge amounts of multilingual data without any \"pre-requirement\" for parallel data or manually built lexicons.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Finally, we may use the knowledge of BWEs obtained by BWESG from document-aligned data to learn bilingual correspondences (e.g., word translation pairs or lists of semantically similar words across languages) which may in turn be used for representation learning from large unaligned multilingual datasets as proposed in (#REF; #REFb; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Tree-based approaches to alignment model translation as a sequence of probabilistic operations transforming the syntactic parse tree of a sentence in one language into that of the other.",
                "The trees may be learned directly from parallel corpora #TARGET_REF), or provided by a parser trained on hand-annotated treebanks (#REF) .",
                "In this paper, we compare these approaches on Chinese-English and French-English datasets, and find that automatically derived trees result in better agreement with human-annotated word-level alignments for unseen test data."
            ],
            "label": [
                "BACK",
                "USE"
            ],
            "context": [
                0,
                1,
                1
            ]
        },
        "input": "Tree-based approaches to alignment model translation as a sequence of probabilistic operations transforming the syntactic parse tree of a sentence in one language into that of the other. The trees may be learned directly from parallel corpora #TARGET_REF), or provided by a parser trained on hand-annotated treebanks (#REF) . In this paper, we compare these approaches on Chinese-English and French-English datasets, and find that automatically derived trees result in better agreement with human-annotated word-level alignments for unseen test data.",
        "output": "{\"label\": [\"BACK\", \"USE\"], \"context\": [\"The trees may be learned directly from parallel corpora #TARGET_REF), or provided by a parser trained on hand-annotated treebanks (#REF) .\", \"In this paper, we compare these approaches on Chinese-English and French-English datasets, and find that automatically derived trees result in better agreement with human-annotated word-level alignments for unseen test data.\"]}"
    },
    {
        "gold": {
            "text": [
                "The tree-based approach allows us to represent the fact that syntactic constituents tend to move as unit, as well as systematic differences in word order in the grammars of the two languages.",
                "Furthermore, the tree structure allows us to make probabilistic independence assumptions that result in polynomial time algorithms for estimating a translation model from parallel training data, and for finding the highest probability translation given a new sentence.",
                "#TARGET_REF modeled the reordering process with binary branching trees, where each production could be either in the same or in reverse order going from source to target language.",
                "The trees of Wu's Inversion Transduction Grammar were derived by synchronously parsing a parallel corpus, using a grammar with lexical translation probabilities at the leaves and a simple grammar with a single nonterminal providing the tree structure.",
                "While this grammar did not represent traditional syntactic categories such as verb phrases and noun phrases, it served to restrict the word-level alignments considered by the system to those allowable by reordering operations on binary trees."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The tree-based approach allows us to represent the fact that syntactic constituents tend to move as unit, as well as systematic differences in word order in the grammars of the two languages. Furthermore, the tree structure allows us to make probabilistic independence assumptions that result in polynomial time algorithms for estimating a translation model from parallel training data, and for finding the highest probability translation given a new sentence. #TARGET_REF modeled the reordering process with binary branching trees, where each production could be either in the same or in reverse order going from source to target language. The trees of Wu's Inversion Transduction Grammar were derived by synchronously parsing a parallel corpus, using a grammar with lexical translation probabilities at the leaves and a simple grammar with a single nonterminal providing the tree structure. While this grammar did not represent traditional syntactic categories such as verb phrases and noun phrases, it served to restrict the word-level alignments considered by the system to those allowable by reordering operations on binary trees.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF modeled the reordering process with binary branching trees, where each production could be either in the same or in reverse order going from source to target language.\"]}"
    },
    {
        "gold": {
            "text": [
                "This restriction corresponds to intuitions about the alignments that could be produced by systematic differences between the two language's grammars, and allows for a polynomial time algorithm for finding the highest-probability alignment, and for re-estimation of the lexical translation and grammar probabilities using the Expectation Maximization algorithm.",
                "#REF present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures.",
                "This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in #TARGET_REF , but the specific bracketing of the parse tree provided.",
                "In this paper, we make a direct comparison of a syntactically unsupervised alignment model, based on #REF , with a syntactically supervised model, based on #REF .",
                "We use the term syntactically supervised to indicate that the syntactic structure in one language is given to the training procedure."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "This restriction corresponds to intuitions about the alignments that could be produced by systematic differences between the two language's grammars, and allows for a polynomial time algorithm for finding the highest-probability alignment, and for re-estimation of the lexical translation and grammar probabilities using the Expectation Maximization algorithm. #REF present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures. This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in #TARGET_REF , but the specific bracketing of the parse tree provided. In this paper, we make a direct comparison of a syntactically unsupervised alignment model, based on #REF , with a syntactically supervised model, based on #REF . We use the term syntactically supervised to indicate that the syntactic structure in one language is given to the training procedure.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in #TARGET_REF , but the specific bracketing of the parse tree provided.\"]}"
    },
    {
        "gold": {
            "text": [
                "The Inversion Transduction Grammar of #TARGET_REF can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous context-free grammar productions.",
                "The grammar is restricted to binary rules, which can have the symbols in the right hand side appear in the same order in both languages, represented with square brackets:",
                "or the symbols may appear in reverse order in the two languages, indicated by angle brackets:",
                "Individual lexical translations between English words e and French words f take place at the leaves of the tree, generated by grammar rules with a single right hand side symbol in each language:",
                "Given a bilingual sentence pair, a synchronous parse can be built using a two-dimensional extension of chart parsing, where chart items are indexed by their nonterminal Y and beginning and ending positions l, m in the source language string, and beginning and ending positions i, j in the target language string."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The Inversion Transduction Grammar of #TARGET_REF can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous context-free grammar productions. The grammar is restricted to binary rules, which can have the symbols in the right hand side appear in the same order in both languages, represented with square brackets: or the symbols may appear in reverse order in the two languages, indicated by angle brackets: Individual lexical translations between English words e and French words f take place at the leaves of the tree, generated by grammar rules with a single right hand side symbol in each language: Given a bilingual sentence pair, a synchronous parse can be built using a two-dimensional extension of chart parsing, where chart items are indexed by their nonterminal Y and beginning and ending positions l, m in the source language string, and beginning and ending positions i, j in the target language string.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The Inversion Transduction Grammar of #TARGET_REF can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous context-free grammar productions.\"]}"
    },
    {
        "gold": {
            "text": [
                "IBM Models 1 and 4 refer to #REF .",
                "We used the GIZA++ package, including the HMM model of #REF .",
                "We ran Model 1 for three iterations, then the HMM model for three iterations, and finally Model 4 for two iterations, training each model until AER began to increase on our held-out cross validation data.",
                "\"Inversion Transduction Grammar\" (ITG) is the model of #TARGET_REF , \"Tree-to-String\" is the model of #REF , and \"Tree-to-String, Clone\" allows the node cloning operation described above.",
                "Our tree-based models were initialized from uniform distributions for both the lexical translation probabilities and the tree reordering operations, and were trained until AER began to rise on our held-out cross-validation data, which turned out to be four iterations for the tree-to-string models and three for the Inversion Transduction Grammar."
            ],
            "label": [
                "BACK",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "IBM Models 1 and 4 refer to #REF . We used the GIZA++ package, including the HMM model of #REF . We ran Model 1 for three iterations, then the HMM model for three iterations, and finally Model 4 for two iterations, training each model until AER began to increase on our held-out cross validation data. \"Inversion Transduction Grammar\" (ITG) is the model of #TARGET_REF , \"Tree-to-String\" is the model of #REF , and \"Tree-to-String, Clone\" allows the node cloning operation described above. Our tree-based models were initialized from uniform distributions for both the lexical translation probabilities and the tree reordering operations, and were trained until AER began to rise on our held-out cross-validation data, which turned out to be four iterations for the tree-to-string models and three for the Inversion Transduction Grammar.",
        "output": "{\"label\": [\"BACK\", \"USE\"], \"context\": [\"We ran Model 1 for three iterations, then the HMM model for three iterations, and finally Model 4 for two iterations, training each model until AER began to increase on our held-out cross validation data.\", \"\\\"Inversion Transduction Grammar\\\" (ITG) is the model of #TARGET_REF , \\\"Tree-to-String\\\" is the model of #REF , and \\\"Tree-to-String, Clone\\\" allows the node cloning operation described above.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF provide a thorough analysis of alignment constraints from the perspective of decoding algorithms.",
                "They train the models of #REF .",
                "Decoding, meaning exact computation of the highest probability translation given a foreign sentence, is not possible in polynomial time for the IBM models, and in practice decoders search through the space of hypothesis translations using a set of additional, hard alignment constraints.",
                "#REF compute the viterbi alignments for German-English and French-English sentences pairs using IBM Model 5, and then measure how many of the resulting alignments fall within the hard constraints of both #TARGET_REF and #REF .",
                "They find higher coverage for an extended version of ITG than for the IBM decoding constraint for both language pairs, with the unmodified ITG implementation covering about the same amount of German-English data as IBM, and significantly less French-English data."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "#REF provide a thorough analysis of alignment constraints from the perspective of decoding algorithms. They train the models of #REF . Decoding, meaning exact computation of the highest probability translation given a foreign sentence, is not possible in polynomial time for the IBM models, and in practice decoders search through the space of hypothesis translations using a set of additional, hard alignment constraints. #REF compute the viterbi alignments for German-English and French-English sentences pairs using IBM Model 5, and then measure how many of the resulting alignments fall within the hard constraints of both #TARGET_REF and #REF . They find higher coverage for an extended version of ITG than for the IBM decoding constraint for both language pairs, with the unmodified ITG implementation covering about the same amount of German-English data as IBM, and significantly less French-English data.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#REF compute the viterbi alignments for German-English and French-English sentences pairs using IBM Model 5, and then measure how many of the resulting alignments fall within the hard constraints of both #TARGET_REF and #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "This restriction corresponds to intuitions about the alignments that could be produced by systematic differences between the two language's grammars, and allows for a polynomial time algorithm for finding the highest-probability alignment, and for re-estimation of the lexical translation and grammar probabilities using the Expectation Maximization algorithm.",
                "#REF present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures.",
                "This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in #REF , but the specific bracketing of the parse tree provided.",
                "In this paper, we make a direct comparison of a syntactically unsupervised alignment model, based on #TARGET_REF , with a syntactically supervised model, based on #REF .",
                "We use the term syntactically supervised to indicate that the syntactic structure in one language is given to the training procedure."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "This restriction corresponds to intuitions about the alignments that could be produced by systematic differences between the two language's grammars, and allows for a polynomial time algorithm for finding the highest-probability alignment, and for re-estimation of the lexical translation and grammar probabilities using the Expectation Maximization algorithm. #REF present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures. This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in #REF , but the specific bracketing of the parse tree provided. In this paper, we make a direct comparison of a syntactically unsupervised alignment model, based on #TARGET_REF , with a syntactically supervised model, based on #REF . We use the term syntactically supervised to indicate that the syntactic structure in one language is given to the training procedure.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In this paper, we make a direct comparison of a syntactically unsupervised alignment model, based on #TARGET_REF , with a syntactically supervised model, based on #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Given a bilingual sentence pair, a synchronous parse can be built using a two-dimensional extension of chart parsing, where chart items are indexed by their nonterminal Y and beginning and ending positions l, m in the source language string, and beginning and ending positions i, j in the target language string.",
                "For Expectation Maximization training, we compute inside probabilities β(Y, l, m, i, j) from the bottom up as outlined below:",
                "A similar recursion is used to compute outside probabilities for each chart item, and the inside and outside probabilities are combined to derive expected counts for occurrence of each grammar rule, including the rules corresponding to individual lexical translations.",
                "In our experiments we use a grammar with a start symbol S, a single preterminal C, and two nonterminals A and B used to ensure that only one parse can generate any given word-level alignment (ignoring insertions and deletions) #TARGET_REF; #REF) .",
                "The individual lexical translations produced by the grammar may include a NULL word on either side, in order to represent insertions and deletions."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Given a bilingual sentence pair, a synchronous parse can be built using a two-dimensional extension of chart parsing, where chart items are indexed by their nonterminal Y and beginning and ending positions l, m in the source language string, and beginning and ending positions i, j in the target language string. For Expectation Maximization training, we compute inside probabilities β(Y, l, m, i, j) from the bottom up as outlined below: A similar recursion is used to compute outside probabilities for each chart item, and the inside and outside probabilities are combined to derive expected counts for occurrence of each grammar rule, including the rules corresponding to individual lexical translations. In our experiments we use a grammar with a start symbol S, a single preterminal C, and two nonterminals A and B used to ensure that only one parse can generate any given word-level alignment (ignoring insertions and deletions) #TARGET_REF; #REF) . The individual lexical translations produced by the grammar may include a NULL word on either side, in order to represent insertions and deletions.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In our experiments we use a grammar with a start symbol S, a single preterminal C, and two nonterminals A and B used to ensure that only one parse can generate any given word-level alignment (ignoring insertions and deletions) #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "They relied on high-dimensional vector representations to model the derived term (e.g., useful) as a result of a compositional process that combines the meanings of the base term (e.g., to use) and the affix (e.g., ful).",
                "For evaluation, they compared the predicted vector of the complex word with the original, corpus-based vector.",
                "More recently, #TARGET_REF put the task of modeling derivation into the perspective of zero-shot-learning: instead of using cosine similarities they predicted the derived term by learning a mapping function between the base term and the derived term.",
                "Once the predicted vector was computed, a nearest neighbor search was applied to validate if the prediction corresponded to the derived term.",
                "In zero-shotlearning the task is to predict novel values, i.e., values that were never seen in training."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "They relied on high-dimensional vector representations to model the derived term (e.g., useful) as a result of a compositional process that combines the meanings of the base term (e.g., to use) and the affix (e.g., ful). For evaluation, they compared the predicted vector of the complex word with the original, corpus-based vector. More recently, #TARGET_REF put the task of modeling derivation into the perspective of zero-shot-learning: instead of using cosine similarities they predicted the derived term by learning a mapping function between the base term and the derived term. Once the predicted vector was computed, a nearest neighbor search was applied to validate if the prediction corresponded to the derived term. In zero-shotlearning the task is to predict novel values, i.e., values that were never seen in training.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"More recently, #TARGET_REF put the task of modeling derivation into the perspective of zero-shot-learning: instead of using cosine similarities they predicted the derived term by learning a mapping function between the base term and the derived term.\"]}"
    },
    {
        "gold": {
            "text": [
                "It is often applied across vector spaces, such as different domains (#REF; .",
                "The experiments by #TARGET_REF were performed over six derivational patterns for German (cf.",
                "Table 1), including particle verbs (PVs) with two different particle prefixes (an and durch), which were particularly difficult to predict.",
                "PVs such as anfangen (to start) are compositions of a base verb (BV) such as fangen (to catch) and a verb particle such as an.",
                "Predicting PV meaning is challenging because German PVs are highly productive (#REFb; #REFa) , and the particles are notoriously ambiguous (Lechler and Roßdeutscher, 2009; #REF; #REF; #REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "It is often applied across vector spaces, such as different domains (#REF; . The experiments by #TARGET_REF were performed over six derivational patterns for German (cf. Table 1), including particle verbs (PVs) with two different particle prefixes (an and durch), which were particularly difficult to predict. PVs such as anfangen (to start) are compositions of a base verb (BV) such as fangen (to catch) and a verb particle such as an. Predicting PV meaning is challenging because German PVs are highly productive (#REFb; #REFa) , and the particles are notoriously ambiguous (Lechler and Roßdeutscher, 2009; #REF; #REF; #REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The experiments by #TARGET_REF were performed over six derivational patterns for German (cf.\"]}"
    },
    {
        "gold": {
            "text": [
                "AvgAdd is a re-implementation of the best method in #TARGET_REF :",
                "3 For each affix, the method learns a difference vector by computing the dimension-wise differences between the vector representations of base term A and derived term B .",
                "The method thus learns a centroid c for all relevant training pairs (N ) with the same affix:",
                "For each PV test instance with this affix, the learned centroid vector is added dimensionwise to the vector representation of the base term to predict a position for the derived term."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "AvgAdd is a re-implementation of the best method in #TARGET_REF : 3 For each affix, the method learns a difference vector by computing the dimension-wise differences between the vector representations of base term A and derived term B . The method thus learns a centroid c for all relevant training pairs (N ) with the same affix: For each PV test instance with this affix, the learned centroid vector is added dimensionwise to the vector representation of the base term to predict a position for the derived term.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"AvgAdd is a re-implementation of the best method in #TARGET_REF :\"]}"
    },
    {
        "gold": {
            "text": [
                "As in #TARGET_REF , we treat every derivation type as a specific learning problem: we take a set of word pairs with a particular derivation pattern (e.g., \"-in\", Bäcker::Bäckerin), and divide this set into training and test pairs by performing 10-fold cross-validation.",
                "For the test pairs, we predict the vectors of the derived terms (e.g.,",
                "The search space includes all corpus words across parts-of-speech, except for the base term.",
                "The performance is measured in terms of recall-out-of-5 (#REF), counting how often the correct derived term is found among the five nearest neighbors of the predicted vector."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "As in #TARGET_REF , we treat every derivation type as a specific learning problem: we take a set of word pairs with a particular derivation pattern (e.g., \"-in\", Bäcker::Bäckerin), and divide this set into training and test pairs by performing 10-fold cross-validation. For the test pairs, we predict the vectors of the derived terms (e.g., The search space includes all corpus words across parts-of-speech, except for the base term. The performance is measured in terms of recall-out-of-5 (#REF), counting how often the correct derived term is found among the five nearest neighbors of the predicted vector.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"As in #TARGET_REF , we treat every derivation type as a specific learning problem: we take a set of word pairs with a particular derivation pattern (e.g., \\\"-in\\\", B\\u00e4cker::B\\u00e4ckerin), and divide this set into training and test pairs by performing 10-fold cross-validation.\"]}"
    },
    {
        "gold": {
            "text": [
                "We created a new collection of German particle verb derivations 1 relying on the same resource as #TARGET_REF , the semiautomatic derivational lexicon for German DErivBase (#REF) .",
                "From DErivBase, we induced all pairs of base verbs and particle verbs across seven different particles.",
                "Nonexisting verbs were manually filtered out.",
                "In total, our collection contains 1 410 BV-PV combinations across seven particles, cf.",
                "Table 2 ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We created a new collection of German particle verb derivations 1 relying on the same resource as #TARGET_REF , the semiautomatic derivational lexicon for German DErivBase (#REF) . From DErivBase, we induced all pairs of base verbs and particle verbs across seven different particles. Nonexisting verbs were manually filtered out. In total, our collection contains 1 410 BV-PV combinations across seven particles, cf. Table 2 .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We created a new collection of German particle verb derivations 1 relying on the same resource as #TARGET_REF , the semiautomatic derivational lexicon for German DErivBase (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "A baseline method that simply guesses the derived term has a chance of approx.",
                "1 460 000 for German and 1 240 000 for English to predict the correct term.",
                "We thus apply a more informed baseline, the same as in #TARGET_REF , and predict the derived term at exactly the same position as the base term."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1
            ]
        },
        "input": "A baseline method that simply guesses the derived term has a chance of approx. 1 460 000 for German and 1 240 000 for English to predict the correct term. We thus apply a more informed baseline, the same as in #TARGET_REF , and predict the derived term at exactly the same position as the base term.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"We thus apply a more informed baseline, the same as in #TARGET_REF , and predict the derived term at exactly the same position as the base term.\"]}"
    },
    {
        "gold": {
            "text": [
                "Nonexisting verbs were manually filtered out.",
                "In total, our collection contains 1 410 BV-PV combinations across seven particles, cf.",
                "Table 2 .",
                "In addition, we apply our models to two existing collections for derivational patterns, the German dataset from #TARGET_REF , comprising six derivational patterns with 80 in-stances each (cf.",
                "Table 3 : English dataset (#REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Nonexisting verbs were manually filtered out. In total, our collection contains 1 410 BV-PV combinations across seven particles, cf. Table 2 . In addition, we apply our models to two existing collections for derivational patterns, the German dataset from #TARGET_REF , comprising six derivational patterns with 80 in-stances each (cf. Table 3 : English dataset (#REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In addition, we apply our models to two existing collections for derivational patterns, the German dataset from #TARGET_REF , comprising six derivational patterns with 80 in-stances each (cf.\"]}"
    },
    {
        "gold": {
            "text": [
                "BLEU (#REF ) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation.",
                "BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (#REF) , summarization (#REF) and text simplification (#REF; #REF; #TARGET_REF , i.e. the rewriting of a sentence as one or more simpler sentences.",
                "Along with the application of parallel corpora and MT techniques for TS (e.g., #REF; #REF; #REF) , BLEU became the main automatic metric for TS, despite its deficiencies (see §2).",
                "Indeed, focusing on lexical simplification, #REF argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used.",
                "In their experiments, BLEU failed to predict simplicity, but obtained a higher correlation with grammaticality and meaning preservation, relative to the SARI metric they proposed."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "BLEU (#REF ) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation. BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (#REF) , summarization (#REF) and text simplification (#REF; #REF; #TARGET_REF , i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., #REF; #REF; #REF) , BLEU became the main automatic metric for TS, despite its deficiencies (see §2). Indeed, focusing on lexical simplification, #REF argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used. In their experiments, BLEU failed to predict simplicity, but obtained a higher correlation with grammaticality and meaning preservation, relative to the SARI metric they proposed.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (#REF) , summarization (#REF) and text simplification (#REF; #REF; #TARGET_REF , i.e. the rewriting of a sentence as one or more simpler sentences.\"]}"
    },
    {
        "gold": {
            "text": [
                "BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (#REF) , summarization (#REF) and text simplification (#REF; #REF; #REF) , i.e. the rewriting of a sentence as one or more simpler sentences.",
                "Along with the application of parallel corpora and MT techniques for TS (e.g., #REF; #REF; #REF) , BLEU became the main automatic metric for TS, despite its deficiencies (see §2).",
                "Indeed, focusing on lexical simplification, #TARGET_REF argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used.",
                "In their experiments, BLEU failed to predict simplicity, but obtained a higher correlation with grammaticality and meaning preservation, relative to the SARI metric they proposed.",
                "In this paper, we further explore the applicability of BLEU for TS evaluation, examining BLEU's informativeness where sentence splitting is involved."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (#REF) , summarization (#REF) and text simplification (#REF; #REF; #REF) , i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., #REF; #REF; #REF) , BLEU became the main automatic metric for TS, despite its deficiencies (see §2). Indeed, focusing on lexical simplification, #TARGET_REF argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used. In their experiments, BLEU failed to predict simplicity, but obtained a higher correlation with grammaticality and meaning preservation, relative to the SARI metric they proposed. In this paper, we further explore the applicability of BLEU for TS evaluation, examining BLEU's informativeness where sentence splitting is involved.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Indeed, focusing on lexical simplification, #TARGET_REF argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used.\"]}"
    },
    {
        "gold": {
            "text": [
                "Metrics.",
                "In addition to BLEU, 7 we also experiment with (1) iBLEU (#REF) which was recently used for TS #TARGET_REF; and which takes into account the BLEU scores of the output against the input and against the references; (2) the Flesch-Kincaid Grade Level (FK; #REF ), computed at the system level, which estimates the readability of the text with a lower value indicating higher 4 Examples are taken from #REF .",
                "5 Examples are not provided in the case of Set 2 so as not to give an a-priori notion of simplicity.",
                "The complete guidelines are found in the supplementary material.",
                "6 Wilicoxon's signed rank test, p = 1.6 · 10 −5 for #Sents and p = 0.002 for SplitSents."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Metrics. In addition to BLEU, 7 we also experiment with (1) iBLEU (#REF) which was recently used for TS #TARGET_REF; and which takes into account the BLEU scores of the output against the input and against the references; (2) the Flesch-Kincaid Grade Level (FK; #REF ), computed at the system level, which estimates the readability of the text with a lower value indicating higher 4 Examples are taken from #REF . 5 Examples are not provided in the case of Set 2 so as not to give an a-priori notion of simplicity. The complete guidelines are found in the supplementary material. 6 Wilicoxon's signed rank test, p = 1.6 · 10 −5 for #Sents and p = 0.002 for SplitSents.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In addition to BLEU, 7 we also experiment with (1) iBLEU (#REF) which was recently used for TS #TARGET_REF; and which takes into account the BLEU scores of the output against the input and against the references; (2) the Flesch-Kincaid Grade Level (FK; #REF ), computed at the system level, which estimates the readability of the text with a lower value indicating higher 4 Examples are taken from #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "For exploring the effect of sentence splitting on BLEU scores, we compile a human-generated gold standard sentence splitting corpus -HSplit, which will also be useful for future studies of splitting in TS, and perform correlation analyses with human judgments.",
                "We consider two reference sets.",
                "First, we experiment with the most common set, proposed by #TARGET_REF , evaluating a variety of system outputs, as well as HSplit.",
                "The references in this setting explicitly emphasize lexical operations, and do not contain splitting or content deletion.",
                "2 Second, we experiment with HSplit as the reference set, evaluating systems that focus on sentence splitting."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "For exploring the effect of sentence splitting on BLEU scores, we compile a human-generated gold standard sentence splitting corpus -HSplit, which will also be useful for future studies of splitting in TS, and perform correlation analyses with human judgments. We consider two reference sets. First, we experiment with the most common set, proposed by #TARGET_REF , evaluating a variety of system outputs, as well as HSplit. The references in this setting explicitly emphasize lexical operations, and do not contain splitting or content deletion. 2 Second, we experiment with HSplit as the reference set, evaluating systems that focus on sentence splitting.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"First, we experiment with the most common set, proposed by #TARGET_REF , evaluating a variety of system outputs, as well as HSplit.\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to investigate the effect of correctly splitting sentences on the automatic metric scores, we build a parallel corpus, where each sentence is modified by 4 annotators, according to specific sentence splitting guidelines.",
                "We use the complex side of the test corpus of #TARGET_REF .",
                "3 While #REF recently proposed the semi-automatically compiled WEB-SPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split.",
                "This corpus enriches the set of references focused on lexical operations that were collected by #REF for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (#REF) .",
                "We use two sets of guidelines."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In order to investigate the effect of correctly splitting sentences on the automatic metric scores, we build a parallel corpus, where each sentence is modified by 4 annotators, according to specific sentence splitting guidelines. We use the complex side of the test corpus of #TARGET_REF . 3 While #REF recently proposed the semi-automatically compiled WEB-SPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split. This corpus enriches the set of references focused on lexical operations that were collected by #REF for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (#REF) . We use two sets of guidelines.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use the complex side of the test corpus of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Systems include six MT-based simplification systems, including outputs of the state-of-the-art neural TS system of #REF , in four variants: either default settings or initialization by word2vec, for each both the highest and the fourth ranked hypotheses in the beam are considered.",
                "10 We further include Moses (#REF) and SBMT-SARI #TARGET_REF , a syntax-based MT system tuned against SARI, and the identity function (outputs are same as inputs).",
                "The case which evaluates outputs with sentence splitting additionally includes the four HSplit corpora and the HSplit average scores.",
                "For \"HSplit as Reference Setting\", we consider the outputs of six simplification systems whose main simplification operation is sentence splitting: DSS, DSS m , SEMoses, SEMoses m , SEMoses LM and SEMoses m LM , taken from (#REFb) .",
                "Human Evaluation."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Systems include six MT-based simplification systems, including outputs of the state-of-the-art neural TS system of #REF , in four variants: either default settings or initialization by word2vec, for each both the highest and the fourth ranked hypotheses in the beam are considered. 10 We further include Moses (#REF) and SBMT-SARI #TARGET_REF , a syntax-based MT system tuned against SARI, and the identity function (outputs are same as inputs). The case which evaluates outputs with sentence splitting additionally includes the four HSplit corpora and the HSplit average scores. For \"HSplit as Reference Setting\", we consider the outputs of six simplification systems whose main simplification operation is sentence splitting: DSS, DSS m , SEMoses, SEMoses m , SEMoses LM and SEMoses m LM , taken from (#REFb) . Human Evaluation.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"10 We further include Moses (#REF) and SBMT-SARI #TARGET_REF , a syntax-based MT system tuned against SARI, and the identity function (outputs are same as inputs).\"]}"
    },
    {
        "gold": {
            "text": [
                "readability; 8 (3) SARI (#REF) , which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems.",
                "For completeness, we also experiment with the negative Levenshtein distance to the source (-LD SC ), which serves as a measure of conservatism.",
                "9 We explore two settings.",
                "In one (\"Standard Reference Setting\", §4.2), we use two sets of references: the Simple Wikipedia reference (yielding BLEU-1ref and iBLEU-1ref), and 8 references obtained by crowdsourcing by #TARGET_REF (yielding BLEU-8ref, iBLEU-8ref and SARI-8ref).",
                "In the other (\"HSplit as Reference Setting\", §4.3), we use HSplit as the reference set."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "readability; 8 (3) SARI (#REF) , which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems. For completeness, we also experiment with the negative Levenshtein distance to the source (-LD SC ), which serves as a measure of conservatism. 9 We explore two settings. In one (\"Standard Reference Setting\", §4.2), we use two sets of references: the Simple Wikipedia reference (yielding BLEU-1ref and iBLEU-1ref), and 8 references obtained by crowdsourcing by #TARGET_REF (yielding BLEU-8ref, iBLEU-8ref and SARI-8ref). In the other (\"HSplit as Reference Setting\", §4.3), we use HSplit as the reference set.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In one (\\\"Standard Reference Setting\\\", \\u00a74.2), we use two sets of references: the Simple Wikipedia reference (yielding BLEU-1ref and iBLEU-1ref), and 8 references obtained by crowdsourcing by #TARGET_REF (yielding BLEU-8ref, iBLEU-8ref and SARI-8ref).\"]}"
    },
    {
        "gold": {
            "text": [
                "The case which evaluates outputs with sentence splitting additionally includes the four HSplit corpora and the HSplit average scores.",
                "For \"HSplit as Reference Setting\", we consider the outputs of six simplification systems whose main simplification operation is sentence splitting: DSS, DSS m , SEMoses, SEMoses m , SEMoses LM and SEMoses m LM , taken from (#REFb) .",
                "Human Evaluation.",
                "We use the evaluation benchmark provided by Sulem et al. (2018b) , 11 including system outputs and human evaluation scores corresponding to the first 70 sentences of the test corpus of #TARGET_REF , and extend it to apply to HSplit as well.",
                "The evaluation of HSplit is carried out by 3 in-house native English annotators, who rated the different input-output pairs for the different systems according to 4 parameters: Grammaticality (G), Meaning preservation (M), Simplicity (S) and Structural Simplicity (StS)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The case which evaluates outputs with sentence splitting additionally includes the four HSplit corpora and the HSplit average scores. For \"HSplit as Reference Setting\", we consider the outputs of six simplification systems whose main simplification operation is sentence splitting: DSS, DSS m , SEMoses, SEMoses m , SEMoses LM and SEMoses m LM , taken from (#REFb) . Human Evaluation. We use the evaluation benchmark provided by Sulem et al. (2018b) , 11 including system outputs and human evaluation scores corresponding to the first 70 sentences of the test corpus of #TARGET_REF , and extend it to apply to HSplit as well. The evaluation of HSplit is carried out by 3 in-house native English annotators, who rated the different input-output pairs for the different systems according to 4 parameters: Grammaticality (G), Meaning preservation (M), Simplicity (S) and Structural Simplicity (StS).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use the evaluation benchmark provided by Sulem et al. (2018b) , 11 including system outputs and human evaluation scores corresponding to the first 70 sentences of the test corpus of #TARGET_REF , and extend it to apply to HSplit as well.\"]}"
    },
    {
        "gold": {
            "text": [
                "While BLEU is standardly used for TS evaluation (e.g., #TARGET_REF; #REF; #REF; #REF ), only few works tested its correlation with human judgments.",
                "Using 20 source sentences from the PWKP test corpus (#REF) with 5 simplified sentences for each of them, #REF reported positive correlation of BLEU with simplicity ratings, but no correlation with adequacy.",
                "T-BLEU (Štajner et al., 2014), a variant of BLEU which uses lower n-grams when no overlapping 4-grams are found, was tested on outputs that applied only structural modifications to the source.",
                "It was found to have moderate positive correlation for meaning preservation, and positive but low correlation for grammaticality.",
                "Correlation with simplicity was not considered in this experiment."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "While BLEU is standardly used for TS evaluation (e.g., #TARGET_REF; #REF; #REF; #REF ), only few works tested its correlation with human judgments. Using 20 source sentences from the PWKP test corpus (#REF) with 5 simplified sentences for each of them, #REF reported positive correlation of BLEU with simplicity ratings, but no correlation with adequacy. T-BLEU (Štajner et al., 2014), a variant of BLEU which uses lower n-grams when no overlapping 4-grams are found, was tested on outputs that applied only structural modifications to the source. It was found to have moderate positive correlation for meaning preservation, and positive but low correlation for grammaticality. Correlation with simplicity was not considered in this experiment.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"While BLEU is standardly used for TS evaluation (e.g., #TARGET_REF; #REF; #REF; #REF ), only few works tested its correlation with human judgments.\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to investigate the effect of correctly splitting sentences on the automatic metric scores, we build a parallel corpus, where each sentence is modified by 4 annotators, according to specific sentence splitting guidelines.",
                "We use the complex side of the test corpus of #REF .",
                "3 While #REF recently proposed the semi-automatically compiled WEB-SPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split.",
                "This corpus enriches the set of references focused on lexical operations that were collected by #TARGET_REF for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (#REF) .",
                "We use two sets of guidelines."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "In order to investigate the effect of correctly splitting sentences on the automatic metric scores, we build a parallel corpus, where each sentence is modified by 4 annotators, according to specific sentence splitting guidelines. We use the complex side of the test corpus of #REF . 3 While #REF recently proposed the semi-automatically compiled WEB-SPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split. This corpus enriches the set of references focused on lexical operations that were collected by #TARGET_REF for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (#REF) . We use two sets of guidelines.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"3 While #REF recently proposed the semi-automatically compiled WEB-SPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split.\", \"This corpus enriches the set of references focused on lexical operations that were collected by #TARGET_REF for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "BLEU-8ref obtains 94.63 for the input and 73.03 for HSplit.",
                "12 The high scores obtained for Identity, also observed by #TARGET_REF , indicate that BLEU is a not a good predictor for relative simplicity to the input.",
                "The drop in the BLEU scores for HSplit is not reflected by the human evaluation scores for grammaticality (4.43 for AvgHSplit vs. 4.80 for Identity) and meaning preservation (4.70 vs. 5.00), where the decrease between Identity and HSplit is much more limited.",
                "For examining these tendencies in more detail, we compute the correlations between the au-tomatic metrics and the human evaluation scores.",
                "They are described in the following paragraph."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "BLEU-8ref obtains 94.63 for the input and 73.03 for HSplit. 12 The high scores obtained for Identity, also observed by #TARGET_REF , indicate that BLEU is a not a good predictor for relative simplicity to the input. The drop in the BLEU scores for HSplit is not reflected by the human evaluation scores for grammaticality (4.43 for AvgHSplit vs. 4.80 for Identity) and meaning preservation (4.70 vs. 5.00), where the decrease between Identity and HSplit is much more limited. For examining these tendencies in more detail, we compute the correlations between the au-tomatic metrics and the human evaluation scores. They are described in the following paragraph.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"12 The high scores obtained for Identity, also observed by #TARGET_REF , indicate that BLEU is a not a good predictor for relative simplicity to the input.\"]}"
    },
    {
        "gold": {
            "text": [
                "Besides, it constitutes the first stage of many NLP pipelines.",
                "Before applying tools trained on specific languages, one must determine the language of the text.",
                "It has attracted considerable attention in recent years [1, 2, #TARGET_REF 4, 5, 6, 7, 8] .",
                "Most of the existing approaches take words as features, and then adopt effective supervised classification algorithms to solve the problem.",
                "Generally speaking, language identification between different languages is a task that can be solved at a high accuracy."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Besides, it constitutes the first stage of many NLP pipelines. Before applying tools trained on specific languages, one must determine the language of the text. It has attracted considerable attention in recent years [1, 2, #TARGET_REF 4, 5, 6, 7, 8] . Most of the existing approaches take words as features, and then adopt effective supervised classification algorithms to solve the problem. Generally speaking, language identification between different languages is a task that can be solved at a high accuracy.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"It has attracted considerable attention in recent years [1, 2, #TARGET_REF 4, 5, 6, 7, 8] .\"]}"
    },
    {
        "gold": {
            "text": [
                "To deal with this problem, Huang and Lee #TARGET_REF proposed a contrastive approach based on documentlevel top-bag-of-word similarity to reflect distances among the three varieties of Mandarin in China, Taiwan and Singapore, which is a kind of word-level uni-gram feature.",
                "The word unigram feature is sufficient for document-level identification of language variants.",
                "More recent studies focus on sentence-level languages identification, such as the Discriminating between Similar Languages (DSL) shared task 2014 and 2015 [7, 8] .",
                "The best system of these shard tasks shows that the uni-gram is an effective feature.",
                "For the sentence-level language identification, you are given a single sentence, and you need to identify the language."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "To deal with this problem, Huang and Lee #TARGET_REF proposed a contrastive approach based on documentlevel top-bag-of-word similarity to reflect distances among the three varieties of Mandarin in China, Taiwan and Singapore, which is a kind of word-level uni-gram feature. The word unigram feature is sufficient for document-level identification of language variants. More recent studies focus on sentence-level languages identification, such as the Discriminating between Similar Languages (DSL) shared task 2014 and 2015 [7, 8] . The best system of these shard tasks shows that the uni-gram is an effective feature. For the sentence-level language identification, you are given a single sentence, and you need to identify the language.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"To deal with this problem, Huang and Lee #TARGET_REF proposed a contrastive approach based on documentlevel top-bag-of-word similarity to reflect distances among the three varieties of Mandarin in China, Taiwan and Singapore, which is a kind of word-level uni-gram feature.\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, #REF focused on Indian languages identification.",
                "Meanwhile, #REF proposed features based on frequencies of character n-grams to identify Malay and Indonesian.",
                "Huang and Lee #TARGET_REF presented the top-bag-of-word similarity based contrastive approach to reflect distances among the three varieties of Mandarin in Mainland China, Taiwan and Singapore.",
                "#REF found that word uni-grams gave very similar performance to character n-gram features in the framework of the probabilistic language model for the Brazilian and European Portuguese language discrimination.",
                "#REF ; #REF showed that the Naïve Bayes classifier with uni-grams achieved high accuracy for the South Slavic languages identification."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "For example, #REF focused on Indian languages identification. Meanwhile, #REF proposed features based on frequencies of character n-grams to identify Malay and Indonesian. Huang and Lee #TARGET_REF presented the top-bag-of-word similarity based contrastive approach to reflect distances among the three varieties of Mandarin in Mainland China, Taiwan and Singapore. #REF found that word uni-grams gave very similar performance to character n-gram features in the framework of the probabilistic language model for the Brazilian and European Portuguese language discrimination. #REF ; #REF showed that the Naïve Bayes classifier with uni-grams achieved high accuracy for the South Slavic languages identification.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Huang and Lee #TARGET_REF presented the top-bag-of-word similarity based contrastive approach to reflect distances among the three varieties of Mandarin in Mainland China, Taiwan and Singapore.\"]}"
    },
    {
        "gold": {
            "text": [
                "In fact, the word alignment-based dictionary can extract both fine-grained representative words and coarse-grained words simultaneously.",
                "The above observation indicates that character form, PMI-based and word alignment-based information are useful information to discriminate dialects in the GCR.",
                "In order to investigate the detailed characteristics of different dialects of Mandarin Chinese, we extend #TARGET_REF dialects in Huang and Lee #TARGET_REF to 6 dialects.",
                "In fact, the more dialects there are, the more difficult the dialects discrimination becomes.",
                "It also has been verified through our experiments."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In fact, the word alignment-based dictionary can extract both fine-grained representative words and coarse-grained words simultaneously. The above observation indicates that character form, PMI-based and word alignment-based information are useful information to discriminate dialects in the GCR. In order to investigate the detailed characteristics of different dialects of Mandarin Chinese, we extend #TARGET_REF dialects in Huang and Lee #TARGET_REF to 6 dialects. In fact, the more dialects there are, the more difficult the dialects discrimination becomes. It also has been verified through our experiments.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"In order to investigate the detailed characteristics of different dialects of Mandarin Chinese, we extend #TARGET_REF dialects in Huang and Lee #TARGET_REF to 6 dialects.\"]}"
    },
    {
        "gold": {
            "text": [
                "Among the above related works, study [3] is the most related work to ours.",
                "The differences between study [3] and our work are two-fold:",
                "(1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore.",
                "In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in Huang and Lee #TARGET_REF to 6 dialects.",
                "Also, the more dialects there are, the more difficult the dialects discrimination becomes."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Among the above related works, study [3] is the most related work to ours. The differences between study [3] and our work are two-fold: (1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore. In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in Huang and Lee #TARGET_REF to 6 dialects. Also, the more dialects there are, the more difficult the dialects discrimination becomes.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in Huang and Lee #TARGET_REF to 6 dialects.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF ; #REF found that bag-of-words features outperformed the syntax or character sequencesbased features for the English varieties.",
                "Besides these works, other recent studies include: Spanish varieties identification [13] , Arabic varieties discrimination [14, 15, 16, 17] , and Persian and Dari identification [18] .",
                "Among the above related works, study #TARGET_REF is the most related work to ours.",
                "The differences between study [3] and our work are two-fold:",
                "(1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "#REF ; #REF found that bag-of-words features outperformed the syntax or character sequencesbased features for the English varieties. Besides these works, other recent studies include: Spanish varieties identification [13] , Arabic varieties discrimination [14, 15, 16, 17] , and Persian and Dari identification [18] . Among the above related works, study #TARGET_REF is the most related work to ours. The differences between study [3] and our work are two-fold: (1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Among the above related works, study #TARGET_REF is the most related work to ours.\"]}"
    },
    {
        "gold": {
            "text": [
                "Similar to Huang and Lee #TARGET_REF 's work, in order to eliminate the trivial issue of character encoding (simplified and traditional character), we convert Taiwan and Hong Kong texts to the same simplified character set using Zhconvertor 6 utility to focus on actual linguistic and textual features.",
                "Table 6 shows the experimental results for the dialect identification in the GCR.",
                "As shown, again, the bi-gram features work better than both uni-gram and tri-gram features on Wikipedia dataset.",
                "Also, the word alignment-based features can contribute about 3.32% performance improvement.",
                "This also confirms our intuition that the word alignment-based information is helpful to discriminate dialects in the GCR, which shows the effectiveness of both fine-grained and coarsegrained characteristic of word alignment based features."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Similar to Huang and Lee #TARGET_REF 's work, in order to eliminate the trivial issue of character encoding (simplified and traditional character), we convert Taiwan and Hong Kong texts to the same simplified character set using Zhconvertor 6 utility to focus on actual linguistic and textual features. Table 6 shows the experimental results for the dialect identification in the GCR. As shown, again, the bi-gram features work better than both uni-gram and tri-gram features on Wikipedia dataset. Also, the word alignment-based features can contribute about 3.32% performance improvement. This also confirms our intuition that the word alignment-based information is helpful to discriminate dialects in the GCR, which shows the effectiveness of both fine-grained and coarsegrained characteristic of word alignment based features.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Similar to Huang and Lee #TARGET_REF 's work, in order to eliminate the trivial issue of character encoding (simplified and traditional character), we convert Taiwan and Hong Kong texts to the same simplified character set using Zhconvertor 6 utility to focus on actual linguistic and textual features.\"]}"
    },
    {
        "gold": {
            "text": [
                "Besides these works, other recent studies include: Spanish varieties identification [13] , Arabic varieties discrimination [14, 15, 16, 17] , and Persian and Dari identification [18] .",
                "Among the above related works, study [3] is the most related work to ours.",
                "The differences between study #TARGET_REF and our work are two-fold:",
                "(1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore.",
                "In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in #REF to 6 dialects."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Besides these works, other recent studies include: Spanish varieties identification [13] , Arabic varieties discrimination [14, 15, 16, 17] , and Persian and Dari identification [18] . Among the above related works, study [3] is the most related work to ours. The differences between study #TARGET_REF and our work are two-fold: (1)They focus on document-level varieties of Mandarin in China, Taiwan and Singapore, while we deal with sentence-level varieties of Mandarin in China, Hong Kong, Taiwan, Macao, Malaysia and Singapore. In order to investigate the detailed characteristic of different dialects of Mandarin Chinese, we extend dialects in #REF to 6 dialects.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The differences between study #TARGET_REF and our work are two-fold:\"]}"
    },
    {
        "gold": {
            "text": [
                "According to the related works [4, 5, 6] , word uni-grams are effective features for discriminating general languages.",
                "Compared with English, no space exists between words in Chinese sentence.",
                "Therefore, we use character uni-grams, bi-grams and tri-grams as features.",
                "However, Huang and Lee #TARGET_REF did not use character-level n-grams."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                1
            ]
        },
        "input": "According to the related works [4, 5, 6] , word uni-grams are effective features for discriminating general languages. Compared with English, no space exists between words in Chinese sentence. Therefore, we use character uni-grams, bi-grams and tri-grams as features. However, Huang and Lee #TARGET_REF did not use character-level n-grams.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Therefore, we use character uni-grams, bi-grams and tri-grams as features.\", \"However, Huang and Lee #TARGET_REF did not use character-level n-grams.\"]}"
    },
    {
        "gold": {
            "text": [
                "If we use a single type of feature, we can see that the uni-gram feature (baseline system 2) is not the best one for Chinese dialect detection in the GCR, although it has been found effective for English detection in previous studies in the DSL shared task.",
                "Instead, bi-gram and word segmentation based features are better than uni-gram one.",
                "Both of the proposed bi-gram and word segmentation based features significantly outperforms the baseline systems with p<0.01 using paired t-test for significance.",
                "Also the bi-gram and word segmentation based features are better than the Huang and Lee #TARGET_REF 's method (baseline system 1) for 6-way, #TARGET_REF-way and 2-way dialect identification in the GCR.",
                "Obviously, the random method does not work for the GCR dialect identification."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "If we use a single type of feature, we can see that the uni-gram feature (baseline system 2) is not the best one for Chinese dialect detection in the GCR, although it has been found effective for English detection in previous studies in the DSL shared task. Instead, bi-gram and word segmentation based features are better than uni-gram one. Both of the proposed bi-gram and word segmentation based features significantly outperforms the baseline systems with p<0.01 using paired t-test for significance. Also the bi-gram and word segmentation based features are better than the Huang and Lee #TARGET_REF 's method (baseline system 1) for 6-way, #TARGET_REF-way and 2-way dialect identification in the GCR. Obviously, the random method does not work for the GCR dialect identification.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Also the bi-gram and word segmentation based features are better than the Huang and Lee #TARGET_REF 's method (baseline system 1) for 6-way, #TARGET_REF-way and 2-way dialect identification in the GCR.\"]}"
    },
    {
        "gold": {
            "text": [
                "(1) 6-way detection: The dialects of Mainland China, Hong Kong, Taiwan, Macao, Malaysia and Singapore are all considered; (2) 3-way detection: We detect dialects of Mainland China, Taiwan and Singapore as in #REF ; (3) 2-way detection: We try to distinguish between two groups of dialects, the ones used in Mainland China, Malaysia and Singapore using simplified characters, and the ones used in Hong Kong, Taiwan and Macao using traditional characters.",
                "For the Wikipedia dataset, we also generate two similar scenarios:",
                "(1) 3-way detection: We detect dialects of Mainland China, Hong Kong and Taiwan; (2) 2-way detection: We try to distinguish between two groups of dialects, the ones used in Mainland China using simplified characters, and the ones used in Hong Kong and Taiwan using traditional characters.",
                "Baseline system 1: As mentioned in Section 2, we take the Huang and Lee #TARGET_REF 's top-bag-of-word similarity-based approach as one of our baseline system.",
                "We re-implement their method in this paper using the similar 3-way news dataset."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "(1) 6-way detection: The dialects of Mainland China, Hong Kong, Taiwan, Macao, Malaysia and Singapore are all considered; (2) 3-way detection: We detect dialects of Mainland China, Taiwan and Singapore as in #REF ; (3) 2-way detection: We try to distinguish between two groups of dialects, the ones used in Mainland China, Malaysia and Singapore using simplified characters, and the ones used in Hong Kong, Taiwan and Macao using traditional characters. For the Wikipedia dataset, we also generate two similar scenarios: (1) 3-way detection: We detect dialects of Mainland China, Hong Kong and Taiwan; (2) 2-way detection: We try to distinguish between two groups of dialects, the ones used in Mainland China using simplified characters, and the ones used in Hong Kong and Taiwan using traditional characters. Baseline system 1: As mentioned in Section 2, we take the Huang and Lee #TARGET_REF 's top-bag-of-word similarity-based approach as one of our baseline system. We re-implement their method in this paper using the similar 3-way news dataset.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Baseline system 1: As mentioned in Section 2, we take the Huang and Lee #TARGET_REF 's top-bag-of-word similarity-based approach as one of our baseline system.\"]}"
    },
    {
        "gold": {
            "text": [
                "There are several works for WSD that do not depend on a sense tagged corpus, and they can be classified into three approaches according to main resources used: raw corpus based approach [2] , dictionary based approach [3, 4] and hierarchical lexical database approach.",
                "The hierarchical lexical database approach can be reclassified into three groups according to usages of the database: gloss based method [5] , conceptual density based method [6, 7] and relative based method #TARGET_REF 9, 10] .",
                "Since our method is a kind of the relative based method, this section describes the related works of the relative based method.",
                "[8] introduced the relative based method using International Roget's Thesaurus as a hierarchical lexical database.",
                "His method is conducted as follows: 1) Get relatives of each sense of a target word from the Roget's Thesaurus."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "There are several works for WSD that do not depend on a sense tagged corpus, and they can be classified into three approaches according to main resources used: raw corpus based approach [2] , dictionary based approach [3, 4] and hierarchical lexical database approach. The hierarchical lexical database approach can be reclassified into three groups according to usages of the database: gloss based method [5] , conceptual density based method [6, 7] and relative based method #TARGET_REF 9, 10] . Since our method is a kind of the relative based method, this section describes the related works of the relative based method. [8] introduced the relative based method using International Roget's Thesaurus as a hierarchical lexical database. His method is conducted as follows: 1) Get relatives of each sense of a target word from the Roget's Thesaurus.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The hierarchical lexical database approach can be reclassified into three groups according to usages of the database: gloss based method [5] , conceptual density based method [6, 7] and relative based method #TARGET_REF 9, 10] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, most of the example sentences of rail are not helpful for WSD of crane.",
                "His method has another problem in disambiguating senses of a large number of target words because it requires a great amount of time and storage space to collect example sentences of relatives of the target words.",
                "[9] followed the method of #TARGET_REF , but tried to resolve the ambiguous relative problem by using just unambiguous relatives.",
                "That is, the ambiguous relative rail is not utilized to build a training data of the word crane because the word rail is ambiguous.",
                "Another difference from [8] is on a lexical database: they utilized WordNet as a lexical database for acquiring relatives of target words instead of International Roget's Thesaurus."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Therefore, most of the example sentences of rail are not helpful for WSD of crane. His method has another problem in disambiguating senses of a large number of target words because it requires a great amount of time and storage space to collect example sentences of relatives of the target words. [9] followed the method of #TARGET_REF , but tried to resolve the ambiguous relative problem by using just unambiguous relatives. That is, the ambiguous relative rail is not utilized to build a training data of the word crane because the word rail is ambiguous. Another difference from [8] is on a lexical database: they utilized WordNet as a lexical database for acquiring relatives of target words instead of International Roget's Thesaurus.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"[9] followed the method of #TARGET_REF , but tried to resolve the ambiguous relative problem by using just unambiguous relatives.\"]}"
    },
    {
        "gold": {
            "text": [
                "That is, the ambiguous relative rail is not utilized to build a training data of the word crane because the word rail is ambiguous.",
                "Another difference from #TARGET_REF is on a lexical database: they utilized WordNet as a lexical database for acquiring relatives of target words instead of International Roget's Thesaurus.",
                "Since WordNet is freely available for research, various kinds of WSD studies based on WordNet can be compared with the method of [9] .",
                "They evaluated their method on 14 ambiguous nouns and achieved a good performance comparable to the methods based on the sense tagged corpus.",
                "However, the evaluation was conducted on a small part of senses of the target words like [8] ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "That is, the ambiguous relative rail is not utilized to build a training data of the word crane because the word rail is ambiguous. Another difference from #TARGET_REF is on a lexical database: they utilized WordNet as a lexical database for acquiring relatives of target words instead of International Roget's Thesaurus. Since WordNet is freely available for research, various kinds of WSD studies based on WordNet can be compared with the method of [9] . They evaluated their method on 14 ambiguous nouns and achieved a good performance comparable to the methods based on the sense tagged corpus. However, the evaluation was conducted on a small part of senses of the target words like [8] .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Another difference from #TARGET_REF is on a lexical database: they utilized WordNet as a lexical database for acquiring relatives of target words instead of International Roget's Thesaurus.\"]}"
    },
    {
        "gold": {
            "text": [
                "They evaluated their method on 14 ambiguous nouns and achieved a good performance comparable to the methods based on the sense tagged corpus.",
                "However, the evaluation was conducted on a small part of senses of the target words like #TARGET_REF .",
                "However, many senses in WordNet do not have unambiguous relatives through relationships such as synonyms, direct hypernyms, and direct hyponyms.",
                "2 A possible alternative is to use the unambiguous relatives in the long distance from a target word, but the way is still problematic because the longer the distance of two senses is, the weaker the relationship between them is.",
                "In other words, the unambiguous relatives in the long distance may provide irrelevant examples for WSD like ambiguous relatives."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "They evaluated their method on 14 ambiguous nouns and achieved a good performance comparable to the methods based on the sense tagged corpus. However, the evaluation was conducted on a small part of senses of the target words like #TARGET_REF . However, many senses in WordNet do not have unambiguous relatives through relationships such as synonyms, direct hypernyms, and direct hyponyms. 2 A possible alternative is to use the unambiguous relatives in the long distance from a target word, but the way is still problematic because the longer the distance of two senses is, the weaker the relationship between them is. In other words, the unambiguous relatives in the long distance may provide irrelevant examples for WSD like ambiguous relatives.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"However, the evaluation was conducted on a small part of senses of the target words like #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In other words, the unambiguous relatives in the long distance may provide irrelevant examples for WSD like ambiguous relatives.",
                "Hence, the method has difficulties in disambiguating senses of words that do not have unambiguous relatives near the target words in the WordNet.",
                "The problem becomes more serious when verbs, which most of the relatives are ambiguous, are disambiguated.",
                "Like #TARGET_REF , the method also has a difficulty in disambiguating senses of many words because the method collects the example sentences of relatives of many words.",
                "[10] reimplemented the method of [9] using a web, which may be a very large corpus, in order to collect example sentences."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "In other words, the unambiguous relatives in the long distance may provide irrelevant examples for WSD like ambiguous relatives. Hence, the method has difficulties in disambiguating senses of words that do not have unambiguous relatives near the target words in the WordNet. The problem becomes more serious when verbs, which most of the relatives are ambiguous, are disambiguated. Like #TARGET_REF , the method also has a difficulty in disambiguating senses of many words because the method collects the example sentences of relatives of many words. [10] reimplemented the method of [9] using a web, which may be a very large corpus, in order to collect example sentences.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Like #TARGET_REF , the method also has a difficulty in disambiguating senses of many words because the method collects the example sentences of relatives of many words.\"]}"
    },
    {
        "gold": {
            "text": [
                "Hence, the method has difficulties in disambiguating senses of words that do not have unambiguous relatives near the target words in the WordNet.",
                "The problem becomes more serious when verbs, which most of the relatives are ambiguous, are disambiguated.",
                "#REF , the method also has a difficulty in disambiguating senses of many words because the method collects the example sentences of relatives of many words.",
                "[10] reimplemented the method of [9] using a web, which may be a very large corpus, in order to collect example sentences.",
                "They built training datum of all noun words in WordNet whose size is larger than 7GB, but evaluated their method on a small number of nouns of lexical sample task of SENSEVAL-2 as #TARGET_REF and [9] ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Hence, the method has difficulties in disambiguating senses of words that do not have unambiguous relatives near the target words in the WordNet. The problem becomes more serious when verbs, which most of the relatives are ambiguous, are disambiguated. #REF , the method also has a difficulty in disambiguating senses of many words because the method collects the example sentences of relatives of many words. [10] reimplemented the method of [9] using a web, which may be a very large corpus, in order to collect example sentences. They built training datum of all noun words in WordNet whose size is larger than 7GB, but evaluated their method on a small number of nouns of lexical sample task of SENSEVAL-2 as #TARGET_REF and [9] .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"They built training datum of all noun words in WordNet whose size is larger than 7GB, but evaluated their method on a small number of nouns of lexical sample task of SENSEVAL-2 as #TARGET_REF and [9] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Comparison with Other Relative Based Methods.",
                "We tried to compare our proposed method with the previous relative based methods.",
                "However, both of #TARGET_REF and [9] did not evaluate their methods on a publicly available data.",
                "We implemented their methods and compared our method with them on the same evaluation data.",
                "When both of the methods are implemented, it is practically difficult to collect example sentences of all target words in the evaluation data."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Comparison with Other Relative Based Methods. We tried to compare our proposed method with the previous relative based methods. However, both of #TARGET_REF and [9] did not evaluate their methods on a publicly available data. We implemented their methods and compared our method with them on the same evaluation data. When both of the methods are implemented, it is practically difficult to collect example sentences of all target words in the evaluation data.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"However, both of #TARGET_REF and [9] did not evaluate their methods on a publicly available data.\"]}"
    },
    {
        "gold": {
            "text": [
                "Since the training data are built by collecting example sentences of relatives in the previous works, the frequencies in Eq. 12 and 13 are calculated with our matrix as follows:",
                "where r l is a relative related to the sense s ij .",
                "f req(r l , w k ) and f req(r l ) are the co-occurrence frequency between r l and w k and the frequency of r l , respectively, and both frequencies can be obtained by looking up the matrix since the matrix contains the frequencies of words and word pairs.",
                "The main difference between #TARGET_REF and [9] is whether ambiguous relatives are utilized or not.",
                "Considering the difference, we implemented the method of [8] to include the ambiguous relatives into relatives, but the method of [9] to exclude the ambiguous relatives."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Since the training data are built by collecting example sentences of relatives in the previous works, the frequencies in Eq. 12 and 13 are calculated with our matrix as follows: where r l is a relative related to the sense s ij . f req(r l , w k ) and f req(r l ) are the co-occurrence frequency between r l and w k and the frequency of r l , respectively, and both frequencies can be obtained by looking up the matrix since the matrix contains the frequencies of words and word pairs. The main difference between #TARGET_REF and [9] is whether ambiguous relatives are utilized or not. Considering the difference, we implemented the method of [8] to include the ambiguous relatives into relatives, but the method of [9] to exclude the ambiguous relatives.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The main difference between #TARGET_REF and [9] is whether ambiguous relatives are utilized or not.\"]}"
    },
    {
        "gold": {
            "text": [
                "Chatbots aim to engage users in open-domain human-computer conversations and are currently receiving increasing attention.",
                "The existing work on building chatbots includes generation-based methods and retrieval-based methods.",
                "The first type of methods synthesize a response with a natural language generation model (#REF; #REF; .",
                "In this paper, we focus on the second type and study the problem of multi-turn response selection.",
                "This task aims to select the best-matched response from a set of candidates, given the context of a conversation which is composed of multiple utterances #TARGET_REF; #REF; #REF )."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                1,
                0,
                1,
                1
            ]
        },
        "input": "Chatbots aim to engage users in open-domain human-computer conversations and are currently receiving increasing attention. The existing work on building chatbots includes generation-based methods and retrieval-based methods. The first type of methods synthesize a response with a natural language generation model (#REF; #REF; . In this paper, we focus on the second type and study the problem of multi-turn response selection. This task aims to select the best-matched response from a set of candidates, given the context of a conversation which is composed of multiple utterances #TARGET_REF; #REF; #REF ).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Chatbots aim to engage users in open-domain human-computer conversations and are currently receiving increasing attention.\", \"The existing work on building chatbots includes generation-based methods and retrieval-based methods.\", \"In this paper, we focus on the second type and study the problem of multi-turn response selection.\", \"This task aims to select the best-matched response from a set of candidates, given the context of a conversation which is composed of multiple utterances #TARGET_REF; #REF; #REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently, some extended work has been made to incorporate external knowledge into generation with specific personas or emotions (#REF; #REFa) .",
                "Our work belongs to the retrieval-based methods, which learn a matching model for a pair of a conversational context and a response candidate.",
                "This approach has the advantage of providing informative and fluent responses because they select a proper response for the current conversation from a repository by means of response selection algorithms #TARGET_REF; #REF EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EB EB EB EB EB E0 E1 E2 E3 E4 E5 E18 E19 E6 E7 E8 E9 E10 E11 E17 E12 E13 E14 E15 E16 E20 E21 E22 E23 E24 E25 [ E0 E0 E0 E0 E0 E0 E0 E0 E1 E1 E1 E1 E1 E1 E0 E1 E1 E1 E1 E1 E0 E1 E1 E1 E1 E1 Speaker Embeddings + + + + + + + + + + + + + + + + + + + + + + + + + + Figure 1 : The input representation of SA-BERT.",
                "The final input embeddings are the sum of the token embeddings, the segmentation embeddings, the position embeddings and the speaker embeddings.",
                "#REF; #REFb) ."
            ],
            "label": [
                "BACK",
                "SIM"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Recently, some extended work has been made to incorporate external knowledge into generation with specific personas or emotions (#REF; #REFa) . Our work belongs to the retrieval-based methods, which learn a matching model for a pair of a conversational context and a response candidate. This approach has the advantage of providing informative and fluent responses because they select a proper response for the current conversation from a repository by means of response selection algorithms #TARGET_REF; #REF EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EB EB EB EB EB E0 E1 E2 E3 E4 E5 E18 E19 E6 E7 E8 E9 E10 E11 E17 E12 E13 E14 E15 E16 E20 E21 E22 E23 E24 E25 [ E0 E0 E0 E0 E0 E0 E0 E0 E1 E1 E1 E1 E1 E1 E0 E1 E1 E1 E1 E1 E0 E1 E1 E1 E1 E1 Speaker Embeddings + + + + + + + + + + + + + + + + + + + + + + + + + + Figure 1 : The input representation of SA-BERT. The final input embeddings are the sum of the token embeddings, the segmentation embeddings, the position embeddings and the speaker embeddings. #REF; #REFb) .",
        "output": "{\"label\": [\"BACK\", \"SIM\"], \"context\": [\"Our work belongs to the retrieval-based methods, which learn a matching model for a pair of a conversational context and a response candidate.\", \"This approach has the advantage of providing informative and fluent responses because they select a proper response for the current conversation from a repository by means of response selection algorithms #TARGET_REF; #REF EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EA EB EB EB EB EB E0 E1 E2 E3 E4 E5 E18 E19 E6 E7 E8 E9 E10 E11 E17 E12 E13 E14 E15 E16 E20 E21 E22 E23 E24 E25 [ E0 E0 E0 E0 E0 E0 E0 E0 E1 E1 E1 E1 E1 E1 E0 E1 E1 E1 E1 E1 E0 E1 E1 E1 E1 E1 Speaker Embeddings + + + + + + + + + + + + + + + + + + + + + + + + + + Figure 1 : The input representation of SA-BERT.\"]}"
    },
    {
        "gold": {
            "text": [
                "The first token of each concatenated sequence is the [CLS] token, with its embedding being used as the aggregated representation for a context-response pair classification.",
                "This embedding captures the matching information between a context-response pair, which is sent into a classifier with a sigmoid output layer.",
                "Parameters of this classifier need to be estimated during the fine-tuning process.",
                "Finally, the classifier returns a score to denote the matching degree of this context-response pair.",
                "We tested SA-BERT on five public multi-turn response selection datasets, Ubuntu Dialogue Corpus V1 #TARGET_REF , Ubuntu Dialogue Corpus V2 (#REF) , Douban Conversation Corpus (#REF) , E-commerce Dialogue Corpus (#REFb) and DSTC 8-Track 2-Subtask 2 Corpus (Seokhwan #REF)."
            ],
            "label": [
                "BACK",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The first token of each concatenated sequence is the [CLS] token, with its embedding being used as the aggregated representation for a context-response pair classification. This embedding captures the matching information between a context-response pair, which is sent into a classifier with a sigmoid output layer. Parameters of this classifier need to be estimated during the fine-tuning process. Finally, the classifier returns a score to denote the matching degree of this context-response pair. We tested SA-BERT on five public multi-turn response selection datasets, Ubuntu Dialogue Corpus V1 #TARGET_REF , Ubuntu Dialogue Corpus V2 (#REF) , Douban Conversation Corpus (#REF) , E-commerce Dialogue Corpus (#REFb) and DSTC 8-Track 2-Subtask 2 Corpus (Seokhwan #REF).",
        "output": "{\"label\": [\"BACK\", \"USE\"], \"context\": [\"We tested SA-BERT on five public multi-turn response selection datasets, Ubuntu Dialogue Corpus V1 #TARGET_REF , Ubuntu Dialogue Corpus V2 (#REF) , Douban Conversation Corpus (#REF) , E-commerce Dialogue Corpus (#REFb) and DSTC 8-Track 2-Subtask 2 Corpus (Seokhwan #REF).\"]}"
    },
    {
        "gold": {
            "text": [
                "We used the same evaluation metrics as those used in previous work #TARGET_REF; #REF; #REF; #REFb; Seokhwan #REF) .",
                "Each model was tasked with selecting the k best-matched responses from n available candidates for the given conversation context c, and we calculated the recall of the true positive replies among the k selected responses, denoted as R n @k, as the main evaluation metric.",
                "In addition to R n @k, we considered the mean average precision (MAP) (#REF), mean reciprocal rank (MRR) (#REF) and precision-at-one (P@1), especially for the Douban corpus, following the settings of previous work."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "We used the same evaluation metrics as those used in previous work #TARGET_REF; #REF; #REF; #REFb; Seokhwan #REF) . Each model was tasked with selecting the k best-matched responses from n available candidates for the given conversation context c, and we calculated the recall of the true positive replies among the k selected responses, denoted as R n @k, as the main evaluation metric. In addition to R n @k, we considered the mean average precision (MAP) (#REF), mean reciprocal rank (MRR) (#REF) and precision-at-one (P@1), especially for the Douban corpus, following the settings of previous work.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We used the same evaluation metrics as those used in previous work #TARGET_REF; #REF; #REF; #REFb; Seokhwan #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Understanding the temporal information in natural language text is an important NLP task (#REF (#REF #REF; #REF; #REF #REF .",
                "A crucial component is temporal relation (TempRel; e.g., before or after) extraction (#REF; #REF; #REF; #REF; #TARGET_REF #REFa .",
                "The TempRels in a document or a sentence can be conveniently modeled as a graph, where the nodes are events, and the edges are labeled by TempRels.",
                "Given all the events in an instance, TempRel annotation is the process of manually labeling all the edges -a highly labor intensive task due to two reasons.",
                "One is that many edges require extensive reasoning over multiple sentences and labeling them is time-consuming."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Understanding the temporal information in natural language text is an important NLP task (#REF (#REF #REF; #REF; #REF #REF . A crucial component is temporal relation (TempRel; e.g., before or after) extraction (#REF; #REF; #REF; #REF; #TARGET_REF #REFa . The TempRels in a document or a sentence can be conveniently modeled as a graph, where the nodes are events, and the edges are labeled by TempRels. Given all the events in an instance, TempRel annotation is the process of manually labeling all the edges -a highly labor intensive task due to two reasons. One is that many edges require extensive reasoning over multiple sentences and labeling them is time-consuming.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Understanding the temporal information in natural language text is an important NLP task (#REF (#REF #REF; #REF; #REF #REF .\", \"A crucial component is temporal relation (TempRel; e.g., before or after) extraction (#REF; #REF; #REF; #REF; #TARGET_REF #REFa .\"]}"
    },
    {
        "gold": {
            "text": [
                "Annotators in this setup usually focus only on salient relations but overlook some others.",
                "It has been reported that many event pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them (#REF; #TARGET_REF .",
                "Consequently, we categorize TimeBank as a partially annotated dataset (P).",
                "The same argument applies to other datasets that adopted this setup, such as AQUAINT (#REF) , CaTeRs (#REF) and RED (O'#REF) .",
                "Most existing systems make use of P, including but not limited to, (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) ; this applies also to the TempEval workshops systems, e.g., (#REF; #REF; #REF) ."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Annotators in this setup usually focus only on salient relations but overlook some others. It has been reported that many event pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them (#REF; #TARGET_REF . Consequently, we categorize TimeBank as a partially annotated dataset (P). The same argument applies to other datasets that adopted this setup, such as AQUAINT (#REF) , CaTeRs (#REF) and RED (O'#REF) . Most existing systems make use of P, including but not limited to, (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) ; this applies also to the TempEval workshops systems, e.g., (#REF; #REF; #REF) .",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Annotators in this setup usually focus only on salient relations but overlook some others.\", \"It has been reported that many event pairs in TimeBank should have been annotated with a specific TempRel but the annotators failed to look at them (#REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Two recent TempRel extraction systems (#REF; #TARGET_REF ) also reported their performances on TB-Dense (F) and on TempEval-3 (P) separately.",
                "However, there are no existing systems that jointly train on both.",
                "Given that the annotation guidelines of F and P are obviously different, it may not be optimal to simply treat P and F uniformly and train on their union.",
                "This situation necessitates further investigation as we do here.",
                "Before introducing our joint learning approach, we have a few remarks about our choice of F and P datasets."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Two recent TempRel extraction systems (#REF; #TARGET_REF ) also reported their performances on TB-Dense (F) and on TempEval-3 (P) separately. However, there are no existing systems that jointly train on both. Given that the annotation guidelines of F and P are obviously different, it may not be optimal to simply treat P and F uniformly and train on their union. This situation necessitates further investigation as we do here. Before introducing our joint learning approach, we have a few remarks about our choice of F and P datasets.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Two recent TempRel extraction systems (#REF; #TARGET_REF ) also reported their performances on TB-Dense (F) and on TempEval-3 (P) separately.\", \"However, there are no existing systems that jointly train on both.\"]}"
    },
    {
        "gold": {
            "text": [
                "A standard way to perform global inference is to formulate it as an Integer Linear Programming (ILP) problem (#REF ) and enforce transitivity rules as constraints.",
                "Let R be the TempRel label set 2 , I r (ij) ∈ {0, 1} be the indicator function of (i, j) = r, and f r (ij) ∈ [0, 1] be the corresponding soft-max score obtained via S F +P .",
                "Then the ILP objective is formulated aŝ",
                "where {r m 3 } is selected based on the general transitivity proposed in #TARGET_REF .",
                "With Eq. (1), different implementations of Line 6 in Algorithm 1 can be described concisely as follows: (i) Local inference is performed by ignoring \"transitivity constraints\"."
            ],
            "label": [
                "BACK",
                "USE"
            ],
            "context": [
                1,
                0,
                1,
                1,
                0
            ]
        },
        "input": "A standard way to perform global inference is to formulate it as an Integer Linear Programming (ILP) problem (#REF ) and enforce transitivity rules as constraints. Let R be the TempRel label set 2 , I r (ij) ∈ {0, 1} be the indicator function of (i, j) = r, and f r (ij) ∈ [0, 1] be the corresponding soft-max score obtained via S F +P . Then the ILP objective is formulated aŝ where {r m 3 } is selected based on the general transitivity proposed in #TARGET_REF . With Eq. (1), different implementations of Line 6 in Algorithm 1 can be described concisely as follows: (i) Local inference is performed by ignoring \"transitivity constraints\".",
        "output": "{\"label\": [\"BACK\", \"USE\"], \"context\": [\"A standard way to perform global inference is to formulate it as an Integer Linear Programming (ILP) problem (#REF ) and enforce transitivity rules as constraints.\", \"Then the ILP objective is formulated a\\u015d\", \"where {r m 3 } is selected based on the general transitivity proposed in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "While incorporating transitivity constraints in inference is widely used, #TARGET_REF proposed to incorporate these constraints in the learning phase as well.",
                "One of the algorithms proposed in #REF is based on #REF 's constraint-driven learning (CoDL), which is the same as our intermediate System 7 in Table 2 ; the fact that System 7 is better than System 1 can thus be considered as a reproduction of #REF .",
                "Despite the technical similarity, this work is motivated differently and is set to achieve a different goal: #REF tried to enforce the transitivity structure, while the current work attempts to use imperfect signals (e.g., partially annotated) taken from additional data, and learn in the incidental supervision framework.",
                "The P used in this work is TBAQ, where only 12% of the edges are annotated.",
                "In practice, every annotation comes at a cost, either time or the expenses paid to annotators, and as more edges are annotated, the marginal \"benefit\" of one edge is going down (an extreme case is that an edge is of no value if it can be inferred from existing edges)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "While incorporating transitivity constraints in inference is widely used, #TARGET_REF proposed to incorporate these constraints in the learning phase as well. One of the algorithms proposed in #REF is based on #REF 's constraint-driven learning (CoDL), which is the same as our intermediate System 7 in Table 2 ; the fact that System 7 is better than System 1 can thus be considered as a reproduction of #REF . Despite the technical similarity, this work is motivated differently and is set to achieve a different goal: #REF tried to enforce the transitivity structure, while the current work attempts to use imperfect signals (e.g., partially annotated) taken from additional data, and learn in the incidental supervision framework. The P used in this work is TBAQ, where only 12% of the edges are annotated. In practice, every annotation comes at a cost, either time or the expenses paid to annotators, and as more edges are annotated, the marginal \"benefit\" of one edge is going down (an extreme case is that an edge is of no value if it can be inferred from existing edges).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"While incorporating transitivity constraints in inference is widely used, #TARGET_REF proposed to incorporate these constraints in the learning phase as well.\"]}"
    },
    {
        "gold": {
            "text": [
                "Then the ILP objective is formulated aŝ",
                "where {r m 3 } is selected based on the general transitivity proposed in (#REF) .",
                "With Eq. (1), different implementations of Line 6 in Algorithm 1 can be described concisely as follows: (i) Local inference is performed by ignoring \"transitivity constraints\".",
                "(ii) Global inference can be performed by adding annotated edges in P as additional constraints.",
                "Note that Algorithm 1 is only for the learning step of TempRel extraction; as for the inference step of this task, we consistently adopt the standard method by solving Eq. (1), as was done by (#REF; #REF; #REF; #REF; #TARGET_REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Then the ILP objective is formulated aŝ where {r m 3 } is selected based on the general transitivity proposed in (#REF) . With Eq. (1), different implementations of Line 6 in Algorithm 1 can be described concisely as follows: (i) Local inference is performed by ignoring \"transitivity constraints\". (ii) Global inference can be performed by adding annotated edges in P as additional constraints. Note that Algorithm 1 is only for the learning step of TempRel extraction; as for the inference step of this task, we consistently adopt the standard method by solving Eq. (1), as was done by (#REF; #REF; #REF; #REF; #TARGET_REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Note that Algorithm 1 is only for the learning step of TempRel extraction; as for the inference step of this task, we consistently adopt the standard method by solving Eq. (1), as was done by (#REF; #REF; #REF; #REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "While incorporating transitivity constraints in inference is widely used, #REF proposed to incorporate these constraints in the learning phase as well.",
                "One of the algorithms proposed in #TARGET_REF is based on #REF 's constraint-driven learning (CoDL), which is the same as our intermediate System 7 in Table 2 ; the fact that System 7 is better than System 1 can thus be considered as a reproduction of #TARGET_REF .",
                "Despite the technical similarity, this work is motivated differently and is set to achieve a different goal: #REF tried to enforce the transitivity structure, while the current work attempts to use imperfect signals (e.g., partially annotated) taken from additional data, and learn in the incidental supervision framework.",
                "The P used in this work is TBAQ, where only 12% of the edges are annotated.",
                "In practice, every annotation comes at a cost, either time or the expenses paid to annotators, and as more edges are annotated, the marginal \"benefit\" of one edge is going down (an extreme case is that an edge is of no value if it can be inferred from existing edges)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "While incorporating transitivity constraints in inference is widely used, #REF proposed to incorporate these constraints in the learning phase as well. One of the algorithms proposed in #TARGET_REF is based on #REF 's constraint-driven learning (CoDL), which is the same as our intermediate System 7 in Table 2 ; the fact that System 7 is better than System 1 can thus be considered as a reproduction of #TARGET_REF . Despite the technical similarity, this work is motivated differently and is set to achieve a different goal: #REF tried to enforce the transitivity structure, while the current work attempts to use imperfect signals (e.g., partially annotated) taken from additional data, and learn in the incidental supervision framework. The P used in this work is TBAQ, where only 12% of the edges are annotated. In practice, every annotation comes at a cost, either time or the expenses paid to annotators, and as more edges are annotated, the marginal \"benefit\" of one edge is going down (an extreme case is that an edge is of no value if it can be inferred from existing edges).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"One of the algorithms proposed in #TARGET_REF is based on #REF 's constraint-driven learning (CoDL), which is the same as our intermediate System 7 in Table 2 ; the fact that System 7 is better than System 1 can thus be considered as a reproduction of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Overall: all edges.",
                "Awareness: the temporal awareness metric used in the TempEval3 workshop, measuring how useful the predicted graphs are (#REF) .",
                "System 7 can also be considered as a reproduction of #TARGET_REF (see the discussion in Sec. 5 for details).",
                "Two bootstrapping algorithms (standard and constrained) are analyzed and the benefit of P, although with missing annotations, is shown on a benchmark dataset.",
                "This work may be a good starting point for further investigations of incidental supervision and data collection schemes of the TempRel extraction task."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Overall: all edges. Awareness: the temporal awareness metric used in the TempEval3 workshop, measuring how useful the predicted graphs are (#REF) . System 7 can also be considered as a reproduction of #TARGET_REF (see the discussion in Sec. 5 for details). Two bootstrapping algorithms (standard and constrained) are analyzed and the benefit of P, although with missing annotations, is shown on a benchmark dataset. This work may be a good starting point for further investigations of incidental supervision and data collection schemes of the TempRel extraction task.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"System 7 can also be considered as a reproduction of #TARGET_REF (see the discussion in Sec. 5 for details).\"]}"
    },
    {
        "gold": {
            "text": [
                "While incorporating transitivity constraints in inference is widely used, #REF proposed to incorporate these constraints in the learning phase as well.",
                "One of the algorithms proposed in #REF is based on #REF 's constraint-driven learning (CoDL), which is the same as our intermediate System 7 in Table 2 ; the fact that System 7 is better than System 1 can thus be considered as a reproduction of #REF .",
                "Despite the technical similarity, this work is motivated differently and is set to achieve a different goal: #TARGET_REF tried to enforce the transitivity structure, while the current work attempts to use imperfect signals (e.g., partially annotated) taken from additional data, and learn in the incidental supervision framework.",
                "The P used in this work is TBAQ, where only 12% of the edges are annotated.",
                "In practice, every annotation comes at a cost, either time or the expenses paid to annotators, and as more edges are annotated, the marginal \"benefit\" of one edge is going down (an extreme case is that an edge is of no value if it can be inferred from existing edges)."
            ],
            "label": [
                "DIFFER",
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "While incorporating transitivity constraints in inference is widely used, #REF proposed to incorporate these constraints in the learning phase as well. One of the algorithms proposed in #REF is based on #REF 's constraint-driven learning (CoDL), which is the same as our intermediate System 7 in Table 2 ; the fact that System 7 is better than System 1 can thus be considered as a reproduction of #REF . Despite the technical similarity, this work is motivated differently and is set to achieve a different goal: #TARGET_REF tried to enforce the transitivity structure, while the current work attempts to use imperfect signals (e.g., partially annotated) taken from additional data, and learn in the incidental supervision framework. The P used in this work is TBAQ, where only 12% of the edges are annotated. In practice, every annotation comes at a cost, either time or the expenses paid to annotators, and as more edges are annotated, the marginal \"benefit\" of one edge is going down (an extreme case is that an edge is of no value if it can be inferred from existing edges).",
        "output": "{\"label\": [\"DIFFER\", \"SIM\"], \"context\": [\"Despite the technical similarity, this work is motivated differently and is set to achieve a different goal: #TARGET_REF tried to enforce the transitivity structure, while the current work attempts to use imperfect signals (e.g., partially annotated) taken from additional data, and learn in the incidental supervision framework.\"]}"
    },
    {
        "gold": {
            "text": [
                "In the experiment, questions are represented by 13 features, 9 of which are semantic features based on WordNet.",
                "#TARGET_REF use a Sparse Network of Winnows (SNoW) to classify questions with respect to their expected answer type.",
                "The taxonomy consists of 6 coarse and 50 fine semantic classes.",
                "The training corpus used consists of 5,500 questions.",
                "Some of these are manually constructed, while other stems from the TREC-8 and TREC-9 conferences."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In the experiment, questions are represented by 13 features, 9 of which are semantic features based on WordNet. #TARGET_REF use a Sparse Network of Winnows (SNoW) to classify questions with respect to their expected answer type. The taxonomy consists of 6 coarse and 50 fine semantic classes. The training corpus used consists of 5,500 questions. Some of these are manually constructed, while other stems from the TREC-8 and TREC-9 conferences.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF use a Sparse Network of Winnows (SNoW) to classify questions with respect to their expected answer type.\"]}"
    },
    {
        "gold": {
            "text": [
                "Apart from these primitive features, a set of operators were used to compose more complex features.",
                "#REF used the same taxonomy as #TARGET_REF , as well as the same training and testing data.",
                "In an initial experiment they compared different machine learning approaches with regards to the question classification problem: Nearest Neighbors (NN), Naïve Bayes (NB), Decision Trees (DT), SNoW, and Support Vector Machines.",
                "The feature extracted and used as input to the machine learning algorithms in the initial experiment was bag-of-words and bag-of-ngrams (all continuous word sequences in the question).",
                "Questions were represented as binary feature vectors."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Apart from these primitive features, a set of operators were used to compose more complex features. #REF used the same taxonomy as #TARGET_REF , as well as the same training and testing data. In an initial experiment they compared different machine learning approaches with regards to the question classification problem: Nearest Neighbors (NN), Naïve Bayes (NB), Decision Trees (DT), SNoW, and Support Vector Machines. The feature extracted and used as input to the machine learning algorithms in the initial experiment was bag-of-words and bag-of-ngrams (all continuous word sequences in the question). Questions were represented as binary feature vectors.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#REF used the same taxonomy as #TARGET_REF , as well as the same training and testing data.\"]}"
    },
    {
        "gold": {
            "text": [
                "For present purposes the micro and macro sign tests established by #REF have been used.",
                "Thses were originally developed for the text categorization task, but as question classification bears many resemblances and can be seen as a special case of text categorization.",
                "The taxonomy used is the taxonomy proposed by #REF .",
                "This taxonomy has been chosen since it is the most frequently used one in earlier work in the field #TARGET_REF; #REF; #REF) .",
                "The corpora used is both the corpus constructed and tagged by #REF , as well as a newly tagged corpus extracted from the AnswerBus logs."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "For present purposes the micro and macro sign tests established by #REF have been used. Thses were originally developed for the text categorization task, but as question classification bears many resemblances and can be seen as a special case of text categorization. The taxonomy used is the taxonomy proposed by #REF . This taxonomy has been chosen since it is the most frequently used one in earlier work in the field #TARGET_REF; #REF; #REF) . The corpora used is both the corpus constructed and tagged by #REF , as well as a newly tagged corpus extracted from the AnswerBus logs.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"This taxonomy has been chosen since it is the most frequently used one in earlier work in the field #TARGET_REF; #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The results in this paper indicate that some of the results found in previous work #TARGET_REF; #REF; #REF) on question classification might be incorrect due to an unbiased training and test corpus.",
                "This bias stems from the fact that the training corpus is derived exclusively from TREC-10 data, while the training data stems from other sources.",
                "Since the TREC conferences have an explicit agenda that shifts from year to year this is perhaps no surprise.",
                "In relation to this, TREC material is maybe not the best source of information if one is interested in how different machine learners might perform on actual user data."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "The results in this paper indicate that some of the results found in previous work #TARGET_REF; #REF; #REF) on question classification might be incorrect due to an unbiased training and test corpus. This bias stems from the fact that the training corpus is derived exclusively from TREC-10 data, while the training data stems from other sources. Since the TREC conferences have an explicit agenda that shifts from year to year this is perhaps no surprise. In relation to this, TREC material is maybe not the best source of information if one is interested in how different machine learners might perform on actual user data.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The results in this paper indicate that some of the results found in previous work #TARGET_REF; #REF; #REF) on question classification might be incorrect due to an unbiased training and test corpus.\"]}"
    },
    {
        "gold": {
            "text": [
                "In essence each class is assigned a codeword of 1's and -1's of length m, where m equals or is greater than the number of classes.",
                "This splits the multi-class data into m binary class data.",
                "Therefore, m SVM classifiers can be designed and their output combined.",
                "The SVM:s also used linear kernels.",
                "The same taxonomy, training and testing data was used as in #TARGET_REF"
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "In essence each class is assigned a codeword of 1's and -1's of length m, where m equals or is greater than the number of classes. This splits the multi-class data into m binary class data. Therefore, m SVM classifiers can be designed and their output combined. The SVM:s also used linear kernels. The same taxonomy, training and testing data was used as in #TARGET_REF",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"The same taxonomy, training and testing data was used as in #TARGET_REF\"]}"
    },
    {
        "gold": {
            "text": [
                "The taxonomy used is the taxonomy proposed by #REF .",
                "This taxonomy has been chosen since it is the most frequently used one in earlier work in the field (#REF; #REF; #REF) .",
                "The corpora used is both the corpus constructed and tagged by #TARGET_REF , as well as a newly tagged corpus extracted from the AnswerBus logs.",
                "AnswerBus is a question answering system that has been online and logged real users questions.",
                "The AnswerBus corpus consists of 25,000 questions."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The taxonomy used is the taxonomy proposed by #REF . This taxonomy has been chosen since it is the most frequently used one in earlier work in the field (#REF; #REF; #REF) . The corpora used is both the corpus constructed and tagged by #TARGET_REF , as well as a newly tagged corpus extracted from the AnswerBus logs. AnswerBus is a question answering system that has been online and logged real users questions. The AnswerBus corpus consists of 25,000 questions.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"The corpora used is both the corpus constructed and tagged by #TARGET_REF , as well as a newly tagged corpus extracted from the AnswerBus logs.\"]}"
    },
    {
        "gold": {
            "text": [
                "Thses were originally developed for the text categorization task, but as question classification bears many resemblances and can be seen as a special case of text categorization.",
                "The taxonomy used is the taxonomy proposed by #TARGET_REF .",
                "This taxonomy has been chosen since it is the most frequently used one in earlier work in the field (#REF; #REF; #REF) .",
                "The corpora used is both the corpus constructed and tagged by #REF , as well as a newly tagged corpus extracted from the AnswerBus logs.",
                "AnswerBus is a question answering system that has been online and logged real users questions."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Thses were originally developed for the text categorization task, but as question classification bears many resemblances and can be seen as a special case of text categorization. The taxonomy used is the taxonomy proposed by #TARGET_REF . This taxonomy has been chosen since it is the most frequently used one in earlier work in the field (#REF; #REF; #REF) . The corpora used is both the corpus constructed and tagged by #REF , as well as a newly tagged corpus extracted from the AnswerBus logs. AnswerBus is a question answering system that has been online and logged real users questions.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The taxonomy used is the taxonomy proposed by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "This experiment has been done under two different settings.",
                "First, we have used the corpus originally developed by #TARGET_REF , but since the test corpus used consists of questions solely from TREC-10 and the TREC conferences have a specific agenda the test corpus might be slightly different from the training data.",
                "Therefore, a second setting was used where the questions from the training and test corpora were pooled together and a randomized test corpus was extracted.",
                "This will be refered to as the repartitioned corpus.",
                "The performance of the different learners on setting 1 can be found in table 1, setting 2 in table 2 while significance testing between the learners is shown in table 3 Classifier π Table 2 : Performance of classifers on repartitioned TREC data."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "This experiment has been done under two different settings. First, we have used the corpus originally developed by #TARGET_REF , but since the test corpus used consists of questions solely from TREC-10 and the TREC conferences have a specific agenda the test corpus might be slightly different from the training data. Therefore, a second setting was used where the questions from the training and test corpora were pooled together and a randomized test corpus was extracted. This will be refered to as the repartitioned corpus. The performance of the different learners on setting 1 can be found in table 1, setting 2 in table 2 while significance testing between the learners is shown in table 3 Classifier π Table 2 : Performance of classifers on repartitioned TREC data.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"First, we have used the corpus originally developed by #TARGET_REF , but since the test corpus used consists of questions solely from TREC-10 and the TREC conferences have a specific agenda the test corpus might be slightly different from the training data.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF simplified their architecture using a highway Bi-LSTM network.",
                "More recently, #TARGET_REF replaced the common recurrent architecture with a self-attention network, directly capturing relationships between tokens regardless of their distance, resulting in better results and faster training.",
                "The work in deep end-to-end SRL has focused heavily on applying deep learning advances without considering the multilingual aspect.",
                "However, language-specific characteristics and the available amount of training data highly influence the optimal model structure.",
                "DAMESRL facilitates exploration and fair evaluation of new SRL models for different languages by providing flexible neural model construction on different modeling levels, the handling of various input and output formats, and clear output visualization."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "#REF simplified their architecture using a highway Bi-LSTM network. More recently, #TARGET_REF replaced the common recurrent architecture with a self-attention network, directly capturing relationships between tokens regardless of their distance, resulting in better results and faster training. The work in deep end-to-end SRL has focused heavily on applying deep learning advances without considering the multilingual aspect. However, language-specific characteristics and the available amount of training data highly influence the optimal model structure. DAMESRL facilitates exploration and fair evaluation of new SRL models for different languages by providing flexible neural model construction on different modeling levels, the handling of various input and output formats, and clear output visualization.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"More recently, #TARGET_REF replaced the common recurrent architecture with a self-attention network, directly capturing relationships between tokens regardless of their distance, resulting in better results and faster training.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, language-specific characteristics and the available amount of training data highly influence the optimal model structure.",
                "DAMESRL facilitates exploration and fair evaluation of new SRL models for different languages by providing flexible neural model construction on different modeling levels, the handling of various input and output formats, and clear output visualization.",
                "Beyond the existing state-of-the-art models (#REF; #REF; #TARGET_REF ), we exploit character-level modeling, beneficial when considering multiple languages.",
                "To demonstrate the merits of easy cross-lingual exploration and evaluation of model structures for SRL provided by DAMESRL, we report performance of several distinct models integrated into our framework for English, German and Arabic, as they have very different linguistic characteristics.",
                "by w p ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "However, language-specific characteristics and the available amount of training data highly influence the optimal model structure. DAMESRL facilitates exploration and fair evaluation of new SRL models for different languages by providing flexible neural model construction on different modeling levels, the handling of various input and output formats, and clear output visualization. Beyond the existing state-of-the-art models (#REF; #REF; #TARGET_REF ), we exploit character-level modeling, beneficial when considering multiple languages. To demonstrate the merits of easy cross-lingual exploration and evaluation of model structures for SRL provided by DAMESRL, we report performance of several distinct models integrated into our framework for English, German and Arabic, as they have very different linguistic characteristics. by w p .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Beyond the existing state-of-the-art models (#REF; #REF; #TARGET_REF ), we exploit character-level modeling, beneficial when considering multiple languages.\"]}"
    },
    {
        "gold": {
            "text": [
                "DAMESRL provides a Bi-LSTM network to learn character-level word representations helping for languages where important SRL cues are given through inflections, such as case markings in German and Arabic.",
                "Despite the foreseen importance, character-level embeddings have not been used in previous work (#REF; #REF; #TARGET_REF) .",
                "Phase II: As core sequence representation component, users can choose between a self-attention encoding (#REF) , a regular Bi-LSTM (#REF) or a highway Bi-LSTM (#REF; #REF) .",
                "Phase III: To compute model probabilities, users can choose a regular softmax, or a linear chain CRF as proposed by (#REF) , which can be useful for languages where word order is an important SRL cue, such as English, or when less training data is available (shown in Section 4).",
                "Phase IV: The inference phase provides two options for label inference from the computed model probabilities including greedy prediction and Viterbi decoding."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "DAMESRL provides a Bi-LSTM network to learn character-level word representations helping for languages where important SRL cues are given through inflections, such as case markings in German and Arabic. Despite the foreseen importance, character-level embeddings have not been used in previous work (#REF; #REF; #TARGET_REF) . Phase II: As core sequence representation component, users can choose between a self-attention encoding (#REF) , a regular Bi-LSTM (#REF) or a highway Bi-LSTM (#REF; #REF) . Phase III: To compute model probabilities, users can choose a regular softmax, or a linear chain CRF as proposed by (#REF) , which can be useful for languages where word order is an important SRL cue, such as English, or when less training data is available (shown in Section 4). Phase IV: The inference phase provides two options for label inference from the computed model probabilities including greedy prediction and Viterbi decoding.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Despite the foreseen importance, character-level embeddings have not been used in previous work (#REF; #REF; #TARGET_REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "DAMESRL provides a Bi-LSTM network to learn character-level word representations helping for languages where important SRL cues are given through inflections, such as case markings in German and Arabic.",
                "Despite the foreseen importance, character-level embeddings have not been used in previous work (#REF; #REF; #REF) .",
                "Phase II: As core sequence representation component, users can choose between a self-attention encoding ( #TARGET_REF) , a regular Bi-LSTM (#REF) or a highway Bi-LSTM (#REF; #REF) .",
                "Phase III: To compute model probabilities, users can choose a regular softmax, or a linear chain CRF as proposed by (#REF) , which can be useful for languages where word order is an important SRL cue, such as English, or when less training data is available (shown in Section 4).",
                "Phase IV: The inference phase provides two options for label inference from the computed model probabilities including greedy prediction and Viterbi decoding."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "DAMESRL provides a Bi-LSTM network to learn character-level word representations helping for languages where important SRL cues are given through inflections, such as case markings in German and Arabic. Despite the foreseen importance, character-level embeddings have not been used in previous work (#REF; #REF; #REF) . Phase II: As core sequence representation component, users can choose between a self-attention encoding ( #TARGET_REF) , a regular Bi-LSTM (#REF) or a highway Bi-LSTM (#REF; #REF) . Phase III: To compute model probabilities, users can choose a regular softmax, or a linear chain CRF as proposed by (#REF) , which can be useful for languages where word order is an important SRL cue, such as English, or when less training data is available (shown in Section 4). Phase IV: The inference phase provides two options for label inference from the computed model probabilities including greedy prediction and Viterbi decoding.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Phase II: As core sequence representation component, users can choose between a self-attention encoding ( #TARGET_REF) , a regular Bi-LSTM (#REF) or a highway Bi-LSTM (#REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "A hoax article typically tries to convince the reader about a cookedup story while propaganda ones usually mislead the reader into believing a false political or social agenda.",
                "#REF defines a satirical article as the one which deliberately exposes real-world individuals, organisations and events to ridicule.",
                "Previous works #TARGET_REF; #REF) rely on various linguistic and handcrafted semantic features for differentiating between news articles.",
                "However, none of them try to model the interaction of sentences within the document.",
                "We observed a pattern in the way sentences cluster in different kind of news articles."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "A hoax article typically tries to convince the reader about a cookedup story while propaganda ones usually mislead the reader into believing a false political or social agenda. #REF defines a satirical article as the one which deliberately exposes real-world individuals, organisations and events to ridicule. Previous works #TARGET_REF; #REF) rely on various linguistic and handcrafted semantic features for differentiating between news articles. However, none of them try to model the interaction of sentences within the document. We observed a pattern in the way sentences cluster in different kind of news articles.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Previous works #TARGET_REF; #REF) rely on various linguistic and handcrafted semantic features for differentiating between news articles.\"]}"
    },
    {
        "gold": {
            "text": [
                "Satire, according to #REF , is complicated because it occupies more than one place in the framework for humor, proposed by #REF : it clearly has an aggressive and social function, and often expresses an intellectual aspect as well.",
                "#TARGET_REF defines news satire as a genre of satire that mimics the format and style of journalistic reporting.",
                "Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources (#REF) .",
                "#REF hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire.",
                "In this work, we show that our proposed model generalizes to articles from unseen publication sources."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Satire, according to #REF , is complicated because it occupies more than one place in the framework for humor, proposed by #REF : it clearly has an aggressive and social function, and often expresses an intellectual aspect as well. #TARGET_REF defines news satire as a genre of satire that mimics the format and style of journalistic reporting. Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources (#REF) . #REF hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire. In this work, we show that our proposed model generalizes to articles from unseen publication sources.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF defines news satire as a genre of satire that mimics the format and style of journalistic reporting.\"]}"
    },
    {
        "gold": {
            "text": [
                "Satire, according to #REF , is complicated because it occupies more than one place in the framework for humor, proposed by #REF : it clearly has an aggressive and social function, and often expresses an intellectual aspect as well.",
                "#REF defines news satire as a genre of satire that mimics the format and style of journalistic reporting.",
                "Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources #TARGET_REF .",
                "#REF hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire.",
                "In this work, we show that our proposed model generalizes to articles from unseen publication sources."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Satire, according to #REF , is complicated because it occupies more than one place in the framework for humor, proposed by #REF : it clearly has an aggressive and social function, and often expresses an intellectual aspect as well. #REF defines news satire as a genre of satire that mimics the format and style of journalistic reporting. Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources #TARGET_REF . #REF hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire. In this work, we show that our proposed model generalizes to articles from unseen publication sources.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We believe that the reason for such kind of behaviour is the presence of factual jumps across sections in a trusted document.",
                "In this work, we propose a graph neural network-based model to classify news articles while capturing the interaction of sentences across the document.",
                "We present a series of experiments on News Corpus with Varying Reliability dataset (#REF) and Satirical Legitimate News dataset #TARGET_REF .",
                "Our results demonstrate that the proposed model achieves state-of-the-art performance on these datasets and provides interesting insights.",
                "Experiments performed in out-of-domain settings establish the generalizability of our proposed method."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We believe that the reason for such kind of behaviour is the presence of factual jumps across sections in a trusted document. In this work, we propose a graph neural network-based model to classify news articles while capturing the interaction of sentences across the document. We present a series of experiments on News Corpus with Varying Reliability dataset (#REF) and Satirical Legitimate News dataset #TARGET_REF . Our results demonstrate that the proposed model achieves state-of-the-art performance on these datasets and provides interesting insights. Experiments performed in out-of-domain settings establish the generalizability of our proposed method.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We present a series of experiments on News Corpus with Varying Reliability dataset (#REF) and Satirical Legitimate News dataset #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We use SLN: Satirical and Legitimate News Database #TARGET_REF , RPN: Random Political News Dataset (#REF) and LUN: Labeled Unreliable News Dataset #REF for our experiments.",
                "Table 1 shows the statistics.",
                "Since all of the previous methods on the aforementioned datasets are non-neural, we implement the following neural baselines,",
                "• CNN: In this model, we apply a 1-d CNN (Convolutional Neural Network) layer (#REF) with filter size 3 over the word embeddings of the sentences within a document.",
                "This is followed by a max-pooling layer to get a single document vector which is passed to a fully connected projection layer to get the logits over output classes."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We use SLN: Satirical and Legitimate News Database #TARGET_REF , RPN: Random Political News Dataset (#REF) and LUN: Labeled Unreliable News Dataset #REF for our experiments. Table 1 shows the statistics. Since all of the previous methods on the aforementioned datasets are non-neural, we implement the following neural baselines, • CNN: In this model, we apply a 1-d CNN (Convolutional Neural Network) layer (#REF) with filter size 3 over the word embeddings of the sentences within a document. This is followed by a max-pooling layer to get a single document vector which is passed to a fully connected projection layer to get the logits over output classes.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use SLN: Satirical and Legitimate News Database #TARGET_REF , RPN: Random Political News Dataset (#REF) and LUN: Labeled Unreliable News Dataset #REF for our experiments.\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF 's work by offering a quantitative study of linguistic differences found in articles of different types of fake news such as hoax, propaganda and satire.",
                "They also proposed predictive models for graded deception across multiple domains.",
                "#REF found that neural methods didn't perform well for this task and proposed to use a Max-Entropy classifier.",
                "We show that our proposed neural network based on graph convolutional layers can outperform this model.",
                "Recent works by #REF ; De #REF show that sophisticated neural models can be used for satirical news detection."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                1,
                1,
                0,
                1,
                0
            ]
        },
        "input": "#TARGET_REF 's work by offering a quantitative study of linguistic differences found in articles of different types of fake news such as hoax, propaganda and satire. They also proposed predictive models for graded deception across multiple domains. #REF found that neural methods didn't perform well for this task and proposed to use a Max-Entropy classifier. We show that our proposed neural network based on graph convolutional layers can outperform this model. Recent works by #REF ; De #REF show that sophisticated neural models can be used for satirical news detection.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"#TARGET_REF 's work by offering a quantitative study of linguistic differences found in articles of different types of fake news such as hoax, propaganda and satire.\", \"They also proposed predictive models for graded deception across multiple domains.\", \"We show that our proposed neural network based on graph convolutional layers can outperform this model.\"]}"
    },
    {
        "gold": {
            "text": [
                "We report macro-averaged scores in Table 3 : 4-way classification results for different models.",
                "We only report F1-score following the SoTA paper.",
                "similarity model does not seem to have much impact on the GCN model, and considering the computing cost, we don't experiment with it for the 4-way classification scenario.",
                "Given that we use SLN as an out of domain test set (just one overlapping source, no overlap in articles), whereas the SoTA paper #TARGET_REF reports a 10fold cross validation number on SLN.",
                "We believe that our results are quite strong, the GAT + 2 Attn Heads model achieves an accuracy of 87% on the entire RPN dataset when used as an out-of-domain test set."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We report macro-averaged scores in Table 3 : 4-way classification results for different models. We only report F1-score following the SoTA paper. similarity model does not seem to have much impact on the GCN model, and considering the computing cost, we don't experiment with it for the 4-way classification scenario. Given that we use SLN as an out of domain test set (just one overlapping source, no overlap in articles), whereas the SoTA paper #TARGET_REF reports a 10fold cross validation number on SLN. We believe that our results are quite strong, the GAT + 2 Attn Heads model achieves an accuracy of 87% on the entire RPN dataset when used as an out-of-domain test set.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Given that we use SLN as an out of domain test set (just one overlapping source, no overlap in articles), whereas the SoTA paper #TARGET_REF reports a 10fold cross validation number on SLN.\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 2 depicts these results.",
                "The correlation between our scores and changes in workforce participation rates are similar to the correlation between the scores from (#REF) and the same (r = 0.8, p = 0.01 and r = 0.81, p < 0.01, respectively, for gender occupation bias; r = 0.84, p < 0.01 and r = 0.79, p = 0.01, respectively, for Asian/White occupation bias).",
                "Qualitative inspection of Figure 2 suggests that our model also produces smoother decade-by-decade scores, suggesting that it not only identifies attribute- #TARGET_REF and our model (blue dotted and green dashed lines, respectively) compared to actual workforce participation rates (solid lines) for gender (top) and Asian/White (bottom) linguistic biases.",
                "To compare all values on a single yaxis, we standardize both sets of bias scores and workforce participation rates by subtracting the mean and dividing by the standard deviation across decades.",
                "specific fluctuations in word semantics, but also, may provide a more general, regularized model for learning attribute-conditioned word embeddings."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Figure 2 depicts these results. The correlation between our scores and changes in workforce participation rates are similar to the correlation between the scores from (#REF) and the same (r = 0.8, p = 0.01 and r = 0.81, p < 0.01, respectively, for gender occupation bias; r = 0.84, p < 0.01 and r = 0.79, p = 0.01, respectively, for Asian/White occupation bias). Qualitative inspection of Figure 2 suggests that our model also produces smoother decade-by-decade scores, suggesting that it not only identifies attribute- #TARGET_REF and our model (blue dotted and green dashed lines, respectively) compared to actual workforce participation rates (solid lines) for gender (top) and Asian/White (bottom) linguistic biases. To compare all values on a single yaxis, we standardize both sets of bias scores and workforce participation rates by subtracting the mean and dividing by the standard deviation across decades. specific fluctuations in word semantics, but also, may provide a more general, regularized model for learning attribute-conditioned word embeddings.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Qualitative inspection of Figure 2 suggests that our model also produces smoother decade-by-decade scores, suggesting that it not only identifies attribute- #TARGET_REF and our model (blue dotted and green dashed lines, respectively) compared to actual workforce participation rates (solid lines) for gender (top) and Asian/White (bottom) linguistic biases.\"]}"
    },
    {
        "gold": {
            "text": [
                "As a comparison, we also compute bias scores by training one Word2Vec model per day and projecting all day-by-day models into the same vector space using orthogonal Procrustes alignment 6 similar to (#REF) .",
                "The resulting scores from this non-dynamic model are depicted in 3(a).",
                "From qualitative inspection, the day-byday scores produced by the non-dynamic model appear much less smooth, and hence, fail to show the relative shift in discourse that likely occurred in response to a major refugee-related news event.",
                "One possible reason for this is that the median number of words for each day in the talk radio corpus is 4 million-over 5x fewer than a median of 22 million words per decade used to train each decade-specific model in #TARGET_REF .",
                "These results suggest that using our dynamic embedding approach is particularly valuable when data is sparse for any given attribute."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "As a comparison, we also compute bias scores by training one Word2Vec model per day and projecting all day-by-day models into the same vector space using orthogonal Procrustes alignment 6 similar to (#REF) . The resulting scores from this non-dynamic model are depicted in 3(a). From qualitative inspection, the day-byday scores produced by the non-dynamic model appear much less smooth, and hence, fail to show the relative shift in discourse that likely occurred in response to a major refugee-related news event. One possible reason for this is that the median number of words for each day in the talk radio corpus is 4 million-over 5x fewer than a median of 22 million words per decade used to train each decade-specific model in #TARGET_REF . These results suggest that using our dynamic embedding approach is particularly valuable when data is sparse for any given attribute.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"From qualitative inspection, the day-byday scores produced by the non-dynamic model appear much less smooth, and hence, fail to show the relative shift in discourse that likely occurred in response to a major refugee-related news event.\", \"One possible reason for this is that the median number of words for each day in the talk radio corpus is 4 million-over 5x fewer than a median of 22 million words per decade used to train each decade-specific model in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "A supervised machine learning model is designed and trained on a single dataset to predict the relation type of pairs of entities.",
                "Traditional methods rely on linguistic or semantic features (#REF; #REF) , or kernels based on syntax or sequences (#REFa,b; #REF) to represent sentences of relations.",
                "More recently, deep neural nets start to show promising results.",
                "Most rely on convolutional neural nets (#REF (#REF #REF, 2016; #TARGET_REF or recurrent neural nets (#REF; #REF; #REF) to learn the representation of relations.",
                "Our supervised base model will be similar to (#REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "A supervised machine learning model is designed and trained on a single dataset to predict the relation type of pairs of entities. Traditional methods rely on linguistic or semantic features (#REF; #REF) , or kernels based on syntax or sequences (#REFa,b; #REF) to represent sentences of relations. More recently, deep neural nets start to show promising results. Most rely on convolutional neural nets (#REF (#REF #REF, 2016; #TARGET_REF or recurrent neural nets (#REF; #REF; #REF) to learn the representation of relations. Our supervised base model will be similar to (#REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Most rely on convolutional neural nets (#REF (#REF #REF, 2016; #TARGET_REF or recurrent neural nets (#REF; #REF; #REF) to learn the representation of relations.\"]}"
    },
    {
        "gold": {
            "text": [
                "The supervised neural model on a single dataset was introduced by #REF and followed by many others (#REF; #REF; #REF; #REF; #TARGET_REF .",
                "We use a similar model as our base model.",
                "It takes word tokens, position of arguments and their entity types as input.",
                "Some work (#REF; #REF) used extra syntax features as input.",
                "However, the parsers that produce syntax features could have errors and vary depending on the domain of text."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The supervised neural model on a single dataset was introduced by #REF and followed by many others (#REF; #REF; #REF; #REF; #TARGET_REF . We use a similar model as our base model. It takes word tokens, position of arguments and their entity types as input. Some work (#REF; #REF) used extra syntax features as input. However, the parsers that produce syntax features could have errors and vary depending on the domain of text.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The supervised neural model on a single dataset was introduced by #REF and followed by many others (#REF; #REF; #REF; #REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "It takes word tokens, position of arguments and their entity types as input.",
                "Some work (#REF; #TARGET_REF used extra syntax features as input.",
                "However, the parsers that produce syntax features could have errors and vary depending on the domain of text.",
                "The syntax features learned could also be too specific for a single dataset.",
                "Thus, we focus on learning representation from scratch, but also compare the models with extra features later in the experiments."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "It takes word tokens, position of arguments and their entity types as input. Some work (#REF; #TARGET_REF used extra syntax features as input. However, the parsers that produce syntax features could have errors and vary depending on the domain of text. The syntax features learned could also be too specific for a single dataset. Thus, we focus on learning representation from scratch, but also compare the models with extra features later in the experiments.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Some work (#REF; #TARGET_REF used extra syntax features as input.\"]}"
    },
    {
        "gold": {
            "text": [
                "Then we obtain the high level summarization φ(x) for the relation example.",
                "The decoder uses this high level representation as features for relation classification.",
                "It usually contains one hidden layer (#REF; #REF; #TARGET_REF ) and a softmax output layer.",
                "We use the same structure which can be formalized as the following:",
                "where W h and b h are the weights for the hidden"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Then we obtain the high level summarization φ(x) for the relation example. The decoder uses this high level representation as features for relation classification. It usually contains one hidden layer (#REF; #REF; #TARGET_REF ) and a softmax output layer. We use the same structure which can be formalized as the following: where W h and b h are the weights for the hidden",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"It usually contains one hidden layer (#REF; #REF; #TARGET_REF ) and a softmax output layer.\"]}"
    },
    {
        "gold": {
            "text": [
                "To apply the multi-task learning, we need at least two datasets.",
                "We pick ACE05 and ERE for our case study.",
                "The ACE05 dataset provides a cross-domain evaluation setting .",
                "It contains 6 domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and weblogs (wl).",
                "Previous work (#REF; #REF; #TARGET_REF set, and the other half of bc, cts and wl as the test sets."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "To apply the multi-task learning, we need at least two datasets. We pick ACE05 and ERE for our case study. The ACE05 dataset provides a cross-domain evaluation setting . It contains 6 domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and weblogs (wl). Previous work (#REF; #REF; #TARGET_REF set, and the other half of bc, cts and wl as the test sets.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Previous work (#REF; #REF; #TARGET_REF set, and the other half of bc, cts and wl as the test sets.\"]}"
    },
    {
        "gold": {
            "text": [
                "Most rely on convolutional neural nets (#REF (#REF #REF, 2016; #REF) or recurrent neural nets (#REF; #REF; #REF) to learn the representation of relations.",
                "Our supervised base model will be similar to (#REF) .",
                "Our initial experiments did not use syntactic features (#REF; #TARGET_REF ) that require additional parsers.",
                "In order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation.",
                "They used their multi-task model to train on the bilingual ACE05 datasets and obtained improvement when there is less training available (10%-50%)."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Most rely on convolutional neural nets (#REF (#REF #REF, 2016; #REF) or recurrent neural nets (#REF; #REF; #REF) to learn the representation of relations. Our supervised base model will be similar to (#REF) . Our initial experiments did not use syntactic features (#REF; #TARGET_REF ) that require additional parsers. In order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation. They used their multi-task model to train on the bilingual ACE05 datasets and obtained improvement when there is less training available (10%-50%).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Our initial experiments did not use syntactic features (#REF; #TARGET_REF ) that require additional parsers.\"]}"
    },
    {
        "gold": {
            "text": [
                "We also convert the entity types of the arguments to entity embeddings.",
                "The setup of word embedding and position embedding was introduced by #REF .",
                "The entity embedding (#REF; #TARGET_REF ) is included for arguments that are entities rather than common nouns.",
                "At the end, each token is converted to an embedding w i as the concatenation of these three types of embeddings, where i ∈ [0, T ), T is the length of the sentence.",
                "A wide range of encoders have been proposed for relation extraction."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We also convert the entity types of the arguments to entity embeddings. The setup of word embedding and position embedding was introduced by #REF . The entity embedding (#REF; #TARGET_REF ) is included for arguments that are entities rather than common nouns. At the end, each token is converted to an embedding w i as the concatenation of these three types of embeddings, where i ∈ [0, T ), T is the length of the sentence. A wide range of encoders have been proposed for relation extraction.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"The entity embedding (#REF; #TARGET_REF ) is included for arguments that are entities rather than common nouns.\"]}"
    },
    {
        "gold": {
            "text": [
                "Training separately on the two corpora (row \"Supervised\" in Table 1 ), we obtain results on ACE05 comparable to previous work (#REF) with substantially fewer features.",
                "With syntactic features as (#REF; #TARGET_REF did, it could be further improved.",
                "In this paper, however, we want to focus on representation learning from scratch first.",
                "Our experiments focus on whether we can improve the representation with more sources of data.",
                "A common way to do so is pre-training."
            ],
            "label": [
                "FUT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Training separately on the two corpora (row \"Supervised\" in Table 1 ), we obtain results on ACE05 comparable to previous work (#REF) with substantially fewer features. With syntactic features as (#REF; #TARGET_REF did, it could be further improved. In this paper, however, we want to focus on representation learning from scratch first. Our experiments focus on whether we can improve the representation with more sources of data. A common way to do so is pre-training.",
        "output": "{\"label\": [\"FUT\"], \"context\": [\"With syntactic features as (#REF; #TARGET_REF did, it could be further improved.\"]}"
    },
    {
        "gold": {
            "text": [
                "Citation texts usually highlight certain contributions of the referenced paper and a set of citation texts to a reference paper can provide useful information about that paper.",
                "Therefore, citation texts have been previously used to enhance many downstream tasks in IR/NLP such as search and summarization (e.g. [2, 15, 16] ).",
                "While useful, citation texts might lack the appropriate context from the reference article #TARGET_REF 5, 18] .",
                "For example, details of the methods, assumptions or conditions for the obtained results are often not mentioned.",
                "Furthermore, in many cases the citing author might misunderstand or misquote the referenced paper and ascribe contributions to it that are not intended in that form."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Citation texts usually highlight certain contributions of the referenced paper and a set of citation texts to a reference paper can provide useful information about that paper. Therefore, citation texts have been previously used to enhance many downstream tasks in IR/NLP such as search and summarization (e.g. [2, 15, 16] ). While useful, citation texts might lack the appropriate context from the reference article #TARGET_REF 5, 18] . For example, details of the methods, assumptions or conditions for the obtained results are often not mentioned. Furthermore, in many cases the citing author might misunderstand or misquote the referenced paper and ascribe contributions to it that are not intended in that form.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"While useful, citation texts might lack the appropriate context from the reference article #TARGET_REF 5, 18] .\"]}"
    },
    {
        "gold": {
            "text": [
                "This dataset contains 220 scienti c biomedical journal articles and 313 total citation texts where the relevant contexts for each citation text are annotated by 4 experts.",
                "Baselines.",
                "To our knowledge, the only published results on TAC 201 #TARGET_REF is #TARGET_REF , where the authors utilized query reformulation (QR) based on UMLS ontology.",
                "In addition to [4] , we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM: Vector Space Model that was used in [4] ; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12] ; and 4) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10] .",
                "All the baseline parameters are tuned for the best performance, and the same preprocessing is applied to all the baselines and our methods."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "This dataset contains 220 scienti c biomedical journal articles and 313 total citation texts where the relevant contexts for each citation text are annotated by 4 experts. Baselines. To our knowledge, the only published results on TAC 201 #TARGET_REF is #TARGET_REF , where the authors utilized query reformulation (QR) based on UMLS ontology. In addition to [4] , we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM: Vector Space Model that was used in [4] ; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12] ; and 4) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10] . All the baseline parameters are tuned for the best performance, and the same preprocessing is applied to all the baselines and our methods.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"To our knowledge, the only published results on TAC 201 #TARGET_REF is #TARGET_REF , where the authors utilized query reformulation (QR) based on UMLS ontology.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our models (last 4 rows of table 1) achieve signi cant improvements over the baselines consistently across most of the metrics.",
                "This shows the e ectiveness of our models viewed from di erent aspects in comparison with the baselines.",
                "The best baseline performance is the query reformulation (QR) method by #TARGET_REF which improves over other baselines.",
                "We observe that using general domain embeddings does not provide much advantage in comparison with the best baseline (compare WE wiki and QR in the Table) .",
                "However, using the domain speci c embeddings (WE Bio ) results in 10% c-F improvement over the best baseline."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Our models (last 4 rows of table 1) achieve signi cant improvements over the baselines consistently across most of the metrics. This shows the e ectiveness of our models viewed from di erent aspects in comparison with the baselines. The best baseline performance is the query reformulation (QR) method by #TARGET_REF which improves over other baselines. We observe that using general domain embeddings does not provide much advantage in comparison with the best baseline (compare WE wiki and QR in the Table) . However, using the domain speci c embeddings (WE Bio ) results in 10% c-F improvement over the best baseline.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The best baseline performance is the query reformulation (QR) method by #TARGET_REF which improves over other baselines.\"]}"
    },
    {
        "gold": {
            "text": [
                "Baselines.",
                "To our knowledge, the only published results on TAC 2014 is [4] , where the authors utilized query reformulation (QR) based on UMLS ontology.",
                "In addition to #TARGET_REF , we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM: Vector Space Model that was used in #TARGET_REF ; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12] ; and #TARGET_REF) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10] .",
                "All the baseline parameters are tuned for the best performance, and the same preprocessing is applied to all the baselines and our methods.",
                "Our methods."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Baselines. To our knowledge, the only published results on TAC 2014 is [4] , where the authors utilized query reformulation (QR) based on UMLS ontology. In addition to #TARGET_REF , we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM: Vector Space Model that was used in #TARGET_REF ; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12] ; and #TARGET_REF) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10] . All the baseline parameters are tuned for the best performance, and the same preprocessing is applied to all the baselines and our methods. Our methods.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"In addition to #TARGET_REF , we also implement several other strong baselines to better evaluate the e ectiveness of our model: 1) BM25; 2) VSM: Vector Space Model that was used in #TARGET_REF ; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12] ; and #TARGET_REF) LMD-LDA: Language modeling with LDA smoothing which is a recent extension of the LMD to also account for the latent topics [10] .\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF proposed dual embedded space model that predicts document aboutness by comparing the centroid of word vectors to query terms.",
                "#REF used embeddings to transform term weights in a translation model for retrieval.",
                "Their model uses embeddings to expand documents and use co-occurrences for estimation.",
                "Unlike these works, we directly use embeddings in estimating the likelihood of query given documents; we furthermore incorporate ways to utilize domain speci c knowledge in our model.",
                "The most relevant prior work to ours is #TARGET_REF where the authors approached the problem using a vector space model similarity ranking and query reformulations."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "#REF proposed dual embedded space model that predicts document aboutness by comparing the centroid of word vectors to query terms. #REF used embeddings to transform term weights in a translation model for retrieval. Their model uses embeddings to expand documents and use co-occurrences for estimation. Unlike these works, we directly use embeddings in estimating the likelihood of query given documents; we furthermore incorporate ways to utilize domain speci c knowledge in our model. The most relevant prior work to ours is #TARGET_REF where the authors approached the problem using a vector space model similarity ranking and query reformulations.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"The most relevant prior work to ours is #TARGET_REF where the authors approached the problem using a vector space model similarity ranking and query reformulations.\"]}"
    },
    {
        "gold": {
            "text": [
                "Only some of these measures generalize tractably beyond two words, for example PMI (#REF) , i.e., the log ratio of the joint probability to the product of the marginal probabilities of the individual words.",
                "Another measure which addresses sequences of longer than two words is the C-value 456 (#REF) which weights term frequency by the log length of the n-gram while penalizing ngrams that appear in frequent larger ones.",
                "Mutual expectation (#REF) involves deriving a normalized statistic that reflects the extent to which a phrase resists the omission of any constituent word.",
                "Similarly, the lexical predictability ratio (LPR) of #TARGET_REF is an association measure applicable to any possible syntactic pattern, which is calculated by discounting syntactic predictability from the overall conditional probability for each word given the other words in the phrase.",
                "Though most association measures involve only usage statistics of the phrase and its subparts, the DRUID measure (#REF) is an exception which uses distributional semantics around the phrase to identify how easily an n-gram could be replaced by a single word."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Only some of these measures generalize tractably beyond two words, for example PMI (#REF) , i.e., the log ratio of the joint probability to the product of the marginal probabilities of the individual words. Another measure which addresses sequences of longer than two words is the C-value 456 (#REF) which weights term frequency by the log length of the n-gram while penalizing ngrams that appear in frequent larger ones. Mutual expectation (#REF) involves deriving a normalized statistic that reflects the extent to which a phrase resists the omission of any constituent word. Similarly, the lexical predictability ratio (LPR) of #TARGET_REF is an association measure applicable to any possible syntactic pattern, which is calculated by discounting syntactic predictability from the overall conditional probability for each word given the other words in the phrase. Though most association measures involve only usage statistics of the phrase and its subparts, the DRUID measure (#REF) is an exception which uses distributional semantics around the phrase to identify how easily an n-gram could be replaced by a single word.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Similarly, the lexical predictability ratio (LPR) of #TARGET_REF is an association measure applicable to any possible syntactic pattern, which is calculated by discounting syntactic predictability from the overall conditional probability for each word given the other words in the phrase.\"]}"
    },
    {
        "gold": {
            "text": [
                "One major motivation for looking beyond MWEs is the ability to carry out broader linguistic analyses.",
                "Within corpus linguistics, multiword sequences have been studied in the form of lexical bundles (#REF) , which are simply n-grams that occur above a certain frequency threshold.",
                "Like FS, lexical bundles generally involve larger phrasal chunks that would be missed by traditional MWE extraction, and so research in this area has tended to focus on how particular formulaic phrases (e.g., if you look at) are indicative of particular genres (e.g., university lectures).",
                "Lexical bundles have been applied, in particular, to learner language: for example, #REF show that non-native student writers use a severely restricted range of lexical bundle types, and tend to overuse those types, while #REF investigate the role of proficiency, demonstrating that intermediate learners underuse lower-frequency bigrams and overuse high-frequency bigrams relative to advanced learners.",
                "#REF demonstrate that improving fluency (closely linked to the use of linguistic formulas) is more important than improving strict grammaticality with respect to native speaker judgments of non-native productions; #TARGET_REF explicitly argue for FS lexicons as a way to identify, track, and improve learner proficiency."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "One major motivation for looking beyond MWEs is the ability to carry out broader linguistic analyses. Within corpus linguistics, multiword sequences have been studied in the form of lexical bundles (#REF) , which are simply n-grams that occur above a certain frequency threshold. Like FS, lexical bundles generally involve larger phrasal chunks that would be missed by traditional MWE extraction, and so research in this area has tended to focus on how particular formulaic phrases (e.g., if you look at) are indicative of particular genres (e.g., university lectures). Lexical bundles have been applied, in particular, to learner language: for example, #REF show that non-native student writers use a severely restricted range of lexical bundle types, and tend to overuse those types, while #REF investigate the role of proficiency, demonstrating that intermediate learners underuse lower-frequency bigrams and overuse high-frequency bigrams relative to advanced learners. #REF demonstrate that improving fluency (closely linked to the use of linguistic formulas) is more important than improving strict grammaticality with respect to native speaker judgments of non-native productions; #TARGET_REF explicitly argue for FS lexicons as a way to identify, track, and improve learner proficiency.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#REF demonstrate that improving fluency (closely linked to the use of linguistic formulas) is more important than improving strict grammaticality with respect to native speaker judgments of non-native productions; #TARGET_REF explicitly argue for FS lexicons as a way to identify, track, and improve learner proficiency.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our approach to FS identification involves optimization of the total explanatory power of a lattice, where each node corresponds to an n-gram type.",
                "The explanatory power of the whole lattice is defined simply as a product of the explainedness of the individual nodes.",
                "Each node can be considered either \"on\" (is an FS) or \"off\" (is not an FS).",
                "The basis of the calculation of explainedness is the syntax-sensitive LPR association measure of #TARGET_REF , but it is calculated differently depending on the on/off status of the node as well as the status of the nodes in its vicinity.",
                "Nodes are linked based on n-gram subsumption and corpus overlap relationships (see Figure 2), with \"on\" nodes typically explaining other nodes."
            ],
            "label": [
                "USE",
                "DIFFER"
            ],
            "context": [
                1,
                1,
                1,
                1,
                0
            ]
        },
        "input": "Our approach to FS identification involves optimization of the total explanatory power of a lattice, where each node corresponds to an n-gram type. The explanatory power of the whole lattice is defined simply as a product of the explainedness of the individual nodes. Each node can be considered either \"on\" (is an FS) or \"off\" (is not an FS). The basis of the calculation of explainedness is the syntax-sensitive LPR association measure of #TARGET_REF , but it is calculated differently depending on the on/off status of the node as well as the status of the nodes in its vicinity. Nodes are linked based on n-gram subsumption and corpus overlap relationships (see Figure 2), with \"on\" nodes typically explaining other nodes.",
        "output": "{\"label\": [\"USE\", \"DIFFER\"], \"context\": [\"Our approach to FS identification involves optimization of the total explanatory power of a lattice, where each node corresponds to an n-gram type.\", \"The explanatory power of the whole lattice is defined simply as a product of the explainedness of the individual nodes.\", \"Each node can be considered either \\\"on\\\" (is an FS) or \\\"off\\\" (is not an FS).\", \"The basis of the calculation of explainedness is the syntax-sensitive LPR association measure of #TARGET_REF , but it is calculated differently depending on the on/off status of the node as well as the status of the nodes in its vicinity.\"]}"
    },
    {
        "gold": {
            "text": [
                "Since our primary association measure is an adaption of LPR, our approach in this section mostly follows #REF up until the last stage.",
                "An initial requirement of any such method is an n-gram frequency threshold, which we set to 1 instance per 10 million words, following #TARGET_REF .",
                "3 We include gapped or non-contiguous n-grams in our analysis, in acknowledgment of the fact that many languages have MWEs where the components can be \"separated\", including verb particle constructions in English (Dehé, 2002) , and noun-verb idioms in Japanese .",
                "Having said this, there are generally strong syntactic and length restrictions on what can constitute a gap (#REF) , which we capture in the form of a language-specific POS-based regular expression (see Section 4 for details).",
                "This greatly lowers the number of potentially gapped n-gram types, increasing precision and efficiency for negligible loss of recall."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Since our primary association measure is an adaption of LPR, our approach in this section mostly follows #REF up until the last stage. An initial requirement of any such method is an n-gram frequency threshold, which we set to 1 instance per 10 million words, following #TARGET_REF . 3 We include gapped or non-contiguous n-grams in our analysis, in acknowledgment of the fact that many languages have MWEs where the components can be \"separated\", including verb particle constructions in English (Dehé, 2002) , and noun-verb idioms in Japanese . Having said this, there are generally strong syntactic and length restrictions on what can constitute a gap (#REF) , which we capture in the form of a language-specific POS-based regular expression (see Section 4 for details). This greatly lowers the number of potentially gapped n-gram types, increasing precision and efficiency for negligible loss of recall.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"An initial requirement of any such method is an n-gram frequency threshold, which we set to 1 instance per 10 million words, following #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate our approach across three different languages, including evaluation sets derived from four different corpora selected for their size and linguistic diversity.",
                "In English, we follow #TARGET_REF in using a 890M token filtered portion of the ICWSM blog corpus (#REF ) tagged with the Tree Tagger (#REF) .",
                "To facilitate a comparison with #REF , which does not scale up to a corpus as large as the ICWSM, we also build a lexicon using the 100M token British National Corpus (#REF) , using the standard CLAWS-derived POS tags for the corpus.",
                "Lemmatization included removing all inflectional marking from both words and POS tags.",
                "For English, gaps are identified using the same POS regex used in #REF , which includes simple nouns and portions thereof, up to a maximum of 4 words."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We evaluate our approach across three different languages, including evaluation sets derived from four different corpora selected for their size and linguistic diversity. In English, we follow #TARGET_REF in using a 890M token filtered portion of the ICWSM blog corpus (#REF ) tagged with the Tree Tagger (#REF) . To facilitate a comparison with #REF , which does not scale up to a corpus as large as the ICWSM, we also build a lexicon using the 100M token British National Corpus (#REF) , using the standard CLAWS-derived POS tags for the corpus. Lemmatization included removing all inflectional marking from both words and POS tags. For English, gaps are identified using the same POS regex used in #REF , which includes simple nouns and portions thereof, up to a maximum of 4 words.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In English, we follow #TARGET_REF in using a 890M token filtered portion of the ICWSM blog corpus (#REF ) tagged with the Tree Tagger (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "To facilitate a comparison with #REF , which does not scale up to a corpus as large as the ICWSM, we also build a lexicon using the 100M token British National Corpus (#REF) , using the standard CLAWS-derived POS tags for the corpus.",
                "Lemmatization included removing all inflectional marking from both words and POS tags.",
                "For English, gaps are identified using the same POS regex used in #TARGET_REF , which includes simple nouns and portions thereof, up to a maximum of 4 words.",
                "The other two languages we include in our evaluation are Croatian and Japanese.",
                "Relative to English, both languages have freer word order: we were interested in probing the challenges associated with using an n-gram approach to FS identification in such languages."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "To facilitate a comparison with #REF , which does not scale up to a corpus as large as the ICWSM, we also build a lexicon using the 100M token British National Corpus (#REF) , using the standard CLAWS-derived POS tags for the corpus. Lemmatization included removing all inflectional marking from both words and POS tags. For English, gaps are identified using the same POS regex used in #TARGET_REF , which includes simple nouns and portions thereof, up to a maximum of 4 words. The other two languages we include in our evaluation are Croatian and Japanese. Relative to English, both languages have freer word order: we were interested in probing the challenges associated with using an n-gram approach to FS identification in such languages.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"For English, gaps are identified using the same POS regex used in #TARGET_REF , which includes simple nouns and portions thereof, up to a maximum of 4 words.\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that the identification of the best ratio across all possible choices of context, not just the largest, is important for longer FS, where the entire POS context alone might uniquely identify the phrase, resulting in the minimum LPR of 1 even for entirely formulaic sequences-an undesirable result.",
                "In the segmentation approach of #TARGET_REF , LPR for an entire span is calculated as a product of the individual LPRs, but here we will use the minimum LPR across the words in the sequence:",
                "Here, minLPR for a particular n-gram does not reflect the overall degree to which it holds together, but rather focuses on the word which is its weakest link.",
                "For example, in the case of be keep * under wraps (Figure 2 ), a general statistical metric might assign it a high score due to the strong association between keep and under or under and wraps, but minLPR is focused on the weaker relationship between be and the rest of the phrase.",
                "This makes it particularly suited to use in a lattice model of competing n-grams, where the choice of be keep * under wraps versus keep * under wraps should be based exactly on the extent to which be is an essential part of the phrase; the other affinities are, in effect, irrelevant, because they occur in the smaller n-gram as well."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Note that the identification of the best ratio across all possible choices of context, not just the largest, is important for longer FS, where the entire POS context alone might uniquely identify the phrase, resulting in the minimum LPR of 1 even for entirely formulaic sequences-an undesirable result. In the segmentation approach of #TARGET_REF , LPR for an entire span is calculated as a product of the individual LPRs, but here we will use the minimum LPR across the words in the sequence: Here, minLPR for a particular n-gram does not reflect the overall degree to which it holds together, but rather focuses on the word which is its weakest link. For example, in the case of be keep * under wraps (Figure 2 ), a general statistical metric might assign it a high score due to the strong association between keep and under or under and wraps, but minLPR is focused on the weaker relationship between be and the rest of the phrase. This makes it particularly suited to use in a lattice model of competing n-grams, where the choice of be keep * under wraps versus keep * under wraps should be based exactly on the extent to which be is an essential part of the phrase; the other affinities are, in effect, irrelevant, because they occur in the smaller n-gram as well.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"In the segmentation approach of #TARGET_REF , LPR for an entire span is calculated as a product of the individual LPRs, but here we will use the minimum LPR across the words in the sequence:\"]}"
    },
    {
        "gold": {
            "text": [
                "Other than the inclusion of new languages, our test sets differ from #TARGET_REF in two ways.",
                "One advantage of a type-based annotation approach, particularly with regards to annotation with a known subjective component, is that it is quite sensible to simply discard borderline cases, improving reliability at the cost of some representativeness.",
                "As such we entirely excluded from our test set n-grams which just one annotator marked as FS.",
                "Table 1 contains the counts for the four test sets after this filtering step and Fleiss' Kappa scores before (\"Pre\") and after (\"Post\").",
                "The second change is that for the main evaluation we collapsed gapped and contiguous n-grams into a single test set."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Other than the inclusion of new languages, our test sets differ from #TARGET_REF in two ways. One advantage of a type-based annotation approach, particularly with regards to annotation with a known subjective component, is that it is quite sensible to simply discard borderline cases, improving reliability at the cost of some representativeness. As such we entirely excluded from our test set n-grams which just one annotator marked as FS. Table 1 contains the counts for the four test sets after this filtering step and Fleiss' Kappa scores before (\"Pre\") and after (\"Post\"). The second change is that for the main evaluation we collapsed gapped and contiguous n-grams into a single test set.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Other than the inclusion of new languages, our test sets differ from #TARGET_REF in two ways.\"]}"
    },
    {
        "gold": {
            "text": [
                "The main results for FS acquisition across the four corpora are shown in Table 2 .",
                "As noted in Section 2, simple statistical association measures like PMI do poorly when faced with syntactically-unrestricted n-grams of variable length: minLPR is clearly a much better statistic for this purpose.",
                "The LPRseg method of #TARGET_REF consistently outperforms simple ranking, and the lattice method proposed here does better still, with a margin that is fairly consistent across the languages.",
                "Generally, clearing and overlap node interactions provide a relatively large increase in precision at the cost of a smaller drop in recall, though the change is fairly symmetrical in Croatian.",
                "When only covering is used, the results are fairly similar to #REF , which is unsurprising given the extent to which decomposition and covering are related."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The main results for FS acquisition across the four corpora are shown in Table 2 . As noted in Section 2, simple statistical association measures like PMI do poorly when faced with syntactically-unrestricted n-grams of variable length: minLPR is clearly a much better statistic for this purpose. The LPRseg method of #TARGET_REF consistently outperforms simple ranking, and the lattice method proposed here does better still, with a margin that is fairly consistent across the languages. Generally, clearing and overlap node interactions provide a relatively large increase in precision at the cost of a smaller drop in recall, though the change is fairly symmetrical in Croatian. When only covering is used, the results are fairly similar to #REF , which is unsurprising given the extent to which decomposition and covering are related.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The LPRseg method of #TARGET_REF consistently outperforms simple ranking, and the lattice method proposed here does better still, with a margin that is fairly consistent across the languages.\"]}"
    },
    {
        "gold": {
            "text": [
                "In English, for instance, it might learn that verb-particle combinations are generally likely to be FS, whereas verb-determiner combinations are not.",
                "Our initial investigations suggest, however, it may be difficult to apply this idea without merely amplifying existing undesirable biases in the LPR measure.",
                "Bringing in other information such as simple distributional statistics might help the model identify non-compositional semantics, and could, in combination with the existing lattice competition, focus the model on MWEs which could provide a reliable basis for generalization.",
                "For all four corpora, the lattice optimization algorithm converged within 10 iterations.",
                "Although the optimization of the lattice is several orders of magnitude more complex than the decomposition heuristics of #TARGET_REF , the time needed to build and optimize the lattice is a fraction of the time required to collect the statistics for LPR calculation, and so the end-to-end runtimes of the two methods are comparable."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "In English, for instance, it might learn that verb-particle combinations are generally likely to be FS, whereas verb-determiner combinations are not. Our initial investigations suggest, however, it may be difficult to apply this idea without merely amplifying existing undesirable biases in the LPR measure. Bringing in other information such as simple distributional statistics might help the model identify non-compositional semantics, and could, in combination with the existing lattice competition, focus the model on MWEs which could provide a reliable basis for generalization. For all four corpora, the lattice optimization algorithm converged within 10 iterations. Although the optimization of the lattice is several orders of magnitude more complex than the decomposition heuristics of #TARGET_REF , the time needed to build and optimize the lattice is a fraction of the time required to collect the statistics for LPR calculation, and so the end-to-end runtimes of the two methods are comparable.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Although the optimization of the lattice is several orders of magnitude more complex than the decomposition heuristics of #TARGET_REF , the time needed to build and optimize the lattice is a fraction of the time required to collect the statistics for LPR calculation, and so the end-to-end runtimes of the two methods are comparable.\"]}"
    },
    {
        "gold": {
            "text": [
                "The first step in the process is to derive a set of ngrams and related statistics from a large, unlabeled corpus of text.",
                "Since our primary association measure is an adaption of LPR, our approach in this section mostly follows #TARGET_REF up until the last stage.",
                "An initial requirement of any such method is an n-gram frequency threshold, which we set to 1 instance per 10 million words, following #REF .",
                "3 We include gapped or non-contiguous n-grams in our analysis, in acknowledgment of the fact that many languages have MWEs where the components can be \"separated\", including verb particle constructions in English (Dehé, 2002) , and noun-verb idioms in Japanese .",
                "Having said this, there are generally strong syntactic and length restrictions on what can constitute a gap (#REF) , which we capture in the form of a language-specific POS-based regular expression (see Section 4 for details)."
            ],
            "label": [
                "SIM",
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The first step in the process is to derive a set of ngrams and related statistics from a large, unlabeled corpus of text. Since our primary association measure is an adaption of LPR, our approach in this section mostly follows #TARGET_REF up until the last stage. An initial requirement of any such method is an n-gram frequency threshold, which we set to 1 instance per 10 million words, following #REF . 3 We include gapped or non-contiguous n-grams in our analysis, in acknowledgment of the fact that many languages have MWEs where the components can be \"separated\", including verb particle constructions in English (Dehé, 2002) , and noun-verb idioms in Japanese . Having said this, there are generally strong syntactic and length restrictions on what can constitute a gap (#REF) , which we capture in the form of a language-specific POS-based regular expression (see Section 4 for details).",
        "output": "{\"label\": [\"SIM\", \"EXT\"], \"context\": [\"Since our primary association measure is an adaption of LPR, our approach in this section mostly follows #TARGET_REF up until the last stage.\"]}"
    },
    {
        "gold": {
            "text": [
                "The LPRseg method of #REF consistently outperforms simple ranking, and the lattice method proposed here does better still, with a margin that is fairly consistent across the languages.",
                "Generally, clearing and overlap node interactions provide a relatively large increase in precision at the cost of a smaller drop in recall, though the change is fairly symmetrical in Croatian.",
                "When only covering is used, the results are fairly similar to #TARGET_REF , which is unsurprising given the extent to which decomposition and covering are related.",
                "The Japanese and ICWSM corpora have relatively high precision and low recall, whereas both the BNC and Croatian corpora have low precision and high recall.",
                "In the contiguous FS test set for the BNC (Ta- Table 2 : Results of FS identification in various test sets: Countrank = ranking with frequency; PMIrank = PMI-based ranking; minLPRrank = ranking with minLPR; LPRseg = the method of #REF ; \"−cl\" = no clearing; \"−ovr\" = no penalization of overlaps; \"P\" = Precision; \"R\" = Recall; and \"F\" = F-score."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The LPRseg method of #REF consistently outperforms simple ranking, and the lattice method proposed here does better still, with a margin that is fairly consistent across the languages. Generally, clearing and overlap node interactions provide a relatively large increase in precision at the cost of a smaller drop in recall, though the change is fairly symmetrical in Croatian. When only covering is used, the results are fairly similar to #TARGET_REF , which is unsurprising given the extent to which decomposition and covering are related. The Japanese and ICWSM corpora have relatively high precision and low recall, whereas both the BNC and Croatian corpora have low precision and high recall. In the contiguous FS test set for the BNC (Ta- Table 2 : Results of FS identification in various test sets: Countrank = ranking with frequency; PMIrank = PMI-based ranking; minLPRrank = ranking with minLPR; LPRseg = the method of #REF ; \"−cl\" = no clearing; \"−ovr\" = no penalization of overlaps; \"P\" = Precision; \"R\" = Recall; and \"F\" = F-score.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"When only covering is used, the results are fairly similar to #TARGET_REF , which is unsurprising given the extent to which decomposition and covering are related.\"]}"
    },
    {
        "gold": {
            "text": [
                "This is done as a preprocessing step, and greatly improves the tractability of the iterative optimization of the lattice.",
                "Of course, a threshold for hard covering must be chosen: during development we found that a ratio of 2/3 (corresponding to a significant majority of the counts of a lower node corresponding to the higher node) worked well.",
                "We also use the concept of hard covering to address the issue of pronouns, based on the observation that specific pronouns often have high LPR values due to pragmatic biases #TARGET_REF ; for instance, private state verbs like feel tend to have first person singular subjects.",
                "In the lattice, n-grams with pronouns are considered covered (inactive) unless they cover at least one other node which does not have a pronoun, which allows us to limit FS with pronouns without excluding them entirely: they are included only in cases where they are definitively formulaic.",
                "Soft covering is used in cases when a single ngram does not entirely account for another, but a turned-on n-gram to some extent may explain some of the statistical irregularity of one lower in the lattice."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "This is done as a preprocessing step, and greatly improves the tractability of the iterative optimization of the lattice. Of course, a threshold for hard covering must be chosen: during development we found that a ratio of 2/3 (corresponding to a significant majority of the counts of a lower node corresponding to the higher node) worked well. We also use the concept of hard covering to address the issue of pronouns, based on the observation that specific pronouns often have high LPR values due to pragmatic biases #TARGET_REF ; for instance, private state verbs like feel tend to have first person singular subjects. In the lattice, n-grams with pronouns are considered covered (inactive) unless they cover at least one other node which does not have a pronoun, which allows us to limit FS with pronouns without excluding them entirely: they are included only in cases where they are definitively formulaic. Soft covering is used in cases when a single ngram does not entirely account for another, but a turned-on n-gram to some extent may explain some of the statistical irregularity of one lower in the lattice.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"We also use the concept of hard covering to address the issue of pronouns, based on the observation that specific pronouns often have high LPR values due to pragmatic biases #TARGET_REF ; for instance, private state verbs like feel tend to have first person singular subjects.\"]}"
    },
    {
        "gold": {
            "text": [
                "We build on recent work trying to learn where to split merged networks [21] , as well as work trying to learn how best to combine private and shared subspaces [5, 18] .",
                "Our model is empirically justified and deals with the dirtiness [16] of loosely related tasks.",
                "We show that it is a generalization of various multi-task learning algorithms such as hard parameter sharing [7] , low supervision #TARGET_REF , and cross-stitch networks [21] , as well as transfer learning algorithms such as frustratingly easy domain adaptation [9] .",
                "Moreover, we study what task properties predict gains, and what properties correlate with learning certain types of sharing, as well as the inductive bias of the resulting architecture.",
                "Figure 1 : A SLUICE NETWORK with one main task A and one auxiliary task B. It consists of a shared input layer (shown left), two task-specific output layers (right), and three hidden layers per task, each partitioned into two subspaces."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "We build on recent work trying to learn where to split merged networks [21] , as well as work trying to learn how best to combine private and shared subspaces [5, 18] . Our model is empirically justified and deals with the dirtiness [16] of loosely related tasks. We show that it is a generalization of various multi-task learning algorithms such as hard parameter sharing [7] , low supervision #TARGET_REF , and cross-stitch networks [21] , as well as transfer learning algorithms such as frustratingly easy domain adaptation [9] . Moreover, we study what task properties predict gains, and what properties correlate with learning certain types of sharing, as well as the inductive bias of the resulting architecture. Figure 1 : A SLUICE NETWORK with one main task A and one auxiliary task B. It consists of a shared input layer (shown left), two task-specific output layers (right), and three hidden layers per task, each partitioned into two subspaces.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Our model is empirically justified and deals with the dirtiness [16] of loosely related tasks.\", \"We show that it is a generalization of various multi-task learning algorithms such as hard parameter sharing [7] , low supervision #TARGET_REF , and cross-stitch networks [21] , as well as transfer learning algorithms such as frustratingly easy domain adaptation [9] .\"]}"
    },
    {
        "gold": {
            "text": [
                "The architecture is very flexible and can be seen as a generalization over several existing algorithms for transfer and multi-task learning, including [7, 9, #TARGET_REF 21] .",
                "We show how to derive each of these below.",
                "Hard Parameter Sharing in the two networks appears if all α values are set to the same constant [7, 8] .",
                "This is equivalent to a mean-constrained 0-regularizer Ω(·) = | · |w i 0 and i λiLi < 1. If the sum of weighted losses are smaller than 1, the loss with penalty is always the highest when all parameters are shared.",
                "Group Lasso The 1/ 2 group lasso regularizer is G g=1 ||G1,i,g||2, a weighted sum over the 2 norms of the groups, often used to enforce subspace sharing [33, 26] ."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The architecture is very flexible and can be seen as a generalization over several existing algorithms for transfer and multi-task learning, including [7, 9, #TARGET_REF 21] . We show how to derive each of these below. Hard Parameter Sharing in the two networks appears if all α values are set to the same constant [7, 8] . This is equivalent to a mean-constrained 0-regularizer Ω(·) = | · |w i 0 and i λiLi < 1. If the sum of weighted losses are smaller than 1, the loss with penalty is always the highest when all parameters are shared. Group Lasso The 1/ 2 group lasso regularizer is G g=1 ||G1,i,g||2, a weighted sum over the 2 norms of the groups, often used to enforce subspace sharing [33, 26] .",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"The architecture is very flexible and can be seen as a generalization over several existing algorithms for transfer and multi-task learning, including [7, 9, #TARGET_REF 21] .\"]}"
    },
    {
        "gold": {
            "text": [
                "dataset provides data annotated for an array of tasks across different languages and domains.",
                "We present experiments with the English portions of datasets, for which we show statistics in Table 1.",
                "2 Tasks In multi-task learning, one task is usually considered the main task, while other tasks are used as auxiliary tasks to improve performance on the main task.",
                "As main tasks, we use chunking (CHUNK), named entity recognition (NER), and a simplified version of semantic role labeling (SRL) where we only identify headwords, and pair them with part-of-speech tagging (POS) as an auxiliary task, following #TARGET_REF .",
                "Example annotations for each task can be found in Table 2 ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "dataset provides data annotated for an array of tasks across different languages and domains. We present experiments with the English portions of datasets, for which we show statistics in Table 1. 2 Tasks In multi-task learning, one task is usually considered the main task, while other tasks are used as auxiliary tasks to improve performance on the main task. As main tasks, we use chunking (CHUNK), named entity recognition (NER), and a simplified version of semantic role labeling (SRL) where we only identify headwords, and pair them with part-of-speech tagging (POS) as an auxiliary task, following #TARGET_REF . Example annotations for each task can be found in Table 2 .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"As main tasks, we use chunking (CHUNK), named entity recognition (NER), and a simplified version of semantic role labeling (SRL) where we only identify headwords, and pair them with part-of-speech tagging (POS) as an auxiliary task, following #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the same hyperparameters for all comparison models across all domains.",
                "We train our models on each domain and evaluate them both on the in-domain test set as well as on the test sets of all other domains to evaluate their out-of-domain generalization ability.",
                "Baseline Models As baselines, we compare against i) a single-task model only trained on chunking; ii) the low supervision model by #TARGET_REF , which predicts the auxiliary task at the first layer; iii) an MTL model based on hard parameter sharing [6] ; and iv) cross-stitch networks [21] .",
                "We compare these against our complete sluice network with subspace constraints and learned α and β parameters.",
                "We provide a detailed ablation analysis of our model in Section 5."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We use the same hyperparameters for all comparison models across all domains. We train our models on each domain and evaluate them both on the in-domain test set as well as on the test sets of all other domains to evaluate their out-of-domain generalization ability. Baseline Models As baselines, we compare against i) a single-task model only trained on chunking; ii) the low supervision model by #TARGET_REF , which predicts the auxiliary task at the first layer; iii) an MTL model based on hard parameter sharing [6] ; and iv) cross-stitch networks [21] . We compare these against our complete sluice network with subspace constraints and learned α and β parameters. We provide a detailed ablation analysis of our model in Section 5.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Baseline Models As baselines, we compare against i) a single-task model only trained on chunking; ii) the low supervision model by #TARGET_REF , which predicts the auxiliary task at the first layer; iii) an MTL model based on hard parameter sharing [6] ; and iv) cross-stitch networks [21] .\"]}"
    },
    {
        "gold": {
            "text": [
                "L K .",
                "We assume that all the deep networks have the same hyper-parameters at the outset.",
                "With loosely related tasks, one task may be better modeled with one hidden layer; another one with two #TARGET_REF .",
                "Our architecture, however, is flexible enough to learn this, if we initially associate each task with the union of the a priori task networks.",
                "Let W ∈ R M ×D be a matrix in which each row i corresponds to a model θ i with D parameters."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "L K . We assume that all the deep networks have the same hyper-parameters at the outset. With loosely related tasks, one task may be better modeled with one hidden layer; another one with two #TARGET_REF . Our architecture, however, is flexible enough to learn this, if we initially associate each task with the union of the a priori task networks. Let W ∈ R M ×D be a matrix in which each row i corresponds to a model θ i with D parameters.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"With loosely related tasks, one task may be better modeled with one hidden layer; another one with two #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The approach to domain adaptation in [9] , which relies on a shared and a private space for each task or domain, can be encoded in sluice networks by setting all αA,B-and αB,A-weights associated with G i,k,1 to 0, while setting all αA,B-weights associated with G i,k,2 to αB,B, and αB,A-weights associated with G i,k,2 to αA,A. Note that [9] discusses three subspaces.",
                "We split the space in two, leading to three subspaces, if we only share one half across the two networks.",
                "Low Supervision #TARGET_REF propose a model where only the inner layers of two deep recurrent works are shared.",
                "This is obtained using heavy mean-constrained L0 regularization over the first layer Li,1, e.g., Ω(W ) = K i ||Li,1||0 with i λiL(i) < 1, while for the auxiliary task, only the first layer β parameter is set to 1.",
                "Cross-Stitch #REF introduce cross-stitch networks that have α values control the flow between layers of two convolutional neural networks."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The approach to domain adaptation in [9] , which relies on a shared and a private space for each task or domain, can be encoded in sluice networks by setting all αA,B-and αB,A-weights associated with G i,k,1 to 0, while setting all αA,B-weights associated with G i,k,2 to αB,B, and αB,A-weights associated with G i,k,2 to αA,A. Note that [9] discusses three subspaces. We split the space in two, leading to three subspaces, if we only share one half across the two networks. Low Supervision #TARGET_REF propose a model where only the inner layers of two deep recurrent works are shared. This is obtained using heavy mean-constrained L0 regularization over the first layer Li,1, e.g., Ω(W ) = K i ||Li,1||0 with i λiL(i) < 1, while for the auxiliary task, only the first layer β parameter is set to 1. Cross-Stitch #REF introduce cross-stitch networks that have α values control the flow between layers of two convolutional neural networks.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Low Supervision #TARGET_REF propose a model where only the inner layers of two deep recurrent works are shared.\"]}"
    },
    {
        "gold": {
            "text": [
                "Sluice networks not only learn the parameters in W , but also some of the parameters of the regularizer Ω, through the α weights, while the β weights are used to learn the parameters of the mixture functions f (·).",
                "Recall that our architecture is partly motivated by the observation that for loosely related tasks, only certain features in specific layers should be shared, while many of the layers and subspaces may remain more taskspecific #TARGET_REF .",
                "We want to learn what to share while inducing models for the different tasks.",
                "For simplicity, we ignore subspaces at first and assume only two tasks A and B. The outputs h A,k,t and h B,k,t of the k-th layer for time step t for task A and B respectively interact through what [21] refer to as cross-stitch units α (see Figure  1 ).",
                "Omitting t for simplicity, the output of the α layers is:"
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Sluice networks not only learn the parameters in W , but also some of the parameters of the regularizer Ω, through the α weights, while the β weights are used to learn the parameters of the mixture functions f (·). Recall that our architecture is partly motivated by the observation that for loosely related tasks, only certain features in specific layers should be shared, while many of the layers and subspaces may remain more taskspecific #TARGET_REF . We want to learn what to share while inducing models for the different tasks. For simplicity, we ignore subspaces at first and assume only two tasks A and B. The outputs h A,k,t and h B,k,t of the k-th layer for time step t for task A and B respectively interact through what [21] refer to as cross-stitch units α (see Figure  1 ). Omitting t for simplicity, the output of the α layers is:",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Recall that our architecture is partly motivated by the observation that for loosely related tasks, only certain features in specific layers should be shared, while many of the layers and subspaces may remain more taskspecific #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "**LEARNING MIXTURES MANY TASKS HAVE AN IMPLICIT HIERARCHY THAT INFORMS THEIR INTERACTION. RATHER THAN**",
                "predefining it #TARGET_REF 11] , we enable our model to learn hierarchical relations by associating different tasks with different layers if this is beneficial for learning.",
                "Inspired by advances in residual learning [12] , we employ skip-connections from each layer, controlled using β parameters.",
                "This layer acts as a mixture model, returning a mixture of expert predictions:",
                "where h A,k is the output of layer k of model A, while hA,t is the linear combination of all layer outputs of model A that is fed into the final softmax layer."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "**LEARNING MIXTURES MANY TASKS HAVE AN IMPLICIT HIERARCHY THAT INFORMS THEIR INTERACTION. RATHER THAN** predefining it #TARGET_REF 11] , we enable our model to learn hierarchical relations by associating different tasks with different layers if this is beneficial for learning. Inspired by advances in residual learning [12] , we employ skip-connections from each layer, controlled using β parameters. This layer acts as a mixture model, returning a mixture of expert predictions: where h A,k is the output of layer k of model A, while hA,t is the linear combination of all layer outputs of model A that is fed into the final softmax layer.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"**LEARNING MIXTURES MANY TASKS HAVE AN IMPLICIT HIERARCHY THAT INFORMS THEIR INTERACTION. RATHER THAN**\", \"predefining it #TARGET_REF 11] , we enable our model to learn hierarchical relations by associating different tasks with different layers if this is beneficial for learning.\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally, concatenation of layer outputs is a viable form to share information across layers.",
                "Chunking, NER, and SRL.",
                "We present inner, middle, and outer layer left to right.",
                "Figure 2 presents the final α weights in the sluice networks for Chunking, NER, and SRL, trained with newswire as training data.",
                "We see that a) for the low-level simplified SRL, there is more sharing at inner layers, which is in line with #TARGET_REF , while Chunking and NER also rely on the outer layer, and b) more information is shared from the more complex target tasks than vice versa."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Finally, concatenation of layer outputs is a viable form to share information across layers. Chunking, NER, and SRL. We present inner, middle, and outer layer left to right. Figure 2 presents the final α weights in the sluice networks for Chunking, NER, and SRL, trained with newswire as training data. We see that a) for the low-level simplified SRL, there is more sharing at inner layers, which is in line with #TARGET_REF , while Chunking and NER also rely on the outer layer, and b) more information is shared from the more complex target tasks than vice versa.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We see that a) for the low-level simplified SRL, there is more sharing at inner layers, which is in line with #TARGET_REF , while Chunking and NER also rely on the outer layer, and b) more information is shared from the more complex target tasks than vice versa.\"]}"
    },
    {
        "gold": {
            "text": [
                "Ability to Fit Noise Sluice networks can learn to disregard sharing completely, so we expect them to be as good as single-task networks to fit random noise, potentially even better.",
                "We verify this by computing a learning curve for random relabelings of 200 sentences annotated with syntactic chunking brackets, as well as 100 gold standard POS-annotated sentences.",
                "The figure in 3 shows that hard parameter sharing, while learning faster because of the smoother loss surface in multi-task learning, is a good regularizer, confirming the findings in #TARGET_REF , whereas the sluice network is even better at fitting noise than the single-task models.",
                "While ability to fit noise is not necessarily a problem [32] , this means that it can be beneficial to add inductive bias to the regularizer, especially when working with small amounts of data."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0
            ]
        },
        "input": "Ability to Fit Noise Sluice networks can learn to disregard sharing completely, so we expect them to be as good as single-task networks to fit random noise, potentially even better. We verify this by computing a learning curve for random relabelings of 200 sentences annotated with syntactic chunking brackets, as well as 100 gold standard POS-annotated sentences. The figure in 3 shows that hard parameter sharing, while learning faster because of the smoother loss surface in multi-task learning, is a good regularizer, confirming the findings in #TARGET_REF , whereas the sluice network is even better at fitting noise than the single-task models. While ability to fit noise is not necessarily a problem [32] , this means that it can be beneficial to add inductive bias to the regularizer, especially when working with small amounts of data.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"The figure in 3 shows that hard parameter sharing, while learning faster because of the smoother loss surface in multi-task learning, is a good regularizer, confirming the findings in #TARGET_REF , whereas the sluice network is even better at fitting noise than the single-task models.\"]}"
    },
    {
        "gold": {
            "text": [
                "Deep neural network models have demonstrated strong performance on a number of challenging tasks, such as image classification (#REF) , speech recognition (#REF) , and various natural language processing tasks #REF; #REF) .",
                "Recently, the augmentation of neural networks with external memory components has been shown to be a powerful means of capturing context of different types (#REF #REF) .",
                "Of particular interest to this work is the work by #TARGET_REF , on end-toend memory networks (N2Ns), which exhibit remarkable reasoning capabilities, e.g. for reasoning and goal-oriented dialogue tasks .",
                "Typically, such tasks consist of three key components: a sequence of supporting facts (the story), a question, and its answer.",
                "An example task is given in Figure 1 ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Deep neural network models have demonstrated strong performance on a number of challenging tasks, such as image classification (#REF) , speech recognition (#REF) , and various natural language processing tasks #REF; #REF) . Recently, the augmentation of neural networks with external memory components has been shown to be a powerful means of capturing context of different types (#REF #REF) . Of particular interest to this work is the work by #TARGET_REF , on end-toend memory networks (N2Ns), which exhibit remarkable reasoning capabilities, e.g. for reasoning and goal-oriented dialogue tasks . Typically, such tasks consist of three key components: a sequence of supporting facts (the story), a question, and its answer. An example task is given in Figure 1 .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Of particular interest to this work is the work by #TARGET_REF , on end-toend memory networks (N2Ns), which exhibit remarkable reasoning capabilities, e.g. for reasoning and goal-oriented dialogue tasks .\"]}"
    },
    {
        "gold": {
            "text": [
                "End-to-End Memory Networks: Building on top of memory networks , #TARGET_REF ing the memory position supervision and making the model trainable in an end-to-end fashion, through the advent of supporting memories and a memory access controller.",
                "Representations of the context sentences x 1 , . . . , x n in the story are encoded using two sets of embedding matrices A and C (both of size d ⇥ |V | where d is the embedding size and |V | the vocabulary size), and stored in the input and output memory cells m 1 , . . . , m n and c 1 , . . . , c n , each of which is obtained via",
                "is a function that maps the input into a bag of dimension |V |.",
                "The input question q is encoded with another embedding matrix B 2 R d⇥|V | such that u = B (q).",
                "N2N utilises the question embedding u and the input memory representations m i to measure the relevance between the question and each supporting context sentence, resulting in a vector of attention weights:"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "End-to-End Memory Networks: Building on top of memory networks , #TARGET_REF ing the memory position supervision and making the model trainable in an end-to-end fashion, through the advent of supporting memories and a memory access controller. Representations of the context sentences x 1 , . . . , x n in the story are encoded using two sets of embedding matrices A and C (both of size d ⇥ |V | where d is the embedding size and |V | the vocabulary size), and stored in the input and output memory cells m 1 , . . . , m n and c 1 , . . . , c n , each of which is obtained via is a function that maps the input into a bag of dimension |V |. The input question q is encoded with another embedding matrix B 2 R d⇥|V | such that u = B (q). N2N utilises the question embedding u and the input memory representations m i to measure the relevance between the question and each supporting context sentence, resulting in a vector of attention weights:",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"End-to-End Memory Networks: Building on top of memory networks , #TARGET_REF ing the memory position supervision and making the model trainable in an end-to-end fashion, through the advent of supporting memories and a memory access controller.\"]}"
    },
    {
        "gold": {
            "text": [
                "N2N utilises the question embedding u and the input memory representations m i to measure the relevance between the question and each supporting context sentence, resulting in a vector of attention weights:",
                "where softmax(a i ) = e a i P j e a j .",
                "Once the attention weights have been computed, the memory access controller receives the response o in the form of a weighted sum over the output memory representations:",
                "To enhance the model's ability to cope with more challenging tasks requiring multiple supporting facts from the memory, #TARGET_REF further extended the model by stacking multiple memory layers (also known as \"hops\"), in which case the output of the k th hop is taken as input to the (k + 1) th hop:",
                "Lastly, N2N predicts the answer to question q using a softmax function: whereŷ is the predicted answer distribution, W 2 R |V |⇥d is a parameter matrix for the model to learn (note that in the context of bAbI tasks, answers are single words), and K is the total number of hops."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "N2N utilises the question embedding u and the input memory representations m i to measure the relevance between the question and each supporting context sentence, resulting in a vector of attention weights: where softmax(a i ) = e a i P j e a j . Once the attention weights have been computed, the memory access controller receives the response o in the form of a weighted sum over the output memory representations: To enhance the model's ability to cope with more challenging tasks requiring multiple supporting facts from the memory, #TARGET_REF further extended the model by stacking multiple memory layers (also known as \"hops\"), in which case the output of the k th hop is taken as input to the (k + 1) th hop: Lastly, N2N predicts the answer to question q using a softmax function: whereŷ is the predicted answer distribution, W 2 R |V |⇥d is a parameter matrix for the model to learn (note that in the context of bAbI tasks, answers are single words), and K is the total number of hops.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"To enhance the model's ability to cope with more challenging tasks requiring multiple supporting facts from the memory, #TARGET_REF further extended the model by stacking multiple memory layers (also known as \\\"hops\\\"), in which case the output of the k th hop is taken as input to the (k + 1) th hop:\"]}"
    },
    {
        "gold": {
            "text": [
                "In #TARGET_REF , two types of weight tying were explored for N2N, namely adjacent (\"ADJ\") and layer-wise (\"LW\").",
                "With LW, the input and output embedding matrices are shared across different hops (i.e., A 1 = A 2 = . . . = A K and C 1 = C 2 = . . . = C K ), resembling RNNs.",
                "With ADJ, on the other hand, not only is the output embedding for a given layer shared with the corresponding input embedding (i.e., A k+1 = C k ), the answer prediction matrix W and question embedding matrix B are also constrained such that W > = C K and B = A 1 .",
                "While both ADJ and LW work well, achieving comparable overall performance in terms of mean error over the 20 bAbI tasks, their performance on a subset of the tasks (i.e., tasks 3, 16, 17 and 19, as shown in Table 1 ) is inconsistent, with one performing very well, and the other performing poorly.",
                "Based on this observation, we propose a unified weight tying mechanism exploiting the benefits of both ADJ and LW, and capable of dynamically determining the best weight tying approach for a given task."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In #TARGET_REF , two types of weight tying were explored for N2N, namely adjacent (\"ADJ\") and layer-wise (\"LW\"). With LW, the input and output embedding matrices are shared across different hops (i.e., A 1 = A 2 = . . . = A K and C 1 = C 2 = . . . = C K ), resembling RNNs. With ADJ, on the other hand, not only is the output embedding for a given layer shared with the corresponding input embedding (i.e., A k+1 = C k ), the answer prediction matrix W and question embedding matrix B are also constrained such that W > = C K and B = A 1 . While both ADJ and LW work well, achieving comparable overall performance in terms of mean error over the 20 bAbI tasks, their performance on a subset of the tasks (i.e., tasks 3, 16, 17 and 19, as shown in Table 1 ) is inconsistent, with one performing very well, and the other performing poorly. Based on this observation, we propose a unified weight tying mechanism exploiting the benefits of both ADJ and LW, and capable of dynamically determining the best weight tying approach for a given task.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In #TARGET_REF , two types of weight tying were explored for N2N, namely adjacent (\\\"ADJ\\\") and layer-wise (\\\"LW\\\").\"]}"
    },
    {
        "gold": {
            "text": [
                "With ADJ, on the other hand, not only is the output embedding for a given layer shared with the corresponding input embedding (i.e., A k+1 = C k ), the answer prediction matrix W and question embedding matrix B are also constrained such that W > = C K and B = A 1 .",
                "While both ADJ and LW work well, achieving comparable overall performance in terms of mean error over the 20 bAbI tasks, their performance on a subset of the tasks (i.e., tasks 3, 16, 17 and 19, as shown in Table 1 ) is inconsistent, with one performing very well, and the other performing poorly.",
                "Based on this observation, we propose a unified weight tying mechanism exploiting the benefits of both ADJ and LW, and capable of dynamically determining the best weight tying approach for a given task.",
                "Table 1 : Accuracy (%) reported in #TARGET_REF on a selected subset of the 20 bAbI 10k tasks.",
                "Note that performance in the LW column is obtained with a larger embedding size d = 100 and ReLU non-linearity applied to the internal state after each hop."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "With ADJ, on the other hand, not only is the output embedding for a given layer shared with the corresponding input embedding (i.e., A k+1 = C k ), the answer prediction matrix W and question embedding matrix B are also constrained such that W > = C K and B = A 1 . While both ADJ and LW work well, achieving comparable overall performance in terms of mean error over the 20 bAbI tasks, their performance on a subset of the tasks (i.e., tasks 3, 16, 17 and 19, as shown in Table 1 ) is inconsistent, with one performing very well, and the other performing poorly. Based on this observation, we propose a unified weight tying mechanism exploiting the benefits of both ADJ and LW, and capable of dynamically determining the best weight tying approach for a given task. Table 1 : Accuracy (%) reported in #TARGET_REF on a selected subset of the 20 bAbI 10k tasks. Note that performance in the LW column is obtained with a larger embedding size d = 100 and ReLU non-linearity applied to the internal state after each hop.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Table 1 : Accuracy (%) reported in #TARGET_REF on a selected subset of the 20 bAbI 10k tasks.\"]}"
    },
    {
        "gold": {
            "text": [
                "Given the first two as input, it is the model's job to reason over the supporting facts and predict the answer to the question.",
                "One drawback of N2Ns is the problem of choosing between two types of weight tying (adjacent and layer-wise; see Section 2 for a technical description).",
                "While N2Ns generally work well with either weight tying approach, as reported in #TARGET_REF , the performance is uneven on some difficult tasks.",
                "That is, for some tasks, one weight tying approach attains nearperfect accuracy and the other performs poorly, but for other tasks, this trend is reversed.",
                "In this paper, focusing on improving N2N, we propose a unified model, UN2N, capable of dynamically determining the appropriate type of weight tying for a given task."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Given the first two as input, it is the model's job to reason over the supporting facts and predict the answer to the question. One drawback of N2Ns is the problem of choosing between two types of weight tying (adjacent and layer-wise; see Section 2 for a technical description). While N2Ns generally work well with either weight tying approach, as reported in #TARGET_REF , the performance is uneven on some difficult tasks. That is, for some tasks, one weight tying approach attains nearperfect accuracy and the other performs poorly, but for other tasks, this trend is reversed. In this paper, focusing on improving N2N, we propose a unified model, UN2N, capable of dynamically determining the appropriate type of weight tying for a given task.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"While N2Ns generally work well with either weight tying approach, as reported in #TARGET_REF , the performance is uneven on some difficult tasks.\", \"That is, for some tasks, one weight tying approach attains nearperfect accuracy and the other performs poorly, but for other tasks, this trend is reversed.\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that the gating vector z can be replaced by a gating scalar z, but we choose to use a vector for more fine-grained control as in LSTMs (#REF) and GRUs .",
                "To simplify the model, we constrain B and W > to share the same parameters as A 1 and C K .",
                "Moreover, following #TARGET_REF , we add a linear mapping H 2 R d⇥d to the update connection between memory hops, but in our case, down-weight it by 1 z, resulting in:",
                "Regularisation: In order to prevent the input and output embedding matrices A k and C k from being dominated by the unconstrained embedding matrices, it is necessary to restrain the magnitude of the values inÃ 1 andC k .",
                "Therefore, in addition to the cross entropy loss over N training instances:"
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Note that the gating vector z can be replaced by a gating scalar z, but we choose to use a vector for more fine-grained control as in LSTMs (#REF) and GRUs . To simplify the model, we constrain B and W > to share the same parameters as A 1 and C K . Moreover, following #TARGET_REF , we add a linear mapping H 2 R d⇥d to the update connection between memory hops, but in our case, down-weight it by 1 z, resulting in: Regularisation: In order to prevent the input and output embedding matrices A k and C k from being dominated by the unconstrained embedding matrices, it is necessary to restrain the magnitude of the values inÃ 1 andC k . Therefore, in addition to the cross entropy loss over N training instances:",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Moreover, following #TARGET_REF , we add a linear mapping H 2 R d\\u21e5d to the update connection between memory hops, but in our case, down-weight it by 1 z, resulting in:\"]}"
    },
    {
        "gold": {
            "text": [
                "either 1k or 10k training instances per task.",
                "In this work, we focus exclusively on the 10k version.",
                "3",
                "Training Details: Following #TARGET_REF, we hold out 10% of the bAbI training set to form a development set.",
                "Position encoding and temporal encoding (with 10% random noise) are also incorporated into the model."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "either 1k or 10k training instances per task. In this work, we focus exclusively on the 10k version. 3 Training Details: Following #TARGET_REF, we hold out 10% of the bAbI training set to form a development set. Position encoding and temporal encoding (with 10% random noise) are also incorporated into the model.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Training Details: Following #TARGET_REF, we hold out 10% of the bAbI training set to form a development set.\"]}"
    },
    {
        "gold": {
            "text": [
                "Training is performed over 100 epochs with a batch size of 32 using the Adam optimiser (#REF) with a learning rate of 0.005.",
                "Following #TARGET_REF , linear start is employed in all our experiments for the first 20 epochs.",
                "All weight parameters are initialised based on a Gaussian distribution with zero mean and = 0.1.",
                "Gradients with an`2 norm of 40 are divided by a scalar to have norm 40.",
                "Also following #REF , we use only the most recent 50 sentences as the memory and set the number of memory hops to 3, the embedding size to 20, and to"
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Training is performed over 100 epochs with a batch size of 32 using the Adam optimiser (#REF) with a learning rate of 0.005. Following #TARGET_REF , linear start is employed in all our experiments for the first 20 epochs. All weight parameters are initialised based on a Gaussian distribution with zero mean and = 0.1. Gradients with an`2 norm of 40 are divided by a scalar to have norm 40. Also following #REF , we use only the most recent 50 sentences as the memory and set the number of memory hops to 3, the embedding size to 20, and to",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following #TARGET_REF , linear start is employed in all our experiments for the first 20 epochs.\"]}"
    },
    {
        "gold": {
            "text": [
                "Training is performed over 100 epochs with a batch size of 32 using the Adam optimiser (#REF) with a learning rate of 0.005.",
                "Following #REF , linear start is employed in all our experiments for the first 20 epochs.",
                "All weight parameters are initialised based on a Gaussian distribution with zero mean and = 0.1.",
                "Gradients with an`2 norm of 40 are divided by a scalar to have norm 40.",
                "Also following #TARGET_REF , we use only the most recent 50 sentences as the memory and set the number of memory hops to 3, the embedding size to 20, and to"
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Training is performed over 100 epochs with a batch size of 32 using the Adam optimiser (#REF) with a learning rate of 0.005. Following #REF , linear start is employed in all our experiments for the first 20 epochs. All weight parameters are initialised based on a Gaussian distribution with zero mean and = 0.1. Gradients with an`2 norm of 40 are divided by a scalar to have norm 40. Also following #TARGET_REF , we use only the most recent 50 sentences as the memory and set the number of memory hops to 3, the embedding size to 20, and to",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Also following #TARGET_REF , we use only the most recent 50 sentences as the memory and set the number of memory hops to 3, the embedding size to 20, and to\"]}"
    },
    {
        "gold": {
            "text": [
                "The results on the 20 bAbI QA tasks are presented in Table 3 .",
                "We benchmark against other memory network based models: (1) N2N with ADJ and LW #TARGET_REF ; (2) DMN (#REF) and its improved version DMN+ (#REF) ; and (3) GN2N (#REF) .",
                "Major improvements on the difficult tasks.",
                "The most noticeable performance gains are over tasks 16, 17 and 19 where, compared with the vanilla N2N, UN2N achieves much better results than the worst of ADJ and LW, surpassing both ADJ and LW in the case of tasks 17 and 18.",
                "This confirms the validity of the model."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The results on the 20 bAbI QA tasks are presented in Table 3 . We benchmark against other memory network based models: (1) N2N with ADJ and LW #TARGET_REF ; (2) DMN (#REF) and its improved version DMN+ (#REF) ; and (3) GN2N (#REF) . Major improvements on the difficult tasks. The most noticeable performance gains are over tasks 16, 17 and 19 where, compared with the vanilla N2N, UN2N achieves much better results than the worst of ADJ and LW, surpassing both ADJ and LW in the case of tasks 17 and 18. This confirms the validity of the model.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We benchmark against other memory network based models: (1) N2N with ADJ and LW #TARGET_REF ; (2) DMN (#REF) and its improved version DMN+ (#REF) ; and (3) GN2N (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Each of these 7 features indicates whether there are any exact matches between words in the candidate and those in the question or memory.",
                "We refer to these 7 features as the match features.",
                "In terms of the training procedure, experiments are carried out with the same configuration as described in Section 4.1.",
                "As a large variance can be observed due to how sensitive memory-based models are to parameter initialisation, following #TARGET_REF and (#REF) , we repeat each training 10 times using the Table 4 : Per-response accuracy on the Dialog bAbI tasks.",
                "N2N: ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Each of these 7 features indicates whether there are any exact matches between words in the candidate and those in the question or memory. We refer to these 7 features as the match features. In terms of the training procedure, experiments are carried out with the same configuration as described in Section 4.1. As a large variance can be observed due to how sensitive memory-based models are to parameter initialisation, following #TARGET_REF and (#REF) , we repeat each training 10 times using the Table 4 : Per-response accuracy on the Dialog bAbI tasks. N2N: .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"As a large variance can be observed due to how sensitive memory-based models are to parameter initialisation, following #TARGET_REF and (#REF) , we repeat each training 10 times using the Table 4 : Per-response accuracy on the Dialog bAbI tasks.\"]}"
    },
    {
        "gold": {
            "text": [
                "The results on the Dialog bAbI tasks are shown in Table 4 .",
                "In terms of baselines, we benchmark against other memory network-based models: 4 (1) N2N #TARGET_REF ; and (2) GN2N #TARGET_REF .",
                "While the results of GN2N is achieved with ADJ, the type of weight tying for N2N is not reported in .",
                "Improvements on task 5.",
                "It can be observed that UN2N offers consistent performance boost on task 5 across all experiments settings, especially in the non-OOV group."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The results on the Dialog bAbI tasks are shown in Table 4 . In terms of baselines, we benchmark against other memory network-based models: 4 (1) N2N #TARGET_REF ; and (2) GN2N #TARGET_REF . While the results of GN2N is achieved with ADJ, the type of weight tying for N2N is not reported in . Improvements on task 5. It can be observed that UN2N offers consistent performance boost on task 5 across all experiments settings, especially in the non-OOV group.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In terms of baselines, we benchmark against other memory network-based models: 4 (1) N2N #TARGET_REF ; and (2) GN2N #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We describe a machine learning based method to identify incorrect entries in translation memories.",
                "It extends previous work by #TARGET_REF through incorporating recall-based machine translation and part-of-speech-tagging features.",
                "Our system ranked first in the Binary Classification (II) task for two out of three language pairs: English-Italian and English-Spanish."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                1,
                0
            ]
        },
        "input": "We describe a machine learning based method to identify incorrect entries in translation memories. It extends previous work by #TARGET_REF through incorporating recall-based machine translation and part-of-speech-tagging features. Our system ranked first in the Binary Classification (II) task for two out of three language pairs: English-Italian and English-Spanish.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"It extends previous work by #TARGET_REF through incorporating recall-based machine translation and part-of-speech-tagging features.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we describe our submitted system for distinguishing correct from incorrect TUs.",
                "Rather than tailoring it to individual languages, we aimed at a languageindependent solution to cover all of the language pairs in this shared task or, looking to the future, Autodesk's production environments.",
                "The system is based on previous work by #TARGET_REF and uses language-independent features with language-specific plug-ins, such as machine translation, part-of-speech tagging, and language classification.",
                "Specifics about previous work are given in the next section.",
                "In Section 3, we describe our method and, in Section 4, show how it compares to Barbu's (2015) approach as well as other submissions to this shared task."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In this paper, we describe our submitted system for distinguishing correct from incorrect TUs. Rather than tailoring it to individual languages, we aimed at a languageindependent solution to cover all of the language pairs in this shared task or, looking to the future, Autodesk's production environments. The system is based on previous work by #TARGET_REF and uses language-independent features with language-specific plug-ins, such as machine translation, part-of-speech tagging, and language classification. Specifics about previous work are given in the next section. In Section 3, we describe our method and, in Section 4, show how it compares to Barbu's (2015) approach as well as other submissions to this shared task.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"The system is based on previous work by #TARGET_REF and uses language-independent features with language-specific plug-ins, such as machine translation, part-of-speech tagging, and language classification.\"]}"
    },
    {
        "gold": {
            "text": [
                "Rather than tailoring it to individual languages, we aimed at a languageindependent solution to cover all of the language pairs in this shared task or, looking to the future, Autodesk's production environments.",
                "The system is based on previous work by #REF and uses language-independent features with language-specific plug-ins, such as machine translation, part-of-speech tagging, and language classification.",
                "Specifics about previous work are given in the next section.",
                "In Section 3, we describe our method and, in Section 4, show how it compares to #TARGET_REF approach as well as other submissions to this shared task.",
                "Lastly, we offer preliminary conclusions and outline future work in Section 5."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Rather than tailoring it to individual languages, we aimed at a languageindependent solution to cover all of the language pairs in this shared task or, looking to the future, Autodesk's production environments. The system is based on previous work by #REF and uses language-independent features with language-specific plug-ins, such as machine translation, part-of-speech tagging, and language classification. Specifics about previous work are given in the next section. In Section 3, we describe our method and, in Section 4, show how it compares to #TARGET_REF approach as well as other submissions to this shared task. Lastly, we offer preliminary conclusions and outline future work in Section 5.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"In Section 3, we describe our method and, in Section 4, show how it compares to #TARGET_REF approach as well as other submissions to this shared task.\"]}"
    },
    {
        "gold": {
            "text": [
                "The \"most important\" of them, according to the author, were bisegment_similarity and lang_diff : the former is defined as the cosine similarity between a target segment and its machine translated source segment, while the latter denotes whether the language codes declared in a translation unit correspond with the codes detected by a language detector.",
                "The best classifier, a support vector machine with linear kernel, achieved 82% precision and 81% recall on a held-out test set of 309 TUs.",
                "To the best of our knowledge, Barbu provided the first and so far only research contribution on automatic TM cleaning, which the author himself described as \"a neglected research area\" #TARGET_REF .",
                "With our participation to this shared task, we seek to extend his work by examining new features based on statistical MT and POS tagging.",
                "As outlined above, comparing machine translated source segments to their actual target segments has proven effective in Barbu's (2015) experiments."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The \"most important\" of them, according to the author, were bisegment_similarity and lang_diff : the former is defined as the cosine similarity between a target segment and its machine translated source segment, while the latter denotes whether the language codes declared in a translation unit correspond with the codes detected by a language detector. The best classifier, a support vector machine with linear kernel, achieved 82% precision and 81% recall on a held-out test set of 309 TUs. To the best of our knowledge, Barbu provided the first and so far only research contribution on automatic TM cleaning, which the author himself described as \"a neglected research area\" #TARGET_REF . With our participation to this shared task, we seek to extend his work by examining new features based on statistical MT and POS tagging. As outlined above, comparing machine translated source segments to their actual target segments has proven effective in Barbu's (2015) experiments.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"To the best of our knowledge, Barbu provided the first and so far only research contribution on automatic TM cleaning, which the author himself described as \\\"a neglected research area\\\" #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Our feature extraction pipeline, including #TARGET_REF as well as our own features (see Section 3.1), is implemented in Scala.",
                "This pipeline is used to transform translation units into feature vectors and train classifiers using the scikitlearn framework (#REF) .",
                "From the various classification algorithms we tested, Random Forests performed best with our selection of features (see below)."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "Our feature extraction pipeline, including #TARGET_REF as well as our own features (see Section 3.1), is implemented in Scala. This pipeline is used to transform translation units into feature vectors and train classifiers using the scikitlearn framework (#REF) . From the various classification algorithms we tested, Random Forests performed best with our selection of features (see below).",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Our feature extraction pipeline, including #TARGET_REF as well as our own features (see Section 3.1), is implemented in Scala.\"]}"
    },
    {
        "gold": {
            "text": [
                "For the reasons mentioned in Section 1, we aimed at finding a combination of features that would perform well with all language pairs rather than tailoring solutions to individual languages.",
                "We focused on gearing our classifiers to distinguish correct or almost correct (classes 1, 2) from incorrect TUs (class 3) -i.e., the Binary Classification (II) task -by optimising the weighted F 1 -score (F 1 ) on training data (see Tables 2a and 2b) .",
                "From the various feature combinations we tested, we found the following to be most successful: ratio_words, pos_sim_all, language_detection, mt_cfs, mt_bleu, ratio_chars (as described in Section 3.1), alongside cg_score, only_capletters_dif, and punctuation_similarity (from #TARGET_REF .",
                "Evaluation results are given in the next section."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0
            ]
        },
        "input": "For the reasons mentioned in Section 1, we aimed at finding a combination of features that would perform well with all language pairs rather than tailoring solutions to individual languages. We focused on gearing our classifiers to distinguish correct or almost correct (classes 1, 2) from incorrect TUs (class 3) -i.e., the Binary Classification (II) task -by optimising the weighted F 1 -score (F 1 ) on training data (see Tables 2a and 2b) . From the various feature combinations we tested, we found the following to be most successful: ratio_words, pos_sim_all, language_detection, mt_cfs, mt_bleu, ratio_chars (as described in Section 3.1), alongside cg_score, only_capletters_dif, and punctuation_similarity (from #TARGET_REF . Evaluation results are given in the next section.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"From the various feature combinations we tested, we found the following to be most successful: ratio_words, pos_sim_all, language_detection, mt_cfs, mt_bleu, ratio_chars (as described in Section 3.1), alongside cg_score, only_capletters_dif, and punctuation_similarity (from #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "While eliminating almost correct TUs might decrease rather than increase MT quality, filtering out incorrect segments can have a positive impact (#REF) .",
                "Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by #TARGET_REF (Baseline 2).",
                "More importantly, however, we compared our system to Barbu's (2015) approach, using the classification algorithms which reportedly worked best with the 17 features in his work.",
                "Our system performed well in this comparison, surpassing Barbu's approach in all language pairs except en-de, where both systems were en par.",
                "Details are shown in Table 2a , where we report weighted precision (P), recall (R), and F 1 -scores averaged over 5-fold cross-validation with 2 /3-1 /3 splits of the training data."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "While eliminating almost correct TUs might decrease rather than increase MT quality, filtering out incorrect segments can have a positive impact (#REF) . Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by #TARGET_REF (Baseline 2). More importantly, however, we compared our system to Barbu's (2015) approach, using the classification algorithms which reportedly worked best with the 17 features in his work. Our system performed well in this comparison, surpassing Barbu's approach in all language pairs except en-de, where both systems were en par. Details are shown in Table 2a , where we report weighted precision (P), recall (R), and F 1 -scores averaged over 5-fold cross-validation with 2 /3-1 /3 splits of the training data.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by #TARGET_REF (Baseline 2).\"]}"
    },
    {
        "gold": {
            "text": [
                "While eliminating almost correct TUs might decrease rather than increase MT quality, filtering out incorrect segments can have a positive impact (#REF) .",
                "Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by #REF (Baseline 2).",
                "More importantly, however, we compared our system to #TARGET_REF approach, using the classification algorithms which reportedly worked best with the 17 features in his work.",
                "Our system performed well in this comparison, surpassing Barbu's approach in all language pairs except en-de, where both systems were en par.",
                "Details are shown in Table 2a , where we report weighted precision (P), recall (R), and F 1 -scores averaged over 5-fold cross-validation with 2 /3-1 /3 splits of the training data."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "While eliminating almost correct TUs might decrease rather than increase MT quality, filtering out incorrect segments can have a positive impact (#REF) . Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by #REF (Baseline 2). More importantly, however, we compared our system to #TARGET_REF approach, using the classification algorithms which reportedly worked best with the 17 features in his work. Our system performed well in this comparison, surpassing Barbu's approach in all language pairs except en-de, where both systems were en par. Details are shown in Table 2a , where we report weighted precision (P), recall (R), and F 1 -scores averaged over 5-fold cross-validation with 2 /3-1 /3 splits of the training data.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"More importantly, however, we compared our system to #TARGET_REF approach, using the classification algorithms which reportedly worked best with the 17 features in his work.\"]}"
    },
    {
        "gold": {
            "text": [
                "Although geared to the Binary Classification (II) task (see above), we also assessed our system on the Fine-Grained Classification task.",
                "Here, the goal was to distinguish between all of the three classes, i.e., determine whether a TU is correct, almost correct, or incorrect.",
                "Again, we compared our system's performance to #TARGET_REF method, using 2 /3-1 /3 splits of the training data (5-fold cross-validation).",
                "The results, shown in Table 2b , implied that the nine features we selected would not suffice for a more fine-grained classification of TUs.",
                "This was confirmed in the official evaluation and ranking: our system scored low on en-de and mediocre on en-es and en-it."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Although geared to the Binary Classification (II) task (see above), we also assessed our system on the Fine-Grained Classification task. Here, the goal was to distinguish between all of the three classes, i.e., determine whether a TU is correct, almost correct, or incorrect. Again, we compared our system's performance to #TARGET_REF method, using 2 /3-1 /3 splits of the training data (5-fold cross-validation). The results, shown in Table 2b , implied that the nine features we selected would not suffice for a more fine-grained classification of TUs. This was confirmed in the official evaluation and ranking: our system scored low on en-de and mediocre on en-es and en-it.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Again, we compared our system's performance to #TARGET_REF method, using 2 /3-1 /3 splits of the training data (5-fold cross-validation).\"]}"
    },
    {
        "gold": {
            "text": [
                "To the best of our knowledge, Barbu provided the first and so far only research contribution on automatic TM cleaning, which the author himself described as \"a neglected research area\" (#REF) .",
                "With our participation to this shared task, we seek to extend his work by examining new features based on statistical MT and POS tagging.",
                "As outlined above, comparing machine translated source segments to their actual target segments has proven effective in #TARGET_REF experiments.",
                "We propose to complement or replace the similarity function used for this comparison (cosine similarity) by two automatic MT evaluation metrics, Bleu (#REF) and characterbased Levenshtein distance, in order to reward higher-order n-gram (n > 1) and partial word overlaps, respectively.",
                "Furthermore, we introduce a recall-based MT feature that takes multiple MT hypotheses (n-best translations) of a given source segment into account, based on the assumption that alternative translations of words (such as \"buy\" and \"purchase\") or phrases (such as \"despite\" and \"in spite of\") should not be punished."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "To the best of our knowledge, Barbu provided the first and so far only research contribution on automatic TM cleaning, which the author himself described as \"a neglected research area\" (#REF) . With our participation to this shared task, we seek to extend his work by examining new features based on statistical MT and POS tagging. As outlined above, comparing machine translated source segments to their actual target segments has proven effective in #TARGET_REF experiments. We propose to complement or replace the similarity function used for this comparison (cosine similarity) by two automatic MT evaluation metrics, Bleu (#REF) and characterbased Levenshtein distance, in order to reward higher-order n-gram (n > 1) and partial word overlaps, respectively. Furthermore, we introduce a recall-based MT feature that takes multiple MT hypotheses (n-best translations) of a given source segment into account, based on the assumption that alternative translations of words (such as \"buy\" and \"purchase\") or phrases (such as \"despite\" and \"in spite of\") should not be punished.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"As outlined above, comparing machine translated source segments to their actual target segments has proven effective in #TARGET_REF experiments.\"]}"
    },
    {
        "gold": {
            "text": [
                "First, for each record of the i th row and j th column in the table, we utilize 1-layer MLP to encode the embeddings of each record's four types of information into a dense vector r i,j , r i,j = ReLU (W a [r i,j .e; r i,j .c; r i,j .v; r i,j .f ] + b a ).",
                "W a and b a are trainable parameters.",
                "The word embeddings for each type of information are trainable and randomly initialized before training following #TARGET_REF .",
                "[; ] denotes the vector concatenation.",
                "Then, we use a LSTM decoder with attention and conditional copy to model the conditional probability P (y t |y <t , S)."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "First, for each record of the i th row and j th column in the table, we utilize 1-layer MLP to encode the embeddings of each record's four types of information into a dense vector r i,j , r i,j = ReLU (W a [r i,j .e; r i,j .c; r i,j .v; r i,j .f ] + b a ). W a and b a are trainable parameters. The word embeddings for each type of information are trainable and randomly initialized before training following #TARGET_REF . [; ] denotes the vector concatenation. Then, we use a LSTM decoder with attention and conditional copy to model the conditional probability P (y t |y <t , S).",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"The word embeddings for each type of information are trainable and randomly initialized before training following #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The decoder was equipped with dual attention (#REF) .",
                "The one with LSTM cell is similar to the one in #REF with 1 layer from {1,2,3}. The one with CNN cell (#REF) has kernel width 3 from {3, 5} and 10 layer from {5,10,15,20}. The one with transformer-style encoder (MHSA) (#REF) has 8 head from {8, 10} and 5 layer from {2,3,4,5,6}. The heads and layers mentioned above were for both record-level encoder and rowlevel encoder respectively.",
                "The self-attention (SA) cell we used, as described in Section 3, achieved better overall performance in terms of F1% of CS, CO and BLEU among the hierarchical encoders.",
                "Also we implemented a template system same as the one used in #TARGET_REF which outputted eight sentences: an introductory sentence (two teams' points and who win), six top players' statistics (ranked by their points) and a conclusion sentence.",
                "We refer the readers to #REF 's paper for more detailed information on templates."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The decoder was equipped with dual attention (#REF) . The one with LSTM cell is similar to the one in #REF with 1 layer from {1,2,3}. The one with CNN cell (#REF) has kernel width 3 from {3, 5} and 10 layer from {5,10,15,20}. The one with transformer-style encoder (MHSA) (#REF) has 8 head from {8, 10} and 5 layer from {2,3,4,5,6}. The heads and layers mentioned above were for both record-level encoder and rowlevel encoder respectively. The self-attention (SA) cell we used, as described in Section 3, achieved better overall performance in terms of F1% of CS, CO and BLEU among the hierarchical encoders. Also we implemented a template system same as the one used in #TARGET_REF which outputted eight sentences: an introductory sentence (two teams' points and who win), six top players' statistics (ranked by their points) and a conclusion sentence. We refer the readers to #REF 's paper for more detailed information on templates.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"Also we implemented a template system same as the one used in #TARGET_REF which outputted eight sentences: an introductory sentence (two teams' points and who win), six top players' statistics (ranked by their points) and a conclusion sentence.\"]}"
    },
    {
        "gold": {
            "text": [
                "We conducted experiments on ROTOWIRE #TARGET_REF .",
                "For each example, it provides three tables as described in Section 2.1 which consists of 628 records in total with a long game summary.",
                "The average length of game summary is 337.1.",
                "In this paper, we followed the data split introduced in #REF"
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                1,
                0,
                0
            ]
        },
        "input": "We conducted experiments on ROTOWIRE #TARGET_REF . For each example, it provides three tables as described in Section 2.1 which consists of 628 records in total with a long game summary. The average length of game summary is 337.1. In this paper, we followed the data split introduced in #REF",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We conducted experiments on ROTOWIRE #TARGET_REF .\", \"For each example, it provides three tables as described in Section 2.1 which consists of 628 records in total with a long game summary.\"]}"
    },
    {
        "gold": {
            "text": [
                "We conducted experiments on ROTOWIRE (#REF) .",
                "For each example, it provides three tables as described in Section 2.1 which consists of 628 records in total with a long game summary.",
                "The average length of game summary is 337.1.",
                "In this paper, we followed the data split introduced in #TARGET_REF"
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1
            ]
        },
        "input": "We conducted experiments on ROTOWIRE (#REF) . For each example, it provides three tables as described in Section 2.1 which consists of 628 records in total with a long game summary. The average length of game summary is 337.1. In this paper, we followed the data split introduced in #TARGET_REF",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In this paper, we followed the data split introduced in #TARGET_REF\"]}"
    },
    {
        "gold": {
            "text": [
                "For inferring, we set beam size as 5.",
                "We also set the history windows size as 3 from {3,5,7} based on the results.",
                "Code of our model can be found at https://github.com/ernestgong/data2text-three-dimensions/. Table 1 displays the automatic evaluation results on both development and test set.",
                "We chose Conditional Copy (CC) model as our baseline, which is the best model in #TARGET_REF .",
                "We included reported scores with updated IE model by #REF and our implementation's result on CC in this paper."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "For inferring, we set beam size as 5. We also set the history windows size as 3 from {3,5,7} based on the results. Code of our model can be found at https://github.com/ernestgong/data2text-three-dimensions/. Table 1 displays the automatic evaluation results on both development and test set. We chose Conditional Copy (CC) model as our baseline, which is the best model in #TARGET_REF . We included reported scores with updated IE model by #REF and our implementation's result on CC in this paper.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We chose Conditional Copy (CC) model as our baseline, which is the best model in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Row, column and time dimension information are important to the modeling of tables because subtracting any of them will result in performance Table 2 : Automatic evaluation results on test set.",
                "Results were obtained using #TARGET_REF 's trained extractive evaluation models with relexicalization (#REF) .",
                "* We include delayed copy (DEL)'s result in the paper (#REF) for comparison.",
                "drop.",
                "Also, position embedding is critical when modeling time dimension information according to the results."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Row, column and time dimension information are important to the modeling of tables because subtracting any of them will result in performance Table 2 : Automatic evaluation results on test set. Results were obtained using #TARGET_REF 's trained extractive evaluation models with relexicalization (#REF) . * We include delayed copy (DEL)'s result in the paper (#REF) for comparison. drop. Also, position embedding is critical when modeling time dimension information according to the results.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Row, column and time dimension information are important to the modeling of tables because subtracting any of them will result in performance Table 2 : Automatic evaluation results on test set.\", \"Results were obtained using #TARGET_REF 's trained extractive evaluation models with relexicalization (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Results show that each component in the model contributes to the overall performance.",
                "In addition, we compare our model with delayed copy model (DEL) (#REF) along with gold text, template system (TEM), conditional copy (CC) (#REF) and NCP+CC (NCP) (#REF) .",
                "#REF 's model generate a template at first and then fill in the slots with delayed copy mechanism.",
                "Since its result in #REF 's paper was evaluated by IE model trained by #TARGET_REF and \"relexicalization\" by #REF , we adopted the corresponding IE model and re-implement \"relexicalization\" as suggested by #REF for fair comparison.",
                "Please note that CC's evaluation results via our reimplemented \"relexicalization\" is comparable to the reported result in #REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Results show that each component in the model contributes to the overall performance. In addition, we compare our model with delayed copy model (DEL) (#REF) along with gold text, template system (TEM), conditional copy (CC) (#REF) and NCP+CC (NCP) (#REF) . #REF 's model generate a template at first and then fill in the slots with delayed copy mechanism. Since its result in #REF 's paper was evaluated by IE model trained by #TARGET_REF and \"relexicalization\" by #REF , we adopted the corresponding IE model and re-implement \"relexicalization\" as suggested by #REF for fair comparison. Please note that CC's evaluation results via our reimplemented \"relexicalization\" is comparable to the reported result in #REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Since its result in #REF 's paper was evaluated by IE model trained by #TARGET_REF and \\\"relexicalization\\\" by #REF , we adopted the corresponding IE model and re-implement \\\"relexicalization\\\" as suggested by #REF for fair comparison.\"]}"
    },
    {
        "gold": {
            "text": [
                "For Chinese, treebanks have been made available under various segmentation granularities (#REF; #REF; #REF) .",
                "These give rise to the research problem * Work done when the first author was visiting SUTD.",
                "of effectively making use of multiple treebanks under heterogeneous annotations for improving output accuracies (#REF; #REF; #TARGET_REF .",
                "The task has been tackled using two typical approaches.",
                "The first is based on stacking (#REF; #REF; #REF) ."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "For Chinese, treebanks have been made available under various segmentation granularities (#REF; #REF; #REF) . These give rise to the research problem * Work done when the first author was visiting SUTD. of effectively making use of multiple treebanks under heterogeneous annotations for improving output accuracies (#REF; #REF; #TARGET_REF . The task has been tackled using two typical approaches. The first is based on stacking (#REF; #REF; #REF) .",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"These give rise to the research problem * Work done when the first author was visiting SUTD.\", \"of effectively making use of multiple treebanks under heterogeneous annotations for improving output accuracies (#REF; #REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "As shown in Figure 1 (a), the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features.",
                "This method has been used for leveraging two different treebanks for word segmentation (#REF; #REF) and dependency parsing (#REF; #REF) .",
                "The second approach is based on multi-view learning (#REF; #TARGET_REF .",
                "The idea is to address both annotation styles simultaneously by sharing common feature representations.",
                "In particular, #REF trained dependency parsers using the domain adaptation method of Daumé III (2007) , keeping a copy of shared features and a separate copy of features for each treebank."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "As shown in Figure 1 (a), the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features. This method has been used for leveraging two different treebanks for word segmentation (#REF; #REF) and dependency parsing (#REF; #REF) . The second approach is based on multi-view learning (#REF; #TARGET_REF . The idea is to address both annotation styles simultaneously by sharing common feature representations. In particular, #REF trained dependency parsers using the domain adaptation method of Daumé III (2007) , keeping a copy of shared features and a separate copy of features for each treebank.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The second approach is based on multi-view learning (#REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "On the other hand, the aforementioned methods on heterogeneous annotations are investigated mainly for discrete models.",
                "It remains an interesting research question how effective multiple treebanks can be utilized by neural NLP models, and we aim to investigate this empirically.",
                "We follow #TARGET_REF , taking POS-tagging for case study, using the methods of #REF and #TARGET_REF as the discrete stacking and multi-view training baselines, respectively, and building neural network counterparts to their models for empirical comparison.",
                "The base tagger is a neural CRF model #REF) , which gives competitive accuracies to discrete CRF taggers.",
                "Results show that neural stacking allows deeper integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "On the other hand, the aforementioned methods on heterogeneous annotations are investigated mainly for discrete models. It remains an interesting research question how effective multiple treebanks can be utilized by neural NLP models, and we aim to investigate this empirically. We follow #TARGET_REF , taking POS-tagging for case study, using the methods of #REF and #TARGET_REF as the discrete stacking and multi-view training baselines, respectively, and building neural network counterparts to their models for empirical comparison. The base tagger is a neural CRF model #REF) , which gives competitive accuracies to discrete CRF taggers. Results show that neural stacking allows deeper integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We follow #TARGET_REF , taking POS-tagging for case study, using the methods of #REF and #TARGET_REF as the discrete stacking and multi-view training baselines, respectively, and building neural network counterparts to their models for empirical comparison.\"]}"
    },
    {
        "gold": {
            "text": [
                "As shown in Figure 1(b) , multi-view learning #TARGET_REF utilizes corpus A and corpus B simultaneously for training.",
                "The coupled tagger directly learns the logistic correspondences between both corpora, therefore can lead a more comprehensive usage of corpus A compared with stacking.",
                "In order to better capture such correlation, specifically designed feature templates between two tag sets are essential.",
                "For each training instances, both A and B labels are needed.",
                "However, one type of tag is missing."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "As shown in Figure 1(b) , multi-view learning #TARGET_REF utilizes corpus A and corpus B simultaneously for training. The coupled tagger directly learns the logistic correspondences between both corpora, therefore can lead a more comprehensive usage of corpus A compared with stacking. In order to better capture such correlation, specifically designed feature templates between two tag sets are essential. For each training instances, both A and B labels are needed. However, one type of tag is missing.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"As shown in Figure 1(b) , multi-view learning #TARGET_REF utilizes corpus A and corpus B simultaneously for training.\"]}"
    },
    {
        "gold": {
            "text": [
                "Input: Two training datasets: where ⃗ y is the model output, s(⃗ y|⃗ x) = logP (⃗ y|⃗ x) is the log probability of ⃗ y and δ(⃗ y, ⃗ y d ) is the Hamming distance between ⃗ y and ⃗ y d .",
                "We adopt online learning, updating parameters using AdaGrad (#REF) .",
                "To train the neural stacking model, we first train a base tagger using corpus A. Then, we train the stacked tagger with corpus B, where the parameters of the A tagger has been pretrained from corpus A and the B tagger is randomly initialized.",
                "For neural multi-view model, we follow #TARGET_REF and take a the corpus-weighting strategy to sample a number of training instances from both corpora for each training iteration, as shown in Algorithm 1.",
                "At each epoch, we randomly sample from the two datasets according to a corpus weights ratio, namely the ratio between the number of sentences in each dataset used for training, to form a training set for the epoch."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Input: Two training datasets: where ⃗ y is the model output, s(⃗ y|⃗ x) = logP (⃗ y|⃗ x) is the log probability of ⃗ y and δ(⃗ y, ⃗ y d ) is the Hamming distance between ⃗ y and ⃗ y d . We adopt online learning, updating parameters using AdaGrad (#REF) . To train the neural stacking model, we first train a base tagger using corpus A. Then, we train the stacked tagger with corpus B, where the parameters of the A tagger has been pretrained from corpus A and the B tagger is randomly initialized. For neural multi-view model, we follow #TARGET_REF and take a the corpus-weighting strategy to sample a number of training instances from both corpora for each training iteration, as shown in Algorithm 1. At each epoch, we randomly sample from the two datasets according to a corpus weights ratio, namely the ratio between the number of sentences in each dataset used for training, to form a training set for the epoch.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For neural multi-view model, we follow #TARGET_REF and take a the corpus-weighting strategy to sample a number of training instances from both corpora for each training iteration, as shown in Algorithm 1.\"]}"
    },
    {
        "gold": {
            "text": [
                "We adopt the Penn Chinese Treebank version 5.0 (CTB5) (#REF) as our main corpus, with the standard data split following previous work (#REF; #TARGET_REF .",
                "People's Daily (PD) is used as second corpus with a different scheme.",
                "We filter out PD sentences longer than 200 words.",
                "Details of the datasets are listed in Table  1 .",
                "The standard token-wise POS tagging accuracy is used as the evaluation metric."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We adopt the Penn Chinese Treebank version 5.0 (CTB5) (#REF) as our main corpus, with the standard data split following previous work (#REF; #TARGET_REF . People's Daily (PD) is used as second corpus with a different scheme. We filter out PD sentences longer than 200 words. Details of the datasets are listed in Table  1 . The standard token-wise POS tagging accuracy is used as the evaluation metric.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We adopt the Penn Chinese Treebank version 5.0 (CTB5) (#REF) as our main corpus, with the standard data split following previous work (#REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Without using dropout, the performance increases in the beginning, but then decreases as the number of training epochs increases beyond 10.",
                "This indicates that the NN models can overfit the training data without dropout.",
                "However, when a 50% dropout rate is used, the initial performances are significantly worse, which implies that the 50% dropout rate can be too large and leads to underfitting.",
                "As a result, we choose a dropout rate of 20% for the remaining experiments, which strikes the balance between over-System Accuracy CRF Baseline #TARGET_REF 94.10 CRF Stacking #TARGET_REF 94.81 CRF Multi-view #TARGET_REF 95 Table 2 : Accuracies on CTB-test.",
                "fitting and underfitting."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Without using dropout, the performance increases in the beginning, but then decreases as the number of training epochs increases beyond 10. This indicates that the NN models can overfit the training data without dropout. However, when a 50% dropout rate is used, the initial performances are significantly worse, which implies that the 50% dropout rate can be too large and leads to underfitting. As a result, we choose a dropout rate of 20% for the remaining experiments, which strikes the balance between over-System Accuracy CRF Baseline #TARGET_REF 94.10 CRF Stacking #TARGET_REF 94.81 CRF Multi-view #TARGET_REF 95 Table 2 : Accuracies on CTB-test. fitting and underfitting.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"As a result, we choose a dropout rate of 20% for the remaining experiments, which strikes the balance between over-System Accuracy CRF Baseline #TARGET_REF 94.10 CRF Stacking #TARGET_REF 94.81 CRF Multi-view #TARGET_REF 95 Table 2 : Accuracies on CTB-test.\"]}"
    },
    {
        "gold": {
            "text": [
                "The ratios of 1:1 and 1:4 give comparable performances.",
                "We choose the former for our final tests because it is a much faster choice.",
                "Table 2 shows the final results on the CTB test data.",
                "We lists the results of stacking method of #REF re-implemented by #TARGET_REF , and CRF multi-view method reported by #TARGET_REF .",
                "We adopt pair-wise significance test (#REF) when comparing the results between two different models."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The ratios of 1:1 and 1:4 give comparable performances. We choose the former for our final tests because it is a much faster choice. Table 2 shows the final results on the CTB test data. We lists the results of stacking method of #REF re-implemented by #TARGET_REF , and CRF multi-view method reported by #TARGET_REF . We adopt pair-wise significance test (#REF) when comparing the results between two different models.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We lists the results of stacking method of #REF re-implemented by #TARGET_REF , and CRF multi-view method reported by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare the efficiencies of neural and discrete multi-view training by running our models and the model of #TARGET_REF 4 with default configurations on the CTB5 training data.",
                "The CRF baseline is adapted from #REF .",
                "All the systems are implemented in C++ running on an Intel E5-1620 CPU.",
                "The results are shown in Table 3 .",
                "The NN baseline model is slower than the CRF baseline model."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We compare the efficiencies of neural and discrete multi-view training by running our models and the model of #TARGET_REF 4 with default configurations on the CTB5 training data. The CRF baseline is adapted from #REF . All the systems are implemented in C++ running on an Intel E5-1620 CPU. The results are shown in Table 3 . The NN baseline model is slower than the CRF baseline model.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We compare the efficiencies of neural and discrete multi-view training by running our models and the model of #TARGET_REF 4 with default configurations on the CTB5 training data.\"]}"
    },
    {
        "gold": {
            "text": [
                "The base tagger is a neural CRF model #REF) , which gives competitive accuracies to discrete CRF taggers.",
                "Results show that neural stacking allows deeper integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training.",
                "In addition, the advantage of neural multi-view learning over its discrete counterpart are many-fold.",
                "First, it is free from the necessity of manual cross-labelset interactive feature engineering, which is far from trivial for representing annotation correspondence #TARGET_REF .",
                "Second, compared to discrete model, parameter sharing in deep neural network eliminates the issue of exponential growth of search space, and allows separated training of each label type, in the same way as multi-task learning (#REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The base tagger is a neural CRF model #REF) , which gives competitive accuracies to discrete CRF taggers. Results show that neural stacking allows deeper integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training. In addition, the advantage of neural multi-view learning over its discrete counterpart are many-fold. First, it is free from the necessity of manual cross-labelset interactive feature engineering, which is far from trivial for representing annotation correspondence #TARGET_REF . Second, compared to discrete model, parameter sharing in deep neural network eliminates the issue of exponential growth of search space, and allows separated training of each label type, in the same way as multi-task learning (#REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"First, it is free from the necessity of manual cross-labelset interactive feature engineering, which is far from trivial for representing annotation correspondence #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "This shows that neural stacking is a preferred choice for stacking.",
                "Multi-view training.",
                "With respect of the multiview training method, the NN model improves over the NN baseline from 94.24 to 95.40, by a margin of +1.16, which is higher than that of 0.90 brought by discrete method of #TARGET_REF over its baseline, from 94.10 to 95.00.",
                "NN multi-view training method gives relatively higher improvements compared with NN stacking method.",
                "This is consistent with the observation of #REF , who showed that discrete label coupling training gives slightly better improvement compared with discrete stacking."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "This shows that neural stacking is a preferred choice for stacking. Multi-view training. With respect of the multiview training method, the NN model improves over the NN baseline from 94.24 to 95.40, by a margin of +1.16, which is higher than that of 0.90 brought by discrete method of #TARGET_REF over its baseline, from 94.10 to 95.00. NN multi-view training method gives relatively higher improvements compared with NN stacking method. This is consistent with the observation of #REF , who showed that discrete label coupling training gives slightly better improvement compared with discrete stacking.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"With respect of the multiview training method, the NN model improves over the NN baseline from 94.24 to 95.40, by a margin of +1.16, which is higher than that of 0.90 brought by discrete method of #TARGET_REF over its baseline, from 94.10 to 95.00.\"]}"
    },
    {
        "gold": {
            "text": [
                "With respect of the multiview training method, the NN model improves over the NN baseline from 94.24 to 95.40, by a margin of +1.16, which is higher than that of 0.90 brought by discrete method of #REF over its baseline, from 94.10 to 95.00.",
                "NN multi-view training method gives relatively higher improvements compared with NN stacking method.",
                "This is consistent with the observation of #TARGET_REF , who showed that discrete label coupling training gives slightly better improvement compared with discrete stacking.",
                "The final accuracies of NN multi-view training is also higher than that of its CRF counterpart, namely 95.00 vs 95.40 at the confidence level p < 10 −3 .",
                "The difference between the final NN multi-view training result of 95.40 and the final NN stacking results is not significant."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "With respect of the multiview training method, the NN model improves over the NN baseline from 94.24 to 95.40, by a margin of +1.16, which is higher than that of 0.90 brought by discrete method of #REF over its baseline, from 94.10 to 95.00. NN multi-view training method gives relatively higher improvements compared with NN stacking method. This is consistent with the observation of #TARGET_REF , who showed that discrete label coupling training gives slightly better improvement compared with discrete stacking. The final accuracies of NN multi-view training is also higher than that of its CRF counterpart, namely 95.00 vs 95.40 at the confidence level p < 10 −3 . The difference between the final NN multi-view training result of 95.40 and the final NN stacking results is not significant.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"This is consistent with the observation of #TARGET_REF , who showed that discrete label coupling training gives slightly better improvement compared with discrete stacking.\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare the efficiencies of neural and discrete multi-view training by running our models and the model of #REF 4 with default configurations on the CTB5 training data.",
                "The CRF baseline is adapted from #TARGET_REF .",
                "All the systems are implemented in C++ running on an Intel E5-1620 CPU.",
                "The results are shown in Table 3 .",
                "The NN baseline model is slower than the CRF baseline model."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We compare the efficiencies of neural and discrete multi-view training by running our models and the model of #REF 4 with default configurations on the CTB5 training data. The CRF baseline is adapted from #TARGET_REF . All the systems are implemented in C++ running on an Intel E5-1620 CPU. The results are shown in Table 3 . The NN baseline model is slower than the CRF baseline model.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"The CRF baseline is adapted from #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Compared with single-sentence captioning, paragraph captioning is a relatively new task.",
                "The main paragraph captioning dataset is the Visual Genome corpus, introduced by #TARGET_REF .",
                "When strong single-sentence captioning models are trained on this dataset, they produce repetitive paragraphs that are unable to describe diverse aspects of images.",
                "The generated paragraphs repeat a slight variant of the same sentence multiple times, even when beam search is used.",
                "Prior work, discussed in the following section, tried to address this repetition with architectural changes, such as hierarchical LSTMs, which separate the generation of sentence topics and words."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Compared with single-sentence captioning, paragraph captioning is a relatively new task. The main paragraph captioning dataset is the Visual Genome corpus, introduced by #TARGET_REF . When strong single-sentence captioning models are trained on this dataset, they produce repetitive paragraphs that are unable to describe diverse aspects of images. The generated paragraphs repeat a slight variant of the same sentence multiple times, even when beam search is used. Prior work, discussed in the following section, tried to address this repetition with architectural changes, such as hierarchical LSTMs, which separate the generation of sentence topics and words.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The main paragraph captioning dataset is the Visual Genome corpus, introduced by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "CIDEr and BLEU measure accuracy with n-gram overlaps, with CIDEr weighting n-grams by TF-IDF (termfrequency inverse-document-frequency), and ME-TEOR uses unigram overlap, incorporating synonym and paraphrase matches.",
                "We discuss these metrics in greater detail when analyzing our experiments.",
                "#TARGET_REF introduced the first large-scale paragraph captioning dataset, a subset of the Visual Genome dataset, along with a number of models for paragraph captioning.",
                "Empirically, they showed that paragraphs contain significantly more pronouns, verbs, coreferences, and greater overall \"diversity\" than singlesentence captions.",
                "Whereas most single-sentence captions in the MSCOCO dataset describe only the most important object or action in an image, paragraph captions usually touch on multiple objects and actions."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "CIDEr and BLEU measure accuracy with n-gram overlaps, with CIDEr weighting n-grams by TF-IDF (termfrequency inverse-document-frequency), and ME-TEOR uses unigram overlap, incorporating synonym and paraphrase matches. We discuss these metrics in greater detail when analyzing our experiments. #TARGET_REF introduced the first large-scale paragraph captioning dataset, a subset of the Visual Genome dataset, along with a number of models for paragraph captioning. Empirically, they showed that paragraphs contain significantly more pronouns, verbs, coreferences, and greater overall \"diversity\" than singlesentence captions. Whereas most single-sentence captions in the MSCOCO dataset describe only the most important object or action in an image, paragraph captions usually touch on multiple objects and actions.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF introduced the first large-scale paragraph captioning dataset, a subset of the Visual Genome dataset, along with a number of models for paragraph captioning.\"]}"
    },
    {
        "gold": {
            "text": [
                "The paragraph captioning models proposed by #TARGET_REF included template-based (nonneural) approaches and two encoder-decoder models.",
                "In both neural models, the encoder is an object detector pre-trained for dense captioning.",
                "In the first model, called the flat model, the decoder is a single LSTM which outputs an entire paragraph word-by-word.",
                "In the second model, called the hierarchical model, the decoder is composed of two LSTMs, where the output of one sentencelevel LSTM is used as input to the other word-level LSTM.",
                "Recently, #REF extended this model with a third (paragraph-level) LSTM and added adversarial training."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The paragraph captioning models proposed by #TARGET_REF included template-based (nonneural) approaches and two encoder-decoder models. In both neural models, the encoder is an object detector pre-trained for dense captioning. In the first model, called the flat model, the decoder is a single LSTM which outputs an entire paragraph word-by-word. In the second model, called the hierarchical model, the decoder is composed of two LSTMs, where the output of one sentencelevel LSTM is used as input to the other word-level LSTM. Recently, #REF extended this model with a third (paragraph-level) LSTM and added adversarial training.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The paragraph captioning models proposed by #TARGET_REF included template-based (nonneural) approaches and two encoder-decoder models.\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently, #REF extended this model with a third (paragraph-level) LSTM and added adversarial training.",
                "In total, their model (RTT-GAN) incorporates three LSTMs, two attention mechanisms, a phrase copy mechanism, and two adversarial discriminators.",
                "To the best of our knowledge, this model achieves state-ofthe-art performance of 16.9 CIDEr on the Visual Genome dataset (without external data).",
                "For our experiments, we use the top-down single-sentence captioning model in #REF .",
                "This model is similar to the \"flat\" model in #TARGET_REF , except that it incorporates attention with a top-down mechanism."
            ],
            "label": [
                "BACK",
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "Recently, #REF extended this model with a third (paragraph-level) LSTM and added adversarial training. In total, their model (RTT-GAN) incorporates three LSTMs, two attention mechanisms, a phrase copy mechanism, and two adversarial discriminators. To the best of our knowledge, this model achieves state-ofthe-art performance of 16.9 CIDEr on the Visual Genome dataset (without external data). For our experiments, we use the top-down single-sentence captioning model in #REF . This model is similar to the \"flat\" model in #TARGET_REF , except that it incorporates attention with a top-down mechanism.",
        "output": "{\"label\": [\"BACK\", \"SIM\"], \"context\": [\"For our experiments, we use the top-down single-sentence captioning model in #REF .\", \"This model is similar to the \\\"flat\\\" model in #TARGET_REF , except that it incorporates attention with a top-down mechanism.\"]}"
    },
    {
        "gold": {
            "text": [
                "For our paragraph captioning model we use the top-down model from #REF .",
                "Our encoder is a convolutional network pretrained for object detection (as opposed to dense captioning, as in #TARGET_REF and #REF ).",
                "METEOR CIDEr BLEU-1 BLEU-2 BLEU-3 BLEU-4 Krause et al. (Template) The encoder extracts between 10 and 100 objects per image and applies spatial max-pooling to yield a single feature vector of dimension 2048 per object.",
                "The decoder is a 1-layer LSTM with hidden dimension 512 and top-down attention.",
                "Evaluation is done on the Visual Genome dataset with the splits provided by #REF ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "For our paragraph captioning model we use the top-down model from #REF . Our encoder is a convolutional network pretrained for object detection (as opposed to dense captioning, as in #TARGET_REF and #REF ). METEOR CIDEr BLEU-1 BLEU-2 BLEU-3 BLEU-4 Krause et al. (Template) The encoder extracts between 10 and 100 objects per image and applies spatial max-pooling to yield a single feature vector of dimension 2048 per object. The decoder is a 1-layer LSTM with hidden dimension 512 and top-down attention. Evaluation is done on the Visual Genome dataset with the splits provided by #REF .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Our encoder is a convolutional network pretrained for object detection (as opposed to dense captioning, as in #TARGET_REF and #REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "The decoder is a 1-layer LSTM with hidden dimension 512 and top-down attention.",
                "Evaluation is done on the Visual Genome dataset with the splits provided by #TARGET_REF .",
                "We first train for 25 epochs with crossentropy (XE) loss, using Adam with learning rate 5 · 10 −4 .",
                "We then train an additional 25 epochs with repetition-penalized SCST targeting a CIDEr-based reward, using Adam with learning rate 5 · 10 −5 .",
                "Our PyTorch-based implementation is available at https://github.com/lukemelas/ image-paragraph-captioning."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The decoder is a 1-layer LSTM with hidden dimension 512 and top-down attention. Evaluation is done on the Visual Genome dataset with the splits provided by #TARGET_REF . We first train for 25 epochs with crossentropy (XE) loss, using Adam with learning rate 5 · 10 −4 . We then train an additional 25 epochs with repetition-penalized SCST targeting a CIDEr-based reward, using Adam with learning rate 5 · 10 −5 . Our PyTorch-based implementation is available at https://github.com/lukemelas/ image-paragraph-captioning.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Evaluation is done on the Visual Genome dataset with the splits provided by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "After analyzing the alignment matrices generated by GroundHog #TARGET_REF , we find that modeling the structural divergence of natural languages is so challenging that unidirectional models can only capture part of alignment regularities.",
                "This finding inspires us to improve attention-based NMT by combining two unidirectional models.",
                "In this work, we only apply agreement-based joint learning to GroundHog.",
                "As our approach does not assume specific network architectures, it is possible to apply it to the models proposed by Luong et al. [2015] ."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "After analyzing the alignment matrices generated by GroundHog #TARGET_REF , we find that modeling the structural divergence of natural languages is so challenging that unidirectional models can only capture part of alignment regularities. This finding inspires us to improve attention-based NMT by combining two unidirectional models. In this work, we only apply agreement-based joint learning to GroundHog. As our approach does not assume specific network architectures, it is possible to apply it to the models proposed by Luong et al. [2015] .",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"After analyzing the alignment matrices generated by GroundHog #TARGET_REF , we find that modeling the structural divergence of natural languages is so challenging that unidirectional models can only capture part of alignment regularities.\"]}"
    },
    {
        "gold": {
            "text": [
                "Such agents may act as mediators or can be helpful for pedagogical purposes (#REF) .",
                "Efforts in agent-human negotiations involving free-form natural language as a means of communication are rather sparse.",
                "Researchers #TARGET_REF recently studied natural language negotiations in buyer-seller bargaining setup, which is comparatively less restricted than previously studied game environments (#REF; #REF) .",
                "Lack of a well-defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario.",
                "Interestingly, this also provides an exciting research opportunity: how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? Understanding the impact of natural language on negotiation outcomes through a data-driven neural framework is the primary objective of this work."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                1
            ]
        },
        "input": "Such agents may act as mediators or can be helpful for pedagogical purposes (#REF) . Efforts in agent-human negotiations involving free-form natural language as a means of communication are rather sparse. Researchers #TARGET_REF recently studied natural language negotiations in buyer-seller bargaining setup, which is comparatively less restricted than previously studied game environments (#REF; #REF) . Lack of a well-defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario. Interestingly, this also provides an exciting research opportunity: how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? Understanding the impact of natural language on negotiation outcomes through a data-driven neural framework is the primary objective of this work.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Researchers #TARGET_REF recently studied natural language negotiations in buyer-seller bargaining setup, which is comparatively less restricted than previously studied game environments (#REF; #REF) .\", \"Lack of a well-defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario.\", \"Interestingly, this also provides an exciting research opportunity: how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? Understanding the impact of natural language on negotiation outcomes through a data-driven neural framework is the primary objective of this work.\"]}"
    },
    {
        "gold": {
            "text": [
                "This number is lower than average for Housing, Bike and Car, resulting in relative better performance of Priceonly model for these categories over others.",
                "The models also show evidence of capturing buyer interest.",
                "By constructing artificial negotiations, we observe that the model predictions at f =0.2 increase when the buyer shows more interest in the product, indicating more willingness to pay.",
                "With the capability to incorporate cues from natural language, such a framework can be used in the future to get negotiation feedback, in order to guide the planning of a negotiating agent.",
                "This can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning (#REF; #TARGET_REF ."
            ],
            "label": [
                "BACK",
                "FUT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "This number is lower than average for Housing, Bike and Car, resulting in relative better performance of Priceonly model for these categories over others. The models also show evidence of capturing buyer interest. By constructing artificial negotiations, we observe that the model predictions at f =0.2 increase when the buyer shows more interest in the product, indicating more willingness to pay. With the capability to incorporate cues from natural language, such a framework can be used in the future to get negotiation feedback, in order to guide the planning of a negotiating agent. This can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning (#REF; #TARGET_REF .",
        "output": "{\"label\": [\"BACK\", \"FUT\"], \"context\": [\"With the capability to incorporate cues from natural language, such a framework can be used in the future to get negotiation feedback, in order to guide the planning of a negotiating agent.\", \"This can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning (#REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Interestingly, this also provides an exciting research opportunity: how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? Understanding the impact of natural language on negotiation outcomes through a data-driven neural framework is the primary objective of this work.",
                "We focus on buyer-seller negotiations #TARGET_REF where two individuals negotiate the price of a given product.",
                "Leveraging the recent advancements (#REF; #REF) in pre-trained language encoders, we attempt to predict negotiation outcomes early on in the conversation, in a completely data-driven manner ( Figure  1 ).",
                "Early prediction of outcomes is essential for effective planning of an automatically negotiating agent.",
                "Although there have been attempts to gain insights into negotiations (#REF; #REF) , to the best of our knowledge, we are the first to study early natural language cues through a datadriven neural system (Section 3)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Interestingly, this also provides an exciting research opportunity: how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? Understanding the impact of natural language on negotiation outcomes through a data-driven neural framework is the primary objective of this work. We focus on buyer-seller negotiations #TARGET_REF where two individuals negotiate the price of a given product. Leveraging the recent advancements (#REF; #REF) in pre-trained language encoders, we attempt to predict negotiation outcomes early on in the conversation, in a completely data-driven manner ( Figure  1 ). Early prediction of outcomes is essential for effective planning of an automatically negotiating agent. Although there have been attempts to gain insights into negotiations (#REF; #REF) , to the best of our knowledge, we are the first to study early natural language cues through a datadriven neural system (Section 3).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We focus on buyer-seller negotiations #TARGET_REF where two individuals negotiate the price of a given product.\"]}"
    },
    {
        "gold": {
            "text": [
                "Early prediction of outcomes is essential for effective planning of an automatically negotiating agent.",
                "Although there have been attempts to gain insights into negotiations (#REF; #REF) , to the best of our knowledge, we are the first to study early natural language cues through a datadriven neural system (Section 3).",
                "Our evaluations show that natural language allows the models to make better predictions by looking at only a fraction of the negotiation.",
                "Rather than just realizing the strategy in natural language, our empirical results suggest that language can be crucial in the planning as well.",
                "We provide a sample negotiation from the test set #TARGET_REF ) along with our model predictions in Table 1 ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Early prediction of outcomes is essential for effective planning of an automatically negotiating agent. Although there have been attempts to gain insights into negotiations (#REF; #REF) , to the best of our knowledge, we are the first to study early natural language cues through a datadriven neural system (Section 3). Our evaluations show that natural language allows the models to make better predictions by looking at only a fraction of the negotiation. Rather than just realizing the strategy in natural language, our empirical results suggest that language can be crucial in the planning as well. We provide a sample negotiation from the test set #TARGET_REF ) along with our model predictions in Table 1 .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We provide a sample negotiation from the test set #TARGET_REF ) along with our model predictions in Table 1 .\"]}"
    },
    {
        "gold": {
            "text": [
                "We study human-human negotiations in the buyerseller bargaining scenario, which has been a key research area in the literature (#REF) .",
                "In this section, we first describe our problem setup and key terminologies by discussing the dataset used.",
                "Later, we formalize our problem definition.",
                "Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by #TARGET_REF .",
                "Instead of focusing on the previously studied game environments (#REF; #REF) , the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist 1 ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We study human-human negotiations in the buyerseller bargaining scenario, which has been a key research area in the literature (#REF) . In this section, we first describe our problem setup and key terminologies by discussing the dataset used. Later, we formalize our problem definition. Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by #TARGET_REF . Instead of focusing on the previously studied game environments (#REF; #REF) , the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist 1 .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Approaches to lexical simplification generally follow a standard pipeline consisting of two main steps: generation and ranking.",
                "In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (#REF) , learning substitution rules from sentence-aligned parallel corpora of complex-simple texts (#REF; #TARGET_REF , and learning word embeddings from a large corpora to obtain similar words of the complex word (Glavaš andŠtajner, 2015; #REF; #REFa, 2017) .",
                "In the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness.",
                "* This research was conducted while the first author was a Post Doctoral Fellow at the City University of Hong Kong.",
                "The ranking step is challenging because the substitution candidates usually have similar meaning to the target word, and thus share similar context features."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Approaches to lexical simplification generally follow a standard pipeline consisting of two main steps: generation and ranking. In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (#REF) , learning substitution rules from sentence-aligned parallel corpora of complex-simple texts (#REF; #TARGET_REF , and learning word embeddings from a large corpora to obtain similar words of the complex word (Glavaš andŠtajner, 2015; #REF; #REFa, 2017) . In the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness. * This research was conducted while the first author was a Post Doctoral Fellow at the City University of Hong Kong. The ranking step is challenging because the substitution candidates usually have similar meaning to the target word, and thus share similar context features.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (#REF) , learning substitution rules from sentence-aligned parallel corpora of complex-simple texts (#REF; #TARGET_REF , and learning word embeddings from a large corpora to obtain similar words of the complex word (Glava\\u0161 and\\u0160tajner, 2015; #REF; #REFa, 2017) .\"]}"
    },
    {
        "gold": {
            "text": [
                "For generating substitution candidates, we utilize the method proposed by #REF , which was recently shown to be the state-of-art method for generating substitution candidates.",
                "They exploit a hybrid substitution generation approach where candidates are first extracted from 550,644 simple-complex aligned sentences from the Newsela corpus (#REF) .",
                "Then, these candidates are complemented with candidates generated with a retrofitted word embedding model.",
                "The word embedding model is retrofitted over WordNet's synonym pairs (for details, please refer to #TARGET_REF ).",
                "For ranking substitution candidates, we use a DSSM, which we elaborate in the next section."
            ],
            "label": [
                "BACK",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "For generating substitution candidates, we utilize the method proposed by #REF , which was recently shown to be the state-of-art method for generating substitution candidates. They exploit a hybrid substitution generation approach where candidates are first extracted from 550,644 simple-complex aligned sentences from the Newsela corpus (#REF) . Then, these candidates are complemented with candidates generated with a retrofitted word embedding model. The word embedding model is retrofitted over WordNet's synonym pairs (for details, please refer to #TARGET_REF ). For ranking substitution candidates, we use a DSSM, which we elaborate in the next section.",
        "output": "{\"label\": [\"BACK\", \"USE\"], \"context\": [\"The word embedding model is retrofitted over WordNet's synonym pairs (for details, please refer to #TARGET_REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "We focus on the ranking step of the standard lexical simplification pipeline.",
                "Given a dataset of tar-get words, their sentential contexts and substitution candidates for the target words, the goal is to train a model that accurately ranks the candidates based on their simplicity and semantic matching.",
                "For generating substitution candidates, we utilize the method proposed by #TARGET_REF , which was recently shown to be the state-of-art method for generating substitution candidates.",
                "They exploit a hybrid substitution generation approach where candidates are first extracted from 550,644 simple-complex aligned sentences from the Newsela corpus (#REF) .",
                "Then, these candidates are complemented with candidates generated with a retrofitted word embedding model."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We focus on the ranking step of the standard lexical simplification pipeline. Given a dataset of tar-get words, their sentential contexts and substitution candidates for the target words, the goal is to train a model that accurately ranks the candidates based on their simplicity and semantic matching. For generating substitution candidates, we utilize the method proposed by #TARGET_REF , which was recently shown to be the state-of-art method for generating substitution candidates. They exploit a hybrid substitution generation approach where candidates are first extracted from 550,644 simple-complex aligned sentences from the Newsela corpus (#REF) . Then, these candidates are complemented with candidates generated with a retrofitted word embedding model.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For generating substitution candidates, we utilize the method proposed by #TARGET_REF , which was recently shown to be the state-of-art method for generating substitution candidates.\"]}"
    },
    {
        "gold": {
            "text": [
                "As baseline features, we use the same n-gram probability features as in #TARGET_REF , who also employ a neural network to rank substitution candidates.",
                "As in #REF , the features were extracted using the SubIMDB corpus (#REF) .",
                "We also experiment with additional features that have been reported as useful in this task.",
                "For each target word and a substitution candidate word we also compute: cosine similarity, word length, and alignment probability in the sentence-aligned Normal-Simple Wikipedia corpus (#REF) .",
                "The cosine similarity feature is computed using the SubIMDB corpus."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "As baseline features, we use the same n-gram probability features as in #TARGET_REF , who also employ a neural network to rank substitution candidates. As in #REF , the features were extracted using the SubIMDB corpus (#REF) . We also experiment with additional features that have been reported as useful in this task. For each target word and a substitution candidate word we also compute: cosine similarity, word length, and alignment probability in the sentence-aligned Normal-Simple Wikipedia corpus (#REF) . The cosine similarity feature is computed using the SubIMDB corpus.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"As baseline features, we use the same n-gram probability features as in #TARGET_REF , who also employ a neural network to rank substitution candidates.\"]}"
    },
    {
        "gold": {
            "text": [
                "As baseline features, we use the same n-gram probability features as in #REF , who also employ a neural network to rank substitution candidates.",
                "As in #TARGET_REF , the features were extracted using the SubIMDB corpus (#REF) .",
                "We also experiment with additional features that have been reported as useful in this task.",
                "For each target word and a substitution candidate word we also compute: cosine similarity, word length, and alignment probability in the sentence-aligned Normal-Simple Wikipedia corpus (#REF) .",
                "The cosine similarity feature is computed using the SubIMDB corpus."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "As baseline features, we use the same n-gram probability features as in #REF , who also employ a neural network to rank substitution candidates. As in #TARGET_REF , the features were extracted using the SubIMDB corpus (#REF) . We also experiment with additional features that have been reported as useful in this task. For each target word and a substitution candidate word we also compute: cosine similarity, word length, and alignment probability in the sentence-aligned Normal-Simple Wikipedia corpus (#REF) . The cosine similarity feature is computed using the SubIMDB corpus.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"As in #TARGET_REF , the features were extracted using the SubIMDB corpus (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Following previous works that used supervised machine learning for ranking in lexical simplification (#REF; #TARGET_REF , we train the DSSM using the LexMTurk dataset (#REF) , which contains 500 instances composed of a sentence, a target word and substitution candidates ranked by simplicity #TARGET_REF .",
                "In order to learn the parameters W t and W s (Figure 1 ) of the DSSM, we use the standard backpropagation algorithm (#REF) .",
                "The objective used in this paper follows the pair-wise learning-to-rank paradigm outlined in (#REF) .",
                "Given a target word and its sentential context T , we obtain a list of candidates L. We set different positive values to the candidates based on their simplicity rankings.",
                "E.g., if the list of the candidates is ordered by simplificity as, L = {A + > B + > C + }, the labels are first constructed as L = {y A + = 3, y B + = 2, y C + = 1}. The values are then normalized by dividing by the maximum value in the list: L = {y A + = 1, y B + = 0.667, y C + = 0.333}. If the target word was not originally in L, we add it with label 0."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Following previous works that used supervised machine learning for ranking in lexical simplification (#REF; #TARGET_REF , we train the DSSM using the LexMTurk dataset (#REF) , which contains 500 instances composed of a sentence, a target word and substitution candidates ranked by simplicity #TARGET_REF . In order to learn the parameters W t and W s (Figure 1 ) of the DSSM, we use the standard backpropagation algorithm (#REF) . The objective used in this paper follows the pair-wise learning-to-rank paradigm outlined in (#REF) . Given a target word and its sentential context T , we obtain a list of candidates L. We set different positive values to the candidates based on their simplicity rankings. E.g., if the list of the candidates is ordered by simplificity as, L = {A + > B + > C + }, the labels are first constructed as L = {y A + = 3, y B + = 2, y C + = 1}. The values are then normalized by dividing by the maximum value in the list: L = {y A + = 1, y B + = 0.667, y C + = 0.333}. If the target word was not originally in L, we add it with label 0.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following previous works that used supervised machine learning for ranking in lexical simplification (#REF; #TARGET_REF , we train the DSSM using the LexMTurk dataset (#REF) , which contains 500 instances composed of a sentence, a target word and substitution candidates ranked by simplicity #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "To evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification: BenchLS (#REFb) , which contains 929 instances, and NNSEval (#REFa) , which contains 239 instances.",
                "Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity #TARGET_REF .",
                "Since both datasets contain instances from the LexMturk dataset (#REF) , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 .",
                "We finally obtain 429 remaining instances in the BenchLS dataset, and 78 instances in the NNEval dataset, which are used in our evaluation.",
                "We adopt the same evaluation metrics featured in Glavaš andŠtajner (2015) and #REF : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "To evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification: BenchLS (#REFb) , which contains 929 instances, and NNSEval (#REFa) , which contains 239 instances. Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity #TARGET_REF . Since both datasets contain instances from the LexMturk dataset (#REF) , which we use for training the DNN, we remove the overlap instances between training and test datasets 1 . We finally obtain 429 remaining instances in the BenchLS dataset, and 78 instances in the NNEval dataset, which are used in our evaluation. We adopt the same evaluation metrics featured in Glavaš andŠtajner (2015) and #REF : 1) precision: ratio of correct simplifications out of all the simplifications made by the system; 2) accuracy: ratio of correct simplifications out of all words that should have been simplified; and 3) changed: ratio of target words changed by the system.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"To evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification: BenchLS (#REFb) , which contains 929 instances, and NNSEval (#REFa) , which contains 239 instances.\", \"Each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We compared the proposed model (DSSM Ranking) to two state-of-the-art approaches to ranking in lexical simplification that exploit supervised machine learning-based methods.",
                "The first baseline is the Neural Substitution Ranking (NSR) approach described in #TARGET_REF , which employs a multi-layer perceptron neural network.",
                "We reimplement their model as part of the LEXenstein toolkit (#REF) .",
                "The network has 3 hidden layers with 8 nodes each.",
                "Unlike the proposed model, they treat ranking in lexical simplification as a standard classification problem."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We compared the proposed model (DSSM Ranking) to two state-of-the-art approaches to ranking in lexical simplification that exploit supervised machine learning-based methods. The first baseline is the Neural Substitution Ranking (NSR) approach described in #TARGET_REF , which employs a multi-layer perceptron neural network. We reimplement their model as part of the LEXenstein toolkit (#REF) . The network has 3 hidden layers with 8 nodes each. Unlike the proposed model, they treat ranking in lexical simplification as a standard classification problem.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We compared the proposed model (DSSM Ranking) to two state-of-the-art approaches to ranking in lexical simplification that exploit supervised machine learning-based methods.\", \"The first baseline is the Neural Substitution Ranking (NSR) approach described in #TARGET_REF , which employs a multi-layer perceptron neural network.\"]}"
    },
    {
        "gold": {
            "text": [
                "n-gram probs.",
                "denotes the n-gram probability features described in #REF , and all denotes all features described in Section 2.3.",
                "All values marked in bold are significantly higher compared to the best baseline, SVM rank , measured by t-test at p-value of 0.05.",
                "with default parameters) for ranking substitution candidates, similar to the method described in (#REF) .",
                "All the three models employ the n-gram probability features extracted from the SubIMDB corpus (#REF) , as described in #TARGET_REF , and are trained using the LexMTurk dataset."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "n-gram probs. denotes the n-gram probability features described in #REF , and all denotes all features described in Section 2.3. All values marked in bold are significantly higher compared to the best baseline, SVM rank , measured by t-test at p-value of 0.05. with default parameters) for ranking substitution candidates, similar to the method described in (#REF) . All the three models employ the n-gram probability features extracted from the SubIMDB corpus (#REF) , as described in #TARGET_REF , and are trained using the LexMTurk dataset.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"All the three models employ the n-gram probability features extracted from the SubIMDB corpus (#REF) , as described in #TARGET_REF , and are trained using the LexMTurk dataset.\"]}"
    },
    {
        "gold": {
            "text": [
                "We follow the Unsupervised Boundary Ranking Substitution Selection method described in #TARGET_REF , which ranks candidates according to how well they fit the context of the target word, and discards 50% of the worst ranking candidates.",
                "The bottom part of the table 1 (Selection Step + Substitution Candidates Ranking) summarizes the results of all ranking systems after performing the selection step on generated substitution candidates.",
                "We obtain similar tendency in the results, with the DSSM Ranking outperforming both baselines.",
                "The results indicate the advantage of using a deep architecture, and of building a semantic representation of the whole sentence on top of the characters.",
                "To illustrate by examples, Table 2 lists the top candidate ranked by the systems for different input sentences."
            ],
            "label": [
                "USE",
                "SIM",
                "DIFFER"
            ],
            "context": [
                1,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We follow the Unsupervised Boundary Ranking Substitution Selection method described in #TARGET_REF , which ranks candidates according to how well they fit the context of the target word, and discards 50% of the worst ranking candidates. The bottom part of the table 1 (Selection Step + Substitution Candidates Ranking) summarizes the results of all ranking systems after performing the selection step on generated substitution candidates. We obtain similar tendency in the results, with the DSSM Ranking outperforming both baselines. The results indicate the advantage of using a deep architecture, and of building a semantic representation of the whole sentence on top of the characters. To illustrate by examples, Table 2 lists the top candidate ranked by the systems for different input sentences.",
        "output": "{\"label\": [\"USE\", \"SIM\", \"DIFFER\"], \"context\": [\"We follow the Unsupervised Boundary Ranking Substitution Selection method described in #TARGET_REF , which ranks candidates according to how well they fit the context of the target word, and discards 50% of the worst ranking candidates.\", \"We obtain similar tendency in the results, with the DSSM Ranking outperforming both baselines.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, from an SMT perspective it is important that the models reflect probability distributions which are preferred by the decoding process, i.e., phrase translations which are likely to be used frequently to achieve better translations should get higher scores and phrases which are less likely to be used should get low scores.",
                "In addition, the heuristic extraction algorithm generates all possible, consistent phrases including overlapping phrases.",
                "This means that translation probabilities are distributed over a very large number of phrase translation candidates most of which never lead to the best possible translation of a sentence.",
                "In this paper, we propose a novel solution which is to re-estimate the models from the best BLEU translation of each source sentence in the bitext.",
                "An important contribution of our approach is that unlike previous approaches such as forced alignment #TARGET_REF , reordering and language models can also be re-estimated."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "However, from an SMT perspective it is important that the models reflect probability distributions which are preferred by the decoding process, i.e., phrase translations which are likely to be used frequently to achieve better translations should get higher scores and phrases which are less likely to be used should get low scores. In addition, the heuristic extraction algorithm generates all possible, consistent phrases including overlapping phrases. This means that translation probabilities are distributed over a very large number of phrase translation candidates most of which never lead to the best possible translation of a sentence. In this paper, we propose a novel solution which is to re-estimate the models from the best BLEU translation of each source sentence in the bitext. An important contribution of our approach is that unlike previous approaches such as forced alignment #TARGET_REF , reordering and language models can also be re-estimated.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"An important contribution of our approach is that unlike previous approaches such as forced alignment #TARGET_REF , reordering and language models can also be re-estimated.\"]}"
    },
    {
        "gold": {
            "text": [
                "Re-estimation of the translation models from the n-best translation of the bitext could re-enforce the probabilities of the low frequency phrase pairs in the re-estimated models leading to over-fitting.",
                "Within forced decoding, #TARGET_REF address this problem by using a leave-one-out approach where they modify the phrase translation probabilities for each sentence pair by removing the counts of all phrases that were extracted from that particular sentence.",
                "However, in our approach, we do not impose a constraint to produce the exact translation, instead we use the highest BLEU translations which may be very different from the references.",
                "Thus it is not strictly necessary to apply leave-one-out in our approach as a solution to over-fitting.",
                "Instead, we handle the problem by simply removing all the phrase pairs below a threshold count which in our case is 2,"
            ],
            "label": [
                "DIFFER",
                "BACK"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Re-estimation of the translation models from the n-best translation of the bitext could re-enforce the probabilities of the low frequency phrase pairs in the re-estimated models leading to over-fitting. Within forced decoding, #TARGET_REF address this problem by using a leave-one-out approach where they modify the phrase translation probabilities for each sentence pair by removing the counts of all phrases that were extracted from that particular sentence. However, in our approach, we do not impose a constraint to produce the exact translation, instead we use the highest BLEU translations which may be very different from the references. Thus it is not strictly necessary to apply leave-one-out in our approach as a solution to over-fitting. Instead, we handle the problem by simply removing all the phrase pairs below a threshold count which in our case is 2,",
        "output": "{\"label\": [\"DIFFER\", \"BACK\"], \"context\": [\"Within forced decoding, #TARGET_REF address this problem by using a leave-one-out approach where they modify the phrase translation probabilities for each sentence pair by removing the counts of all phrases that were extracted from that particular sentence.\", \"However, in our approach, we do not impose a constraint to produce the exact translation, instead we use the highest BLEU translations which may be very different from the references.\"]}"
    },
    {
        "gold": {
            "text": [
                "We discuss the experimental results of our oracle-BLEU re-estimation approach for different models and settings and provide a comparison with the baseline (heuristic training) and forced alignment.",
                "Re-estimated models with three different values of n ∈ {1, 10, 100} were evaluated under three settings: phrase table re-estimation, interpolation, and BiLM re-estimation.",
                "The best improvements over the baseline are obtained by using only 1-best (n= 1) alignments as shown in Table 1 .",
                "Surprisingly, this is in contrast with forced decoding as discussed in #TARGET_REF , where the best improvements are obtained for n = 100.",
                "Table 2 provides a comparison between BLEU improvements achieved by forced decoding (n = 100 best) and our oracle-BLEU re-estimation approach (n = 1 best) over the baseline for different models."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "We discuss the experimental results of our oracle-BLEU re-estimation approach for different models and settings and provide a comparison with the baseline (heuristic training) and forced alignment. Re-estimated models with three different values of n ∈ {1, 10, 100} were evaluated under three settings: phrase table re-estimation, interpolation, and BiLM re-estimation. The best improvements over the baseline are obtained by using only 1-best (n= 1) alignments as shown in Table 1 . Surprisingly, this is in contrast with forced decoding as discussed in #TARGET_REF , where the best improvements are obtained for n = 100. Table 2 provides a comparison between BLEU improvements achieved by forced decoding (n = 100 best) and our oracle-BLEU re-estimation approach (n = 1 best) over the baseline for different models.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The best improvements over the baseline are obtained by using only 1-best (n= 1) alignments as shown in Table 1 .\", \"Surprisingly, this is in contrast with forced decoding as discussed in #TARGET_REF , where the best improvements are obtained for n = 100.\"]}"
    },
    {
        "gold": {
            "text": [
                "The forced alignment technique of #TARGET_REF forms the main motivation for our work.",
                "In forced alignment, given a sentence pair (F, E), a decoder determines the best phrase segmentation and alignment which will result in a translation of F into E. The best segmentation is defined as the one which maximizes the probability of translating the source sentence into the given target sentence.",
                "At the end, the phrase table is re-estimated using the phrase pair segmentations obtained from forced decoding.",
                "Thus forced alignment is a reestimation technique where translation probabilities are calculated based on their frequency in best-scoring hypotheses instead of the frequencies of all possible phrase pairs in the bitext.",
                "However, one limitation of forced alignment is that only the phrase translation model can be re-estimated since it is restricted to align the source sentence to the given target reference, thus fixing the choice of reordering decisions."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The forced alignment technique of #TARGET_REF forms the main motivation for our work. In forced alignment, given a sentence pair (F, E), a decoder determines the best phrase segmentation and alignment which will result in a translation of F into E. The best segmentation is defined as the one which maximizes the probability of translating the source sentence into the given target sentence. At the end, the phrase table is re-estimated using the phrase pair segmentations obtained from forced decoding. Thus forced alignment is a reestimation technique where translation probabilities are calculated based on their frequency in best-scoring hypotheses instead of the frequencies of all possible phrase pairs in the bitext. However, one limitation of forced alignment is that only the phrase translation model can be re-estimated since it is restricted to align the source sentence to the given target reference, thus fixing the choice of reordering decisions.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"The forced alignment technique of #TARGET_REF forms the main motivation for our work.\"]}"
    },
    {
        "gold": {
            "text": [
                "A similar line of work is proposed by and who use a self-enhancing strategy to utilize additional mono- lingual source language data by aligning it to its target language translation obtained by using an SMT system to rank sentence translation probabilities.",
                "However, the main focus of their work is translation model adaptation by augmenting the bitext with additional training data and not the reestimation of the translation models trained on the parallel data.",
                "In this work, we propose that aligning source sentences to their oracle BLEU translations provides a more realistic estimate of the models from the decoding perspective instead of aligning them to high quality human translations as in forced decoding.",
                "Another relevant line of research relates tuning (weight optimisation), where our work lies between forced decoding #TARGET_REF and the bold updating approach of (#REF) .",
                "However, our approach specifically proposes a novel method for training models using oracle BLEU translations."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "A similar line of work is proposed by and who use a self-enhancing strategy to utilize additional mono- lingual source language data by aligning it to its target language translation obtained by using an SMT system to rank sentence translation probabilities. However, the main focus of their work is translation model adaptation by augmenting the bitext with additional training data and not the reestimation of the translation models trained on the parallel data. In this work, we propose that aligning source sentences to their oracle BLEU translations provides a more realistic estimate of the models from the decoding perspective instead of aligning them to high quality human translations as in forced decoding. Another relevant line of research relates tuning (weight optimisation), where our work lies between forced decoding #TARGET_REF and the bold updating approach of (#REF) . However, our approach specifically proposes a novel method for training models using oracle BLEU translations.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Another relevant line of research relates tuning (weight optimisation), where our work lies between forced decoding #TARGET_REF and the bold updating approach of (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "However, improvements achieved with this interpolation did not surpass the best result obtained for the oracle-BLEU re-estimation.",
                "Additionally, we also compare oracle-BLEU re-estimation to forced decoding with leave-oneout #TARGET_REF by evaluating both on a concatenation of 5 test sets (MT03, MT05-MT09).",
                "As shown in Table 3 , even with leaveone-out, forced decoding performance drops below the baseline by -0.3 BLEU.",
                "In contrast, phrase tables re-estimated from oracle-BLEU translation achieves the same performance as the baseline.",
                "When interpolated with the baseline phrase table, both approaches show significant improvements over the baseline."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "However, improvements achieved with this interpolation did not surpass the best result obtained for the oracle-BLEU re-estimation. Additionally, we also compare oracle-BLEU re-estimation to forced decoding with leave-oneout #TARGET_REF by evaluating both on a concatenation of 5 test sets (MT03, MT05-MT09). As shown in Table 3 , even with leaveone-out, forced decoding performance drops below the baseline by -0.3 BLEU. In contrast, phrase tables re-estimated from oracle-BLEU translation achieves the same performance as the baseline. When interpolated with the baseline phrase table, both approaches show significant improvements over the baseline.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Additionally, we also compare oracle-BLEU re-estimation to forced decoding with leave-oneout #TARGET_REF by evaluating both on a concatenation of 5 test sets (MT03, MT05-MT09).\"]}"
    },
    {
        "gold": {
            "text": [
                "Current state-of-the-art parsers score over 90% on the standard newswire evaluation, but the remaining errors are difficult to overcome using only the training corpus.",
                "Features from n-gram counts over resources like Web1T (#REF ) have proven to be useful proxies for syntax #TARGET_REF; #REF) , but they enforce linear word order, and are unable to distinguish between syntactic and non-syntactic co-occurrences.",
                "Longer n-grams are also noisier and sparser, limiting the range of potential features.",
                "In this paper we develop new features for the graph-based MSTParser (#REF) from the Google Syntactic Ngrams corpus (#REF) , a collection of Stanford dependency subtree counts.",
                "These features capture information collated across millions of subtrees produced by a shift-reduce parser, trading off potential systemic parser errors for data that is better aligned with the parsing task."
            ],
            "label": [
                "MOT",
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Current state-of-the-art parsers score over 90% on the standard newswire evaluation, but the remaining errors are difficult to overcome using only the training corpus. Features from n-gram counts over resources like Web1T (#REF ) have proven to be useful proxies for syntax #TARGET_REF; #REF) , but they enforce linear word order, and are unable to distinguish between syntactic and non-syntactic co-occurrences. Longer n-grams are also noisier and sparser, limiting the range of potential features. In this paper we develop new features for the graph-based MSTParser (#REF) from the Google Syntactic Ngrams corpus (#REF) , a collection of Stanford dependency subtree counts. These features capture information collated across millions of subtrees produced by a shift-reduce parser, trading off potential systemic parser errors for data that is better aligned with the parsing task.",
        "output": "{\"label\": [\"MOT\", \"BACK\"], \"context\": [\"Features from n-gram counts over resources like Web1T (#REF ) have proven to be useful proxies for syntax #TARGET_REF; #REF) , but they enforce linear word order, and are unable to distinguish between syntactic and non-syntactic co-occurrences.\"]}"
    },
    {
        "gold": {
            "text": [
                "Experiments on the development set led to a minimum cutoff frequency of 10,000 for each feature to avoid noise from parser and OCR errors.",
                "3 Surface n-gram Features #TARGET_REF demonstrate that features generated from bucketing simple surface n-gram counts and collecting the top paraphrase-based contextual words over Web1T are useful for almost all attachment decisions, boosting dependency parsing accuracy by up to 0.6%.",
                "However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts.",
                "#REF also tested only on in-domain text, though these external count features should be useful out of domain.",
                "We extract #REF 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts."
            ],
            "label": [
                "MOT",
                "BACK"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Experiments on the development set led to a minimum cutoff frequency of 10,000 for each feature to avoid noise from parser and OCR errors. 3 Surface n-gram Features #TARGET_REF demonstrate that features generated from bucketing simple surface n-gram counts and collecting the top paraphrase-based contextual words over Web1T are useful for almost all attachment decisions, boosting dependency parsing accuracy by up to 0.6%. However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts. #REF also tested only on in-domain text, though these external count features should be useful out of domain. We extract #REF 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts.",
        "output": "{\"label\": [\"MOT\", \"BACK\"], \"context\": [\"3 Surface n-gram Features #TARGET_REF demonstrate that features generated from bucketing simple surface n-gram counts and collecting the top paraphrase-based contextual words over Web1T are useful for almost all attachment decisions, boosting dependency parsing accuracy by up to 0.6%.\", \"However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts.\"]}"
    },
    {
        "gold": {
            "text": [
                "Affinity features rely on the intuition that frequently co-occurring words in large unlabeled text collections are likely to be in a syntactic relationship (#REF; #TARGET_REF .",
                "N-gram resources such as Web1T and Google Books provide large offline collections from which these co-occurrence statistics can be harvested; given each head and argument ambiguity in a training and test corpus, the corpora can be linearly scanned ahead of parsing time to reduce the impact of querying in the parser.",
                "When scanning, the head and argument word may appear immediately adjacent to one another in linear order (CONTIG), or with up to three intervening words (GAP1, GAP2, and GAP3) as the maximum n-gram length is five.",
                "The total count is then discretized as per Equation 1 previously.",
                "The final parser features include the POS tags of the potential head and argument, the discretized count, directionality, and the binned length of the dependency."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Affinity features rely on the intuition that frequently co-occurring words in large unlabeled text collections are likely to be in a syntactic relationship (#REF; #TARGET_REF . N-gram resources such as Web1T and Google Books provide large offline collections from which these co-occurrence statistics can be harvested; given each head and argument ambiguity in a training and test corpus, the corpora can be linearly scanned ahead of parsing time to reduce the impact of querying in the parser. When scanning, the head and argument word may appear immediately adjacent to one another in linear order (CONTIG), or with up to three intervening words (GAP1, GAP2, and GAP3) as the maximum n-gram length is five. The total count is then discretized as per Equation 1 previously. The final parser features include the POS tags of the potential head and argument, the discretized count, directionality, and the binned length of the dependency.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Affinity features rely on the intuition that frequently co-occurring words in large unlabeled text collections are likely to be in a syntactic relationship (#REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In #TARGET_REF , paraphrase features are generated for all full-parse attachment ambiguities from the surface n-gram corpus.",
                "For each attachment ambiguity, 3-grams of the form ( q 1 q 2 ), (q 1 q 2 ), and (q 1 q 2 ) are extracted, where q 1 and q 2 are the head and argument in their linear order of appearance in the original sentence, and is any single context word appearing before, in between, or after the query words.",
                "Then the most frequent words appearing in each of these configurations for each head-argument ambiguity is encoded as a feature with the POS tags of the head and argument 2 .",
                "Given the arc hold → hearing in Figure 2 , public is the most frequent word appearing in the n-gram (hold hearing) in Web1T.",
                "Thus, the final encoded feature is POS (hold) ∧ POS (hearing) ∧ public ∧ mid."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In #TARGET_REF , paraphrase features are generated for all full-parse attachment ambiguities from the surface n-gram corpus. For each attachment ambiguity, 3-grams of the form ( q 1 q 2 ), (q 1 q 2 ), and (q 1 q 2 ) are extracted, where q 1 and q 2 are the head and argument in their linear order of appearance in the original sentence, and is any single context word appearing before, in between, or after the query words. Then the most frequent words appearing in each of these configurations for each head-argument ambiguity is encoded as a feature with the POS tags of the head and argument 2 . Given the arc hold → hearing in Figure 2 , public is the most frequent word appearing in the n-gram (hold hearing) in Web1T. Thus, the final encoded feature is POS (hold) ∧ POS (hearing) ∧ public ∧ mid.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In #TARGET_REF , paraphrase features are generated for all full-parse attachment ambiguities from the surface n-gram corpus.\"]}"
    },
    {
        "gold": {
            "text": [
                "Surface n-gram counts from large web corpora have been used to address NP and PP attachment errors (#REF; #REF) Aside from #TARGET_REF , other feature-based approaches to improving dependency parsing include #REF , who exploits Brown clusters and point-wise mutual information of surface n-gram counts to specifically address PP and coordination errors.",
                "#REF describe a novel way of generating meta-features that work to emphasise important feature types used by the parser.",
                "#REF generate subtree-based features that are similar to ours.",
                "However, they use the in-domain BLLIP newswire corpus to generate their subtree counts, whereas the Syntactic Ngrams corpus is out-of-domain and an order of magnitude larger.",
                "They also use the same underlying parser to generate the BLLIP subtree counts and as the final test-time parser, while Syntactic Ngrams is parsed with a simpler, shift-reduce parser compared to the graph-based MSTParser used during test time."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Surface n-gram counts from large web corpora have been used to address NP and PP attachment errors (#REF; #REF) Aside from #TARGET_REF , other feature-based approaches to improving dependency parsing include #REF , who exploits Brown clusters and point-wise mutual information of surface n-gram counts to specifically address PP and coordination errors. #REF describe a novel way of generating meta-features that work to emphasise important feature types used by the parser. #REF generate subtree-based features that are similar to ours. However, they use the in-domain BLLIP newswire corpus to generate their subtree counts, whereas the Syntactic Ngrams corpus is out-of-domain and an order of magnitude larger. They also use the same underlying parser to generate the BLLIP subtree counts and as the final test-time parser, while Syntactic Ngrams is parsed with a simpler, shift-reduce parser compared to the graph-based MSTParser used during test time.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Surface n-gram counts from large web corpora have been used to address NP and PP attachment errors (#REF; #REF) Aside from #TARGET_REF , other feature-based approaches to improving dependency parsing include #REF , who exploits Brown clusters and point-wise mutual information of surface n-gram counts to specifically address PP and coordination errors.\"]}"
    },
    {
        "gold": {
            "text": [
                "Longer n-grams are also noisier and sparser, limiting the range of potential features.",
                "In this paper we develop new features for the graph-based MSTParser (#REF) from the Google Syntactic Ngrams corpus (#REF) , a collection of Stanford dependency subtree counts.",
                "These features capture information collated across millions of subtrees produced by a shift-reduce parser, trading off potential systemic parser errors for data that is better aligned with the parsing task.",
                "We compare the performance of our syntactic n-gram features against the surface n-gram features of #TARGET_REF in-domain on newswire and out-of-domain on the English Web Treebank (#REF) across CoNLL-style (LTH) dependencies.",
                "We also extend the first-order surface n-gram features to second-order, and compare the utility of Web1T and the Google Books Ngram corpus (#REF) as surface n-gram sources."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Longer n-grams are also noisier and sparser, limiting the range of potential features. In this paper we develop new features for the graph-based MSTParser (#REF) from the Google Syntactic Ngrams corpus (#REF) , a collection of Stanford dependency subtree counts. These features capture information collated across millions of subtrees produced by a shift-reduce parser, trading off potential systemic parser errors for data that is better aligned with the parsing task. We compare the performance of our syntactic n-gram features against the surface n-gram features of #TARGET_REF in-domain on newswire and out-of-domain on the English Web Treebank (#REF) across CoNLL-style (LTH) dependencies. We also extend the first-order surface n-gram features to second-order, and compare the utility of Web1T and the Google Books Ngram corpus (#REF) as surface n-gram sources.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We compare the performance of our syntactic n-gram features against the surface n-gram features of #TARGET_REF in-domain on newswire and out-of-domain on the English Web Treebank (#REF) across CoNLL-style (LTH) dependencies.\"]}"
    },
    {
        "gold": {
            "text": [
                "Additional features for each bucket value up to the maximum are also encoded.",
                "We also develop paraphrase-style features like those of #TARGET_REF based on the most frequently occurring words and POS tags before, in between, and after each head-argument ambiguity (see Section 3.2).",
                "Figure 1 depicts the potential context words available the hold → hearing dependency.",
                "We experiment with a number of second-order features, mirroring those extracted for surface ngrams in Section 3.3.",
                "We extract all triple and sibling word and POS structures considered by the parser in the training and test corpora (following the factorization depicted in Figure 2 ), and counted their frequency in the Syntactic Ngrams corpus."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Additional features for each bucket value up to the maximum are also encoded. We also develop paraphrase-style features like those of #TARGET_REF based on the most frequently occurring words and POS tags before, in between, and after each head-argument ambiguity (see Section 3.2). Figure 1 depicts the potential context words available the hold → hearing dependency. We experiment with a number of second-order features, mirroring those extracted for surface ngrams in Section 3.3. We extract all triple and sibling word and POS structures considered by the parser in the training and test corpora (following the factorization depicted in Figure 2 ), and counted their frequency in the Syntactic Ngrams corpus.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"We also develop paraphrase-style features like those of #TARGET_REF based on the most frequently occurring words and POS tags before, in between, and after each head-argument ambiguity (see Section 3.2).\"]}"
    },
    {
        "gold": {
            "text": [
                "However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts.",
                "#REF also tested only on in-domain text, though these external count features should be useful out of domain.",
                "We extract #REF 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts.",
                "Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts.",
                "We also extend #TARGET_REF Klein's affinity and paraphrase features to second-order."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts. #REF also tested only on in-domain text, though these external count features should be useful out of domain. We extract #REF 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts. Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts. We also extend #TARGET_REF Klein's affinity and paraphrase features to second-order.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We also extend #TARGET_REF Klein's affinity and paraphrase features to second-order.\"]}"
    },
    {
        "gold": {
            "text": [
                "As with #TARGET_REF #REF , we convert the Penn Treebank to dependencies using pennconverter 3 (#REF) (henceforth LTH) and generate POS tags with MX-POST (#REF) .",
                "We used sections 02-21 of the WSJ for training, 22 for development, and 23 for final testing.",
                "The test sections of the answers, newsgroups, and reviews sections of the English Web Treebank as per the SANCL 2012 Shared Task (#REF) were converted to LTH and used for out-of-domain evaluation.",
                "We used MSTParser (#REF) , trained with the parameters order:2, training-k:5, iters:10, and loss-type:nopunc.",
                "We omit labeled attachment scores in this paper for brevity, but they are consistent with the reported UAS scores."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "As with #TARGET_REF #REF , we convert the Penn Treebank to dependencies using pennconverter 3 (#REF) (henceforth LTH) and generate POS tags with MX-POST (#REF) . We used sections 02-21 of the WSJ for training, 22 for development, and 23 for final testing. The test sections of the answers, newsgroups, and reviews sections of the English Web Treebank as per the SANCL 2012 Shared Task (#REF) were converted to LTH and used for out-of-domain evaluation. We used MSTParser (#REF) , trained with the parameters order:2, training-k:5, iters:10, and loss-type:nopunc. We omit labeled attachment scores in this paper for brevity, but they are consistent with the reported UAS scores.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"As with #TARGET_REF #REF , we convert the Penn Treebank to dependencies using pennconverter 3 (#REF) (henceforth LTH) and generate POS tags with MX-POST (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts.",
                "#REF also tested only on in-domain text, though these external count features should be useful out of domain.",
                "We extract #TARGET_REF 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts.",
                "Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts.",
                "We also extend Bansal and Klein's affinity and paraphrase features to second-order."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "However, this technique is restricted to counts based purely on the linear order of the adjacent words, and is unable to incorporate disambiguating information such as POS tags to avoid spurious counts. #REF also tested only on in-domain text, though these external count features should be useful out of domain. We extract #TARGET_REF 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts. Both corpora are very large, contain different types of noise, and are sourced from very different underlying texts. We also extend Bansal and Klein's affinity and paraphrase features to second-order.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"We extract #TARGET_REF 's affinity and paraphrase-style first-order features from the Google Books English Ngrams corpus, and compare their performance against Web1T counts.\"]}"
    },
    {
        "gold": {
            "text": [
                "The premier example for word embeddings is skip-gram negative sampling, which is part of the word2vec family of algorithms (#REF) .",
                "The random processes involved in training these embeddings lead to a lack of reliability which is dangerous during interpretationexperiments cannot be repeated without predicting severely different relationships between words #REFa, 2017) .",
                "Word embeddings based on singular value decomposition (SVD; historically popular in the form of Latent Semantic Analysis (#REF) ) are not affected by this problem.",
                "#REF created SVD PPMI after investigating the implicit operations performed while training neural word embeddings (#REF) .",
                "As SVD PPMI performs very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the Digital Humanities #TARGET_REF; #REFa) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The premier example for word embeddings is skip-gram negative sampling, which is part of the word2vec family of algorithms (#REF) . The random processes involved in training these embeddings lead to a lack of reliability which is dangerous during interpretationexperiments cannot be repeated without predicting severely different relationships between words #REFa, 2017) . Word embeddings based on singular value decomposition (SVD; historically popular in the form of Latent Semantic Analysis (#REF) ) are not affected by this problem. #REF created SVD PPMI after investigating the implicit operations performed while training neural word embeddings (#REF) . As SVD PPMI performs very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the Digital Humanities #TARGET_REF; #REFa) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"As SVD PPMI performs very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the Digital Humanities #TARGET_REF; #REFa) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Meanwhile, word embeddings and their application to diachronic semantics have become a novel state-of-the-art methodology lacking, however, off-the-shelves analysis tools easy to use for a typically non-technical audience.",
                "Most work is centered around word2vec (e.g., #REF ; #REF ; Hellrich and Hahn (2016b) ), whereas alternative approaches are rare, e.g., #REF using GloVe (#REF) and #TARGET_REF using SVD PPMI .",
                "Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (#REF; #REF; #REF) and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space.",
                "This information can then be exploited for automatic (#REF) or manual (#REF) interpretation.",
                "research."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Meanwhile, word embeddings and their application to diachronic semantics have become a novel state-of-the-art methodology lacking, however, off-the-shelves analysis tools easy to use for a typically non-technical audience. Most work is centered around word2vec (e.g., #REF ; #REF ; Hellrich and Hahn (2016b) ), whereas alternative approaches are rare, e.g., #REF using GloVe (#REF) and #TARGET_REF using SVD PPMI . Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (#REF; #REF; #REF) and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space. This information can then be exploited for automatic (#REF) or manual (#REF) interpretation. research.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Most work is centered around word2vec (e.g., #REF ; #REF ; Hellrich and Hahn (2016b) ), whereas alternative approaches are rare, e.g., #REF using GloVe (#REF) and #TARGET_REF using SVD PPMI .\"]}"
    },
    {
        "gold": {
            "text": [
                "Most work is centered around word2vec (e.g., #REF ; #REF ; Hellrich and Hahn (2016b) ), whereas alternative approaches are rare, e.g., #REF using GloVe (#REF) and #REF using SVD PPMI .",
                "Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (#REF; #REF; #TARGET_REF and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space.",
                "This information can then be exploited for automatic (#REF) or manual (#REF) interpretation.",
                "research.",
                "We employ five corpora, including the four largest diachronic corpora of acceptable quality for English and German."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Most work is centered around word2vec (e.g., #REF ; #REF ; Hellrich and Hahn (2016b) ), whereas alternative approaches are rare, e.g., #REF using GloVe (#REF) and #REF using SVD PPMI . Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (#REF; #REF; #TARGET_REF and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space. This information can then be exploited for automatic (#REF) or manual (#REF) interpretation. research. We employ five corpora, including the four largest diachronic corpora of acceptable quality for English and German.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (#REF; #REF; #TARGET_REF and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space.\"]}"
    },
    {
        "gold": {
            "text": [
                "9 To ensure JESEME's responsiveness, we finally pre-computed similarity (by cosine between word embeddings), as well as context specificity based on PPMI and χ 2 .",
                "These values are stored in a POSTGRESQL 10 database, occupying about 60GB of space.",
                "Due to both space constraints (scaling with O(n 2 ) for vocabulary size n) and the lower quality of representations for infrequent words, we limited this step to words which were among the 10k most frequent words for all slices of a corpus, resulting in 3,1k -6,5k words per corpus.",
                "In accordance with this limit, we also discarded slices with less than 10k (5k for RSC) 7 Parameters were chosen in accordance with #REF and #TARGET_REF words above the minimum frequency threshold used during PPMI and χ 2 calculation, e.g., the 1810s and 1820s COHA slices.",
                "Figure 1 illustrates this sequence of processing steps, while Table 1 summarizes the resulting models for each corpus."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "9 To ensure JESEME's responsiveness, we finally pre-computed similarity (by cosine between word embeddings), as well as context specificity based on PPMI and χ 2 . These values are stored in a POSTGRESQL 10 database, occupying about 60GB of space. Due to both space constraints (scaling with O(n 2 ) for vocabulary size n) and the lower quality of representations for infrequent words, we limited this step to words which were among the 10k most frequent words for all slices of a corpus, resulting in 3,1k -6,5k words per corpus. In accordance with this limit, we also discarded slices with less than 10k (5k for RSC) 7 Parameters were chosen in accordance with #REF and #TARGET_REF words above the minimum frequency threshold used during PPMI and χ 2 calculation, e.g., the 1810s and 1820s COHA slices. Figure 1 illustrates this sequence of processing steps, while Table 1 summarizes the resulting models for each corpus.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"In accordance with this limit, we also discarded slices with less than 10k (5k for RSC) 7 Parameters were chosen in accordance with #REF and #TARGET_REF words above the minimum frequency threshold used during PPMI and \\u03c7 2 calculation, e.g., the 1810s and 1820s COHA slices.\"]}"
    },
    {
        "gold": {
            "text": [
                "The result page provides three kinds of graphs, i.e., Similar Words, Typical Context and Relative Frequency.",
                "Similar Words depicts the words with the highest similarity relative to the query term for the first and last time slice and how their similarity values changed over time.",
                "We follow #REF in choosing such a visualization, while we refrain from using the two-dimensional projection used in other studies (#REF; #TARGET_REF .",
                "We stipulate that the latter could Figure 2 : Screenshot of JESEME's result page when searching for the lexical item \"heart\" in COHA.",
                "be potentially misleading by implying a constant meaning of those words used as the background (which are actually positioned by their meaning at a single point in time)."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The result page provides three kinds of graphs, i.e., Similar Words, Typical Context and Relative Frequency. Similar Words depicts the words with the highest similarity relative to the query term for the first and last time slice and how their similarity values changed over time. We follow #REF in choosing such a visualization, while we refrain from using the two-dimensional projection used in other studies (#REF; #TARGET_REF . We stipulate that the latter could Figure 2 : Screenshot of JESEME's result page when searching for the lexical item \"heart\" in COHA. be potentially misleading by implying a constant meaning of those words used as the background (which are actually positioned by their meaning at a single point in time).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We follow #REF in choosing such a visualization, while we refrain from using the two-dimensional projection used in other studies (#REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In contrast to other corpus exploration tools, JESEME is based on cutting-edge word embedding technology (#REF; #REF; #REFa, 2017) and provides access to five popular corpora for the English and German language.",
                "JESEME is also the first tool of its kind and under continuous development.",
                "Future technical work will add functionality to compare words across corpora which might require a mapping between embeddings (#REF; #TARGET_REF and provide optional stemming routines.",
                "Both goals come with an increase in precomputed similarity values and will thus necessitate storage optimizations to ensure long-term availability.",
                "Finally, we will conduct a user study to investigate JESEME's potential for the Digital Humanities community."
            ],
            "label": [
                "FUT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In contrast to other corpus exploration tools, JESEME is based on cutting-edge word embedding technology (#REF; #REF; #REFa, 2017) and provides access to five popular corpora for the English and German language. JESEME is also the first tool of its kind and under continuous development. Future technical work will add functionality to compare words across corpora which might require a mapping between embeddings (#REF; #TARGET_REF and provide optional stemming routines. Both goals come with an increase in precomputed similarity values and will thus necessitate storage optimizations to ensure long-term availability. Finally, we will conduct a user study to investigate JESEME's potential for the Digital Humanities community.",
        "output": "{\"label\": [\"FUT\"], \"context\": [\"Future technical work will add functionality to compare words across corpora which might require a mapping between embeddings (#REF; #TARGET_REF and provide optional stemming routines.\"]}"
    },
    {
        "gold": {
            "text": [
                "In automatic scoring, phraseological expres-sions have long been used almost exclusively for detecting errors, a task for which they have been very useful (e.g., #REF; #REF; #REF) .",
                "It is noteworthy that a feature tracking the correct use of collocations was considered for inclusion in eRater, but its usefulness for predicting text quality seems rather limited (#REF) .",
                "Very recently, however, #REF and #TARGET_REF Even if these results were extremely promising, they leave a number of questions unanswered.",
                "First, they were obtained by studying short oral responses.",
                "Can they be generalized to longer written texts, a situation that allows the learner to spend much more time on its production? Then one can wonder whether the use of MI is sufficient, or if additional benefits can be obtained by taking into account other associational measures for collocations."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In automatic scoring, phraseological expres-sions have long been used almost exclusively for detecting errors, a task for which they have been very useful (e.g., #REF; #REF; #REF) . It is noteworthy that a feature tracking the correct use of collocations was considered for inclusion in eRater, but its usefulness for predicting text quality seems rather limited (#REF) . Very recently, however, #REF and #TARGET_REF Even if these results were extremely promising, they leave a number of questions unanswered. First, they were obtained by studying short oral responses. Can they be generalized to longer written texts, a situation that allows the learner to spend much more time on its production? Then one can wonder whether the use of MI is sufficient, or if additional benefits can be obtained by taking into account other associational measures for collocations.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Very recently, however, #REF and #TARGET_REF Even if these results were extremely promising, they leave a number of questions unanswered.\"]}"
    },
    {
        "gold": {
            "text": [
                "z (#REF), the signed squareroot of the cell contribution to the Pearson Chi-square for a 2x2 contingency table, 3. simple-ll (#REF), the signed cell contribution to the log-likelihood Chi-square test recommended by #REF , 4.",
                "Fisher's exact test (#REF) , which corresponds to the probability of observing, under the null hypothesis of independence, at least as many collocations as the number actually observed, 5.",
                "Mutual rank ratio (mrr, #REF), a nonparametric measure that has been successful in detecting collocation errors in EFL texts (#REF), 6 . logDice (#REF), a logarithmic transformation of the Dice coefficient used in the Sketch Engine (#REF) .",
                "In order to extract more information from the distribution of the ASs in each text than the mean or the median, #REF and #TARGET_REF used a standard procedure in descriptive statistics and automatic information processing known as discretization, binning or quantization (#REF) .",
                "It divides a continuous variable into bins and counts the proportion of scores that fall into each bin."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "z (#REF), the signed squareroot of the cell contribution to the Pearson Chi-square for a 2x2 contingency table, 3. simple-ll (#REF), the signed cell contribution to the log-likelihood Chi-square test recommended by #REF , 4. Fisher's exact test (#REF) , which corresponds to the probability of observing, under the null hypothesis of independence, at least as many collocations as the number actually observed, 5. Mutual rank ratio (mrr, #REF), a nonparametric measure that has been successful in detecting collocation errors in EFL texts (#REF), 6 . logDice (#REF), a logarithmic transformation of the Dice coefficient used in the Sketch Engine (#REF) . In order to extract more information from the distribution of the ASs in each text than the mean or the median, #REF and #TARGET_REF used a standard procedure in descriptive statistics and automatic information processing known as discretization, binning or quantization (#REF) . It divides a continuous variable into bins and counts the proportion of scores that fall into each bin.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In order to extract more information from the distribution of the ASs in each text than the mean or the median, #REF and #TARGET_REF used a standard procedure in descriptive statistics and automatic information processing known as discretization, binning or quantization (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Extracted from the Cambridge Learner Corpus, this dataset consists of 1238 texts of between 200 and 400 words, to which an overall mark has been assigned.",
                "As in #REF , the 1141 texts from the year 2000 were used for training, while the 97 texts from the year 2001 were used for testing.",
                "Collocational Features: The global statistical features in #TARGET_REF and were used: the mean, the median, the maximum and the minimum of the ASs, and the proportion of bigrams that are present in the learner text but absent from the reference corpus.",
                "Because the best number of bins for discretizing the distributions was not known, the following ones were compared: 3, 5, 8, 10, 15, 20, 25, 33, 50, 75 and 100. To get all these features, each learner text was tokenized and POS-tagged by means of CLAWS7 2 and all bigrams were extracted.",
                "Punctuation marks and any sequence of characters that did not correspond to a word interrupt the bigram extraction."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Extracted from the Cambridge Learner Corpus, this dataset consists of 1238 texts of between 200 and 400 words, to which an overall mark has been assigned. As in #REF , the 1141 texts from the year 2000 were used for training, while the 97 texts from the year 2001 were used for testing. Collocational Features: The global statistical features in #TARGET_REF and were used: the mean, the median, the maximum and the minimum of the ASs, and the proportion of bigrams that are present in the learner text but absent from the reference corpus. Because the best number of bins for discretizing the distributions was not known, the following ones were compared: 3, 5, 8, 10, 15, 20, 25, 33, 50, 75 and 100. To get all these features, each learner text was tokenized and POS-tagged by means of CLAWS7 2 and all bigrams were extracted. Punctuation marks and any sequence of characters that did not correspond to a word interrupt the bigram extraction.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Collocational Features: The global statistical features in #TARGET_REF and were used: the mean, the median, the maximum and the minimum of the ASs, and the proportion of bigrams that are present in the learner text but absent from the reference corpus.\"]}"
    },
    {
        "gold": {
            "text": [
                "From eight bins and beyond, using all the ASs gave the best result, but the gain was relatively small.",
                "Regarding the number of bins, at least five seems necessary, but using many more did not harm performance.",
                "It is noteworthy that all the correlations reported in table 1 are much larger that the correlation of a baseline system based purely on length (r = 0.27).",
                "To determine if the automatic procedure for discretizing the ASs is at least as effective as the bin boundaries manually set by #TARGET_REF , I used them instead of the automatic bins for the model with eight bins based on MI.",
                "The correlation obtained was 0.60, a value slightly lower than that reported in Table 1 (0.61)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "From eight bins and beyond, using all the ASs gave the best result, but the gain was relatively small. Regarding the number of bins, at least five seems necessary, but using many more did not harm performance. It is noteworthy that all the correlations reported in table 1 are much larger that the correlation of a baseline system based purely on length (r = 0.27). To determine if the automatic procedure for discretizing the ASs is at least as effective as the bin boundaries manually set by #TARGET_REF , I used them instead of the automatic bins for the model with eight bins based on MI. The correlation obtained was 0.60, a value slightly lower than that reported in Table 1 (0.61).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"To determine if the automatic procedure for discretizing the ASs is at least as effective as the bin boundaries manually set by #TARGET_REF , I used them instead of the automatic bins for the model with eight bins based on MI.\"]}"
    },
    {
        "gold": {
            "text": [
                "It is also necessary to determine whether the collocational features can improve not only the baseline used here, but also a predictive model that includes many other features known for their effectiveness.",
                "Further developments are worth mentioning.",
                "Unlike #TARGET_REF , I only used bigrams' collocational features.",
                "Whether adding trigrams would further improve the performance is an open question.",
                "Trying to answer it requires a thorough study of the association measures for ngrams longer than two words since they have received much less attention (#REF; #REF) ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "It is also necessary to determine whether the collocational features can improve not only the baseline used here, but also a predictive model that includes many other features known for their effectiveness. Further developments are worth mentioning. Unlike #TARGET_REF , I only used bigrams' collocational features. Whether adding trigrams would further improve the performance is an open question. Trying to answer it requires a thorough study of the association measures for ngrams longer than two words since they have received much less attention (#REF; #REF) .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Unlike #TARGET_REF , I only used bigrams' collocational features.\"]}"
    },
    {
        "gold": {
            "text": [
                "Unsupervised speech representation learning [2, 3, 4, 5, 6, #TARGET_REF 8, 9, 10] is effective in extracting high-level properties from speech.",
                "SLP downstream tasks can be improved through speech representations because surface features such as log Mel-spectrograms or waveform can poorly reveal the abundant information within speech.",
                "Contrastive Predictive Coding (CPC) [5] and wav2vec [7] use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task.",
                "Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss.",
                "Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6, 7] ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Unsupervised speech representation learning [2, 3, 4, 5, 6, #TARGET_REF 8, 9, 10] is effective in extracting high-level properties from speech. SLP downstream tasks can be improved through speech representations because surface features such as log Mel-spectrograms or waveform can poorly reveal the abundant information within speech. Contrastive Predictive Coding (CPC) [5] and wav2vec [7] use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task. Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss. Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6, 7] .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Unsupervised speech representation learning [2, 3, 4, 5, 6, #TARGET_REF 8, 9, 10] is effective in extracting high-level properties from speech.\"]}"
    },
    {
        "gold": {
            "text": [
                "Unsupervised speech representation learning [2, 3, 4, 5, 6, 7, 8, 9, 10] is effective in extracting high-level properties from speech.",
                "SLP downstream tasks can be improved through speech representations because surface features such as log Mel-spectrograms or waveform can poorly reveal the abundant information within speech.",
                "Contrastive Predictive Coding (CPC) [5] and wav2vec #TARGET_REF use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task.",
                "Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss.",
                "Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6, 7] ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Unsupervised speech representation learning [2, 3, 4, 5, 6, 7, 8, 9, 10] is effective in extracting high-level properties from speech. SLP downstream tasks can be improved through speech representations because surface features such as log Mel-spectrograms or waveform can poorly reveal the abundant information within speech. Contrastive Predictive Coding (CPC) [5] and wav2vec #TARGET_REF use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task. Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss. Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6, 7] .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Contrastive Predictive Coding (CPC) [5] and wav2vec #TARGET_REF use a multi-layer CNN to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task.\"]}"
    },
    {
        "gold": {
            "text": [
                "Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss.",
                "Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6, #TARGET_REF .",
                "However, this constraint on model architectures limits the potential of speech representation learning.",
                "The recently proposed vq-wav2vec [8] approach attempts to apply the well-performing Natural Language Processing (NLP) algorithm BERT [12] on continuous speech.",
                "Input speech is discretized to a K-way quantized embedding space, so continuous speech could act like discrete units similar to word tokens in NLP tasks."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Autoregressive Predictive Coding (APC) [6] uses autoregressive models to encode temporal information of past acoustic sequences; the model predicts future frames like an RNN-based language model [11] , optimized with reconstruction loss. Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6, #TARGET_REF . However, this constraint on model architectures limits the potential of speech representation learning. The recently proposed vq-wav2vec [8] approach attempts to apply the well-performing Natural Language Processing (NLP) algorithm BERT [12] on continuous speech. Input speech is discretized to a K-way quantized embedding space, so continuous speech could act like discrete units similar to word tokens in NLP tasks.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Unidirectional models are commonly used in the previous approaches [2, 3, 4, 5, 6, #TARGET_REF .\", \"However, this constraint on model architectures limits the potential of speech representation learning.\"]}"
    },
    {
        "gold": {
            "text": [
                "Unlike previous left-to-right unidirectional approaches that only consider past sequences to predict information about future frames, the proposed method allows us to train a bidirectional speech representation model, alleviating the unidirectionality constraint of previous methods.",
                "As a result, the Mockingjay model obtains substantial improvements in several SLP tasks.",
                "Moreover, as previous approaches restrict the power of the pre-trained models to representation extraction only [5, 6, #TARGET_REF 8] , the proposed method is robust and can be fine-tuned easily on downstream tasks.",
                "We show that finetuning for 2 epochs easily acquires significant improvement.",
                "The proposed approach outperforms other representations and features."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Unlike previous left-to-right unidirectional approaches that only consider past sequences to predict information about future frames, the proposed method allows us to train a bidirectional speech representation model, alleviating the unidirectionality constraint of previous methods. As a result, the Mockingjay model obtains substantial improvements in several SLP tasks. Moreover, as previous approaches restrict the power of the pre-trained models to representation extraction only [5, 6, #TARGET_REF 8] , the proposed method is robust and can be fine-tuned easily on downstream tasks. We show that finetuning for 2 epochs easily acquires significant improvement. The proposed approach outperforms other representations and features.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Moreover, as previous approaches restrict the power of the pre-trained models to representation extraction only [5, 6, #TARGET_REF 8] , the proposed method is robust and can be fine-tuned easily on downstream tasks.\"]}"
    },
    {
        "gold": {
            "text": [
                "The proposed approaches are mainly compared with APC [6] representations, as they also experiment on phone classification and speaker verification.",
                "As reported in [6] , the APC approach outperformed CPC representations [5, #TARGET_REF 9] in both two tasks, which makes APC suitable as a strong baseline.",
                "APC uses an unidirectional autoregressive model.",
                "We compare the proposed approach with APC to show that our bidirectional approach has advantages in speech representation learning.",
                "For fair comparison, we pre-train APC using their official implementations with the reported ideal parameters and settings, but expand the model's hidden size to H dim =768 to match ours."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The proposed approaches are mainly compared with APC [6] representations, as they also experiment on phone classification and speaker verification. As reported in [6] , the APC approach outperformed CPC representations [5, #TARGET_REF 9] in both two tasks, which makes APC suitable as a strong baseline. APC uses an unidirectional autoregressive model. We compare the proposed approach with APC to show that our bidirectional approach has advantages in speech representation learning. For fair comparison, we pre-train APC using their official implementations with the reported ideal parameters and settings, but expand the model's hidden size to H dim =768 to match ours.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"As reported in [6] , the APC approach outperformed CPC representations [5, #TARGET_REF 9] in both two tasks, which makes APC suitable as a strong baseline.\"]}"
    },
    {
        "gold": {
            "text": [
                "Following previous works [2, 3, 4, 5, 6, #TARGET_REF 8] , we evaluate different features and representations on downstream tasks, including: phoneme classification, speaker recognition, and sentiment classification on spoken content.",
                "For a fair comparison, each downstream task uses an identical model architecture and hyperparameters despite different input features.",
                "We report results from 5 of our models: 1) BASE and 2) LARGE where Mockingjay representations are extracted from the last encoder layer, 3) the BASE-FT2 where we finetune BASE with random initialized downstream models for 2 epochs, and 4) the BASE-FT500 where we fine-tune for 500k steps, and finally 5) the LARGE-WS where we incorporate hidden states from all encoder layers of the LARGE model through a learnable weighted sum.",
                "We did not fine-tune the LARGE model, as it is meant for extracting representations.",
                "Empirically we found that even with supervised training, a random initialized Mockingjay model followed by any downstream model is hard to be trained from scratch."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Following previous works [2, 3, 4, 5, 6, #TARGET_REF 8] , we evaluate different features and representations on downstream tasks, including: phoneme classification, speaker recognition, and sentiment classification on spoken content. For a fair comparison, each downstream task uses an identical model architecture and hyperparameters despite different input features. We report results from 5 of our models: 1) BASE and 2) LARGE where Mockingjay representations are extracted from the last encoder layer, 3) the BASE-FT2 where we finetune BASE with random initialized downstream models for 2 epochs, and 4) the BASE-FT500 where we fine-tune for 500k steps, and finally 5) the LARGE-WS where we incorporate hidden states from all encoder layers of the LARGE model through a learnable weighted sum. We did not fine-tune the LARGE model, as it is meant for extracting representations. Empirically we found that even with supervised training, a random initialized Mockingjay model followed by any downstream model is hard to be trained from scratch.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following previous works [2, 3, 4, 5, 6, #TARGET_REF 8] , we evaluate different features and representations on downstream tasks, including: phoneme classification, speaker recognition, and sentiment classification on spoken content.\"]}"
    },
    {
        "gold": {
            "text": [
                "State-of-the-art results on neural machine translation often use attentional sequence-to-sequence models with some form of convolution or recursion.",
                "#TARGET_REF propose a new architecture that avoids recurrence and convolution completely.",
                "Instead, it uses only self-attention and feed-forward layers.",
                "While the proposed architecture achieves state-of-the-art results on several machine translation tasks, it requires a large number of parameters and training iterations to converge.",
                "We propose Weighted Transformer, a Transformer with modified attention layers, that not only outperforms the baseline network in BLEU score but also converges 15 − 40% faster."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "State-of-the-art results on neural machine translation often use attentional sequence-to-sequence models with some form of convolution or recursion. #TARGET_REF propose a new architecture that avoids recurrence and convolution completely. Instead, it uses only self-attention and feed-forward layers. While the proposed architecture achieves state-of-the-art results on several machine translation tasks, it requires a large number of parameters and training iterations to converge. We propose Weighted Transformer, a Transformer with modified attention layers, that not only outperforms the baseline network in BLEU score but also converges 15 − 40% faster.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF propose a new architecture that avoids recurrence and convolution completely.\"]}"
    },
    {
        "gold": {
            "text": [
                "Self-attention, which relies on dot-products between elements of the input sequence to compute a weighted sum (#REF; #REF; #REF) , has also been a critical ingredient in modern NMT architectures.",
                "The Transformer network #TARGET_REF avoids the recurrence completely and uses only self-attention.",
                "We propose a modified Transformer network wherein the multi-head attention layer is replaced by a branched self-attention layer.",
                "The contributions of the various branches is learned as part of the training procedure.",
                "The idea of multi-branch networks has been explored in several domains (Ahmed & #REF; #REF; #REF) ."
            ],
            "label": [
                "BACK",
                "EXT"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Self-attention, which relies on dot-products between elements of the input sequence to compute a weighted sum (#REF; #REF; #REF) , has also been a critical ingredient in modern NMT architectures. The Transformer network #TARGET_REF avoids the recurrence completely and uses only self-attention. We propose a modified Transformer network wherein the multi-head attention layer is replaced by a branched self-attention layer. The contributions of the various branches is learned as part of the training procedure. The idea of multi-branch networks has been explored in several domains (Ahmed & #REF; #REF; #REF) .",
        "output": "{\"label\": [\"BACK\", \"EXT\"], \"context\": [\"The Transformer network #TARGET_REF avoids the recurrence completely and uses only self-attention.\", \"We propose a modified Transformer network wherein the multi-head attention layer is replaced by a branched self-attention layer.\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that",
                "where h denotes the number of heads in the multi-head attention.",
                "#TARGET_REF proportionally reduce d k = d v = d model so that the computational load of the multi-head attention is the same as simple self-attention.",
                "The second component of each layer of the Transformer network is a feed-forward network.",
                "The authors propose using a two-layered network with a ReLU activation."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Note that where h denotes the number of heads in the multi-head attention. #TARGET_REF proportionally reduce d k = d v = d model so that the computational load of the multi-head attention is the same as simple self-attention. The second component of each layer of the Transformer network is a feed-forward network. The authors propose using a two-layered network with a ReLU activation.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF proportionally reduce d k = d v = d model so that the computational load of the multi-head attention is the same as simple self-attention.\"]}"
    },
    {
        "gold": {
            "text": [
                "The second component of each layer of the Transformer network is a feed-forward network.",
                "The authors propose using a two-layered network with a ReLU activation.",
                "Given trainable weights W 1 , W 2 , b 1 , b 2 , the sub-layer is defined as:",
                "The dimension of the inner layer is d f f which is set to 2048 in their experiments.",
                "For the sake of brevity, we refer the reader to #TARGET_REF for additional details regarding the architecture."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The second component of each layer of the Transformer network is a feed-forward network. The authors propose using a two-layered network with a ReLU activation. Given trainable weights W 1 , W 2 , b 1 , b 2 , the sub-layer is defined as: The dimension of the inner layer is d f f which is set to 2048 in their experiments. For the sake of brevity, we refer the reader to #TARGET_REF for additional details regarding the architecture.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"For the sake of brevity, we refer the reader to #TARGET_REF for additional details regarding the architecture.\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF state three reasons for the preference: (a) computational complexity of each layer, (b) concurrency, and (c) path length between long-range dependencies.",
                "Assuming a sequence length of n and vector dimension d, the complexity of each layer is O(n 2 d) for self-attention layers while it is O(nd 2 ) for recurrent layers.",
                "Given that typically d > n, the complexity of self-attention layers is lower than that of recurrent layers.",
                "Further, the number of sequential computations is O(1) for self-attention layers and O(n) for recurrent layers.",
                "This helps improved utilization of parallel computing architectures."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "#TARGET_REF state three reasons for the preference: (a) computational complexity of each layer, (b) concurrency, and (c) path length between long-range dependencies. Assuming a sequence length of n and vector dimension d, the complexity of each layer is O(n 2 d) for self-attention layers while it is O(nd 2 ) for recurrent layers. Given that typically d > n, the complexity of self-attention layers is lower than that of recurrent layers. Further, the number of sequential computations is O(1) for self-attention layers and O(n) for recurrent layers. This helps improved utilization of parallel computing architectures.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF state three reasons for the preference: (a) computational complexity of each layer, (b) concurrency, and (c) path length between long-range dependencies.\"]}"
    },
    {
        "gold": {
            "text": [
                "We now describe the proposed architecture, the Weighted Transformer, which is more efficient to train and makes better use of representational power.",
                "In Equations (3) and (4), we described the attention layer proposed in #TARGET_REF comprising the multi-head attention sub-layer and a FFN sub-layer.",
                "For the Weighted Transformer, we propose a branched attention that modifies the entire attention layer in the Transformer network (including both the multi-head attention and the feed-forward network).",
                "The proposed attention layer can be described as:",
                "where M denotes the total number of branches, κ i , α i ∈ R + are learned parameters and W Oi ∈ R dv×dmodel ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We now describe the proposed architecture, the Weighted Transformer, which is more efficient to train and makes better use of representational power. In Equations (3) and (4), we described the attention layer proposed in #TARGET_REF comprising the multi-head attention sub-layer and a FFN sub-layer. For the Weighted Transformer, we propose a branched attention that modifies the entire attention layer in the Transformer network (including both the multi-head attention and the feed-forward network). The proposed attention layer can be described as: where M denotes the total number of branches, κ i , α i ∈ R + are learned parameters and W Oi ∈ R dv×dmodel .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In Equations (3) and (4), we described the attention layer proposed in #TARGET_REF comprising the multi-head attention sub-layer and a FFN sub-layer.\"]}"
    },
    {
        "gold": {
            "text": [
                "Transformer (small) #TARGET_REF 27.3 38.1 Weighted Transformer (small) 28.4 38.9",
                "Transformer (large) (#REF) 28.4 41.0 Weighted Transformer (large) 28.9 41.4",
                "ByteNet (#REF) 23.7 -Deep-Att+PosUnk (#REF) -39.2 GNMT+RL (#REF) 24.6 39.9 ConvS2S (#REF) 25.2 40.5 MoE 26.0 40.6 Table 1 : Experimental results on the WMT 2014 English-to-German (EN-DE) and English-toFrench (EN-FR) translation tasks.",
                "Our proposed model outperforms the state-of-the-art models including the Transformer (#REF) .",
                "The small model corresponds to configuration (A) in Table 2 while large corresponds to configuration (B)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Transformer (small) #TARGET_REF 27.3 38.1 Weighted Transformer (small) 28.4 38.9 Transformer (large) (#REF) 28.4 41.0 Weighted Transformer (large) 28.9 41.4 ByteNet (#REF) 23.7 -Deep-Att+PosUnk (#REF) -39.2 GNMT+RL (#REF) 24.6 39.9 ConvS2S (#REF) 25.2 40.5 MoE 26.0 40.6 Table 1 : Experimental results on the WMT 2014 English-to-German (EN-DE) and English-toFrench (EN-FR) translation tasks. Our proposed model outperforms the state-of-the-art models including the Transformer (#REF) . The small model corresponds to configuration (A) in Table 2 while large corresponds to configuration (B).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Transformer (small) #TARGET_REF 27.3 38.1 Weighted Transformer (small) 28.4 38.9\"]}"
    },
    {
        "gold": {
            "text": [
                "Transformer (small) (#REF) 27.3 38.1 Weighted Transformer (small) 28.4 38.9",
                "Transformer (large) #TARGET_REF 28.4 41.0 Weighted Transformer (large) 28.9 41.4",
                "ByteNet (#REF) 23.7 -Deep-Att+PosUnk (#REF) -39.2 GNMT+RL (#REF) 24.6 39.9 ConvS2S (#REF) 25.2 40.5 MoE 26.0 40.6 Table 1 : Experimental results on the WMT 2014 English-to-German (EN-DE) and English-toFrench (EN-FR) translation tasks.",
                "Our proposed model outperforms the state-of-the-art models including the Transformer (#REF) .",
                "The small model corresponds to configuration (A) in Table 2 while large corresponds to configuration (B)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Transformer (small) (#REF) 27.3 38.1 Weighted Transformer (small) 28.4 38.9 Transformer (large) #TARGET_REF 28.4 41.0 Weighted Transformer (large) 28.9 41.4 ByteNet (#REF) 23.7 -Deep-Att+PosUnk (#REF) -39.2 GNMT+RL (#REF) 24.6 39.9 ConvS2S (#REF) 25.2 40.5 MoE 26.0 40.6 Table 1 : Experimental results on the WMT 2014 English-to-German (EN-DE) and English-toFrench (EN-FR) translation tasks. Our proposed model outperforms the state-of-the-art models including the Transformer (#REF) . The small model corresponds to configuration (A) in Table 2 while large corresponds to configuration (B).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Transformer (large) #TARGET_REF 28.4 41.0 Weighted Transformer (large) 28.9 41.4\"]}"
    },
    {
        "gold": {
            "text": [
                "Attention dropout randomly drops out elements (#REF) from the softmax in (1).",
                "As in #TARGET_REF , we used the Adam optimizer (Kingma & #REF) with (β 1 , β 2 ) = (0.9, 0.98) and = 10 −9 .",
                "We also use the learning rate warm-up strategy for Adam wherein the learning rate lr takes on the form:",
                "for the all parameters except (α, κ) and",
                "for (α, κ)."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Attention dropout randomly drops out elements (#REF) from the softmax in (1). As in #TARGET_REF , we used the Adam optimizer (Kingma & #REF) with (β 1 , β 2 ) = (0.9, 0.98) and = 10 −9 . We also use the learning rate warm-up strategy for Adam wherein the learning rate lr takes on the form: for the all parameters except (α, κ) and for (α, κ).",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"As in #TARGET_REF , we used the Adam optimizer (Kingma & #REF) with (\\u03b2 1 , \\u03b2 2 ) = (0.9, 0.98) and = 10 \\u22129 .\"]}"
    },
    {
        "gold": {
            "text": [
                "We found that the objective did not significantly improve by running it for longer.",
                "Further, we do not use any averaging strategies employed in #TARGET_REF and simply return the final model for testing purposes.",
                "In order to reduce the computational load associated with padding, sentences were batched such that they were approximately of the same length.",
                "All sentences were encoded using byte-pair encoding (#REF) and shared a common vocabulary.",
                "Weights for word embeddings were tied to corresponding entries in the final softmax layer (#REF; Press & #REF) ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We found that the objective did not significantly improve by running it for longer. Further, we do not use any averaging strategies employed in #TARGET_REF and simply return the final model for testing purposes. In order to reduce the computational load associated with padding, sentences were batched such that they were approximately of the same length. All sentences were encoded using byte-pair encoding (#REF) and shared a common vocabulary. Weights for word embeddings were tied to corresponding entries in the final softmax layer (#REF; Press & #REF) .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Further, we do not use any averaging strategies employed in #TARGET_REF and simply return the final model for testing purposes.\"]}"
    },
    {
        "gold": {
            "text": [
                "Transformer (small) (#REF) 27.3 38.1 Weighted Transformer (small) 28.4 38.9",
                "Transformer (large) (#REF) 28.4 41.0 Weighted Transformer (large) 28.9 41.4",
                "ByteNet (#REF) 23.7 -Deep-Att+PosUnk (#REF) -39.2 GNMT+RL (#REF) 24.6 39.9 ConvS2S (#REF) 25.2 40.5 MoE 26.0 40.6 Table 1 : Experimental results on the WMT 2014 English-to-German (EN-DE) and English-toFrench (EN-FR) translation tasks.",
                "Our proposed model outperforms the state-of-the-art models including the Transformer #TARGET_REF .",
                "The small model corresponds to configuration (A) in Table 2 while large corresponds to configuration (B)."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Transformer (small) (#REF) 27.3 38.1 Weighted Transformer (small) 28.4 38.9 Transformer (large) (#REF) 28.4 41.0 Weighted Transformer (large) 28.9 41.4 ByteNet (#REF) 23.7 -Deep-Att+PosUnk (#REF) -39.2 GNMT+RL (#REF) 24.6 39.9 ConvS2S (#REF) 25.2 40.5 MoE 26.0 40.6 Table 1 : Experimental results on the WMT 2014 English-to-German (EN-DE) and English-toFrench (EN-FR) translation tasks. Our proposed model outperforms the state-of-the-art models including the Transformer #TARGET_REF . The small model corresponds to configuration (A) in Table 2 while large corresponds to configuration (B).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Our proposed model outperforms the state-of-the-art models including the Transformer #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 1 : Overview of our model.",
                "Using the billions of precomputed phrase representations, we perform a maximum inner product search between the phrase vectors and an input question vector.",
                "We propose to learn contextualized sparse phrase representations which are also very interpretable.",
                "We demonstrate the effectiveness of our model in open-domain question answering (QA), the task of retrieving answer phrases given a web-scale collection of documents.",
                "Following #TARGET_REF , we concatenate both sparse and dense vectors to encode every phrase in Wikipedia and use maximum similarity search to find the closest candidate phrase to answer each question."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Figure 1 : Overview of our model. Using the billions of precomputed phrase representations, we perform a maximum inner product search between the phrase vectors and an input question vector. We propose to learn contextualized sparse phrase representations which are also very interpretable. We demonstrate the effectiveness of our model in open-domain question answering (QA), the task of retrieving answer phrases given a web-scale collection of documents. Following #TARGET_REF , we concatenate both sparse and dense vectors to encode every phrase in Wikipedia and use maximum similarity search to find the closest candidate phrase to answer each question.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following #TARGET_REF , we concatenate both sparse and dense vectors to encode every phrase in Wikipedia and use maximum similarity search to find the closest candidate phrase to answer each question.\"]}"
    },
    {
        "gold": {
            "text": [
                "The loss to minimize is computed from the negative log likelihood over the sum of the dense and sparse logits:",
                "where i * , j * denote the true start and end positions of the answer phrase.",
                "While the loss above is an unbiased estimator, in practice, we adopt early loss summation as suggested by #TARGET_REF for larger gradient signals in early training.",
                "Additionally, we also add dense-only loss that omits the sparse logits (i.e. original loss in #REF ) to the final loss, in which case we find that we obtain higher-quality dense phrase representations.",
                "Negative Sampling We train our model on SQuAD v1.1 which always has a positive paragraph that contains the answer."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The loss to minimize is computed from the negative log likelihood over the sum of the dense and sparse logits: where i * , j * denote the true start and end positions of the answer phrase. While the loss above is an unbiased estimator, in practice, we adopt early loss summation as suggested by #TARGET_REF for larger gradient signals in early training. Additionally, we also add dense-only loss that omits the sparse logits (i.e. original loss in #REF ) to the final loss, in which case we find that we obtain higher-quality dense phrase representations. Negative Sampling We train our model on SQuAD v1.1 which always has a positive paragraph that contains the answer.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"While the loss above is an unbiased estimator, in practice, we adopt early loss summation as suggested by #TARGET_REF for larger gradient signals in early training.\"]}"
    },
    {
        "gold": {
            "text": [
                "The loss to minimize is computed from the negative log likelihood over the sum of the dense and sparse logits:",
                "where i * , j * denote the true start and end positions of the answer phrase.",
                "While the loss above is an unbiased estimator, in practice, we adopt early loss summation as suggested by #REF for larger gradient signals in early training.",
                "Additionally, we also add dense-only loss that omits the sparse logits (i.e. original loss in #TARGET_REF ) to the final loss, in which case we find that we obtain higher-quality dense phrase representations.",
                "Negative Sampling We train our model on SQuAD v1.1 which always has a positive paragraph that contains the answer."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The loss to minimize is computed from the negative log likelihood over the sum of the dense and sparse logits: where i * , j * denote the true start and end positions of the answer phrase. While the loss above is an unbiased estimator, in practice, we adopt early loss summation as suggested by #REF for larger gradient signals in early training. Additionally, we also add dense-only loss that omits the sparse logits (i.e. original loss in #TARGET_REF ) to the final loss, in which case we find that we obtain higher-quality dense phrase representations. Negative Sampling We train our model on SQuAD v1.1 which always has a positive paragraph that contains the answer.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Additionally, we also add dense-only loss that omits the sparse logits (i.e. original loss in #TARGET_REF ) to the final loss, in which case we find that we obtain higher-quality dense phrase representations.\"]}"
    },
    {
        "gold": {
            "text": [
                "Negative Sampling We train our model on SQuAD v1.1 which always has a positive paragraph that contains the answer.",
                "To learn robust phrase representations, we concatenate negative paragraphs to the original SQuAD paragraphs.",
                "To each paragraph x, we concatenate the paragraph x neg which was paired with the question whose dense representation h neg is most similar to the original dense question representation h , following #TARGET_REF .",
                "Note the difference, however, that we concatenate the negative example instead of considering it as an independent example with noanswer option #REF .",
                "During training, we find that adding tf-idf matching scores on the word-level logits of the negative paragraphs further improves the quality of sparse representations as our sparse models have to give stronger attentions to positively related words in this biased setting."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Negative Sampling We train our model on SQuAD v1.1 which always has a positive paragraph that contains the answer. To learn robust phrase representations, we concatenate negative paragraphs to the original SQuAD paragraphs. To each paragraph x, we concatenate the paragraph x neg which was paired with the question whose dense representation h neg is most similar to the original dense question representation h , following #TARGET_REF . Note the difference, however, that we concatenate the negative example instead of considering it as an independent example with noanswer option #REF . During training, we find that adding tf-idf matching scores on the word-level logits of the negative paragraphs further improves the quality of sparse representations as our sparse models have to give stronger attentions to positively related words in this biased setting.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"To each paragraph x, we concatenate the paragraph x neg which was paired with the question whose dense representation h neg is most similar to the original dense question representation h , following #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We do not finetune the word embedding during training.",
                "We pre-compute and store all encoded phrase representations of all documents in Wikipedia (more than 5 million documents).",
                "It takes 600 GPU hours to index all phrases in Wikipedia.",
                "Each phrase representation has 2d se + 2F + 1 dimensions.",
                "We use the same storage reduction and search techniques by #TARGET_REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "We do not finetune the word embedding during training. We pre-compute and store all encoded phrase representations of all documents in Wikipedia (more than 5 million documents). It takes 600 GPU hours to index all phrases in Wikipedia. Each phrase representation has 2d se + 2F + 1 dimensions. We use the same storage reduction and search techniques by #TARGET_REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use the same storage reduction and search techniques by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that the scale of weights in tf-idf vectors is normalized in open-domain setups to match the scale between tf-idf vectors and dense vectors.",
                "We observe that tf-idf vectors usually assign high weights on infrequent (often meaningless) n-grams, while COSPR focuses on contextually important entities such as 1991 for 415,000 or california state, state university for 12.",
                "Our sparse question representation also learns meaningful n-gram weights compared to tf-idf vectors.",
                "Table 4 shows the outputs of three OpenQA models: DrQA (#REF) , DENSPI #TARGET_REF , and our DENSPI+COSPR.",
                "DENSPI+COSPR is able to retrieve various correct answers from different documents, and it often correctly answers questions with specific dates or numbers compared to DENSPI showing the effectiveness of learned sparse representations."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Note that the scale of weights in tf-idf vectors is normalized in open-domain setups to match the scale between tf-idf vectors and dense vectors. We observe that tf-idf vectors usually assign high weights on infrequent (often meaningless) n-grams, while COSPR focuses on contextually important entities such as 1991 for 415,000 or california state, state university for 12. Our sparse question representation also learns meaningful n-gram weights compared to tf-idf vectors. Table 4 shows the outputs of three OpenQA models: DrQA (#REF) , DENSPI #TARGET_REF , and our DENSPI+COSPR. DENSPI+COSPR is able to retrieve various correct answers from different documents, and it often correctly answers questions with specific dates or numbers compared to DENSPI showing the effectiveness of learned sparse representations.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Table 4 shows the outputs of three OpenQA models: DrQA (#REF) , DENSPI #TARGET_REF , and our DENSPI+COSPR.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our empirical results demonstrate its state-of-the-art performance in open-domain QA datasets, SQUAD OPEN and CURATEDTREC.",
                "Notably, our method significantly outperforms DENSPI #TARGET_REF , the previous end-toend QA model, by more than 4% with negligible drop in inference speed.",
                "Moreover, our method achieves up to 2% better accuracy and x97 speedup in inference compared to pipeline (retrievalbased) approaches.",
                "Our analysis particularly shows that fine-grained sparse representation is crucial for doing well in phrase retrieval task.",
                "In summary, the contributions of our paper are:"
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Our empirical results demonstrate its state-of-the-art performance in open-domain QA datasets, SQUAD OPEN and CURATEDTREC. Notably, our method significantly outperforms DENSPI #TARGET_REF , the previous end-toend QA model, by more than 4% with negligible drop in inference speed. Moreover, our method achieves up to 2% better accuracy and x97 speedup in inference compared to pipeline (retrievalbased) approaches. Our analysis particularly shows that fine-grained sparse representation is crucial for doing well in phrase retrieval task. In summary, the contributions of our paper are:",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Notably, our method significantly outperforms DENSPI #TARGET_REF , the previous end-toend QA model, by more than 4% with negligible drop in inference speed.\"]}"
    },
    {
        "gold": {
            "text": [
                "Given a pair of question q and a golden document x (a paragraph in the case of SQuAD), we first compute the dense logit of each phrase x i:j by l i,j = h i:j · h .",
                "Unlike #TARGET_REF , each phrase's sparse embedding is also trained, so it needs to be considered in the loss function.",
                "We define the sparse logit for phrase x i:j as l sparse",
                ". For brevity, we describe how we compute the first term s start i ·s start",
                "[CLS] corresponding to the start word (and dropping the superscript 'start'); the second term can be computed in the same way."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Given a pair of question q and a golden document x (a paragraph in the case of SQuAD), we first compute the dense logit of each phrase x i:j by l i,j = h i:j · h . Unlike #TARGET_REF , each phrase's sparse embedding is also trained, so it needs to be considered in the loss function. We define the sparse logit for phrase x i:j as l sparse . For brevity, we describe how we compute the first term s start i ·s start [CLS] corresponding to the start word (and dropping the superscript 'start'); the second term can be computed in the same way.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Unlike #TARGET_REF , each phrase's sparse embedding is also trained, so it needs to be considered in the loss function.\"]}"
    },
    {
        "gold": {
            "text": [
                "To improve the performance of the open-domain question answering, various modifications have been studied to the pipelined models which include improving the retriever-reader interaction (#REFa; #REF) , re-ranking paragraphs and/or answers (#REFb; #REF; #REF; #REF) , learning end-to-end models with weak supervision , or simply making a better retriever and a reader model (#REF; #REF) .",
                "Due to the pipeline nature, however, these models inevitably suffer error propagation from the retrievers.",
                "To mitigate this problem, #TARGET_REF propose to learn query-agnostic representations of phrases in Wikipedia and retrieve phrases that best answers a question.",
                "While #REF have shown that encoding both dense and sparse representations for each phrase could keep the lexically important words of a phrase to some extent, their sparse representations are based on static tf-idf vectors which have globally the same weight for each n-gram.",
                "Phrase Representations In NLP, phrase representations can be either obtained in a similar manner as word representations (#REF) , or by learning a parametric function of word representations (#REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "To improve the performance of the open-domain question answering, various modifications have been studied to the pipelined models which include improving the retriever-reader interaction (#REFa; #REF) , re-ranking paragraphs and/or answers (#REFb; #REF; #REF; #REF) , learning end-to-end models with weak supervision , or simply making a better retriever and a reader model (#REF; #REF) . Due to the pipeline nature, however, these models inevitably suffer error propagation from the retrievers. To mitigate this problem, #TARGET_REF propose to learn query-agnostic representations of phrases in Wikipedia and retrieve phrases that best answers a question. While #REF have shown that encoding both dense and sparse representations for each phrase could keep the lexically important words of a phrase to some extent, their sparse representations are based on static tf-idf vectors which have globally the same weight for each n-gram. Phrase Representations In NLP, phrase representations can be either obtained in a similar manner as word representations (#REF) , or by learning a parametric function of word representations (#REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"To mitigate this problem, #TARGET_REF propose to learn query-agnostic representations of phrases in Wikipedia and retrieve phrases that best answers a question.\"]}"
    },
    {
        "gold": {
            "text": [
                "and d c are chosen to make d = 2d se + 2d c ).",
                "Then each phrase x i:j is densely represented as follows:",
                "where · denotes inner product operation.",
                "h 1 i and h 2 j are start/end representations of a phrase, and the inner product of h 3 i and h 4 j is used for computing coherency of the phrase.",
                "Refer to #TARGET_REF for details; we mostly reuse its architecture."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "and d c are chosen to make d = 2d se + 2d c ). Then each phrase x i:j is densely represented as follows: where · denotes inner product operation. h 1 i and h 2 j are start/end representations of a phrase, and the inner product of h 3 i and h 4 j is used for computing coherency of the phrase. Refer to #TARGET_REF for details; we mostly reuse its architecture.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Refer to #TARGET_REF for details; we mostly reuse its architecture.\"]}"
    },
    {
        "gold": {
            "text": [
                "While #TARGET_REF have shown that encoding both dense and sparse representations for each phrase could keep the lexically important words of a phrase to some extent, their sparse representations are based on static tf-idf vectors which have globally the same weight for each n-gram.",
                "Phrase Representations In NLP, phrase representations can be either obtained in a similar manner as word representations (#REF) , or by learning a parametric function of word representations (#REF) .",
                "In extractive question answering, phrases are often referred to as spans, but most models do not consider explicitly learning phrase representations as these answer spans can be obtained by predicting only start and end positions in a paragraph (Wang & #REF; .",
                "Nevertheless, few studies have focused on directly learning and classifying phrase representations (#REF) which achieve strong performance when combined with attention mechanism.",
                "In this work, we are interested in learning query-agnostic sparse phrase representations which enables the precomputation of re-usable phrase representations (#REF) ."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "While #TARGET_REF have shown that encoding both dense and sparse representations for each phrase could keep the lexically important words of a phrase to some extent, their sparse representations are based on static tf-idf vectors which have globally the same weight for each n-gram. Phrase Representations In NLP, phrase representations can be either obtained in a similar manner as word representations (#REF) , or by learning a parametric function of word representations (#REF) . In extractive question answering, phrases are often referred to as spans, but most models do not consider explicitly learning phrase representations as these answer spans can be obtained by predicting only start and end positions in a paragraph (Wang & #REF; . Nevertheless, few studies have focused on directly learning and classifying phrase representations (#REF) which achieve strong performance when combined with attention mechanism. In this work, we are interested in learning query-agnostic sparse phrase representations which enables the precomputation of re-usable phrase representations (#REF) .",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"While #TARGET_REF have shown that encoding both dense and sparse representations for each phrase could keep the lexically important words of a phrase to some extent, their sparse representations are based on static tf-idf vectors which have globally the same weight for each n-gram.\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate the effectiveness of COSPR by augmenting DENSPI #TARGET_REF with contextualized sparse representations (DENSPI+COSPR).",
                "We extensively compare the model with the original DENSPI and previous pipeline-based QA models.",
                "Model SQUADOPEN CURATEDTREC EM F1 Exact Match s/Q DrQA (#REF) 29.8 ** -25.4 * 35 R 3 (#REFa) 29.1 37.5 28.4 * -Paragraph Ranker (#REF) 30.2 -35.4 * -Multi-Step-Reasoner (#REF) 31.9 39.2 --BERTserini (#REF) 38.6 46.1 -115 ORQA 20.2 -30.1 -Multi-passage BERT † † (#REF) 53.0 60.9 --DENSPI (#REF) 36 (#REF) while being almost two orders of magnitude faster.",
                "We expect much bigger speed gaps between ours and other pipeline methods as most of them put additional complex components to the original pipelined methods.",
                "On CURATEDTREC, which is constructed from real user queries, our model also achieves the stateof-the-art performance."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We evaluate the effectiveness of COSPR by augmenting DENSPI #TARGET_REF with contextualized sparse representations (DENSPI+COSPR). We extensively compare the model with the original DENSPI and previous pipeline-based QA models. Model SQUADOPEN CURATEDTREC EM F1 Exact Match s/Q DrQA (#REF) 29.8 ** -25.4 * 35 R 3 (#REFa) 29.1 37.5 28.4 * -Paragraph Ranker (#REF) 30.2 -35.4 * -Multi-Step-Reasoner (#REF) 31.9 39.2 --BERTserini (#REF) 38.6 46.1 -115 ORQA 20.2 -30.1 -Multi-passage BERT † † (#REF) 53.0 60.9 --DENSPI (#REF) 36 (#REF) while being almost two orders of magnitude faster. We expect much bigger speed gaps between ours and other pipeline methods as most of them put additional complex components to the original pipelined methods. On CURATEDTREC, which is constructed from real user queries, our model also achieves the stateof-the-art performance.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"We evaluate the effectiveness of COSPR by augmenting DENSPI #TARGET_REF with contextualized sparse representations (DENSPI+COSPR).\"]}"
    },
    {
        "gold": {
            "text": [
                "Self-attention networks (SANs) (#REF; #REF) have shown promising empirical results in various natural language processing (NLP) tasks, such as machine translation #TARGET_REF , natural language inference (#REFa) , and acoustic modeling (#REF) .",
                "One appealing strength of SANs lies in their ability to capture dependencies regardless of distance by explicitly attending to all the elements.",
                "In addition, the performance of SANs can be improved by multi-head attention (#REF) , which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace.",
                "Despite their success, SANs have two major limitations.",
                "First, the model fully take into ac- * Zhaopeng Tu is the corresponding author of the paper."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Self-attention networks (SANs) (#REF; #REF) have shown promising empirical results in various natural language processing (NLP) tasks, such as machine translation #TARGET_REF , natural language inference (#REFa) , and acoustic modeling (#REF) . One appealing strength of SANs lies in their ability to capture dependencies regardless of distance by explicitly attending to all the elements. In addition, the performance of SANs can be improved by multi-head attention (#REF) , which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace. Despite their success, SANs have two major limitations. First, the model fully take into ac- * Zhaopeng Tu is the corresponding author of the paper.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Self-attention networks (SANs) (#REF; #REF) have shown promising empirical results in various natural language processing (NLP) tasks, such as machine translation #TARGET_REF , natural language inference (#REFa) , and acoustic modeling (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Self-attention networks (SANs) (#REF; #REF) have shown promising empirical results in various natural language processing (NLP) tasks, such as machine translation (#REF) , natural language inference (#REFa) , and acoustic modeling (#REF) .",
                "One appealing strength of SANs lies in their ability to capture dependencies regardless of distance by explicitly attending to all the elements.",
                "In addition, the performance of SANs can be improved by multi-head attention #TARGET_REF , which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace.",
                "Despite their success, SANs have two major limitations.",
                "First, the model fully take into ac- * Zhaopeng Tu is the corresponding author of the paper."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Self-attention networks (SANs) (#REF; #REF) have shown promising empirical results in various natural language processing (NLP) tasks, such as machine translation (#REF) , natural language inference (#REFa) , and acoustic modeling (#REF) . One appealing strength of SANs lies in their ability to capture dependencies regardless of distance by explicitly attending to all the elements. In addition, the performance of SANs can be improved by multi-head attention #TARGET_REF , which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace. Despite their success, SANs have two major limitations. First, the model fully take into ac- * Zhaopeng Tu is the corresponding author of the paper.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In addition, the performance of SANs can be improved by multi-head attention #TARGET_REF , which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace.\"]}"
    },
    {
        "gold": {
            "text": [
                "Given an input sequence X = {x 1 , . . . , x I } ∈ R I×d , the model first transforms it into queries Q, keys K, and values V:",
                "where",
                "where ATT(·) is an attention model (#REF; #TARGET_REF that retrieves the keys K h with the query q h i .",
                "The final output representation O is the concatenation of outputs generated by multiple attention models:",
                "3 Approach"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Given an input sequence X = {x 1 , . . . , x I } ∈ R I×d , the model first transforms it into queries Q, keys K, and values V: where where ATT(·) is an attention model (#REF; #TARGET_REF that retrieves the keys K h with the query q h i . The final output representation O is the concatenation of outputs generated by multiple attention models: 3 Approach",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"where ATT(\\u00b7) is an attention model (#REF; #TARGET_REF that retrieves the keys K h with the query q h i .\"]}"
    },
    {
        "gold": {
            "text": [
                "While they modeled locality from position embedding, we improve locality modeling from revising attention scope.",
                "To make a fair comparison, we re-implemented the above approaches under a same framework.",
                "Empirical results on machine translation tasks show the superiority of our approach in both translation quality and training efficiency.",
                "Multi-Head Attention Multi-head attention mechanism #TARGET_REF employs different attention heads to capture distinct features (#REF) .",
                "Along this direction, Shen et al. (2018a) explicitly used multiple attention heads to model different dependencies of the same word pair, and #REF employed different attention heads to capture different linguistic features."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "While they modeled locality from position embedding, we improve locality modeling from revising attention scope. To make a fair comparison, we re-implemented the above approaches under a same framework. Empirical results on machine translation tasks show the superiority of our approach in both translation quality and training efficiency. Multi-Head Attention Multi-head attention mechanism #TARGET_REF employs different attention heads to capture distinct features (#REF) . Along this direction, Shen et al. (2018a) explicitly used multiple attention heads to model different dependencies of the same word pair, and #REF employed different attention heads to capture different linguistic features.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Multi-Head Attention Multi-head attention mechanism #TARGET_REF employs different attention heads to capture distinct features (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We expect that the interaction across different subspaces can further improve the performance of SANs.",
                "We evaluate the effectiveness of the proposed model on three widely-used translation tasks: WMT14 English-to-German, WMT17 Chineseto-English, and WAT17 Japanese-to-English.",
                "Experimental results demonstrate that our approach consistently improves performance over the strong TRANSFORMER model #TARGET_REF across language pairs.",
                "Comparing with previous work on modeling locality for SANs (e.g. #REF; #REF) , our model boosts performance on both translation quality and training efficiency.",
                "2 Multi-Head Self-Attention Networks"
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We expect that the interaction across different subspaces can further improve the performance of SANs. We evaluate the effectiveness of the proposed model on three widely-used translation tasks: WMT14 English-to-German, WMT17 Chineseto-English, and WAT17 Japanese-to-English. Experimental results demonstrate that our approach consistently improves performance over the strong TRANSFORMER model #TARGET_REF across language pairs. Comparing with previous work on modeling locality for SANs (e.g. #REF; #REF) , our model boosts performance on both translation quality and training efficiency. 2 Multi-Head Self-Attention Networks",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Experimental results demonstrate that our approach consistently improves performance over the strong TRANSFORMER model #TARGET_REF across language pairs.\"]}"
    },
    {
        "gold": {
            "text": [
                "We conducted experiments with the Transformer model #TARGET_REF on English⇒German (En⇒De), Chinese⇒English (Zh⇒En) and Japanese⇒English (Ja⇒En) translation tasks.",
                "For the En⇒De and Zh⇒En tasks, the models were trained on widely-used WMT14 and WMT17 corpora, consisting of around 4.5 and 20.62 million sentence pairs, respectively.",
                "Concerning Ja⇒En, we used the first two sections of WAT17 corpus as the training data, which consists of 2M sentence pairs.",
                "To reduce the vocabulary size, all the data were tokenized and segmented into subword symbols using byte-pair encoding (#REF) with 32K merge operations.",
                "Following #REF , we incorporated the proposed model into the encoder, which is a stack of 6 SAN layers."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We conducted experiments with the Transformer model #TARGET_REF on English⇒German (En⇒De), Chinese⇒English (Zh⇒En) and Japanese⇒English (Ja⇒En) translation tasks. For the En⇒De and Zh⇒En tasks, the models were trained on widely-used WMT14 and WMT17 corpora, consisting of around 4.5 and 20.62 million sentence pairs, respectively. Concerning Ja⇒En, we used the first two sections of WAT17 corpus as the training data, which consists of 2M sentence pairs. To reduce the vocabulary size, all the data were tokenized and segmented into subword symbols using byte-pair encoding (#REF) with 32K merge operations. Following #REF , we incorporated the proposed model into the encoder, which is a stack of 6 SAN layers.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We conducted experiments with the Transformer model #TARGET_REF on English\\u21d2German (En\\u21d2De), Chinese\\u21d2English (Zh\\u21d2En) and Japanese\\u21d2English (Ja\\u21d2En) translation tasks.\"]}"
    },
    {
        "gold": {
            "text": [
                "Concerning Ja⇒En, we used the first two sections of WAT17 corpus as the training data, which consists of 2M sentence pairs.",
                "To reduce the vocabulary size, all the data were tokenized and segmented into subword symbols using byte-pair encoding (#REF) with 32K merge operations.",
                "Following #REF , we incorporated the proposed model into the encoder, which is a stack of 6 SAN layers.",
                "Prior studies revealed that modeling locality in lower layers can achieve better performance (#REFb; #REF; , we applied our approach to the lowest three layers of the encoder.",
                "About configurations of NMT models, we used the Base and Big settings same as #TARGET_REF , and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Concerning Ja⇒En, we used the first two sections of WAT17 corpus as the training data, which consists of 2M sentence pairs. To reduce the vocabulary size, all the data were tokenized and segmented into subword symbols using byte-pair encoding (#REF) with 32K merge operations. Following #REF , we incorporated the proposed model into the encoder, which is a stack of 6 SAN layers. Prior studies revealed that modeling locality in lower layers can achieve better performance (#REFb; #REF; , we applied our approach to the lowest three layers of the encoder. About configurations of NMT models, we used the Base and Big settings same as #TARGET_REF , and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"About configurations of NMT models, we used the Base and Big settings same as #TARGET_REF , and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens.\"]}"
    },
    {
        "gold": {
            "text": [
                "We study the problem of jointly embedding a knowledge base and a text corpus.",
                "The key issue is the alignment model making sure the vectors of entities, relations and words are in the same space.",
                "#TARGET_REF rely on Wikipedia anchors, making the applicable scope quite limited.",
                "In this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors.",
                "We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We study the problem of jointly embedding a knowledge base and a text corpus. The key issue is the alignment model making sure the vectors of entities, relations and words are in the same space. #TARGET_REF rely on Wikipedia anchors, making the applicable scope quite limited. In this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors. We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"#TARGET_REF rely on Wikipedia anchors, making the applicable scope quite limited.\"]}"
    },
    {
        "gold": {
            "text": [
                "This task is to extract facts (h, r, t) from plain text. show that combing scores from TransE and some text side base extractor achieved much better precision-recall curve compared to the base extractor.",
                "#TARGET_REF confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as #TARGET_REF to investigate the performance of our new alignment model.",
                "We use the same public dataset NYT+FB, released by #REF and used in and (#REFa) .",
                "We use Mintz (#REF ) and MIML (#REF) as our base extractors.",
                "In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment."
            ],
            "label": [
                "MOT",
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "This task is to extract facts (h, r, t) from plain text. show that combing scores from TransE and some text side base extractor achieved much better precision-recall curve compared to the base extractor. #TARGET_REF confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as #TARGET_REF to investigate the performance of our new alignment model. We use the same public dataset NYT+FB, released by #REF and used in and (#REFa) . We use Mintz (#REF ) and MIML (#REF) as our base extractors. In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment.",
        "output": "{\"label\": [\"MOT\", \"USE\"], \"context\": [\"#TARGET_REF confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as #TARGET_REF to investigate the performance of our new alignment model.\"]}"
    },
    {
        "gold": {
            "text": [
                "The key issue is the alignment model making sure the vectors of entities, relations and words are in the same space.",
                "Wang et al. (2014a) rely on Wikipedia anchors, making the applicable scope quite limited.",
                "In this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors.",
                "We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description.",
                "Extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of #TARGET_REF , which is encouraging as we do not use any anchor information."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The key issue is the alignment model making sure the vectors of entities, relations and words are in the same space. Wang et al. (2014a) rely on Wikipedia anchors, making the applicable scope quite limited. In this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors. We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description. Extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of #TARGET_REF , which is encouraging as we do not use any anchor information.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of #TARGET_REF , which is encouraging as we do not use any anchor information.\"]}"
    },
    {
        "gold": {
            "text": [
                "We only assume some entities in KBs have text descriptions, which almost always holds in practice.",
                "We require the embedding of an entity not only fits the structured constraints in KBs but also equals the vector computed from the text description.",
                "Meanwhile, if an entity has few facts, the description will provide information for embedding, thus the issue of KB sparsity is also well handled.",
                "We conduct extensive experiments on the tasks of triplet classification, link prediction, relational fact extraction, and analogical reasoning to compare with the previous approach #TARGET_REF .",
                "Results show that our approach consistently achieves better or comparable performance."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "We only assume some entities in KBs have text descriptions, which almost always holds in practice. We require the embedding of an entity not only fits the structured constraints in KBs but also equals the vector computed from the text description. Meanwhile, if an entity has few facts, the description will provide information for embedding, thus the issue of KB sparsity is also well handled. We conduct extensive experiments on the tasks of triplet classification, link prediction, relational fact extraction, and analogical reasoning to compare with the previous approach #TARGET_REF . Results show that our approach consistently achieves better or comparable performance.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We conduct extensive experiments on the tasks of triplet classification, link prediction, relational fact extraction, and analogical reasoning to compare with the previous approach #TARGET_REF .\", \"Results show that our approach consistently achieves better or comparable performance.\"]}"
    },
    {
        "gold": {
            "text": [
                "Text Model Defines the probability of a pair of words w and v co-occurring in a text window:",
                "where",
                "Then the loss function of text model is",
                "Alignment Model This part is different from #TARGET_REF .",
                "For each word w in the description of entity e, we define Pr(w|e), the conditional probability of predicting w given e:"
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Text Model Defines the probability of a pair of words w and v co-occurring in a text window: where Then the loss function of text model is Alignment Model This part is different from #TARGET_REF . For each word w in the description of entity e, we define Pr(w|e), the conditional probability of predicting w given e:",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Alignment Model This part is different from #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "(2) \"Jointly(desp)\" achieves the best results, especially for the case of \"Phrases\".",
                "Both \"Jointly(anchor)\" and \"Skip-gram\" only consider the context of words, while \"Jointly(desp)\" not only consider the context but also use the whole document to disambiguate words.",
                "Intuitively, the whole document is also a valuable resource to disambiguate words.",
                "(3) We further verify that \"Jointly(name)\", i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in (#REFa) .",
                "The above four experiments are consistent in results: without using any anchor information, alignment by entity description is able to achieve better or comparable performance, compared to alignment by Wikipedia anchors proposed by #TARGET_REF ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "(2) \"Jointly(desp)\" achieves the best results, especially for the case of \"Phrases\". Both \"Jointly(anchor)\" and \"Skip-gram\" only consider the context of words, while \"Jointly(desp)\" not only consider the context but also use the whole document to disambiguate words. Intuitively, the whole document is also a valuable resource to disambiguate words. (3) We further verify that \"Jointly(name)\", i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in (#REFa) . The above four experiments are consistent in results: without using any anchor information, alignment by entity description is able to achieve better or comparable performance, compared to alignment by Wikipedia anchors proposed by #TARGET_REF .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The above four experiments are consistent in results: without using any anchor information, alignment by entity description is able to achieve better or comparable performance, compared to alignment by Wikipedia anchors proposed by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The above approaches are only based on structured facts of KBs, and thus cannot work well on entities with few facts.",
                "An important milestone, the approach of #TARGET_REF solves issue (1) by jointly embedding entities, relations, and words into the same vector space and hence is able to deal with words/phrases beyond entities in KBs.",
                "The key component is the so-called alignment model, which makes sure the embeddings of entities, relations, and words are in the same space.",
                "Two alignment models are introduced there: one uses entity names and another uses Wikipedia anchors.",
                "However, both of them have drawbacks."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The above approaches are only based on structured facts of KBs, and thus cannot work well on entities with few facts. An important milestone, the approach of #TARGET_REF solves issue (1) by jointly embedding entities, relations, and words into the same vector space and hence is able to deal with words/phrases beyond entities in KBs. The key component is the so-called alignment model, which makes sure the embeddings of entities, relations, and words are in the same space. Two alignment models are introduced there: one uses entity names and another uses Wikipedia anchors. However, both of them have drawbacks.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"An important milestone, the approach of #TARGET_REF solves issue (1) by jointly embedding entities, relations, and words into the same vector space and hence is able to deal with words/phrases beyond entities in KBs.\"]}"
    },
    {
        "gold": {
            "text": [
                "For a fact (h, r, t) in KBs, where h is the head entity, r is the relation, and t is the tail entity, TransE models the relation r as a translation vector r connecting the embeddings h and t of the two entities, i.e., h + r is close to t. The model is simple, effective and efficient.",
                "Most knowledge embedding models thereafter including this paper are variants of this model (#REFb; #TARGET_REF; #REF) .",
                "Skip-gram This is an efficient word embedding method proposed by Mikolov et al. (2013a) , which learns word embeddings from word concurrencies in text windows.",
                "Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as 'King' − 'Queen' ≈ 'Man' − 'Women'.",
                "However, as it is unsupervised, it cannot tell the exact relation between two words."
            ],
            "label": [
                "BACK",
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "For a fact (h, r, t) in KBs, where h is the head entity, r is the relation, and t is the tail entity, TransE models the relation r as a translation vector r connecting the embeddings h and t of the two entities, i.e., h + r is close to t. The model is simple, effective and efficient. Most knowledge embedding models thereafter including this paper are variants of this model (#REFb; #TARGET_REF; #REF) . Skip-gram This is an efficient word embedding method proposed by Mikolov et al. (2013a) , which learns word embeddings from word concurrencies in text windows. Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as 'King' − 'Queen' ≈ 'Man' − 'Women'. However, as it is unsupervised, it cannot tell the exact relation between two words.",
        "output": "{\"label\": [\"BACK\", \"USE\"], \"context\": [\"Most knowledge embedding models thereafter including this paper are variants of this model (#REFb; #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Skip-gram This is an efficient word embedding method proposed by Mikolov et al. (2013a) , which learns word embeddings from word concurrencies in text windows.",
                "Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as 'King' − 'Queen' ≈ 'Man' − 'Women'.",
                "However, as it is unsupervised, it cannot tell the exact relation between two words.",
                "#TARGET_REF combines knowledge embedding and word embedding in a joint framework so that the entities/relations and words are in the same vector space and hence operators like inner product (similarity) between them are meaningful.",
                "This brings convenience to tasks requiring computation between knowledge bases and text."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Skip-gram This is an efficient word embedding method proposed by Mikolov et al. (2013a) , which learns word embeddings from word concurrencies in text windows. Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as 'King' − 'Queen' ≈ 'Man' − 'Women'. However, as it is unsupervised, it cannot tell the exact relation between two words. #TARGET_REF combines knowledge embedding and word embedding in a joint framework so that the entities/relations and words are in the same vector space and hence operators like inner product (similarity) between them are meaningful. This brings convenience to tasks requiring computation between knowledge bases and text.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF combines knowledge embedding and word embedding in a joint framework so that the entities/relations and words are in the same vector space and hence operators like inner product (similarity) between them are meaningful.\"]}"
    },
    {
        "gold": {
            "text": [
                "The vocabulary of words is V. The union vocabulary of entities and words together is I = E ∪ V. In this paper \"word(s)\" refers to \"word(s)/phrase(s)\".",
                "We follow the jointly embedding framework of #TARGET_REF , i.e., learning optimal embeddings by minimizing the following loss",
                "where L K , L T and L A are the component loss functions of the knowledge model, text model and alignment model respectively.",
                "Our focus is on a new alignment model L A while the knowledge model L K and text model L T are the same as the counterparts in (#REFa) .",
                "However, to make the content self-contained, we still need to briefly explain L K and L T ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The vocabulary of words is V. The union vocabulary of entities and words together is I = E ∪ V. In this paper \"word(s)\" refers to \"word(s)/phrase(s)\". We follow the jointly embedding framework of #TARGET_REF , i.e., learning optimal embeddings by minimizing the following loss where L K , L T and L A are the component loss functions of the knowledge model, text model and alignment model respectively. Our focus is on a new alignment model L A while the knowledge model L K and text model L T are the same as the counterparts in (#REFa) . However, to make the content self-contained, we still need to briefly explain L K and L T .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We follow the jointly embedding framework of #TARGET_REF , i.e., learning optimal embeddings by minimizing the following loss\"]}"
    },
    {
        "gold": {
            "text": [
                "where L K , L T and L A are the component loss functions of the knowledge model, text model and alignment model respectively.",
                "Our focus is on a new alignment model L A while the knowledge model L K and text model L T are the same as the counterparts in #TARGET_REF .",
                "However, to make the content self-contained, we still need to briefly explain L K and L T .",
                "Knowledge Model Describes the plausibility of a triplet (h, r, t) by defining",
                "where z(h, r, t) = b − 0.5 · h + r − t 2 2 , b = 7 as suggested by Wang et al. (2014a) ."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "where L K , L T and L A are the component loss functions of the knowledge model, text model and alignment model respectively. Our focus is on a new alignment model L A while the knowledge model L K and text model L T are the same as the counterparts in #TARGET_REF . However, to make the content self-contained, we still need to briefly explain L K and L T . Knowledge Model Describes the plausibility of a triplet (h, r, t) by defining where z(h, r, t) = b − 0.5 · h + r − t 2 2 , b = 7 as suggested by Wang et al. (2014a) .",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"Our focus is on a new alignment model L A while the knowledge model L K and text model L T are the same as the counterparts in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "However, to make the content self-contained, we still need to briefly explain L K and L T .",
                "Knowledge Model Describes the plausibility of a triplet (h, r, t) by defining",
                "where z(h, r, t) = b − 0.5 · h + r − t 2 2 , b = 7 as suggested by #TARGET_REF .",
                "Pr(r|h, t) and Pr(t|h, r) are defined in the same way.",
                "The loss function of knowledge model is then defined as"
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "However, to make the content self-contained, we still need to briefly explain L K and L T . Knowledge Model Describes the plausibility of a triplet (h, r, t) by defining where z(h, r, t) = b − 0.5 · h + r − t 2 2 , b = 7 as suggested by #TARGET_REF . Pr(r|h, t) and Pr(t|h, r) are defined in the same way. The loss function of knowledge model is then defined as",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Knowledge Model Describes the plausibility of a triplet (h, r, t) by defining\", \"where z(h, r, t) = b \\u2212 0.5 \\u00b7 h + r \\u2212 t 2 2 , b = 7 as suggested by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The last one evaluates quality of word embeddings.",
                "We try to study whether the proposed alignment model, without using any anchor information, is able to achieve comparable or better performance than alignment by anchors.",
                "As to the methods, \"Separately\" denotes the method of separately embedding knowledge bases and text. \"Jointly(anchor)\" and \"Jointly(name)\" denote the jointly embedding methods based on Alignment by Wikipedia Anchors and Alignment by Entity Names in #TARGET_REF respectively.",
                "\"Jointly(desp)\" is the joint embedding method based on alignment by entity descriptions.",
                "Data For link prediction, FB15K from ) is used as the knowledge base."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The last one evaluates quality of word embeddings. We try to study whether the proposed alignment model, without using any anchor information, is able to achieve comparable or better performance than alignment by anchors. As to the methods, \"Separately\" denotes the method of separately embedding knowledge bases and text. \"Jointly(anchor)\" and \"Jointly(name)\" denote the jointly embedding methods based on Alignment by Wikipedia Anchors and Alignment by Entity Names in #TARGET_REF respectively. \"Jointly(desp)\" is the joint embedding method based on alignment by entity descriptions. Data For link prediction, FB15K from ) is used as the knowledge base.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"As to the methods, \\\"Separately\\\" denotes the method of separately embedding knowledge bases and text. \\\"Jointly(anchor)\\\" and \\\"Jointly(name)\\\" denote the jointly embedding methods based on Alignment by Wikipedia Anchors and Alignment by Entity Names in #TARGET_REF respectively.\"]}"
    },
    {
        "gold": {
            "text": [
                "Data For link prediction, FB15K from ) is used as the knowledge base.",
                "For triplet classification, a large dataset provided by #TARGET_REF is used as the knowledge base.",
                "Both sets are subsets of Freebase.",
                "For all tasks, Wikipedia articles are used as the text corpus.",
                "As many Wikipedia articles can be mapped to Freebase entities, we regard a Wikipedia article as the description for the corresponding entity in Freebase."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Data For link prediction, FB15K from ) is used as the knowledge base. For triplet classification, a large dataset provided by #TARGET_REF is used as the knowledge base. Both sets are subsets of Freebase. For all tasks, Wikipedia articles are used as the text corpus. As many Wikipedia articles can be mapped to Freebase entities, we regard a Wikipedia article as the description for the corresponding entity in Freebase.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For triplet classification, a large dataset provided by #TARGET_REF is used as the knowledge base.\"]}"
    },
    {
        "gold": {
            "text": [
                "Following the settings in #TARGET_REF , we apply the same preprocessing steps, including sentence segmentation, tokenization, and named entity recognition.",
                "We combine the consecutive tokens covered by an anchor or identically tagged as \"Location/Person/Organization\" and regard them as phrases.",
                "Link Prediction This task aims to complete a fact (h, r, t) in absence of h or t, simply based on h + r − t .",
                "We follow the same protocol in .",
                "We directly copy the results of the baseline (TransE) from and implement \"Jointly(anchor)\"."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Following the settings in #TARGET_REF , we apply the same preprocessing steps, including sentence segmentation, tokenization, and named entity recognition. We combine the consecutive tokens covered by an anchor or identically tagged as \"Location/Person/Organization\" and regard them as phrases. Link Prediction This task aims to complete a fact (h, r, t) in absence of h or t, simply based on h + r − t . We follow the same protocol in . We directly copy the results of the baseline (TransE) from and implement \"Jointly(anchor)\".",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"Following the settings in #TARGET_REF , we apply the same preprocessing steps, including sentence segmentation, tokenization, and named entity recognition.\"]}"
    },
    {
        "gold": {
            "text": [
                "In other settings \"Jointly(desp)\" wins.",
                "Triplet Classification This is a binary classification task, predicting whether a candidate triplet (h, r, t) is a correct fact or not.",
                "It is used in (#REF; #REFb; #REFa) .",
                "We follow the same protocol in #TARGET_REF .",
                "We train their models via our own implementation on our dataset."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "In other settings \"Jointly(desp)\" wins. Triplet Classification This is a binary classification task, predicting whether a candidate triplet (h, r, t) is a correct fact or not. It is used in (#REF; #REFb; #REFa) . We follow the same protocol in #TARGET_REF . We train their models via our own implementation on our dataset.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"We follow the same protocol in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "This task is to extract facts (h, r, t) from plain text. show that combing scores from TransE and some text side base extractor achieved much better precision-recall curve compared to the base extractor.",
                "Wang et al. (2014a) confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as (#REFa) to investigate the performance of our new alignment model.",
                "We use the same public dataset NYT+FB, released by #REF and used in and #TARGET_REF .",
                "We use Mintz (#REF ) and MIML (#REF) as our base extractors.",
                "In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "This task is to extract facts (h, r, t) from plain text. show that combing scores from TransE and some text side base extractor achieved much better precision-recall curve compared to the base extractor. Wang et al. (2014a) confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as (#REFa) to investigate the performance of our new alignment model. We use the same public dataset NYT+FB, released by #REF and used in and #TARGET_REF . We use Mintz (#REF ) and MIML (#REF) as our base extractors. In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"We use the same public dataset NYT+FB, released by #REF and used in and #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment.",
                "Since both Mintz and MIML are probabilistic models, we use the same method in #TARGET_REF to linearly combine the scores.",
                "The precision-recall curves are plot in Fig. (1) .",
                "On both base extractors, the jointly embedding methods outperform separate embedding.",
                "Moreover, \"Jointly(desp)\" is slightly better than \"Jointly(anchor)\", which is in accordance with the results from the link prediction experiment and the triplet classification experiment."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment. Since both Mintz and MIML are probabilistic models, we use the same method in #TARGET_REF to linearly combine the scores. The precision-recall curves are plot in Fig. (1) . On both base extractors, the jointly embedding methods outperform separate embedding. Moreover, \"Jointly(desp)\" is slightly better than \"Jointly(anchor)\", which is in accordance with the results from the link prediction experiment and the triplet classification experiment.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"Since both Mintz and MIML are probabilistic models, we use the same method in #TARGET_REF to linearly combine the scores.\"]}"
    },
    {
        "gold": {
            "text": [
                "The precision-recall curves are plot in Fig. (1) .",
                "On both base extractors, the jointly embedding methods outperform separate embedding.",
                "Moreover, \"Jointly(desp)\" is slightly better than \"Jointly(anchor)\", which is in accordance with the results from the link prediction experiment and the triplet classification experiment.",
                "Analogical Reasoning This task evaluates the quality of word embeddings (#REFb) .",
                "We use the original dataset released by (#REFb) and follow the same evaluation protocol of #TARGET_REF ."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The precision-recall curves are plot in Fig. (1) . On both base extractors, the jointly embedding methods outperform separate embedding. Moreover, \"Jointly(desp)\" is slightly better than \"Jointly(anchor)\", which is in accordance with the results from the link prediction experiment and the triplet classification experiment. Analogical Reasoning This task evaluates the quality of word embeddings (#REFb) . We use the original dataset released by (#REFb) and follow the same evaluation protocol of #TARGET_REF .",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"We use the original dataset released by (#REFb) and follow the same evaluation protocol of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "(2) \"Jointly(desp)\" achieves the best results, especially for the case of \"Phrases\".",
                "Both \"Jointly(anchor)\" and \"Skip-gram\" only consider the context of words, while \"Jointly(desp)\" not only consider the context but also use the whole document to disambiguate words.",
                "Intuitively, the whole document is also a valuable resource to disambiguate words.",
                "(3) We further verify that \"Jointly(name)\", i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in #TARGET_REF .",
                "The above four experiments are consistent in results: without using any anchor information, alignment by entity description is able to achieve better or comparable performance, compared to alignment by Wikipedia anchors proposed by Wang et al. (2014a) ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "(2) \"Jointly(desp)\" achieves the best results, especially for the case of \"Phrases\". Both \"Jointly(anchor)\" and \"Skip-gram\" only consider the context of words, while \"Jointly(desp)\" not only consider the context but also use the whole document to disambiguate words. Intuitively, the whole document is also a valuable resource to disambiguate words. (3) We further verify that \"Jointly(name)\", i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in #TARGET_REF . The above four experiments are consistent in results: without using any anchor information, alignment by entity description is able to achieve better or comparable performance, compared to alignment by Wikipedia anchors proposed by Wang et al. (2014a) .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"(3) We further verify that \\\"Jointly(name)\\\", i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Orthogonally, the geolocation task can be viewed as a regression task over real-valued geographical coordinates, or a classification task over discretised region-based locations.",
                "Most previous research on user geolocation has focused either on text-based classification approaches (#REF; #REF; #TARGET_REF; #REF) or, to a lesser extent, network-based regression approaches (#REF; #REF; #TARGET_REF .",
                "Methods which combine the two, however, are rare.",
                "In this paper, we present work on Twitter user geolocation using both text and network information.",
                "Our contributions are as follows: (1) we propose the use of Modified Adsorption (#REF) as a baseline networkbased geolocation model, and show that it outperforms previous network-based approaches (#REF; #REF) ; (2) we demonstrate that removing \"celebrity\" nodes (nodes with high in-degrees) from the network increases geolocation accuracy and dramatically decreases network edge size; and (3) we integrate textbased geolocation priors into Modified Adsorption, and show that our unified geolocation model outperforms both text-only and network-only approaches, and achieves state-of-the-art results over three standard datasets."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Orthogonally, the geolocation task can be viewed as a regression task over real-valued geographical coordinates, or a classification task over discretised region-based locations. Most previous research on user geolocation has focused either on text-based classification approaches (#REF; #REF; #TARGET_REF; #REF) or, to a lesser extent, network-based regression approaches (#REF; #REF; #TARGET_REF . Methods which combine the two, however, are rare. In this paper, we present work on Twitter user geolocation using both text and network information. Our contributions are as follows: (1) we propose the use of Modified Adsorption (#REF) as a baseline networkbased geolocation model, and show that it outperforms previous network-based approaches (#REF; #REF) ; (2) we demonstrate that removing \"celebrity\" nodes (nodes with high in-degrees) from the network increases geolocation accuracy and dramatically decreases network edge size; and (3) we integrate textbased geolocation priors into Modified Adsorption, and show that our unified geolocation model outperforms both text-only and network-only approaches, and achieves state-of-the-art results over three standard datasets.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Most previous research on user geolocation has focused either on text-based classification approaches (#REF; #REF; #TARGET_REF; #REF) or, to a lesser extent, network-based regression approaches (#REF; #REF; #TARGET_REF .\", \"Methods which combine the two, however, are rare.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF and #REF used a Twitter reciprocal mention network, and geolocated users based on the geographical coordinates of their friends, by minimising the weighted distance of a given user to their friends.",
                "For a reciprocal mention network to be effective, however, a huge amount of Twitter data is required.",
                "#REF showed that this assumption could be relaxed to use an undirected mention network for smaller datasets, and still attain state-of-theart results.",
                "The greatest shortcoming of networkbased models is that they completely fail to geolocate users who are not connected to geolocated components of the graph.",
                "As shown by #TARGET_REF , geolocation predictions from text can be used as a backoff for disconnected users, but there has been little work that has investigated a more integrated text-and network-based approach to user geolocation."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "#REF and #REF used a Twitter reciprocal mention network, and geolocated users based on the geographical coordinates of their friends, by minimising the weighted distance of a given user to their friends. For a reciprocal mention network to be effective, however, a huge amount of Twitter data is required. #REF showed that this assumption could be relaxed to use an undirected mention network for smaller datasets, and still attain state-of-theart results. The greatest shortcoming of networkbased models is that they completely fail to geolocate users who are not connected to geolocated components of the graph. As shown by #TARGET_REF , geolocation predictions from text can be used as a backoff for disconnected users, but there has been little work that has investigated a more integrated text-and network-based approach to user geolocation.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"As shown by #TARGET_REF , geolocation predictions from text can be used as a backoff for disconnected users, but there has been little work that has investigated a more integrated text-and network-based approach to user geolocation.\"]}"
    },
    {
        "gold": {
            "text": [
                "Orthogonally, the geolocation task can be viewed as a regression task over real-valued geographical coordinates, or a classification task over discretised region-based locations.",
                "Most previous research on user geolocation has focused either on text-based classification approaches (#REF; #REF; #REF; #REF) or, to a lesser extent, network-based regression approaches (#REF; #REF; #REF) .",
                "Methods which combine the two, however, are rare.",
                "In this paper, we present work on Twitter user geolocation using both text and network information.",
                "Our contributions are as follows: (1) we propose the use of Modified Adsorption (#REF) as a baseline networkbased geolocation model, and show that it outperforms previous network-based approaches (#REF; #TARGET_REF ; (2) we demonstrate that removing \"celebrity\" nodes (nodes with high in-degrees) from the network increases geolocation accuracy and dramatically decreases network edge size; and (3) we integrate textbased geolocation priors into Modified Adsorption, and show that our unified geolocation model outperforms both text-only and network-only approaches, and achieves state-of-the-art results over three standard datasets."
            ],
            "label": [
                "DIFFER",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Orthogonally, the geolocation task can be viewed as a regression task over real-valued geographical coordinates, or a classification task over discretised region-based locations. Most previous research on user geolocation has focused either on text-based classification approaches (#REF; #REF; #REF; #REF) or, to a lesser extent, network-based regression approaches (#REF; #REF; #REF) . Methods which combine the two, however, are rare. In this paper, we present work on Twitter user geolocation using both text and network information. Our contributions are as follows: (1) we propose the use of Modified Adsorption (#REF) as a baseline networkbased geolocation model, and show that it outperforms previous network-based approaches (#REF; #TARGET_REF ; (2) we demonstrate that removing \"celebrity\" nodes (nodes with high in-degrees) from the network increases geolocation accuracy and dramatically decreases network edge size; and (3) we integrate textbased geolocation priors into Modified Adsorption, and show that our unified geolocation model outperforms both text-only and network-only approaches, and achieves state-of-the-art results over three standard datasets.",
        "output": "{\"label\": [\"DIFFER\", \"USE\"], \"context\": [\"Our contributions are as follows: (1) we propose the use of Modified Adsorption (#REF) as a baseline networkbased geolocation model, and show that it outperforms previous network-based approaches (#REF; #TARGET_REF ; (2) we demonstrate that removing \\\"celebrity\\\" nodes (nodes with high in-degrees) from the network increases geolocation accuracy and dramatically decreases network edge size; and (3) we integrate textbased geolocation priors into Modified Adsorption, and show that our unified geolocation model outperforms both text-only and network-only approaches, and achieves state-of-the-art results over three standard datasets.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our baseline network-based model of MAD-B outperforms the text-based models and also previous network-based models (#REF; #REF; #TARGET_REF .",
                "The inference, however, is intractable for TWITTER-US and TWITTER-WORLD due to the size of the network.",
                "Celebrity removal in MADCEL-B and MADCEL-W has a positive effect on geolocation accuracy, and results in a 47% reduction in Median over GEOTEXT.",
                "It also makes graph inference over TWITTER-US and TWITTER-WORLD tractable, and results in superior Acc@161 and Median, but slightly inferior Mean, compared to the state-of-the-art results of LR, based on text-based classification (#REF) .",
                "MADCEL-W (weighted graph) outperforms MADCEL-B (binary graph) over the smaller GEOTEXT dataset where it compensates for the sparsity of network information, but doesn't improve the results for the two larger datasets where network information is denser."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Our baseline network-based model of MAD-B outperforms the text-based models and also previous network-based models (#REF; #REF; #TARGET_REF . The inference, however, is intractable for TWITTER-US and TWITTER-WORLD due to the size of the network. Celebrity removal in MADCEL-B and MADCEL-W has a positive effect on geolocation accuracy, and results in a 47% reduction in Median over GEOTEXT. It also makes graph inference over TWITTER-US and TWITTER-WORLD tractable, and results in superior Acc@161 and Median, but slightly inferior Mean, compared to the state-of-the-art results of LR, based on text-based classification (#REF) . MADCEL-W (weighted graph) outperforms MADCEL-B (binary graph) over the smaller GEOTEXT dataset where it compensates for the sparsity of network information, but doesn't improve the results for the two larger datasets where network information is denser.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Our baseline network-based model of MAD-B outperforms the text-based models and also previous network-based models (#REF; #REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Celebrity removal in MADCEL-B and MADCEL-W has a positive effect on geolocation accuracy, and results in a 47% reduction in Median over GEOTEXT.",
                "It also makes graph inference over TWITTER-US and TWITTER-WORLD tractable, and results in superior Acc@161 and Median, but slightly inferior Mean, compared to the state-of-the-art results of LR, based on text-based classification #TARGET_REF .",
                "MADCEL-W (weighted graph) outperforms MADCEL-B (binary graph) over the smaller GEOTEXT dataset where it compensates for the sparsity of network information, but doesn't improve the results for the two larger datasets where network information is denser.",
                "Adding text to the network-based geolocation models in the form of MADCEL-B-LR (binary edges) and MADCEL-W-LR (weighted edges), we achieve state-of-the-art results over all three datasets.",
                "The inclusion of text-based priors has the greatest impact on Mean, resulting in an additional 26% and 23% error reduction over TWITTER-US and TWITTER-WORLD, respectively."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Celebrity removal in MADCEL-B and MADCEL-W has a positive effect on geolocation accuracy, and results in a 47% reduction in Median over GEOTEXT. It also makes graph inference over TWITTER-US and TWITTER-WORLD tractable, and results in superior Acc@161 and Median, but slightly inferior Mean, compared to the state-of-the-art results of LR, based on text-based classification #TARGET_REF . MADCEL-W (weighted graph) outperforms MADCEL-B (binary graph) over the smaller GEOTEXT dataset where it compensates for the sparsity of network information, but doesn't improve the results for the two larger datasets where network information is denser. Adding text to the network-based geolocation models in the form of MADCEL-B-LR (binary edges) and MADCEL-W-LR (weighted edges), we achieve state-of-the-art results over all three datasets. The inclusion of text-based priors has the greatest impact on Mean, resulting in an additional 26% and 23% error reduction over TWITTER-US and TWITTER-WORLD, respectively.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"It also makes graph inference over TWITTER-US and TWITTER-WORLD tractable, and results in superior Acc@161 and Median, but slightly inferior Mean, compared to the state-of-the-art results of LR, based on text-based classification #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate our models over three pre-existing geotagged Twitter datasets: (1) GEOTEXT (Eisen-stein et al., 2010), (2) TWITTER-US #TARGET_REF , and (3) TWITTER-WORLD (#REF) .",
                "In each dataset, users are represented by a single meta-document, generated by concatenating their tweets.",
                "The datasets are pre-partitioned into training, development and test sets, and rebuilt from the original version to include mention information.",
                "The first two datasets were constructed to contain mostly English messages.",
                "GEOTEXT consists of tweets from 9.5K users: 1895 users are held out for each of development and test data."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We evaluate our models over three pre-existing geotagged Twitter datasets: (1) GEOTEXT (Eisen-stein et al., 2010), (2) TWITTER-US #TARGET_REF , and (3) TWITTER-WORLD (#REF) . In each dataset, users are represented by a single meta-document, generated by concatenating their tweets. The datasets are pre-partitioned into training, development and test sets, and rebuilt from the original version to include mention information. The first two datasets were constructed to contain mostly English messages. GEOTEXT consists of tweets from 9.5K users: 1895 users are held out for each of development and test data.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We evaluate our models over three pre-existing geotagged Twitter datasets: (1) GEOTEXT (Eisen-stein et al., 2010), (2) TWITTER-US #TARGET_REF , and (3) TWITTER-WORLD (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The effect of celebrity removal over the development set of TWITTER-US is shown in Figure 2 where it dramatically reduces the graph edge size and simultaneously leads to an improvement in the mean error.",
                "A Unified Geolocation Model To address the issue of disconnected test users, we incorporate text information into the model by attaching a labelled dongle node to every test node (#REF; #REF) .",
                "The label for the dongle node is based on a textbased l 1 regularised logistic regression model, using the method of #TARGET_REF .",
                "The dongle nodes with their corresponding label confidences are added to the seed set, and are treated in the same way as other labelled nodes (i.e. the training nodes).",
                "Once again, we experiment with text-based labelled dongle nodes over both binary (\"MADCEL-B-LR\") and weighted (\"MADCEL-W-LR\") networks."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The effect of celebrity removal over the development set of TWITTER-US is shown in Figure 2 where it dramatically reduces the graph edge size and simultaneously leads to an improvement in the mean error. A Unified Geolocation Model To address the issue of disconnected test users, we incorporate text information into the model by attaching a labelled dongle node to every test node (#REF; #REF) . The label for the dongle node is based on a textbased l 1 regularised logistic regression model, using the method of #TARGET_REF . The dongle nodes with their corresponding label confidences are added to the seed set, and are treated in the same way as other labelled nodes (i.e. the training nodes). Once again, we experiment with text-based labelled dongle nodes over both binary (\"MADCEL-B-LR\") and weighted (\"MADCEL-W-LR\") networks.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The label for the dongle node is based on a textbased l 1 regularised logistic regression model, using the method of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #REF and #REF , we evaluate using the mean and median error (in km) over all test users (\"Mean\" and \"Median\", resp.), and also accuracy within 161km of the actual location (\"Acc@161\").",
                "Note that higher numbers are better for Acc@161, but lower numbers are better for mean and median error, with a lower bound of 0 and no (theoretical) upper bound.",
                "To generate a continuous-valued latitude/longitude coordinate for a given user from the k-d tree cell, we use the median coordinates of all training points in the predicted region.",
                "Table 1 shows the performance of MAD-B, MADCEL-B, MADCEL-W, MADCEL-B-LR and MADCEL-W-LR over the GEOTEXT, TWITTER-US and TWITTER-WORLD datasets.",
                "The results are also compared with prior work on network-based geolocation using label propagation (LP) #TARGET_REF , text-based classification models (#REF; #REF; #TARGET_REF; #REF) , textbased graphical models (#REF) , and network-text hybrid models (LP-LR) #TARGET_REF ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Following #REF and #REF , we evaluate using the mean and median error (in km) over all test users (\"Mean\" and \"Median\", resp.), and also accuracy within 161km of the actual location (\"Acc@161\"). Note that higher numbers are better for Acc@161, but lower numbers are better for mean and median error, with a lower bound of 0 and no (theoretical) upper bound. To generate a continuous-valued latitude/longitude coordinate for a given user from the k-d tree cell, we use the median coordinates of all training points in the predicted region. Table 1 shows the performance of MAD-B, MADCEL-B, MADCEL-W, MADCEL-B-LR and MADCEL-W-LR over the GEOTEXT, TWITTER-US and TWITTER-WORLD datasets. The results are also compared with prior work on network-based geolocation using label propagation (LP) #TARGET_REF , text-based classification models (#REF; #REF; #TARGET_REF; #REF) , textbased graphical models (#REF) , and network-text hybrid models (LP-LR) #TARGET_REF .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"The results are also compared with prior work on network-based geolocation using label propagation (LP) #TARGET_REF , text-based classification models (#REF; #REF; #TARGET_REF; #REF) , textbased graphical models (#REF) , and network-text hybrid models (LP-LR) #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "However, the main focus is to identify only the \"correct\" answers that support a given question.",
                "Since the ability to refute is as important as to support, it does not fully address the verification problem of factchecking.",
                "Recently, #TARGET_REF proposed a public dataset to explore the complete process of the large-scale fact-checking.",
                "It is designed not only to verify claims but also to extract sets of related evidence.",
                "Nevertheless, the pipeline solution proposed in that paper suffers from following problems: 1) The overall performance (30.88% accuracy) still needs further improvement to be applicable to the evidence selection and classification, which also highlights the challenging nature of this task."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "However, the main focus is to identify only the \"correct\" answers that support a given question. Since the ability to refute is as important as to support, it does not fully address the verification problem of factchecking. Recently, #TARGET_REF proposed a public dataset to explore the complete process of the large-scale fact-checking. It is designed not only to verify claims but also to extract sets of related evidence. Nevertheless, the pipeline solution proposed in that paper suffers from following problems: 1) The overall performance (30.88% accuracy) still needs further improvement to be applicable to the evidence selection and classification, which also highlights the challenging nature of this task.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Recently, #TARGET_REF proposed a public dataset to explore the complete process of the large-scale fact-checking.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this module, l sentences are extracted as possible evidence for the claim.",
                "Instead of selecting the sentences by recomputing sentence-level TF-IDF features between claim and document text as in #TARGET_REF , we propose a neural ranker using decomposable attention (DA) model (#REF) to perform evidence selection.",
                "DA model does not require the input text to be parsed syntactically, nor is an ensemble, and it is faster without any recurrent structure.",
                "In general, using neural methods is better for the following reasons: 1) The TF-IDF may have limited ability to capture semantics compared to word representation learning 2) Faster inference time compared to TF-IDF methods that need real-time reconstruction.",
                "The neural ranker DA rank is trained using a fake task, which is to classify whether a given sentence is an evidence of a given claim or not."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In this module, l sentences are extracted as possible evidence for the claim. Instead of selecting the sentences by recomputing sentence-level TF-IDF features between claim and document text as in #TARGET_REF , we propose a neural ranker using decomposable attention (DA) model (#REF) to perform evidence selection. DA model does not require the input text to be parsed syntactically, nor is an ensemble, and it is faster without any recurrent structure. In general, using neural methods is better for the following reasons: 1) The TF-IDF may have limited ability to capture semantics compared to word representation learning 2) Faster inference time compared to TF-IDF methods that need real-time reconstruction. The neural ranker DA rank is trained using a fake task, which is to classify whether a given sentence is an evidence of a given claim or not.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Instead of selecting the sentences by recomputing sentence-level TF-IDF features between claim and document text as in #TARGET_REF , we propose a neural ranker using decomposable attention (DA) model (#REF) to perform evidence selection.\"]}"
    },
    {
        "gold": {
            "text": [
                "Given a claim and l possible evidence, a DA rte classifier is trained to recognize the textual entailment to be support, refute or not enough information to verify (NEI).",
                "Same as #TARGET_REF , we use the decomposable attention (DA) between the claim and the evidence for RTE.",
                "DA model decomposes the RTE problem into subproblems, which can be considered as bi-direction wordlevel attention features.",
                "Note that the DA model is utilized over other models such as as Chen et al. (2017b) ; #REF , because it is a simple but effective model.",
                "Our DA rte model must correctly decide whether a claim is NEI, when the evidence retrieved is irrelevant and insufficient."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Given a claim and l possible evidence, a DA rte classifier is trained to recognize the textual entailment to be support, refute or not enough information to verify (NEI). Same as #TARGET_REF , we use the decomposable attention (DA) between the claim and the evidence for RTE. DA model decomposes the RTE problem into subproblems, which can be considered as bi-direction wordlevel attention features. Note that the DA model is utilized over other models such as as Chen et al. (2017b) ; #REF , because it is a simple but effective model. Our DA rte model must correctly decide whether a claim is NEI, when the evidence retrieved is irrelevant and insufficient.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Same as #TARGET_REF , we use the decomposable attention (DA) between the claim and the evidence for RTE.\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that the DA model is utilized over other models such as as Chen et al. (2017b) ; #REF , because it is a simple but effective model.",
                "Our DA rte model must correctly decide whether a claim is NEI, when the evidence retrieved is irrelevant and insufficient.",
                "However, NEI claims have no annotated evidence, thus cannot be used to train RTE.",
                "To overcome this issue, same as #TARGET_REF , the most probable NEI evidence are simulated by sampling sentences from the nearest page to the claim using the document retrieval module.",
                "MLP DA rte DA rte +NER Accuracy (%) 63.2 78.4 79.9 Table 4 : Oracle RTE classification accuracy in the test set using gold evidence."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Note that the DA model is utilized over other models such as as Chen et al. (2017b) ; #REF , because it is a simple but effective model. Our DA rte model must correctly decide whether a claim is NEI, when the evidence retrieved is irrelevant and insufficient. However, NEI claims have no annotated evidence, thus cannot be used to train RTE. To overcome this issue, same as #TARGET_REF , the most probable NEI evidence are simulated by sampling sentences from the nearest page to the claim using the document retrieval module. MLP DA rte DA rte +NER Accuracy (%) 63.2 78.4 79.9 Table 4 : Oracle RTE classification accuracy in the test set using gold evidence.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"To overcome this issue, same as #TARGET_REF , the most probable NEI evidence are simulated by sampling sentences from the nearest page to the claim using the document retrieval module.\"]}"
    },
    {
        "gold": {
            "text": [
                "Prior work (#REF; #REF) have proposed fact-checking through entailment from knowledge bases.",
                "Some works have investigated fact verification using PolitiFact data (#REF; #REF) or FakeNews challenge (Pomerleau and Rao) .",
                "Most closely related to our work, #TARGET_REF addresses large-scale fact extraction and verification task using a pipeline approach.",
                "In addition, question answering (#REF; #REFa; #REF; #REF) and task-oriented dialog systems (#REF;  Table 7 : Full-pipeline evaluation on the test set using k = 2 and th = 0.6.",
                "The first and the second one (with *) are the baselines from #REF ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Prior work (#REF; #REF) have proposed fact-checking through entailment from knowledge bases. Some works have investigated fact verification using PolitiFact data (#REF; #REF) or FakeNews challenge (Pomerleau and Rao) . Most closely related to our work, #TARGET_REF addresses large-scale fact extraction and verification task using a pipeline approach. In addition, question answering (#REF; #REFa; #REF; #REF) and task-oriented dialog systems (#REF;  Table 7 : Full-pipeline evaluation on the test set using k = 2 and th = 0.6. The first and the second one (with *) are the baselines from #REF .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Most closely related to our work, #TARGET_REF addresses large-scale fact extraction and verification task using a pipeline approach.\"]}"
    },
    {
        "gold": {
            "text": [
                "Dataset: FEVER dataset #TARGET_REF ) is a relatively large-scale dataset compared to other previous fact extraction and verification works, with around 5.4M Wikipedia documents and 185k samples.",
                "The claims are generated by altering sentences extracted from Wikipedia, with humanannotated evidence sentences and verification labels (e.g. Table 1 ).",
                "The training/validation/test sets of these three datasets are split in advance by the providers.",
                "Note that the test-set was equally split into 3 classes: #REF, #REF, NEI (3333).",
                "Training: We trained our models end-to-end using Adagrad optimizer (#REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Dataset: FEVER dataset #TARGET_REF ) is a relatively large-scale dataset compared to other previous fact extraction and verification works, with around 5.4M Wikipedia documents and 185k samples. The claims are generated by altering sentences extracted from Wikipedia, with humanannotated evidence sentences and verification labels (e.g. Table 1 ). The training/validation/test sets of these three datasets are split in advance by the providers. Note that the test-set was equally split into 3 classes: #REF, #REF, NEI (3333). Training: We trained our models end-to-end using Adagrad optimizer (#REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Dataset: FEVER dataset #TARGET_REF ) is a relatively large-scale dataset compared to other previous fact extraction and verification works, with around 5.4M Wikipedia documents and 185k samples.\"]}"
    },
    {
        "gold": {
            "text": [
                "The dropout rate is set to 0.2.",
                "In all the datasets, we tuned the hyper-parameters with grid-search over the validation set.",
                "Evaluation: For each module, we independently measure oracle performance, where we assume gold standard documents and set of evidence are provided (oracle evaluation).",
                "For the final fullpipeline, we compare to and follow the metric defined in #TARGET_REF .",
                "NoScoreEv is a simple classification accuracy that only considers the correctness of the verification label."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The dropout rate is set to 0.2. In all the datasets, we tuned the hyper-parameters with grid-search over the validation set. Evaluation: For each module, we independently measure oracle performance, where we assume gold standard documents and set of evidence are provided (oracle evaluation). For the final fullpipeline, we compare to and follow the metric defined in #TARGET_REF . NoScoreEv is a simple classification accuracy that only considers the correctness of the verification label.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For the final fullpipeline, we compare to and follow the metric defined in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "This is because providing a succinct set of evidence makes the verification task easier for the RTE module.",
                "Therefore, we choose DA rank +NER model with th = 0.6 for the fullpipeline.",
                "Recognizing Textual Entailment: The oracle classification accuracy for RTE is shown in Table 4.",
                "The MLP is a simple multi-layer perceptron using TF and TF-IDF cosine similarity between the claim and evidence as features as shown in #TARGET_REF .",
                "The highest accuracy achieved is 79.9% using DA rte with NER information, thus, we further evaluate the full-pipeline accuracy on this setting."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "This is because providing a succinct set of evidence makes the verification task easier for the RTE module. Therefore, we choose DA rank +NER model with th = 0.6 for the fullpipeline. Recognizing Textual Entailment: The oracle classification accuracy for RTE is shown in Table 4. The MLP is a simple multi-layer perceptron using TF and TF-IDF cosine similarity between the claim and evidence as features as shown in #TARGET_REF . The highest accuracy achieved is 79.9% using DA rte with NER information, thus, we further evaluate the full-pipeline accuracy on this setting.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The MLP is a simple multi-layer perceptron using TF and TF-IDF cosine similarity between the claim and evidence as features as shown in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently, translation scholars have made some general claims about translation properties.",
                "Some of these are source language independent while others are not.",
                "#TARGET_REF performed empirical studies to validate both types of properties using English source texts and other texts translated into English.",
                "Obviously, corpora of this sort, which focus on a single language, are not adequate for claiming universality of translation properties.",
                "In this paper, we are validating both types of translation properties using original and translated texts from six European languages."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                1,
                1,
                1,
                0
            ]
        },
        "input": "Recently, translation scholars have made some general claims about translation properties. Some of these are source language independent while others are not. #TARGET_REF performed empirical studies to validate both types of properties using English source texts and other texts translated into English. Obviously, corpora of this sort, which focus on a single language, are not adequate for claiming universality of translation properties. In this paper, we are validating both types of translation properties using original and translated texts from six European languages.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Recently, translation scholars have made some general claims about translation properties.\", \"Some of these are source language independent while others are not.\", \"#TARGET_REF performed empirical studies to validate both types of properties using English source texts and other texts translated into English.\", \"Obviously, corpora of this sort, which focus on a single language, are not adequate for claiming universality of translation properties.\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF have built a classifier that can identify the correct source of the translated text (given different possible source languages).",
                "They have built another classifier which can identify source text and translated text.",
                "Furthermore, they have shown that the degree of difference between two translated texts, translated from two different languages into the same target language reflects, the degree of difference of the source languages.",
                "They have gained impressive results for both of the tasks.",
                "However, the limitation of this study is that they only used a corpus of English original text and English text translated from various European languages."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "#TARGET_REF have built a classifier that can identify the correct source of the translated text (given different possible source languages). They have built another classifier which can identify source text and translated text. Furthermore, they have shown that the degree of difference between two translated texts, translated from two different languages into the same target language reflects, the degree of difference of the source languages. They have gained impressive results for both of the tasks. However, the limitation of this study is that they only used a corpus of English original text and English text translated from various European languages.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"#TARGET_REF have built a classifier that can identify the correct source of the translated text (given different possible source languages).\", \"They have built another classifier which can identify source text and translated text.\", \"Furthermore, they have shown that the degree of difference between two translated texts, translated from two different languages into the same target language reflects, the degree of difference of the source languages.\", \"They have gained impressive results for both of the tasks.\", \"However, the limitation of this study is that they only used a corpus of English original text and English text translated from various European languages.\"]}"
    },
    {
        "gold": {
            "text": [
                "The system works on a character level rather than on a word level.",
                "The system performs poorly when the source language of the training corpus is different from the one of the test corpus.",
                "We can not compare our findings directly with #TARGET_REF even though we use text from the same corpus and similar techniques.",
                "The English language is not considered for this study due to unavailability of English translations for some languages included in this work.",
                "Furthermore, instead of the list of 300 function words used by #REF , we used the 100 most frequent words for each candidate language."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The system works on a character level rather than on a word level. The system performs poorly when the source language of the training corpus is different from the one of the test corpus. We can not compare our findings directly with #TARGET_REF even though we use text from the same corpus and similar techniques. The English language is not considered for this study due to unavailability of English translations for some languages included in this work. Furthermore, instead of the list of 300 function words used by #REF , we used the 100 most frequent words for each candidate language.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We can not compare our findings directly with #TARGET_REF even though we use text from the same corpus and similar techniques.\"]}"
    },
    {
        "gold": {
            "text": [
                "A customized version of the Europarl corpus (#REF ) is freely available for corpus-based translation studies.",
                "However, this corpus is not suitable for the experiment we are performing here.",
                "We extract a suitable corpus from the Europarl corpus in a way similar to #REF and #REF .",
                "Our target is to extract texts that are translated from and to the languages considered here.",
                "We trust the source language marker that has been put by the respective translator, as did #REF and #TARGET_REF ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "A customized version of the Europarl corpus (#REF ) is freely available for corpus-based translation studies. However, this corpus is not suitable for the experiment we are performing here. We extract a suitable corpus from the Europarl corpus in a way similar to #REF and #REF . Our target is to extract texts that are translated from and to the languages considered here. We trust the source language marker that has been put by the respective translator, as did #REF and #TARGET_REF .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We trust the source language marker that has been put by the respective translator, as did #REF and #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Each chunk contains at least seven sentences.",
                "Our hypothesis is again similar to #TARGET_REF , that is, if the classifier's accuracy is close to 20%, then we cannot say that there is an interference effect in translated text.",
                "If the classifier's accuracy is close to 100% then our conclusion will be that interference effects exist in translated text.",
                "Table 3 and Table 4 show the evaluation results.",
                "Table 4 : Source language identification evaluation (Accuracy)"
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Each chunk contains at least seven sentences. Our hypothesis is again similar to #TARGET_REF , that is, if the classifier's accuracy is close to 20%, then we cannot say that there is an interference effect in translated text. If the classifier's accuracy is close to 100% then our conclusion will be that interference effects exist in translated text. Table 3 and Table 4 show the evaluation results. Table 4 : Source language identification evaluation (Accuracy)",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Our hypothesis is again similar to #TARGET_REF , that is, if the classifier's accuracy is close to 20%, then we cannot say that there is an interference effect in translated text.\", \"If the classifier's accuracy is close to 100% then our conclusion will be that interference effects exist in translated text.\"]}"
    },
    {
        "gold": {
            "text": [
                "The results show that training a classifier based on the 100 most frequent words of a language is sufficient to obtain interpretable results.",
                "We find our results to be compatible with #TARGET_REF who used 300 function words.",
                "A list of the 100 most frequent words is easily obtainable for a vast number of languages, while lists consisting strictly of function words are rare and cannot be produced without considerable additional effort.",
                "While the 100 most frequent words of a language are sufficient to train a classifier for Germanic or Romance languages, it fails to perform equally well for Slavic languages.",
                "#REF claim that Toury's (1995) findings of interference of a translation hold true; we find the assumption to be too simplistic, since for Slavic text either as a source or target language this statement cannot supported."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The results show that training a classifier based on the 100 most frequent words of a language is sufficient to obtain interpretable results. We find our results to be compatible with #TARGET_REF who used 300 function words. A list of the 100 most frequent words is easily obtainable for a vast number of languages, while lists consisting strictly of function words are rare and cannot be produced without considerable additional effort. While the 100 most frequent words of a language are sufficient to train a classifier for Germanic or Romance languages, it fails to perform equally well for Slavic languages. #REF claim that Toury's (1995) findings of interference of a translation hold true; we find the assumption to be too simplistic, since for Slavic text either as a source or target language this statement cannot supported.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"The results show that training a classifier based on the 100 most frequent words of a language is sufficient to obtain interpretable results.\", \"We find our results to be compatible with #TARGET_REF who used 300 function words.\"]}"
    },
    {
        "gold": {
            "text": [
                "The system works on a character level rather than on a word level.",
                "The system performs poorly when the source language of the training corpus is different from the one of the test corpus.",
                "We can not compare our findings directly with #REF even though we use text from the same corpus and similar techniques.",
                "The English language is not considered for this study due to unavailability of English translations for some languages included in this work.",
                "Furthermore, instead of the list of 300 function words used by #TARGET_REF , we used the 100 most frequent words for each candidate language."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The system works on a character level rather than on a word level. The system performs poorly when the source language of the training corpus is different from the one of the test corpus. We can not compare our findings directly with #REF even though we use text from the same corpus and similar techniques. The English language is not considered for this study due to unavailability of English translations for some languages included in this work. Furthermore, instead of the list of 300 function words used by #TARGET_REF , we used the 100 most frequent words for each candidate language.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Furthermore, instead of the list of 300 function words used by #TARGET_REF , we used the 100 most frequent words for each candidate language.\"]}"
    },
    {
        "gold": {
            "text": [
                "Expected F-Scores are calculated from 100 samples.",
                "Table 5 shows the evaluation results.",
                "Even though the classifier for German achieves around 99% accuracy, we cannot compare the result with #TARGET_REF as the amount of chunks for the classes are different.",
                "The classifiers for other languages also display very high accuracy.",
                "The result of Table 5 shows that general translation properties exist for all languages used in this experiment."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Expected F-Scores are calculated from 100 samples. Table 5 shows the evaluation results. Even though the classifier for German achieves around 99% accuracy, we cannot compare the result with #TARGET_REF as the amount of chunks for the classes are different. The classifiers for other languages also display very high accuracy. The result of Table 5 shows that general translation properties exist for all languages used in this experiment.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Even though the classifier for German achieves around 99% accuracy, we cannot compare the result with #TARGET_REF as the amount of chunks for the classes are different.\"]}"
    },
    {
        "gold": {
            "text": [
                "A customized version of the Europarl corpus (#REF ) is freely available for corpus-based translation studies.",
                "However, this corpus is not suitable for the experiment we are performing here.",
                "We extract a suitable corpus from the Europarl corpus in a way similar to #REF and #TARGET_REF .",
                "Our target is to extract texts that are translated from and to the languages considered here.",
                "We trust the source language marker that has been put by the respective translator, as did #REF and #REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "A customized version of the Europarl corpus (#REF ) is freely available for corpus-based translation studies. However, this corpus is not suitable for the experiment we are performing here. We extract a suitable corpus from the Europarl corpus in a way similar to #REF and #TARGET_REF . Our target is to extract texts that are translated from and to the languages considered here. We trust the source language marker that has been put by the respective translator, as did #REF and #REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We extract a suitable corpus from the Europarl corpus in a way similar to #REF and #TARGET_REF .\", \"Our target is to extract texts that are translated from and to the languages considered here.\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, mapping a text into a time series of sentence lengths is a natural way to investigate text structures.",
                "Recent methods applied to study texts at sentence level include probability distributions [24, 25, 26, 27] and correlations [26, 27, 17, #TARGET_REF .",
                "In recent studies, sentence length analysis have been related to style and authorship [29, 26, 27] .",
                "In general, sentence lengths have been quantified by the number of words [24, 29, 25, 28] or characters [30, 31, 26, 27] .",
                "Also, note that the recurrence time of full stops also quantifies the sentence length [3] ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Therefore, mapping a text into a time series of sentence lengths is a natural way to investigate text structures. Recent methods applied to study texts at sentence level include probability distributions [24, 25, 26, 27] and correlations [26, 27, 17, #TARGET_REF . In recent studies, sentence length analysis have been related to style and authorship [29, 26, 27] . In general, sentence lengths have been quantified by the number of words [24, 29, 25, 28] or characters [30, 31, 26, 27] . Also, note that the recurrence time of full stops also quantifies the sentence length [3] .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Recent methods applied to study texts at sentence level include probability distributions [24, 25, 26, 27] and correlations [26, 27, 17, #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In recent studies, sentence length analysis have been related to style and authorship [29, 26, 27] .",
                "In general, sentence lengths have been quantified by the number of words [24, 29, 25, #TARGET_REF or characters [30, 31, 26, 27] .",
                "Also, note that the recurrence time of full stops also quantifies the sentence length [3] .",
                "However, there are other possible variations, such as word length or word frequency mappings, where all words are brought to lower case and punctuation marks are removed.",
                "Other possible variations are the removal of stop words and the lemmatization [21] ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In recent studies, sentence length analysis have been related to style and authorship [29, 26, 27] . In general, sentence lengths have been quantified by the number of words [24, 29, 25, #TARGET_REF or characters [30, 31, 26, 27] . Also, note that the recurrence time of full stops also quantifies the sentence length [3] . However, there are other possible variations, such as word length or word frequency mappings, where all words are brought to lower case and punctuation marks are removed. Other possible variations are the removal of stop words and the lemmatization [21] .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In general, sentence lengths have been quantified by the number of words [24, 29, 25, #TARGET_REF or characters [30, 31, 26, 27] .\"]}"
    },
    {
        "gold": {
            "text": [
                "All comparisons for all books rejected the null hypothesis for these tests.",
                "Before concluding this subsection, we discuss Fig. 2a in connection with the MenzerathAltmann law.",
                "Basically, this law states that the bigger the whole, the smaller its parts and vice-versa.",
                "However, an issue about this relationship was addressed in [37] : \"Somewhat more problematic is the relation of sentence length to the word length.\" This comment is consistent with the one in #TARGET_REF asserting that the Menzerath-Altmann law does not hold if the sentence length is measured in terms of characters instead of the number of words.",
                "In a similar way, the data of Fig. 2a indicates, in the context of this law, that the relationship between the number of words and characters is also problematic."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "All comparisons for all books rejected the null hypothesis for these tests. Before concluding this subsection, we discuss Fig. 2a in connection with the MenzerathAltmann law. Basically, this law states that the bigger the whole, the smaller its parts and vice-versa. However, an issue about this relationship was addressed in [37] : \"Somewhat more problematic is the relation of sentence length to the word length.\" This comment is consistent with the one in #TARGET_REF asserting that the Menzerath-Altmann law does not hold if the sentence length is measured in terms of characters instead of the number of words. In a similar way, the data of Fig. 2a indicates, in the context of this law, that the relationship between the number of words and characters is also problematic.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"However, an issue about this relationship was addressed in [37] : \\\"Somewhat more problematic is the relation of sentence length to the word length.\\\" This comment is consistent with the one in #TARGET_REF asserting that the Menzerath-Altmann law does not hold if the sentence length is measured in terms of characters instead of the number of words.\"]}"
    },
    {
        "gold": {
            "text": [
                "All the series from the other books reflects this behavior (h ∼ 0.75).",
                "This result is consistent with the multifractal analysis performed in #TARGET_REF .",
                "In addition, the values for the difference between Hurst exponents (∆h) of a given book are shown in Fig. 4c , where we can observe that the variation of the Hurst exponents is small from one series to another.",
                "Also, the standard deviation for h within each book was close to 0.001.",
                "Lastly, no correlations were found between the number of sentences in a text and the Hurst exponent."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "All the series from the other books reflects this behavior (h ∼ 0.75). This result is consistent with the multifractal analysis performed in #TARGET_REF . In addition, the values for the difference between Hurst exponents (∆h) of a given book are shown in Fig. 4c , where we can observe that the variation of the Hurst exponents is small from one series to another. Also, the standard deviation for h within each book was close to 0.001. Lastly, no correlations were found between the number of sentences in a text and the Hurst exponent.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"This result is consistent with the multifractal analysis performed in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The present work differs from #REF in two respects, viz. we focus solely on textual information and we directly address the problem of tutorial dialogue.",
                "In this study we apply the methods of #REF , #TARGET_REF Hearst ( , 1997 , and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue.",
                "All three are vector space methods that measure lexical cohesion to determine topic shifts.",
                "Our results show that the new using an orthonormal basis significantly outperforms the other methods.",
                "Section 2 reviews previous work, and Section 3 reviews the vector space model."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The present work differs from #REF in two respects, viz. we focus solely on textual information and we directly address the problem of tutorial dialogue. In this study we apply the methods of #REF , #TARGET_REF Hearst ( , 1997 , and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue. All three are vector space methods that measure lexical cohesion to determine topic shifts. Our results show that the new using an orthonormal basis significantly outperforms the other methods. Section 2 reviews previous work, and Section 3 reviews the vector space model.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In this study we apply the methods of #REF , #TARGET_REF Hearst ( , 1997 , and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue.\"]}"
    },
    {
        "gold": {
            "text": [
                "The JTextTile software was used to implement #TARGET_REF on dialogue.",
                "As with #REF , a text unit and window size had to be determined for dialogue.",
                "#REF recommends using the average paragraph size as the window size.",
                "Using the development corpus's average topic length of 16 utterances as a reference point, F-measures were calculated for the combinations of window size and text unit size in Table 1 .",
                "The optimal combination of parameters (Fmeasure = .17) is a unit size of 16 words and a window size of 16 units."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The JTextTile software was used to implement #TARGET_REF on dialogue. As with #REF , a text unit and window size had to be determined for dialogue. #REF recommends using the average paragraph size as the window size. Using the development corpus's average topic length of 16 utterances as a reference point, F-measures were calculated for the combinations of window size and text unit size in Table 1 . The optimal combination of parameters (Fmeasure = .17) is a unit size of 16 words and a window size of 16 units.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The JTextTile software was used to implement #TARGET_REF on dialogue.\"]}"
    },
    {
        "gold": {
            "text": [
                "This combination matches #TARGET_REF 's heuristic of choosing the window size to be the average paragraph length.",
                "On the test set, this combination of parameters yielded an F-measure of .14 as opposed to the Fmeasure for monologue reported by #REF , .70.",
                "For dialogue, the algorithm is 20% as effective as it is for monologue.",
                "It is unclear, however, exactly what part of the algorithm contributes to this poor performance.",
                "The two most obvious possibilities are the segmentation criterion, i.e. depth scores, or the standard vector space method."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "This combination matches #TARGET_REF 's heuristic of choosing the window size to be the average paragraph length. On the test set, this combination of parameters yielded an F-measure of .14 as opposed to the Fmeasure for monologue reported by #REF , .70. For dialogue, the algorithm is 20% as effective as it is for monologue. It is unclear, however, exactly what part of the algorithm contributes to this poor performance. The two most obvious possibilities are the segmentation criterion, i.e. depth scores, or the standard vector space method.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"This combination matches #TARGET_REF 's heuristic of choosing the window size to be the average paragraph length.\"]}"
    },
    {
        "gold": {
            "text": [
                "Though the idea of using lexical cohesion to segment text has the advantages of simplicity and intuitive appeal, it lacks a unique implementation.",
                "An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment.",
                "Both #TARGET_REF Hearst ( , 1997 and #REF use vector space methods discussed below to represent and compare units of text.",
                "The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.",
                "However, Hearst (1994 Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Though the idea of using lexical cohesion to segment text has the advantages of simplicity and intuitive appeal, it lacks a unique implementation. An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment. Both #TARGET_REF Hearst ( , 1997 and #REF use vector space methods discussed below to represent and compare units of text. The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text. However, Hearst (1994 Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Both #TARGET_REF Hearst ( , 1997 and #REF use vector space methods discussed below to represent and compare units of text.\"]}"
    },
    {
        "gold": {
            "text": [
                "Though the idea of using lexical cohesion to segment text has the advantages of simplicity and intuitive appeal, it lacks a unique implementation.",
                "An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment.",
                "Both Hearst (1994 Hearst ( , 1997 and #REF use vector space methods discussed below to represent and compare units of text.",
                "The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.",
                "However, #TARGET_REF Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Though the idea of using lexical cohesion to segment text has the advantages of simplicity and intuitive appeal, it lacks a unique implementation. An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment. Both Hearst (1994 Hearst ( , 1997 and #REF use vector space methods discussed below to represent and compare units of text. The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text. However, #TARGET_REF Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"However, #TARGET_REF Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, Hearst (1994 Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison.",
                "The text unit's definition in #TARGET_REF Hearst ( , 1997 and #REF is generally task dependent, depending on what size gives the best results.",
                "For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level.",
                "However, when using LSA to segment text, #REF use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion.",
                "Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens (#REF) , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "However, Hearst (1994 Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison. The text unit's definition in #TARGET_REF Hearst ( , 1997 and #REF is generally task dependent, depending on what size gives the best results. For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level. However, when using LSA to segment text, #REF use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion. Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens (#REF) , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The text unit's definition in #TARGET_REF Hearst ( , 1997 and #REF is generally task dependent, depending on what size gives the best results.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, when using LSA to segment text, #REF use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion.",
                "Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens #TARGET_REF , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size.",
                "Under a vector space model, comparisons are performed by calculating the cosine of vectors representing text.",
                "As stated previously, these comparisons reflect the cohesion between units of text.",
                "In order to use these comparisons to segment text, however, one must have a criterion in place."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "However, when using LSA to segment text, #REF use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion. Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens #TARGET_REF , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size. Under a vector space model, comparisons are performed by calculating the cosine of vectors representing text. As stated previously, these comparisons reflect the cohesion between units of text. In order to use these comparisons to segment text, however, one must have a criterion in place.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens #TARGET_REF , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size.\"]}"
    },
    {
        "gold": {
            "text": [
                "To replicate #REF , software was written in Java that created a moving window of varying sizes on the input text, and the software retrieved the LSA vector and calculated the cosine of each window.",
                "Hearst (1994 Hearst ( , 1997 was replicated using the JTextTile (#REF ) Java software.",
                "A variant of #TARGET_REF Hearst ( , 1997 was created by using LSA instead of the standard vector space method.",
                "The orthonormal basis method also used a moving window; however, in contrast to the previous methods, the window is not treated just as a large block of text.",
                "Instead, the window consists of two orthonormal bases, one on either side of an utterance."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "To replicate #REF , software was written in Java that created a moving window of varying sizes on the input text, and the software retrieved the LSA vector and calculated the cosine of each window. Hearst (1994 Hearst ( , 1997 was replicated using the JTextTile (#REF ) Java software. A variant of #TARGET_REF Hearst ( , 1997 was created by using LSA instead of the standard vector space method. The orthonormal basis method also used a moving window; however, in contrast to the previous methods, the window is not treated just as a large block of text. Instead, the window consists of two orthonormal bases, one on either side of an utterance.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"A variant of #TARGET_REF Hearst ( , 1997 was created by using LSA instead of the standard vector space method.\"]}"
    },
    {
        "gold": {
            "text": [
                "Unfortunately, the large difference in F-measure between the Foltz algorithm and the Hearst + LSA algorithm is more difficult to explain.",
                "These two methods differ by their segmentation criterion and by their training (Foltz is a regression model and Hearst is not).",
                "It may be that #TARGET_REF Hearst ( , 1997 )'s segmentation criterion, i.e. depth scores, do not translate well to dialogue.",
                "Perhaps the assignment of segment boundaries based on the relative difference between a candidate score and its surrounding peaks is highly sensitive to cohesion gaps created by conversational implicatures.",
                "On the other hand the differences between these two methods may be entirely attributable to the amount of training they received."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                1,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Unfortunately, the large difference in F-measure between the Foltz algorithm and the Hearst + LSA algorithm is more difficult to explain. These two methods differ by their segmentation criterion and by their training (Foltz is a regression model and Hearst is not). It may be that #TARGET_REF Hearst ( , 1997 )'s segmentation criterion, i.e. depth scores, do not translate well to dialogue. Perhaps the assignment of segment boundaries based on the relative difference between a candidate score and its surrounding peaks is highly sensitive to cohesion gaps created by conversational implicatures. On the other hand the differences between these two methods may be entirely attributable to the amount of training they received.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Unfortunately, the large difference in F-measure between the Foltz algorithm and the Hearst + LSA algorithm is more difficult to explain.\", \"These two methods differ by their segmentation criterion and by their training (Foltz is a regression model and Hearst is not).\", \"It may be that #TARGET_REF Hearst ( , 1997 )'s segmentation criterion, i.e. depth scores, do not translate well to dialogue.\"]}"
    },
    {
        "gold": {
            "text": [
                "To be able to answer the questions \"What causes ebola?\", \"What are the duties of a medical doctor?\", \"What are the differences between a terrorist and a victim?\", \"Which are the animals that have wings but cannot fly?\" one requires knowledge about verb-based relations.",
                "Over the years, researchers have developed various relation learning algorithms.",
                "Some (#REF; #REF) targeted specific relations like BornInYear, CorporationAcquired, others (#REF; #TARGET_REF ) extracted any phrase denoting a relation in an English sentence.",
                "(#REF) used labeled data to learn relations, (#REF) used information encoded in the structured Wikipedia documents, (#REF) bootstrapped patterns.",
                "As a result various knowledge bases have been produced like TopicSignatures (#REF) , ConceptNet (#REF) , Yago (#REF) , NELL (#REF) and ReVerb (#REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "To be able to answer the questions \"What causes ebola?\", \"What are the duties of a medical doctor?\", \"What are the differences between a terrorist and a victim?\", \"Which are the animals that have wings but cannot fly?\" one requires knowledge about verb-based relations. Over the years, researchers have developed various relation learning algorithms. Some (#REF; #REF) targeted specific relations like BornInYear, CorporationAcquired, others (#REF; #TARGET_REF ) extracted any phrase denoting a relation in an English sentence. (#REF) used labeled data to learn relations, (#REF) used information encoded in the structured Wikipedia documents, (#REF) bootstrapped patterns. As a result various knowledge bases have been produced like TopicSignatures (#REF) , ConceptNet (#REF) , Yago (#REF) , NELL (#REF) and ReVerb (#REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Some (#REF; #REF) targeted specific relations like BornInYear, CorporationAcquired, others (#REF; #TARGET_REF ) extracted any phrase denoting a relation in an English sentence.\"]}"
    },
    {
        "gold": {
            "text": [
                "Some (#REF; #REF) targeted specific relations like BornInYear, CorporationAcquired, others (#REF; #REF ) extracted any phrase denoting a relation in an English sentence.",
                "(#REF) used labeled data to learn relations, (#REF) used information encoded in the structured Wikipedia documents, (#REF) bootstrapped patterns.",
                "As a result various knowledge bases have been produced like TopicSignatures (#REF) , ConceptNet (#REF) , Yago (#REF) , NELL (#REF) and ReVerb #TARGET_REF .",
                "Despite the many efforts to date, yet there is no universal repository (or even a system), which for a given term it can immediately return all verb relations related to the term.",
                "However, one would still like to dispose of an automated procedure, which on the fly can accurately and quickly produce such information for any term."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Some (#REF; #REF) targeted specific relations like BornInYear, CorporationAcquired, others (#REF; #REF ) extracted any phrase denoting a relation in an English sentence. (#REF) used labeled data to learn relations, (#REF) used information encoded in the structured Wikipedia documents, (#REF) bootstrapped patterns. As a result various knowledge bases have been produced like TopicSignatures (#REF) , ConceptNet (#REF) , Yago (#REF) , NELL (#REF) and ReVerb #TARGET_REF . Despite the many efforts to date, yet there is no universal repository (or even a system), which for a given term it can immediately return all verb relations related to the term. However, one would still like to dispose of an automated procedure, which on the fly can accurately and quickly produce such information for any term.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"As a result various knowledge bases have been produced like TopicSignatures (#REF) , ConceptNet (#REF) , Yago (#REF) , NELL (#REF) and ReVerb #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Lots of attention has been payed on learning is-a and part-of relations (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) .",
                "Others (#REF; #REF) have focused on learning specific relations like BornInYear, EmployedBy and CorporationAcquired.",
                "However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (#REF; #REF) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance #TARGET_REF .",
                "However, recently developed OpenIE systems like TextRunner (#REF; #REF) and ReVerb (#REF) surmount the necessity of labeled data by extracting arbitrary phrases denoting relations in English sentences.",
                "(#REF; #REF ) define relation to be any verb-prep, adj-noun construction."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Lots of attention has been payed on learning is-a and part-of relations (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) . Others (#REF; #REF) have focused on learning specific relations like BornInYear, EmployedBy and CorporationAcquired. However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (#REF; #REF) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance #TARGET_REF . However, recently developed OpenIE systems like TextRunner (#REF; #REF) and ReVerb (#REF) surmount the necessity of labeled data by extracting arbitrary phrases denoting relations in English sentences. (#REF; #REF ) define relation to be any verb-prep, adj-noun construction.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (#REF; #REF) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Lots of attention has been payed on learning is-a and part-of relations (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) .",
                "Others (#REF; #REF) have focused on learning specific relations like BornInYear, EmployedBy and CorporationAcquired.",
                "However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (#REF; #REF) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance (#REF) .",
                "However, recently developed OpenIE systems like TextRunner (#REF; #REF) and ReVerb #TARGET_REF surmount the necessity of labeled data by extracting arbitrary phrases denoting relations in English sentences.",
                "(#REF; #REF ) define relation to be any verb-prep, adj-noun construction."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Lots of attention has been payed on learning is-a and part-of relations (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) . Others (#REF; #REF) have focused on learning specific relations like BornInYear, EmployedBy and CorporationAcquired. However to build a system that can learn a richer set of relations is not trivial, because often labeled training data is required (#REF; #REF) and most methods do not scale to corpora where the number of relations is very large or when the relations are not specified in advance (#REF) . However, recently developed OpenIE systems like TextRunner (#REF; #REF) and ReVerb #TARGET_REF surmount the necessity of labeled data by extracting arbitrary phrases denoting relations in English sentences. (#REF; #REF ) define relation to be any verb-prep, adj-noun construction.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"However, recently developed OpenIE systems like TextRunner (#REF; #REF) and ReVerb #TARGET_REF surmount the necessity of labeled data by extracting arbitrary phrases denoting relations in English sentences.\"]}"
    },
    {
        "gold": {
            "text": [
                "For our comparative study with existing systems, we used ReVerb 4 (#REF) , which similarly to our approach was specifically designed to learn verb-based relations from unstructured texts.",
                "Currently, ReVerb has extracted relations from ClueWeb09 5 and Wikipedia, which have been freely distributed to the public.",
                "ReVerb learns relations by taking as input any document and applies POS-tagging, NP-chunking and a set of rules over all sentences in the document to generate triples containing the verbs and the arguments associated with them.",
                "According to #TARGET_REF ReVerb outperforms TextRunner (#REF) and the open Wikipedia extractor WOE (#REF) in terms of the quantity and quality of the learned relations.",
                "For comparison, we took five terms from our experiment: ant, bomb, president, terrorists, virus and collected all verbs found by ReVerb in the ClueWeb09 and Wikipedia triples."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "For our comparative study with existing systems, we used ReVerb 4 (#REF) , which similarly to our approach was specifically designed to learn verb-based relations from unstructured texts. Currently, ReVerb has extracted relations from ClueWeb09 5 and Wikipedia, which have been freely distributed to the public. ReVerb learns relations by taking as input any document and applies POS-tagging, NP-chunking and a set of rules over all sentences in the document to generate triples containing the verbs and the arguments associated with them. According to #TARGET_REF ReVerb outperforms TextRunner (#REF) and the open Wikipedia extractor WOE (#REF) in terms of the quantity and quality of the learned relations. For comparison, we took five terms from our experiment: ant, bomb, president, terrorists, virus and collected all verbs found by ReVerb in the ClueWeb09 and Wikipedia triples.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"According to #TARGET_REF ReVerb outperforms TextRunner (#REF) and the open Wikipedia extractor WOE (#REF) in terms of the quantity and quality of the learned relations.\"]}"
    },
    {
        "gold": {
            "text": [
                "The main contributions of the paper are:",
                "• We develop an automatic procedure, which on the fly can learn a diverse set of verb and verb-preposition relations for a given term.",
                "• We establish the effectiveness of our approach through human-based evaluation.",
                "• We conduct a comparative study with the verb-based relation extraction system ReVerb #TARGET_REF and show that our approach accurately extracts more verb-based relations.",
                "• We also compare the verb relations produced by our system with those available in existing knowledge bases, and observe that despite their completeness these repositories lack many verb-based relations."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The main contributions of the paper are: • We develop an automatic procedure, which on the fly can learn a diverse set of verb and verb-preposition relations for a given term. • We establish the effectiveness of our approach through human-based evaluation. • We conduct a comparative study with the verb-based relation extraction system ReVerb #TARGET_REF and show that our approach accurately extracts more verb-based relations. • We also compare the verb relations produced by our system with those available in existing knowledge bases, and observe that despite their completeness these repositories lack many verb-based relations.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"\\u2022 We conduct a comparative study with the verb-based relation extraction system ReVerb #TARGET_REF and show that our approach accurately extracts more verb-based relations.\"]}"
    },
    {
        "gold": {
            "text": [
                "For our comparative study with existing systems, we used ReVerb 4 #TARGET_REF , which similarly to our approach was specifically designed to learn verb-based relations from unstructured texts.",
                "Currently, ReVerb has extracted relations from ClueWeb09 5 and Wikipedia, which have been freely distributed to the public.",
                "ReVerb learns relations by taking as input any document and applies POS-tagging, NP-chunking and a set of rules over all sentences in the document to generate triples containing the verbs and the arguments associated with them.",
                "According to (#REF) ReVerb outperforms TextRunner (#REF) and the open Wikipedia extractor WOE (#REF) in terms of the quantity and quality of the learned relations.",
                "For comparison, we took five terms from our experiment: ant, bomb, president, terrorists, virus and collected all verbs found by ReVerb in the ClueWeb09 and Wikipedia triples."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "For our comparative study with existing systems, we used ReVerb 4 #TARGET_REF , which similarly to our approach was specifically designed to learn verb-based relations from unstructured texts. Currently, ReVerb has extracted relations from ClueWeb09 5 and Wikipedia, which have been freely distributed to the public. ReVerb learns relations by taking as input any document and applies POS-tagging, NP-chunking and a set of rules over all sentences in the document to generate triples containing the verbs and the arguments associated with them. According to (#REF) ReVerb outperforms TextRunner (#REF) and the open Wikipedia extractor WOE (#REF) in terms of the quantity and quality of the learned relations. For comparison, we took five terms from our experiment: ant, bomb, president, terrorists, virus and collected all verbs found by ReVerb in the ClueWeb09 and Wikipedia triples.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"For our comparative study with existing systems, we used ReVerb 4 #TARGET_REF , which similarly to our approach was specifically designed to learn verb-based relations from unstructured texts.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our key contribution is the development of a semi-supervised procedure, which starts with a term and a verb to learn from Web documents a large and diverse set of verb relations.",
                "We have conducted an experimental evaluation with 36 terms and have collected 26, 678 unique candidate verbs and 1, 040, 651 candidate argument fillers.",
                "We have evaluated the accuracy of our approach using human based evaluation and have compared results against the ReVerb #TARGET_REF system and existing knowledge bases like NELL (#REF) , Yago (#REF) and ConceptNet (#REF) .",
                "Our study showed that despite their completeness these resources lack verb-based information and there is plenty of room for improvement since they can be further enriched with verbs using our harvesting procedure.",
                "In the future, we would like to test the usefulness of the generated resources in NLP applications."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Our key contribution is the development of a semi-supervised procedure, which starts with a term and a verb to learn from Web documents a large and diverse set of verb relations. We have conducted an experimental evaluation with 36 terms and have collected 26, 678 unique candidate verbs and 1, 040, 651 candidate argument fillers. We have evaluated the accuracy of our approach using human based evaluation and have compared results against the ReVerb #TARGET_REF system and existing knowledge bases like NELL (#REF) , Yago (#REF) and ConceptNet (#REF) . Our study showed that despite their completeness these resources lack verb-based information and there is plenty of room for improvement since they can be further enriched with verbs using our harvesting procedure. In the future, we would like to test the usefulness of the generated resources in NLP applications.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We have evaluated the accuracy of our approach using human based evaluation and have compared results against the ReVerb #TARGET_REF system and existing knowledge bases like NELL (#REF) , Yago (#REF) and ConceptNet (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Unfortunately, this approach requires a control of the samples variance, which can lead otherwise to unstable learning [12] .",
                "In a similar work, Mnih et al. #TARGET_REF proposed to use Noise Contrastive Estimation (NCE) [14] to speed-up the training.",
                "NCE treats the learning as a binary classification problem between a target word and noise samples, which are drawn from a noise distribution.",
                "Moreover, NCE considers the normalization term as an additional parameter that can be learned during training or fixed beforehand.",
                "In this case, the network learns to self-normalize."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Unfortunately, this approach requires a control of the samples variance, which can lead otherwise to unstable learning [12] . In a similar work, Mnih et al. #TARGET_REF proposed to use Noise Contrastive Estimation (NCE) [14] to speed-up the training. NCE treats the learning as a binary classification problem between a target word and noise samples, which are drawn from a noise distribution. Moreover, NCE considers the normalization term as an additional parameter that can be learned during training or fixed beforehand. In this case, the network learns to self-normalize.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In a similar work, Mnih et al. #TARGET_REF proposed to use Noise Contrastive Estimation (NCE) [14] to speed-up the training.\"]}"
    },
    {
        "gold": {
            "text": [
                "Moreover, training using IS can be very difficult and requires a careful control of the samples variance, which can lead otherwise to unstable learning as was reported in [12] .",
                "Hence, an adaptive IS may use a large number of samples to solve this problem whereas NCE is more stable and requires a fixed small number of noise samples (e.g., 100) to achieve a good performance #TARGET_REF 16] .",
                "Furthermore, the network learns to self-normalize during training using NCE.",
                "As a results, and on the contrary to IS, the softmax is no longer required during evaluation, which makes NCE an attractive choice to train large vocabulary NNLM.",
                "The next section will show how NCE can be efficiently implemented in batch mode training."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Moreover, training using IS can be very difficult and requires a careful control of the samples variance, which can lead otherwise to unstable learning as was reported in [12] . Hence, an adaptive IS may use a large number of samples to solve this problem whereas NCE is more stable and requires a fixed small number of noise samples (e.g., 100) to achieve a good performance #TARGET_REF 16] . Furthermore, the network learns to self-normalize during training using NCE. As a results, and on the contrary to IS, the softmax is no longer required during evaluation, which makes NCE an attractive choice to train large vocabulary NNLM. The next section will show how NCE can be efficiently implemented in batch mode training.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Hence, an adaptive IS may use a large number of samples to solve this problem whereas NCE is more stable and requires a fixed small number of noise samples (e.g., 100) to achieve a good performance #TARGET_REF 16] .\"]}"
    },
    {
        "gold": {
            "text": [
                "To alleviate this problem, noise samples can be shared across the batch [16] .",
                "This paper proposes an extension of NCE to batch mode (B-NCE) training.",
                "This approach does not require any sampling and can be formulated using dense matrix operations.",
                "Furthermore, we can show that this solution optimally approximates the sampling from a unigram distribution, which has been shown to be a good noise distribution choice #TARGET_REF 16] .",
                "The main idea here is to restrict the vocabulary, at each forward-backward pass, to the target words in the batch (words to predict) and then replace the softmax function by NCE."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "To alleviate this problem, noise samples can be shared across the batch [16] . This paper proposes an extension of NCE to batch mode (B-NCE) training. This approach does not require any sampling and can be formulated using dense matrix operations. Furthermore, we can show that this solution optimally approximates the sampling from a unigram distribution, which has been shown to be a good noise distribution choice #TARGET_REF 16] . The main idea here is to restrict the vocabulary, at each forward-backward pass, to the target words in the batch (words to predict) and then replace the softmax function by NCE.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Furthermore, we can show that this solution optimally approximates the sampling from a unigram distribution, which has been shown to be a good noise distribution choice #TARGET_REF 16] .\"]}"
    },
    {
        "gold": {
            "text": [
                "For the LTCB corpus, we also report results of the models trained with the full softmax function.",
                "This is the primary motive for using this corpus.",
                "We would like also to highlight that the goal of this paper is not about improving LMs performance but rather showing how a significant training speed-up can be achieved without compromising the models performance for large vocabulary LMs.",
                "Hence, we solely focus our experiments on NCE as a major approach to achieve this goal [17, #TARGET_REF 16] in comparison to the reference full softmax function.",
                "Comparison to other training approaches such as importance sampling will be conducted in future work."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "For the LTCB corpus, we also report results of the models trained with the full softmax function. This is the primary motive for using this corpus. We would like also to highlight that the goal of this paper is not about improving LMs performance but rather showing how a significant training speed-up can be achieved without compromising the models performance for large vocabulary LMs. Hence, we solely focus our experiments on NCE as a major approach to achieve this goal [17, #TARGET_REF 16] in comparison to the reference full softmax function. Comparison to other training approaches such as importance sampling will be conducted in future work.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Hence, we solely focus our experiments on NCE as a major approach to achieve this goal [17, #TARGET_REF 16] in comparison to the reference full softmax function.\"]}"
    },
    {
        "gold": {
            "text": [
                "Moreover, B-NCE and S-NCE use the unigram as noise distribution pn.",
                "Following the setup proposed in #TARGET_REF 16] , S-NCE uses K = 100 noise samples, whereas B-NCE uses only the target words in the batch (K=0).",
                "Note that S-NCE will process and update B + K words at its output layer during each forward-backward pass, whereas B-NCE updates only B words.",
                "Similarly to [17] , the NCE normalization constant is set to Z = exp(9), which approximates the mean of the normalization term using softmax.",
                "Table 2 clearly show that B-NCE reduces the training time by a factor of 4 to 8 with a slight degradation in the models performance compared to softmax."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Moreover, B-NCE and S-NCE use the unigram as noise distribution pn. Following the setup proposed in #TARGET_REF 16] , S-NCE uses K = 100 noise samples, whereas B-NCE uses only the target words in the batch (K=0). Note that S-NCE will process and update B + K words at its output layer during each forward-backward pass, whereas B-NCE updates only B words. Similarly to [17] , the NCE normalization constant is set to Z = exp(9), which approximates the mean of the normalization term using softmax. Table 2 clearly show that B-NCE reduces the training time by a factor of 4 to 8 with a slight degradation in the models performance compared to softmax.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following the setup proposed in #TARGET_REF 16] , S-NCE uses K = 100 noise samples, whereas B-NCE uses only the target words in the batch (K=0).\"]}"
    },
    {
        "gold": {
            "text": [
                "Topic models are a newer development in machine learning that play an important role in document analysis and information retrieval.",
                "It turns out there is a surprising connection between the two that suggests novel ways of extending both grammars and topic models.",
                "After explaining this connection, I go on to describe extensions which identify topical multiword collocations and automatically learn the internal structure of namedentity phrases.",
                "The adaptor grammar framework is a nonparametric extension of probabilistic context-free grammars (#REF) , which was initially intended to allow fast prototyping of models of unsupervised language acquisition (#REF), but it has been shown to have applications in text data mining and information retrieval as well #TARGET_REF) .",
                "We'll see how learning the referents of words and learning the roles of social cues in language acquisition (#REF) can be viewed as a kind of topic modelling problem that can be reduced to a grammatical inference problem using the techniques described in this talk."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Topic models are a newer development in machine learning that play an important role in document analysis and information retrieval. It turns out there is a surprising connection between the two that suggests novel ways of extending both grammars and topic models. After explaining this connection, I go on to describe extensions which identify topical multiword collocations and automatically learn the internal structure of namedentity phrases. The adaptor grammar framework is a nonparametric extension of probabilistic context-free grammars (#REF) , which was initially intended to allow fast prototyping of models of unsupervised language acquisition (#REF), but it has been shown to have applications in text data mining and information retrieval as well #TARGET_REF) . We'll see how learning the referents of words and learning the roles of social cues in language acquisition (#REF) can be viewed as a kind of topic modelling problem that can be reduced to a grammatical inference problem using the techniques described in this talk.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The adaptor grammar framework is a nonparametric extension of probabilistic context-free grammars (#REF) , which was initially intended to allow fast prototyping of models of unsupervised language acquisition (#REF), but it has been shown to have applications in text data mining and information retrieval as well #TARGET_REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Measuring relational similarity between two word pairs plays important roles in natural language processing (NLP).",
                "The techniques for solving this problem can be applied to a variety of NLP tasks, such as query expansion, word sense disambiguation, machine translation, information extraction and question answering.",
                "Previous work addressing the problem can be roughly classified into three categories: (1) learning word embeddings from large collections of text using variants of neural networks (Mikolov et al. (2013a) ; Mikolov et al. (2013b) ; Mikolov et al. (2013c) ; #TARGET_REF ) or global matrix factorization (#REF ; #REF ); (2) extracting knowledge from existing semantic networks, such as WordNet (#REF ; #REF ; #REF ) and ConceptNet (#REF ); (3) combining the above two models by various ways (#REF ; #REF ; #REF ; #REF ).",
                "The empirical evidence shows that the word representations learned from neural network models do an especially good job in capturing not only attributional similarities between words but also similarities between pairs of words (Mikolov et al. (2013c) ).",
                "#REF generalize the skip-gram model with negative sampling to include arbitrary word contexts and present the dependency-based word embeddings, which are learned from syntactic contexts derived from dependency parse-trees."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Measuring relational similarity between two word pairs plays important roles in natural language processing (NLP). The techniques for solving this problem can be applied to a variety of NLP tasks, such as query expansion, word sense disambiguation, machine translation, information extraction and question answering. Previous work addressing the problem can be roughly classified into three categories: (1) learning word embeddings from large collections of text using variants of neural networks (Mikolov et al. (2013a) ; Mikolov et al. (2013b) ; Mikolov et al. (2013c) ; #TARGET_REF ) or global matrix factorization (#REF ; #REF ); (2) extracting knowledge from existing semantic networks, such as WordNet (#REF ; #REF ; #REF ) and ConceptNet (#REF ); (3) combining the above two models by various ways (#REF ; #REF ; #REF ; #REF ). The empirical evidence shows that the word representations learned from neural network models do an especially good job in capturing not only attributional similarities between words but also similarities between pairs of words (Mikolov et al. (2013c) ). #REF generalize the skip-gram model with negative sampling to include arbitrary word contexts and present the dependency-based word embeddings, which are learned from syntactic contexts derived from dependency parse-trees.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Previous work addressing the problem can be roughly classified into three categories: (1) learning word embeddings from large collections of text using variants of neural networks (Mikolov et al. (2013a) ; Mikolov et al. (2013b) ; Mikolov et al. (2013c) ; #TARGET_REF ) or global matrix factorization (#REF ; #REF ); (2) extracting knowledge from existing semantic networks, such as WordNet (#REF ; #REF ; #REF ) and ConceptNet (#REF ); (3) combining the above two models by various ways (#REF ; #REF ; #REF ; #REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "So for k =2, the contexts of the target word w t are w t−2 , w t−1 , w t+1, w t+2 and we are predicting each of these from the word w t .",
                "However, a context window with a smaller size k may miss some important contexts while including some accidental ones.",
                "Recently, Levy et al. propose the dependency-based word embeddings (DEP), which generalize the skip-gram model with negative sampling, and move from linear bag-of-words contexts to syntactic contexts that are derived from automatically produced dependency parse-trees.",
                "Embeddings produced from different kinds of contexts can induce different word similarities.",
                "The original skip-gram embeddings can yield broad topical similarities, while the dependency-based word embeddings can capture more functional similarities #TARGET_REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "So for k =2, the contexts of the target word w t are w t−2 , w t−1 , w t+1, w t+2 and we are predicting each of these from the word w t . However, a context window with a smaller size k may miss some important contexts while including some accidental ones. Recently, Levy et al. propose the dependency-based word embeddings (DEP), which generalize the skip-gram model with negative sampling, and move from linear bag-of-words contexts to syntactic contexts that are derived from automatically produced dependency parse-trees. Embeddings produced from different kinds of contexts can induce different word similarities. The original skip-gram embeddings can yield broad topical similarities, while the dependency-based word embeddings can capture more functional similarities #TARGET_REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The original skip-gram embeddings can yield broad topical similarities, while the dependency-based word embeddings can capture more functional similarities #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The first is the advent of deep learning techniques (#REF) , which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data.",
                "The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries #TARGET_REF; #REF) , which permit fast integration loops between model conception and experimental evaluation.",
                "Cloze-style queries (#REF) are created by deleting a particular word in a natural-language statement.",
                "The task is to guess which word was deleted.",
                "In a pragmatic approach, recent work (#REF) formed such questions by extracting a sentence from a larger document."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The first is the advent of deep learning techniques (#REF) , which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data. The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries #TARGET_REF; #REF) , which permit fast integration loops between model conception and experimental evaluation. Cloze-style queries (#REF) are created by deleting a particular word in a natural-language statement. The task is to guess which word was deleted. In a pragmatic approach, recent work (#REF) formed such questions by extracting a sentence from a larger document.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries #TARGET_REF; #REF) , which permit fast integration loops between model conception and experimental evaluation.\"]}"
    },
    {
        "gold": {
            "text": [
                "The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries (#REF; #REF) , which permit fast integration loops between model conception and experimental evaluation.",
                "Cloze-style queries (#REF) are created by deleting a particular word in a natural-language statement.",
                "The task is to guess which word was deleted.",
                "In a pragmatic approach, recent work #TARGET_REF formed such questions by extracting a sentence from a larger document.",
                "In contrast to considering a stand-alone statement, the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries (#REF; #REF) , which permit fast integration loops between model conception and experimental evaluation. Cloze-style queries (#REF) are created by deleting a particular word in a natural-language statement. The task is to guess which word was deleted. In a pragmatic approach, recent work #TARGET_REF formed such questions by extracting a sentence from a larger document. In contrast to considering a stand-alone statement, the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In a pragmatic approach, recent work #TARGET_REF formed such questions by extracting a sentence from a larger document.\"]}"
    },
    {
        "gold": {
            "text": [
                "One of the advantages of using Cloze-style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention.",
                "The CBT #TARGET_REF and CNN (#REF) corpora are two such datasets.",
                "The CBT 1 corpus was generated from well-known children's books available through Project Gutenberg.",
                "Documents consist of 20-sentence excerpts from these books.",
                "The related query is formed from an excerpt's 21st sentence by replacing a single word with an anonymous placeholder token."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "One of the advantages of using Cloze-style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention. The CBT #TARGET_REF and CNN (#REF) corpora are two such datasets. The CBT 1 corpus was generated from well-known children's books available through Project Gutenberg. Documents consist of 20-sentence excerpts from these books. The related query is formed from an excerpt's 21st sentence by replacing a single word with an anonymous placeholder token.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"One of the advantages of using Cloze-style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention.\", \"The CBT #TARGET_REF and CNN (#REF) corpora are two such datasets.\"]}"
    },
    {
        "gold": {
            "text": [
                "After a fixed number of iterations, the model uses a summary of its inference process to predict the answer.",
                "This paper makes the following contributions.",
                "We present a novel iterative, alternating attention mechanism that, unlike existing models #TARGET_REF; #REF) , does not compress the query to a single representation, but instead alternates its attention between the query and the document to obtain a fine-grained query representation within a fixed computation time.",
                "Our architecture tightly integrates previous ideas related to bidirectional readers (#REF) and iterative attention processes (#REF; #REF) .",
                "It obtains state-of-theart results on two machine comprehension datasets and shows promise for application to a broad range of natural language processing tasks."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "After a fixed number of iterations, the model uses a summary of its inference process to predict the answer. This paper makes the following contributions. We present a novel iterative, alternating attention mechanism that, unlike existing models #TARGET_REF; #REF) , does not compress the query to a single representation, but instead alternates its attention between the query and the document to obtain a fine-grained query representation within a fixed computation time. Our architecture tightly integrates previous ideas related to bidirectional readers (#REF) and iterative attention processes (#REF; #REF) . It obtains state-of-theart results on two machine comprehension datasets and shows promise for application to a broad range of natural language processing tasks.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We present a novel iterative, alternating attention mechanism that, unlike existing models #TARGET_REF; #REF) , does not compress the query to a single representation, but instead alternates its attention between the query and the document to obtain a fine-grained query representation within a fixed computation time.\"]}"
    },
    {
        "gold": {
            "text": [
                "where q i, t are the query attention weights and A q ∈ R 2h×s , where s is the dimensionality of the inference GRU state, and a q ∈ R 2h .",
                "The attention we use here is similar to the formulation used in #TARGET_REF; #REF) , but with two differences.",
                "First, we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step.",
                "This simple bilinear attention has been successfully used in (#REF) .",
                "Second, we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t−1 ."
            ],
            "label": [
                "DIFFER",
                "SIM"
            ],
            "context": [
                0,
                1,
                1,
                1,
                1
            ]
        },
        "input": "where q i, t are the query attention weights and A q ∈ R 2h×s , where s is the dimensionality of the inference GRU state, and a q ∈ R 2h . The attention we use here is similar to the formulation used in #TARGET_REF; #REF) , but with two differences. First, we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step. This simple bilinear attention has been successfully used in (#REF) . Second, we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t−1 .",
        "output": "{\"label\": [\"DIFFER\", \"SIM\"], \"context\": [\"The attention we use here is similar to the formulation used in #TARGET_REF; #REF) , but with two differences.\", \"First, we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step.\", \"This simple bilinear attention has been successfully used in (#REF) .\", \"Second, we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t\\u22121 .\"]}"
    },
    {
        "gold": {
            "text": [
                "The subsets are named entity, common noun, verb, and preposition.",
                "We will focus our evaluation solely on the first two subsets, i.e. CBT-NE (named entity) and CBT-CN (common nouns), since the latter two are relatively simple as demonstrated by #TARGET_REF .",
                "The CNN 2 corpus was generated from news articles available through the CNN website.",
                "The documents are given by the full articles themselves, which are accompanied by short, bullet-point summary statements.",
                "Instead of extracting a query from the articles themselves, the authors replace a named entity within each article summary with an anonymous placeholder token."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The subsets are named entity, common noun, verb, and preposition. We will focus our evaluation solely on the first two subsets, i.e. CBT-NE (named entity) and CBT-CN (common nouns), since the latter two are relatively simple as demonstrated by #TARGET_REF . The CNN 2 corpus was generated from news articles available through the CNN website. The documents are given by the full articles themselves, which are accompanied by short, bullet-point summary statements. Instead of extracting a query from the articles themselves, the authors replace a named entity within each article summary with an anonymous placeholder token.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"We will focus our evaluation solely on the first two subsets, i.e. CBT-NE (named entity) and CBT-CN (common nouns), since the latter two are relatively simple as demonstrated by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Neural attention models have been applied recently to a smörgåsbord of machine learning and natural language processing problems.",
                "These include, but are not limited to, handwriting recognition (#REF) , digit classification (#REF) , machine translation (#REF) , question answering (#REF; #REF) and caption generation (#REF) .",
                "In general, attention models keep a memory of states that can be accessed at will by learned attention policies.",
                "In our case, the memory is represented by the set of document and query contextual encodings.",
                "Our model is closely related to (#REF; #REF; #REF; #REF; #TARGET_REF , which were also applied to question answering."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Neural attention models have been applied recently to a smörgåsbord of machine learning and natural language processing problems. These include, but are not limited to, handwriting recognition (#REF) , digit classification (#REF) , machine translation (#REF) , question answering (#REF; #REF) and caption generation (#REF) . In general, attention models keep a memory of states that can be accessed at will by learned attention policies. In our case, the memory is represented by the set of document and query contextual encodings. Our model is closely related to (#REF; #REF; #REF; #REF; #TARGET_REF , which were also applied to question answering.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Our model is closely related to (#REF; #REF; #REF; #REF; #TARGET_REF , which were also applied to question answering.\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition to achieving state-ofthe-art performance, this technique may also prove to be more scalable than alternative query attention models.",
                "Finally, our iterative inference process shares similarities to the iterative hops in Memory Networks (#REF; #TARGET_REF .",
                "In that model, the query representation is updated iteratively from hop to hop, although its different components are not attended to separately.",
                "Moreover, we substitute the simple linear update with a GRU network.",
                "The gating mechanism of the GRU network made it possible to use multiple steps of attention and to propagate the learning signal effectively back through to the first timestep."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In addition to achieving state-ofthe-art performance, this technique may also prove to be more scalable than alternative query attention models. Finally, our iterative inference process shares similarities to the iterative hops in Memory Networks (#REF; #TARGET_REF . In that model, the query representation is updated iteratively from hop to hop, although its different components are not attended to separately. Moreover, we substitute the simple linear update with a GRU network. The gating mechanism of the GRU network made it possible to use multiple steps of attention and to propagate the learning signal effectively back through to the first timestep.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Finally, our iterative inference process shares similarities to the iterative hops in Memory Networks (#REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We report the results of our model on the CBT-CN, CBT-NE and CNN datasets, previously described in Section 2.",
                "Table 2 reports our results on the CBT-CN and CBT-NE dataset.",
                "The Humans, LSTMs and Memory Networks (MemNNs) results are taken from #TARGET_REF and the Attention-Sum Reader (AS Reader) is a state-of-the-art result recently obtained by (#REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1
            ]
        },
        "input": "We report the results of our model on the CBT-CN, CBT-NE and CNN datasets, previously described in Section 2. Table 2 reports our results on the CBT-CN and CBT-NE dataset. The Humans, LSTMs and Memory Networks (MemNNs) results are taken from #TARGET_REF and the Attention-Sum Reader (AS Reader) is a state-of-the-art result recently obtained by (#REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The Humans, LSTMs and Memory Networks (MemNNs) results are taken from #TARGET_REF and the Attention-Sum Reader (AS Reader) is a state-of-the-art result recently obtained by (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "There is a deep tension in statistical modeling of grammatical structure between providing good expressivity -to allow accurate modeling of the data with sparse grammars -and low complexitymaking induction of the grammars and parsing of novel sentences computationally practical.",
                "Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the problem of segmenting training data trees into elementary parse tree fragments to form the grammar (#REF; #TARGET_REF; #REF) .",
                "DP inference tackles this problem by exploring the space of all possible segmentations of the data, in search for fragments that are on the one hand large enough so that they incorporate the useful dependencies, and on the other small enough so that they recur and have a chance to be useful in analyzing unseen data.",
                "The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) \"split them up\", preventing their reuse, leading to less sparse grammars than might be ideal.",
                "For instance, imagine modeling the following set of structures:"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "There is a deep tension in statistical modeling of grammatical structure between providing good expressivity -to allow accurate modeling of the data with sparse grammars -and low complexitymaking induction of the grammars and parsing of novel sentences computationally practical. Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the problem of segmenting training data trees into elementary parse tree fragments to form the grammar (#REF; #TARGET_REF; #REF) . DP inference tackles this problem by exploring the space of all possible segmentations of the data, in search for fragments that are on the one hand large enough so that they incorporate the useful dependencies, and on the other small enough so that they recur and have a chance to be useful in analyzing unseen data. The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) \"split them up\", preventing their reuse, leading to less sparse grammars than might be ideal. For instance, imagine modeling the following set of structures:",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the problem of segmenting training data trees into elementary parse tree fragments to form the grammar (#REF; #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "In the basic nonparametric TSG model, there is an independent DP for every grammar category (such as c = N P ), each of which uses a base distribution P 0 that generates an initial tree by making stepwise decisions.",
                "The canonical P 0 uses a probabilistic CFGP that is fixed a priori to sample CFG rules top-down and Bernoulli variables for determining where substitutions should occur (#REF; #TARGET_REF .",
                "We extend this model by adding specialized DPs for left and right auxiliary trees.",
                "3",
                "Therefore, we have an exchangeable process for generating right auxiliary trees"
            ],
            "label": [
                "BACK",
                "EXT"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "In the basic nonparametric TSG model, there is an independent DP for every grammar category (such as c = N P ), each of which uses a base distribution P 0 that generates an initial tree by making stepwise decisions. The canonical P 0 uses a probabilistic CFGP that is fixed a priori to sample CFG rules top-down and Bernoulli variables for determining where substitutions should occur (#REF; #TARGET_REF . We extend this model by adding specialized DPs for left and right auxiliary trees. 3 Therefore, we have an exchangeable process for generating right auxiliary trees",
        "output": "{\"label\": [\"BACK\", \"EXT\"], \"context\": [\"The canonical P 0 uses a probabilistic CFGP that is fixed a priori to sample CFG rules top-down and Bernoulli variables for determining where substitutions should occur (#REF; #TARGET_REF .\", \"We extend this model by adding specialized DPs for left and right auxiliary trees.\"]}"
    },
    {
        "gold": {
            "text": [
                "Since TIG derivations are highly structured objects, a basic sampling strategy based on local node-level moves such as Gibbs sampling (#REF) would not hold much promise.",
                "Following previous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion #TARGET_REF; #REF) .",
                "This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (#REF) .",
                "Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a CFG rule CFG probability proposal distribution.",
                "Fortunately, #REF provide an (exact) transformation from a fully general TIG into a TSG that generates the same string languages."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Since TIG derivations are highly structured objects, a basic sampling strategy based on local node-level moves such as Gibbs sampling (#REF) would not hold much promise. Following previous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion #TARGET_REF; #REF) . This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (#REF) . Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a CFG rule CFG probability proposal distribution. Fortunately, #REF provide an (exact) transformation from a fully general TIG into a TSG that generates the same string languages.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following previous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (#REF) .",
                "Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a CFG rule CFG probability proposal distribution.",
                "Fortunately, #REF provide an (exact) transformation from a fully general TIG into a TSG that generates the same string languages.",
                "It is then straightforward to represent this TSG as a CFG using the Goodman transform (#REF; #TARGET_REF .",
                "Figure 4 lists the additional CFG productions we have designed, as well as the rules used that trigger them."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (#REF) . Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a CFG rule CFG probability proposal distribution. Fortunately, #REF provide an (exact) transformation from a fully general TIG into a TSG that generates the same string languages. It is then straightforward to represent this TSG as a CFG using the Goodman transform (#REF; #TARGET_REF . Figure 4 lists the additional CFG productions we have designed, as well as the rules used that trigger them.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"It is then straightforward to represent this TSG as a CFG using the Goodman transform (#REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "All reported numbers are averages over three runs.",
                "Parsing results are based on the maximum probability parse which was obtained by sampling derivations under the transform CFG.",
                "We compare our system (referred to as TIG) to our implementation of the TSG system of #TARGET_REF ) (referred to as TSG) and the constrained TIG variant of (#REF ) (referred to as TIG 0 ).",
                "The upshot of our experiments is that, while on the large training set all models have similar performance (85.6, 85.3, 85 .4 for TSG, TIG 0 and TIG respectively), on the small dataset insertion helps nonparametric model to find more compact and generalizable representations for the data, which affects parsing performance (Figure 4 ).",
                "Although TIG 0 has performance close to TIG, note that TIG achieves this performance using a more succinct representation and extracting a rich set of auxiliary trees."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "All reported numbers are averages over three runs. Parsing results are based on the maximum probability parse which was obtained by sampling derivations under the transform CFG. We compare our system (referred to as TIG) to our implementation of the TSG system of #TARGET_REF ) (referred to as TSG) and the constrained TIG variant of (#REF ) (referred to as TIG 0 ). The upshot of our experiments is that, while on the large training set all models have similar performance (85.6, 85.3, 85 .4 for TSG, TIG 0 and TIG respectively), on the small dataset insertion helps nonparametric model to find more compact and generalizable representations for the data, which affects parsing performance (Figure 4 ). Although TIG 0 has performance close to TIG, note that TIG achieves this performance using a more succinct representation and extracting a rich set of auxiliary trees.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"We compare our system (referred to as TIG) to our implementation of the TSG system of #TARGET_REF ) (referred to as TSG) and the constrained TIG variant of (#REF ) (referred to as TIG 0 ).\", \"The upshot of our experiments is that, while on the large training set all models have similar performance (85.6, 85.3, 85 .4 for TSG, TIG 0 and TIG respectively), on the small dataset insertion helps nonparametric model to find more compact and generalizable representations for the data, which affects parsing performance (Figure 4 ).\"]}"
    },
    {
        "gold": {
            "text": [
                "Hikers could immediately learn about their bug bites and whether to seek out emergency medical care.",
                "Pilots could learn how many birds are in their path to decide whether to change course and so avoid costly, life-threatening collisions.",
                "These examples illustrate several of the interests from a visual question answering (VQA) system, including tackling problems that involve classification, detection, and counting.",
                "More generally, the goal for VQA is to have a single system that can accurately answer any natural language question about an image or video [2] , #TARGET_REF , [4] .",
                "Entangled in the dream of a VQA system is an unavoidable issue that, when asking multiple people a visual question, sometimes they all agree on a single answer while other times they offer different answers ( Figure 1) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Hikers could immediately learn about their bug bites and whether to seek out emergency medical care. Pilots could learn how many birds are in their path to decide whether to change course and so avoid costly, life-threatening collisions. These examples illustrate several of the interests from a visual question answering (VQA) system, including tackling problems that involve classification, detection, and counting. More generally, the goal for VQA is to have a single system that can accurately answer any natural language question about an image or video [2] , #TARGET_REF , [4] . Entangled in the dream of a VQA system is an unavoidable issue that, when asking multiple people a visual question, sometimes they all agree on a single answer while other times they offer different answers ( Figure 1) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"More generally, the goal for VQA is to have a single system that can accurately answer any natural language question about an image or video [2] , #TARGET_REF , [4] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Our work is partially inspired by the goal to improve how to employ crowds as the computing power at run-time.",
                "Towards satisfying existing users, gaining new users, and supporting a wide range of applications, a crowd-powered VQA system should be low cost, have fast response times, and yield high quality answers.",
                "Today's status quo is to assume a fixed number of human responses per visual question and so a fixed cost, delay, and potential diversity of answers for every visual question #TARGET_REF , [1] , [5] .",
                "We instead propose to dynamically solicit the number of human responses based on each visual question.",
                "In particular, we aim to accrue additional costs and delays from collecting extra answers only when extra responses are needed to discover all plausible answers."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Our work is partially inspired by the goal to improve how to employ crowds as the computing power at run-time. Towards satisfying existing users, gaining new users, and supporting a wide range of applications, a crowd-powered VQA system should be low cost, have fast response times, and yield high quality answers. Today's status quo is to assume a fixed number of human responses per visual question and so a fixed cost, delay, and potential diversity of answers for every visual question #TARGET_REF , [1] , [5] . We instead propose to dynamically solicit the number of human responses based on each visual question. In particular, we aim to accrue additional costs and delays from collecting extra answers only when extra responses are needed to discover all plausible answers.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"Today's status quo is to assume a fixed number of human responses per visual question and so a fixed cost, delay, and potential diversity of answers for every visual question #TARGET_REF , [1] , [5] .\", \"We instead propose to dynamically solicit the number of human responses based on each visual question.\"]}"
    },
    {
        "gold": {
            "text": [
                "We show in our experiments that our system saves 19 40-hour work weeks and $1800 to answer 121,512 visual questions, compared to today's status quo approach [1] .",
                "Our work is also inspired by the goal to improve how to employ crowds to produce the information needed to train and evaluate automated methods.",
                "Specifically, researchers in fields as diverse as computer vision #TARGET_REF , computational linguistics [2] , and machine learning [4] rely on large datasets to improve their VQA algorithms.",
                "These datasets include visual questions and human-supplied answers.",
                "Such data is critical for teaching machine learning algorithms how to answer questions by example."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We show in our experiments that our system saves 19 40-hour work weeks and $1800 to answer 121,512 visual questions, compared to today's status quo approach [1] . Our work is also inspired by the goal to improve how to employ crowds to produce the information needed to train and evaluate automated methods. Specifically, researchers in fields as diverse as computer vision #TARGET_REF , computational linguistics [2] , and machine learning [4] rely on large datasets to improve their VQA algorithms. These datasets include visual questions and human-supplied answers. Such data is critical for teaching machine learning algorithms how to answer questions by example.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Specifically, researchers in fields as diverse as computer vision #TARGET_REF , computational linguistics [2] , and machine learning [4] rely on large datasets to improve their VQA algorithms.\"]}"
    },
    {
        "gold": {
            "text": [
                "These datasets include visual questions and human-supplied answers.",
                "Such data is critical for teaching machine learning algorithms how to answer questions by example.",
                "Such data is also critical for evaluating how well VQA algorithms perform.",
                "In general, \"bigger\" data is better.",
                "Current methods to create these datasets assume a fixed number of human answers per visual question #TARGET_REF , [5] , thereby either compromising on quality by not collecting all plausible answers or cost by collecting additional answers when they are redundant."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "These datasets include visual questions and human-supplied answers. Such data is critical for teaching machine learning algorithms how to answer questions by example. Such data is also critical for evaluating how well VQA algorithms perform. In general, \"bigger\" data is better. Current methods to create these datasets assume a fixed number of human answers per visual question #TARGET_REF , [5] , thereby either compromising on quality by not collecting all plausible answers or cost by collecting additional answers when they are redundant.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Current methods to create these datasets assume a fixed number of human answers per visual question #TARGET_REF , [5] , thereby either compromising on quality by not collecting all plausible answers or cost by collecting additional answers when they are redundant.\"]}"
    },
    {
        "gold": {
            "text": [
                "Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2] , #TARGET_REF , [1] , [4] .",
                "Yet a commonality across these communities is they adopt a one-size-fits-all approach when deciding the number of answers for any visual question.",
                "For example, crowd-powered systems aim to supply a prespecified, fixed number of answers per visual question [1] and automated systems return a single answer for every visual question [2] , [3] , [4] .",
                "Inspired by the observation that there can be multiple plausible answers per visual question, we propose a richer representation of visual question answering that accounts for whether different people would agree on a single answer.",
                "We propose a system that automatically predicts whether humans will disagree."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2] , #TARGET_REF , [1] , [4] . Yet a commonality across these communities is they adopt a one-size-fits-all approach when deciding the number of answers for any visual question. For example, crowd-powered systems aim to supply a prespecified, fixed number of answers per visual question [1] and automated systems return a single answer for every visual question [2] , [3] , [4] . Inspired by the observation that there can be multiple plausible answers per visual question, we propose a richer representation of visual question answering that accounts for whether different people would agree on a single answer. We propose a system that automatically predicts whether humans will disagree.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2] , #TARGET_REF , [1] , [4] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2] , [3] , [1] , [4] .",
                "Yet a commonality across these communities is they adopt a one-size-fits-all approach when deciding the number of answers for any visual question.",
                "For example, crowd-powered systems aim to supply a prespecified, fixed number of answers per visual question [1] and automated systems return a single answer for every visual question [2] , #TARGET_REF , [4] .",
                "Inspired by the observation that there can be multiple plausible answers per visual question, we propose a richer representation of visual question answering that accounts for whether different people would agree on a single answer.",
                "We propose a system that automatically predicts whether humans will disagree."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2] , [3] , [1] , [4] . Yet a commonality across these communities is they adopt a one-size-fits-all approach when deciding the number of answers for any visual question. For example, crowd-powered systems aim to supply a prespecified, fixed number of answers per visual question [1] and automated systems return a single answer for every visual question [2] , #TARGET_REF , [4] . Inspired by the observation that there can be multiple plausible answers per visual question, we propose a richer representation of visual question answering that accounts for whether different people would agree on a single answer. We propose a system that automatically predicts whether humans will disagree.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"For example, crowd-powered systems aim to supply a prespecified, fixed number of answers per visual question [1] and automated systems return a single answer for every visual question [2] , #TARGET_REF , [4] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Such approaches aim to collect a pre-specified, fixed number of answers per visual question.",
                "For those systems that treat response time as a first priority, a variable number of answers may arise but this is due to varying crowdsourcing conditions such as the available supply of workers [1] , [6] .",
                "Other systems ensure a fixed number of answers are collected per visual question #TARGET_REF , [5] .",
                "Unlike prior work, our goal is to collect answers in a way that is both economical and complete in capturing the diversity of plausible answers for all visual questions.",
                "To our knowledge, our work is the first to predict the number of answers to collect for a visual question."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Such approaches aim to collect a pre-specified, fixed number of answers per visual question. For those systems that treat response time as a first priority, a variable number of answers may arise but this is due to varying crowdsourcing conditions such as the available supply of workers [1] , [6] . Other systems ensure a fixed number of answers are collected per visual question #TARGET_REF , [5] . Unlike prior work, our goal is to collect answers in a way that is both economical and complete in capturing the diversity of plausible answers for all visual questions. To our knowledge, our work is the first to predict the number of answers to collect for a visual question.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"Other systems ensure a fixed number of answers are collected per visual question #TARGET_REF , [5] .\", \"Unlike prior work, our goal is to collect answers in a way that is both economical and complete in capturing the diversity of plausible answers for all visual questions.\"]}"
    },
    {
        "gold": {
            "text": [
                "Specifically, 80% (i.e., 369,861) of VQAs are about real images that show 91 types of objects that would be \"easily recognizable by a 4 year old\" in their natural context [21] .",
                "The remaining 90,000 VQAs are about abstract scenes that were created with clipart and show 100 types of everyday objects often observed in real images #TARGET_REF .",
                "The benchmark includes a diversity of visual questions intentionally collected to be both grounded in images and taskindependent.",
                "Towards this aim, visual questions were collected by asking three Amazon Mechanical Turk (AMT) crowd workers to look at a given image and generate a text-based question about it that would \"stump a smart robot\" [3] .",
                "Three open-ended questions were collected about each of 153,287 images, resulting in a total of 459,861 visual questions."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Specifically, 80% (i.e., 369,861) of VQAs are about real images that show 91 types of objects that would be \"easily recognizable by a 4 year old\" in their natural context [21] . The remaining 90,000 VQAs are about abstract scenes that were created with clipart and show 100 types of everyday objects often observed in real images #TARGET_REF . The benchmark includes a diversity of visual questions intentionally collected to be both grounded in images and taskindependent. Towards this aim, visual questions were collected by asking three Amazon Mechanical Turk (AMT) crowd workers to look at a given image and generate a text-based question about it that would \"stump a smart robot\" [3] . Three open-ended questions were collected about each of 153,287 images, resulting in a total of 459,861 visual questions.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The remaining 90,000 VQAs are about abstract scenes that were created with clipart and show 100 types of everyday objects often observed in real images #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "While this approach does not fully resolve all conceptually equivalent responses, it does reveal an upper bound of expected answer diversity.",
                "In other words, more lenient agreement schemes (e.g., employing sophisticated natural language processing methods) would lead to either the same or less answer diversity.",
                "We establish valid answers by tallying the number of times each unique answer is given and then only accepting answers observed from at least m people, where m is an application-specific parameter to set.",
                "In practice, prior work deems answers as 100% valid using blind trust (i.e., m = 1 person) [22] as well as more conservative answer validation schemes (i.e., m = 3 people) #TARGET_REF .",
                "Measuring Answer Diversity: We now turn to the question of how much answer diversity is observed in practice for visual questions."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "While this approach does not fully resolve all conceptually equivalent responses, it does reveal an upper bound of expected answer diversity. In other words, more lenient agreement schemes (e.g., employing sophisticated natural language processing methods) would lead to either the same or less answer diversity. We establish valid answers by tallying the number of times each unique answer is given and then only accepting answers observed from at least m people, where m is an application-specific parameter to set. In practice, prior work deems answers as 100% valid using blind trust (i.e., m = 1 person) [22] as well as more conservative answer validation schemes (i.e., m = 3 people) #TARGET_REF . Measuring Answer Diversity: We now turn to the question of how much answer diversity is observed in practice for visual questions.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"We establish valid answers by tallying the number of times each unique answer is given and then only accepting answers observed from at least m people, where m is an application-specific parameter to set.\", \"In practice, prior work deems answers as 100% valid using blind trust (i.e., m = 1 person) [22] as well as more conservative answer validation schemes (i.e., m = 3 people) #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We next present a novel resource allocation system for efficiently capturing the diversity of true answers for a batch of visual questions.",
                "Today's status quo is to either uniformly collect N answers for every visual question #TARGET_REF or collect multiple answers where the number is determined by external crowdsourcing conditions [1] .",
                "Our system instead spends a human budget by predicting the number of answers to collect Fig. 6 : We propose a novel application of predicting the number of redundant answers to collect from the crowd per visual question to efficiently capture the diversity of all answers for all visual questions.",
                "(a) For a batch of visual questions, our system first produces a relative ordering using the predicted confidence in whether a crowd would agree on an answer (upper half).",
                "Then, the system allocates a minimum number of annotations to all visual questions (bottom, left half) and then the extra available human budget to visual questions most confidently predicted to lead to crowd disagreement (bottom, right half)."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "We next present a novel resource allocation system for efficiently capturing the diversity of true answers for a batch of visual questions. Today's status quo is to either uniformly collect N answers for every visual question #TARGET_REF or collect multiple answers where the number is determined by external crowdsourcing conditions [1] . Our system instead spends a human budget by predicting the number of answers to collect Fig. 6 : We propose a novel application of predicting the number of redundant answers to collect from the crowd per visual question to efficiently capture the diversity of all answers for all visual questions. (a) For a batch of visual questions, our system first produces a relative ordering using the predicted confidence in whether a crowd would agree on an answer (upper half). Then, the system allocates a minimum number of annotations to all visual questions (bottom, left half) and then the extra available human budget to visual questions most confidently predicted to lead to crowd disagreement (bottom, right half).",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"Today's status quo is to either uniformly collect N answers for every visual question #TARGET_REF or collect multiple answers where the number is determined by external crowdsourcing conditions [1] .\", \"Our system instead spends a human budget by predicting the number of answers to collect Fig. 6 : We propose a novel application of predicting the number of redundant answers to collect from the crowd per visual question to efficiently capture the diversity of all answers for all visual questions.\"]}"
    },
    {
        "gold": {
            "text": [
                "Status Quo:",
                "The system randomly prioritizes which images receive redundancy.",
                "This predictor illustrates the best a user can achieve today with crowd-powered systems [1] , [6] or with current dataset collection methods #TARGET_REF , [5] .",
                "Evaluation Methodology: We quantify the total diversity of answers captured by a resource allocation system for a batch of visual questions Q as follows:",
                "where q i represents the set of all true answers for the i-th visual question, r i represents the set of unique answers captured in the R answers collected for the i-th visual question, and s j represents the set of unique answers captured in the S answers collected for the j-th visual question."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Status Quo: The system randomly prioritizes which images receive redundancy. This predictor illustrates the best a user can achieve today with crowd-powered systems [1] , [6] or with current dataset collection methods #TARGET_REF , [5] . Evaluation Methodology: We quantify the total diversity of answers captured by a resource allocation system for a batch of visual questions Q as follows: where q i represents the set of all true answers for the i-th visual question, r i represents the set of unique answers captured in the R answers collected for the i-th visual question, and s j represents the set of unique answers captured in the S answers collected for the j-th visual question.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"This predictor illustrates the best a user can achieve today with crowd-powered systems [1] , [6] or with current dataset collection methods #TARGET_REF , [5] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Inspired by the observation that there can be multiple plausible answers per visual question, we propose a richer representation of visual question answering that accounts for whether different people would agree on a single answer.",
                "We propose a system that automatically predicts whether humans will disagree.",
                "We demonstrate the predictive advantage of our system over relying on the uncertainty of a VQA algorithm in its predicted answer #TARGET_REF .",
                "Answer Collection from a Crowd: Our work relates to methods that propose how to employ crowd workers to answer questions about images.",
                "Such approaches aim to collect a pre-specified, fixed number of answers per visual question."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Inspired by the observation that there can be multiple plausible answers per visual question, we propose a richer representation of visual question answering that accounts for whether different people would agree on a single answer. We propose a system that automatically predicts whether humans will disagree. We demonstrate the predictive advantage of our system over relying on the uncertainty of a VQA algorithm in its predicted answer #TARGET_REF . Answer Collection from a Crowd: Our work relates to methods that propose how to employ crowd workers to answer questions about images. Such approaches aim to collect a pre-specified, fixed number of answers per visual question.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We demonstrate the predictive advantage of our system over relying on the uncertainty of a VQA algorithm in its predicted answer #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "To our knowledge, no prior work has directly addressed predicting answer (dis)agreement for visual questions.",
                "Therefore, we employ as a baseline a related VQA algorithm [25] , #TARGET_REF which produces for a given visual question an answer with a confidence score.",
                "This system parallels the deep learning architecture we adapt.",
                "However, it predicts the system's uncertainty in its own answer, whereas we are interested in the humans' collective disagreement on the answer.",
                "Still, it is a useful baseline to see if an existing algorithm could serve our purpose."
            ],
            "label": [
                "DIFFER",
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                1,
                0
            ]
        },
        "input": "To our knowledge, no prior work has directly addressed predicting answer (dis)agreement for visual questions. Therefore, we employ as a baseline a related VQA algorithm [25] , #TARGET_REF which produces for a given visual question an answer with a confidence score. This system parallels the deep learning architecture we adapt. However, it predicts the system's uncertainty in its own answer, whereas we are interested in the humans' collective disagreement on the answer. Still, it is a useful baseline to see if an existing algorithm could serve our purpose.",
        "output": "{\"label\": [\"DIFFER\", \"USE\"], \"context\": [\"Therefore, we employ as a baseline a related VQA algorithm [25] , #TARGET_REF which produces for a given visual question an answer with a confidence score.\", \"However, it predicts the system's uncertainty in its own answer, whereas we are interested in the humans' collective disagreement on the answer.\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 4a shows precision-recall curves for all prediction systems.",
                "Both our proposed classification systems outperform the VQA Algorithm #TARGET_REF baseline; e.g., Ours -RF yields a 12 percentage point improvement with respect to AP.",
                "This is interesting because it shows there is value in learning the disagreement task specifically, rather than employing an algorithm's confidence in its answers.",
                "More generally, our results demonstrate it is possible to predict whether a crowd will agree on a single answer from a given image and associated question.",
                "Despite the significant variety of questions and image content, and despite the variety of reasons for which the crowd can disagree, our learned model is able to produce quite accurate results 1 ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Figure 4a shows precision-recall curves for all prediction systems. Both our proposed classification systems outperform the VQA Algorithm #TARGET_REF baseline; e.g., Ours -RF yields a 12 percentage point improvement with respect to AP. This is interesting because it shows there is value in learning the disagreement task specifically, rather than employing an algorithm's confidence in its answers. More generally, our results demonstrate it is possible to predict whether a crowd will agree on a single answer from a given image and associated question. Despite the significant variety of questions and image content, and despite the variety of reasons for which the crowd can disagree, our learned model is able to produce quite accurate results 1 .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Both our proposed classification systems outperform the VQA Algorithm #TARGET_REF baseline; e.g., Ours -RF yields a 12 percentage point improvement with respect to AP.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our approach fills an important gap in the crowdsourcing answer collection literature for targeting the allocation of extra answers only to visual questions where a diversity of answers is expected.",
                "Figure 6b also illustrates the advantage of our system over a related VQA algorithm #TARGET_REF for our novel application of costsensitive answer collection from a crowd.",
                "As observed, relying on an algorithm's confidence in its answer offers a valuable indicator over today's status quo of passively budgeting.",
                "While we acknowledge this method is not intended for our task specifically, it still serves as an important baseline (as discussed above).",
                "We attribute the further performance gains of our prediction system to it directly predicting whether humans will disagree rather than predicting a property of a specific algorithm (e.g., confidence of the Antol et al. algorithm in its answer prediction)."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Our approach fills an important gap in the crowdsourcing answer collection literature for targeting the allocation of extra answers only to visual questions where a diversity of answers is expected. Figure 6b also illustrates the advantage of our system over a related VQA algorithm #TARGET_REF for our novel application of costsensitive answer collection from a crowd. As observed, relying on an algorithm's confidence in its answer offers a valuable indicator over today's status quo of passively budgeting. While we acknowledge this method is not intended for our task specifically, it still serves as an important baseline (as discussed above). We attribute the further performance gains of our prediction system to it directly predicting whether humans will disagree rather than predicting a property of a specific algorithm (e.g., confidence of the Antol et al. algorithm in its answer prediction).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Figure 6b also illustrates the advantage of our system over a related VQA algorithm #TARGET_REF for our novel application of costsensitive answer collection from a crowd.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our first aim is to answer the following questions: 1) How much answer diversity arises for visual questions? and 2) Why do people disagree?",
                "VQA Datasets: We conduct our analysis on a total of 459,861 visual questions and 4,598,610 answers coming from today's largest freely-available VQA benchmark #TARGET_REF .",
                "We chose this benchmark because it both represents a diversity of visual questions and includes many crowdsourced answers for every visual question.",
                "The benchmark consists of two datasets that reflect VQAs for real images and abstract scenes.",
                "Specifically, 80% (i.e., 369,861) of VQAs are about real images that show 91 types of objects that would be \"easily recognizable by a 4 year old\" in their natural context [21] ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Our first aim is to answer the following questions: 1) How much answer diversity arises for visual questions? and 2) Why do people disagree? VQA Datasets: We conduct our analysis on a total of 459,861 visual questions and 4,598,610 answers coming from today's largest freely-available VQA benchmark #TARGET_REF . We chose this benchmark because it both represents a diversity of visual questions and includes many crowdsourced answers for every visual question. The benchmark consists of two datasets that reflect VQAs for real images and abstract scenes. Specifically, 80% (i.e., 369,861) of VQAs are about real images that show 91 types of objects that would be \"easily recognizable by a 4 year old\" in their natural context [21] .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"VQA Datasets: We conduct our analysis on a total of 459,861 visual questions and 4,598,610 answers coming from today's largest freely-available VQA benchmark #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Specifically, 80% (i.e., 369,861) of VQAs are about real images that show 91 types of objects that would be \"easily recognizable by a 4 year old\" in their natural context [21] .",
                "The remaining 90,000 VQAs are about abstract scenes that were created with clipart and show 100 types of everyday objects often observed in real images [3] .",
                "The benchmark includes a diversity of visual questions intentionally collected to be both grounded in images and taskindependent.",
                "Towards this aim, visual questions were collected by asking three Amazon Mechanical Turk (AMT) crowd workers to look at a given image and generate a text-based question about it that would \"stump a smart robot\" #TARGET_REF .",
                "Three open-ended questions were collected about each of 153,287 images, resulting in a total of 459,861 visual questions."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Specifically, 80% (i.e., 369,861) of VQAs are about real images that show 91 types of objects that would be \"easily recognizable by a 4 year old\" in their natural context [21] . The remaining 90,000 VQAs are about abstract scenes that were created with clipart and show 100 types of everyday objects often observed in real images [3] . The benchmark includes a diversity of visual questions intentionally collected to be both grounded in images and taskindependent. Towards this aim, visual questions were collected by asking three Amazon Mechanical Turk (AMT) crowd workers to look at a given image and generate a text-based question about it that would \"stump a smart robot\" #TARGET_REF . Three open-ended questions were collected about each of 153,287 images, resulting in a total of 459,861 visual questions.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Towards this aim, visual questions were collected by asking three Amazon Mechanical Turk (AMT) crowd workers to look at a given image and generate a text-based question about it that would \\\"stump a smart robot\\\" #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The benchmark also includes 10 open-ended natural language answers from 10 AMT crowd workers per visual question.",
                "Each answer was collected by showing a worker an image with associated question and asking him/her to respond with \"a brief phrase and not a complete sentence\" #TARGET_REF .",
                "Finally, to enrich our analysis, we leverage the included labels which indicate the type of answer elicited for each visual question.",
                "Specifically, each visual question is labeled as eliciting one of the following types of answers: \"yes/no\", \"number\", or \"other\".",
                "This label was determined as the most popular option from 10 labels assigned to the associated 10 answers for each visual question."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The benchmark also includes 10 open-ended natural language answers from 10 AMT crowd workers per visual question. Each answer was collected by showing a worker an image with associated question and asking him/her to respond with \"a brief phrase and not a complete sentence\" #TARGET_REF . Finally, to enrich our analysis, we leverage the included labels which indicate the type of answer elicited for each visual question. Specifically, each visual question is labeled as eliciting one of the following types of answers: \"yes/no\", \"number\", or \"other\". This label was determined as the most popular option from 10 labels assigned to the associated 10 answers for each visual question.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Each answer was collected by showing a worker an image with associated question and asking him/her to respond with \\\"a brief phrase and not a complete sentence\\\" #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We establish unique answers by pre-processing each answer to eliminate cosmetic differences and then applying exact string matching to identify the number of different answers.",
                "We pre-process each answer by converting all letters to lower case, converting numbers to digits, and removing punctuation and articles (i.e., \"a\", \"an\", \"the\"), as was done in prior work #TARGET_REF .",
                "While this approach does not fully resolve all conceptually equivalent responses, it does reveal an upper bound of expected answer diversity.",
                "In other words, more lenient agreement schemes (e.g., employing sophisticated natural language processing methods) would lead to either the same or less answer diversity.",
                "We establish valid answers by tallying the number of times each unique answer is given and then only accepting answers observed from at least m people, where m is an application-specific parameter to set."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We establish unique answers by pre-processing each answer to eliminate cosmetic differences and then applying exact string matching to identify the number of different answers. We pre-process each answer by converting all letters to lower case, converting numbers to digits, and removing punctuation and articles (i.e., \"a\", \"an\", \"the\"), as was done in prior work #TARGET_REF . While this approach does not fully resolve all conceptually equivalent responses, it does reveal an upper bound of expected answer diversity. In other words, more lenient agreement schemes (e.g., employing sophisticated natural language processing methods) would lead to either the same or less answer diversity. We establish valid answers by tallying the number of times each unique answer is given and then only accepting answers observed from at least m people, where m is an application-specific parameter to set.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We pre-process each answer by converting all letters to lower case, converting numbers to digits, and removing punctuation and articles (i.e., \\\"a\\\", \\\"an\\\", \\\"the\\\"), as was done in prior work #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We now describe our studies to assess the predictive power of our classification systems to decide whether visual questions will lead to answer (dis)agreement.",
                "We capitalize on today's largest visual question answering dataset #TARGET_REF to evaluate our prediction system, which includes 369,861 visual questions about real images.",
                "Of these, 248,349 visual questions (i.e., Training questions 2015 v1.0) are kept for for training and the remaining 121,512 visual questions (i.e., Validation questions 2015 v1.0) are employed for testing our classification system.",
                "This separation of training and testing samples enables us to estimate how well a classifier will generalize when applied to an unseen, independent set of visual questions.",
                "To our knowledge, no prior work has directly addressed predicting answer (dis)agreement for visual questions."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We now describe our studies to assess the predictive power of our classification systems to decide whether visual questions will lead to answer (dis)agreement. We capitalize on today's largest visual question answering dataset #TARGET_REF to evaluate our prediction system, which includes 369,861 visual questions about real images. Of these, 248,349 visual questions (i.e., Training questions 2015 v1.0) are kept for for training and the remaining 121,512 visual questions (i.e., Validation questions 2015 v1.0) are employed for testing our classification system. This separation of training and testing samples enables us to estimate how well a classifier will generalize when applied to an unseen, independent set of visual questions. To our knowledge, no prior work has directly addressed predicting answer (dis)agreement for visual questions.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We capitalize on today's largest visual question answering dataset #TARGET_REF to evaluate our prediction system, which includes 369,861 visual questions about real images.\"]}"
    },
    {
        "gold": {
            "text": [
                "While more elaborate schemes for distributing responses may be possible, we will show this approach already proves quite effective in our experiments.",
                "We simulate answer collection by randomly selecting answers from the 10 crowd answers per visual question.",
                "Baselines: We compare our approach to the following baselines:",
                "VQA Algorithm #TARGET_REF :",
                "As in the previous section, we leverage the output confidence score from the publicly-shared model [25] learned from a LSTM-CNN deep learning architecture to rank the order of priority for visual questions to receive redundancy."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "While more elaborate schemes for distributing responses may be possible, we will show this approach already proves quite effective in our experiments. We simulate answer collection by randomly selecting answers from the 10 crowd answers per visual question. Baselines: We compare our approach to the following baselines: VQA Algorithm #TARGET_REF : As in the previous section, we leverage the output confidence score from the publicly-shared model [25] learned from a LSTM-CNN deep learning architecture to rank the order of priority for visual questions to receive redundancy.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Baselines: We compare our approach to the following baselines:\", \"VQA Algorithm #TARGET_REF :\"]}"
    },
    {
        "gold": {
            "text": [
                "We observe that question-based features yield greater predictive performance than image-based features for all visual questions, when comparing AP scores for Q and I classification results ( Figure 5 ).",
                "In fact, image features contribute to performance improvements only for our random forest classifier for visual questions that lead to \"number\" answers, as illustrated by comparing AP scores for Our RF: Q+I and Our RF: Q (Figure 5b) .",
                "Our overall finding that most of the predictive power stems from language-based features parallels feature analysis findings in the automated VQA literature #TARGET_REF , [22] .",
                "This does not mean, however, that the image content is not predictive.",
                "Further work improving visual content cues for VQA agreement is warranted."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We observe that question-based features yield greater predictive performance than image-based features for all visual questions, when comparing AP scores for Q and I classification results ( Figure 5 ). In fact, image features contribute to performance improvements only for our random forest classifier for visual questions that lead to \"number\" answers, as illustrated by comparing AP scores for Our RF: Q+I and Our RF: Q (Figure 5b) . Our overall finding that most of the predictive power stems from language-based features parallels feature analysis findings in the automated VQA literature #TARGET_REF , [22] . This does not mean, however, that the image content is not predictive. Further work improving visual content cues for VQA agreement is warranted.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Our overall finding that most of the predictive power stems from language-based features parallels feature analysis findings in the automated VQA literature #TARGET_REF , [22] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , we consider two document collections heterogeneous if #TARGET_REF documents differ systematically with respect to vocabulary and / or level of abstraction.",
                "With these defining differences, there often also comes a difference in length, which, however, by itself does not make document collections heterogeneous.",
                "Examples include collections in which expert answers are mapped to non-expert questions (e.g. InsuranceQA by #REF ), but also so-called community QA collections (#REF ), where the lexical mismatch between Q and A documents is often less pronounced than the length difference.",
                "Like many other approaches, the proposed method is based on word embeddings as universal meaning representations, and on vector cosine as the similarity metric.",
                "However, instead of computing pairs of document representations and measuring their similarity, our method assesses the document-pair similarity on the basis of selected pairwise word similarities."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Following #TARGET_REF , we consider two document collections heterogeneous if #TARGET_REF documents differ systematically with respect to vocabulary and / or level of abstraction. With these defining differences, there often also comes a difference in length, which, however, by itself does not make document collections heterogeneous. Examples include collections in which expert answers are mapped to non-expert questions (e.g. InsuranceQA by #REF ), but also so-called community QA collections (#REF ), where the lexical mismatch between Q and A documents is often less pronounced than the length difference. Like many other approaches, the proposed method is based on word embeddings as universal meaning representations, and on vector cosine as the similarity metric. However, instead of computing pairs of document representations and measuring their similarity, our method assesses the document-pair similarity on the basis of selected pairwise word similarities.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Following #TARGET_REF , we consider two document collections heterogeneous if #TARGET_REF documents differ systematically with respect to vocabulary and / or level of abstraction.\", \"With these defining differences, there often also comes a difference in length, which, however, by itself does not make document collections heterogeneous.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF do not provide any specification, or annotation guidelines, of the semantics of the 'matches' relation to be annotated.",
                "Instead, they create gold standard annotations based on a majority vote of three manual annotations.",
                "Figure 1 provides an example of a matching C-P pair.",
                "The concept labels can be very specific, potentially introducing vocabulary that is not present in the actual concept descriptions.",
                "The extent to which this information is used by #TARGET_REF is not entirely clear, so we experiment with several setups (cf. Section 4)."
            ],
            "label": [
                "MOT",
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "#REF do not provide any specification, or annotation guidelines, of the semantics of the 'matches' relation to be annotated. Instead, they create gold standard annotations based on a majority vote of three manual annotations. Figure 1 provides an example of a matching C-P pair. The concept labels can be very specific, potentially introducing vocabulary that is not present in the actual concept descriptions. The extent to which this information is used by #TARGET_REF is not entirely clear, so we experiment with several setups (cf. Section 4).",
        "output": "{\"label\": [\"MOT\", \"EXT\"], \"context\": [\"The extent to which this information is used by #TARGET_REF is not entirely clear, so we experiment with several setups (cf. Section 4).\"]}"
    },
    {
        "gold": {
            "text": [
                "We presented a simple method for semantic matching of documents from heterogeneous collections as a solution to the Concept-Project matching task by #TARGET_REF .",
                "Although much simpler, our method clearly outperformed the original system in most input settings.",
                "Another result is that, contrary to the claim made by #REF , the standard averaging approach does indeed work very well even for heterogeneous document collections, if appropriate weighting is applied.",
                "Due to its simplicity, we believe that our method can also be applied to other text matching tasks, including more 'standard' ones which do not necessarily involve heterogeneous document collections.",
                "This seems desirable because our method offers additional transparency by providing not only a similarity score, but also the subset of words on which the similarity score is based."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We presented a simple method for semantic matching of documents from heterogeneous collections as a solution to the Concept-Project matching task by #TARGET_REF . Although much simpler, our method clearly outperformed the original system in most input settings. Another result is that, contrary to the claim made by #REF , the standard averaging approach does indeed work very well even for heterogeneous document collections, if appropriate weighting is applied. Due to its simplicity, we believe that our method can also be applied to other text matching tasks, including more 'standard' ones which do not necessarily involve heterogeneous document collections. This seems desirable because our method offers additional transparency by providing not only a similarity score, but also the subset of words on which the similarity score is based.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"We presented a simple method for semantic matching of documents from heterogeneous collections as a solution to the Concept-Project matching task by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Examples include collections in which expert answers are mapped to non-expert questions (e.g. InsuranceQA by #REF ), but also so-called community QA collections (#REF ), where the lexical mismatch between Q and A documents is often less pronounced than the length difference.",
                "Like many other approaches, the proposed method is based on word embeddings as universal meaning representations, and on vector cosine as the similarity metric.",
                "However, instead of computing pairs of document representations and measuring their similarity, our method assesses the document-pair similarity on the basis of selected pairwise word similarities.",
                "This has the following advantages, which make our method a viable candidate for practical, real-world applications: efficiency, because pairwise word similarities can be efficiently (pre-)computed and cached, and transparency, because the selected words from each document are available as evidence for what the similarity computation was based on.",
                "We demonstrate our method with the Concept-Project matching task ( #TARGET_REF ), which is described in the next section."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Examples include collections in which expert answers are mapped to non-expert questions (e.g. InsuranceQA by #REF ), but also so-called community QA collections (#REF ), where the lexical mismatch between Q and A documents is often less pronounced than the length difference. Like many other approaches, the proposed method is based on word embeddings as universal meaning representations, and on vector cosine as the similarity metric. However, instead of computing pairs of document representations and measuring their similarity, our method assesses the document-pair similarity on the basis of selected pairwise word similarities. This has the following advantages, which make our method a viable candidate for practical, real-world applications: efficiency, because pairwise word similarities can be efficiently (pre-)computed and cached, and transparency, because the selected words from each document are available as evidence for what the similarity computation was based on. We demonstrate our method with the Concept-Project matching task ( #TARGET_REF ), which is described in the next section.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We demonstrate our method with the Concept-Project matching task ( #TARGET_REF ), which is described in the next section.\"]}"
    },
    {
        "gold": {
            "text": [
                "In the first step, weighting can be applied by multiplying a vector with the TF, IDF, or TF*IDF score of its pertaining word.",
                "We implement this standard measure (AVG COS SIM) as a baseline for both our method and for the method by #TARGET_REF .",
                "It yields a single scalar similarity score.",
                "The core idea of our alternative method is to turn the above process upside down, by computing the cosine similarity of selected pairs of words from c and p first, and to average over the similarity scores afterwards (cf. also Section 6).",
                "More precisely, we implement a measure TOP n COS SIM AVG as the average of the n highest pairwise cosine similarities of the n top-ranking words in c and p. Ranking, again, is done by TF, IDF, and TF*IDF."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In the first step, weighting can be applied by multiplying a vector with the TF, IDF, or TF*IDF score of its pertaining word. We implement this standard measure (AVG COS SIM) as a baseline for both our method and for the method by #TARGET_REF . It yields a single scalar similarity score. The core idea of our alternative method is to turn the above process upside down, by computing the cosine similarity of selected pairs of words from c and p first, and to average over the similarity scores afterwards (cf. also Section 6). More precisely, we implement a measure TOP n COS SIM AVG as the average of the n highest pairwise cosine similarities of the n top-ranking words in c and p. Ranking, again, is done by TF, IDF, and TF*IDF.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"We implement this standard measure (AVG COS SIM) as a baseline for both our method and for the method by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The first interesting finding is that the AVG COS SIM measure again performs very well: In all three settings, it beats both the system based on general-purpose embeddings (topic wiki) and the one that is adapted to the science domain (topic science), with again the Both setting yielding the best overall result (.926).",
                "Note that our Both setting is probably the one most similar to the concept input used by #TARGET_REF .",
                "This result corroborates our findings on the tuning data, and clearly contradicts the (implicit) claim made by #REF regarding the infeasibility of document-level matching for documents of different lengths.",
                "The second, more important finding is that our proposed TOP n COS SIM AVG measure is also very competitive, as it also outperforms both systems by #REF in two out of three settings.",
                "It only fails in the setting using only the Description input."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The first interesting finding is that the AVG COS SIM measure again performs very well: In all three settings, it beats both the system based on general-purpose embeddings (topic wiki) and the one that is adapted to the science domain (topic science), with again the Both setting yielding the best overall result (.926). Note that our Both setting is probably the one most similar to the concept input used by #TARGET_REF . This result corroborates our findings on the tuning data, and clearly contradicts the (implicit) claim made by #REF regarding the infeasibility of document-level matching for documents of different lengths. The second, more important finding is that our proposed TOP n COS SIM AVG measure is also very competitive, as it also outperforms both systems by #REF in two out of three settings. It only fails in the setting using only the Description input.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Note that our Both setting is probably the one most similar to the concept input used by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "For comparison, the two top rows provide the best results of #REF .",
                "The first interesting finding is that the AVG COS SIM measure again performs very well: In all three settings, it beats both the system based on general-purpose embeddings (topic wiki) and the one that is adapted to the science domain (topic science), with again the Both setting yielding the best overall result (.926).",
                "Note that our Both setting is probably the one most similar to the concept input used by #REF .",
                "This result corroborates our findings on the tuning data, and clearly contradicts the (implicit) claim made by #REF regarding the infeasibility of document-level matching for documents of different lengths.",
                "The second, more important finding is that our proposed TOP n COS SIM AVG measure is also very competitive, as it also outperforms both systems by #TARGET_REF in two out of three settings."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "For comparison, the two top rows provide the best results of #REF . The first interesting finding is that the AVG COS SIM measure again performs very well: In all three settings, it beats both the system based on general-purpose embeddings (topic wiki) and the one that is adapted to the science domain (topic science), with again the Both setting yielding the best overall result (.926). Note that our Both setting is probably the one most similar to the concept input used by #REF . This result corroborates our findings on the tuning data, and clearly contradicts the (implicit) claim made by #REF regarding the infeasibility of document-level matching for documents of different lengths. The second, more important finding is that our proposed TOP n COS SIM AVG measure is also very competitive, as it also outperforms both systems by #TARGET_REF in two out of three settings.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"The second, more important finding is that our proposed TOP n COS SIM AVG measure is also very competitive, as it also outperforms both systems by #TARGET_REF in two out of three settings.\"]}"
    },
    {
        "gold": {
            "text": [
                "In recent years, word embeddings (#REF; #REF; #REF; #REF) have had a huge impact in natural language processing (NLP) and related fields, being used in many tasks including sentiment analysis (Dos #REF; #REF) , information retrieval (#REF; #REF) and word sense disambiguation (#REF; #REF; #REF) , among many others.",
                "Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations #REF; #REF; #REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF .",
                "Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (#REF) , it seems that more complex approaches usually yield better performance (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF .",
                "To this end, we propose a simple yet effective approach for aggregating word embeddings into document embeddings.",
                "Our approach is inspired by the Vector of Locally-Aggregated Descriptors (VLAD) (Jégou et al., 2010 (Jégou et al., , 2012 used in computer vision to efficiently represent images for various image classification and retrieval tasks."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In recent years, word embeddings (#REF; #REF; #REF; #REF) have had a huge impact in natural language processing (NLP) and related fields, being used in many tasks including sentiment analysis (Dos #REF; #REF) , information retrieval (#REF; #REF) and word sense disambiguation (#REF; #REF; #REF) , among many others. Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations #REF; #REF; #REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF . Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (#REF) , it seems that more complex approaches usually yield better performance (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF . To this end, we propose a simple yet effective approach for aggregating word embeddings into document embeddings. Our approach is inspired by the Vector of Locally-Aggregated Descriptors (VLAD) (Jégou et al., 2010 (Jégou et al., , 2012 used in computer vision to efficiently represent images for various image classification and retrieval tasks.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations #REF; #REF; #REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In recent years, word embeddings (#REF; #REF; #REF; #REF) have had a huge impact in natural language processing (NLP) and related fields, being used in many tasks including sentiment analysis (Dos #REF; #REF) , information retrieval (#REF; #REF) and word sense disambiguation (#REF; #REF; #REF) , among many others.",
                "Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF .",
                "Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (#REF) , it seems that more complex approaches usually yield better performance (#REF; #REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF #REF .",
                "To this end, we propose a simple yet effective approach for aggregating word embeddings into document embeddings.",
                "Our approach is inspired by the Vector of Locally-Aggregated Descriptors (VLAD) (Jégou et al., 2010 (Jégou et al., , 2012 used in computer vision to efficiently represent images for various image classification and retrieval tasks."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In recent years, word embeddings (#REF; #REF; #REF; #REF) have had a huge impact in natural language processing (NLP) and related fields, being used in many tasks including sentiment analysis (Dos #REF; #REF) , information retrieval (#REF; #REF) and word sense disambiguation (#REF; #REF; #REF) , among many others. Starting from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF . Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (#REF) , it seems that more complex approaches usually yield better performance (#REF; #REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF #REF . To this end, we propose a simple yet effective approach for aggregating word embeddings into document embeddings. Our approach is inspired by the Vector of Locally-Aggregated Descriptors (VLAD) (Jégou et al., 2010 (Jégou et al., , 2012 used in computer vision to efficiently represent images for various image classification and retrieval tasks.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (#REF) , it seems that more complex approaches usually yield better performance (#REF; #REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier, namely Support Vector Machines (SVM), and show that it is useful for a diverse set of text classification tasks.",
                "We consider five benchmark data sets: #REF8 (#REF) , RT-2k (#REF) , MR (#REF) , TREC (#REF) and Subj (#REF) .",
                "We compare VLAWE with recent stateof-the-art methods #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF , demonstrating the effectiveness of our approach.",
                "Furthermore, we obtain a considerable improvement on the Movie Review (MR) data set, surpassing the state-of-the-art approach of #REF by almost 10%.",
                "The rest of the paper is organized as follows."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier, namely Support Vector Machines (SVM), and show that it is useful for a diverse set of text classification tasks. We consider five benchmark data sets: #REF8 (#REF) , RT-2k (#REF) , MR (#REF) , TREC (#REF) and Subj (#REF) . We compare VLAWE with recent stateof-the-art methods #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF , demonstrating the effectiveness of our approach. Furthermore, we obtain a considerable improvement on the Movie Review (MR) data set, surpassing the state-of-the-art approach of #REF by almost 10%. The rest of the paper is organized as follows.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"We compare VLAWE with recent stateof-the-art methods #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF , demonstrating the effectiveness of our approach.\"]}"
    },
    {
        "gold": {
            "text": [
                "There are various works #REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF ) that propose to build effective sentence-level or document-level representations based on word embeddings.",
                "While most of these approaches are based on deep learning (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (#REF) .",
                "The relationship between the bag-of-visual-words, Fisher Vectors and VLAD is discussed in (Jégou et al., 2012) .",
                "The discussion can be transferred to describe the relantionship of our work and the closely-related works of and #REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "There are various works #REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF ) that propose to build effective sentence-level or document-level representations based on word embeddings. While most of these approaches are based on deep learning (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (#REF) . The relationship between the bag-of-visual-words, Fisher Vectors and VLAD is discussed in (Jégou et al., 2012) . The discussion can be transferred to describe the relantionship of our work and the closely-related works of and #REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"There are various works #REF; #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF ) that propose to build effective sentence-level or document-level representations based on word embeddings.\"]}"
    },
    {
        "gold": {
            "text": [
                "There are various works #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF ) that propose to build effective sentence-level or document-level representations based on word embeddings.",
                "While most of these approaches are based on deep learning (#REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (#REF) .",
                "The relationship between the bag-of-visual-words, Fisher Vectors and VLAD is discussed in (Jégou et al., 2012) .",
                "The discussion can be transferred to describe the relantionship of our work and the closely-related works of and #REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0
            ]
        },
        "input": "There are various works #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF ) that propose to build effective sentence-level or document-level representations based on word embeddings. While most of these approaches are based on deep learning (#REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (#REF) . The relationship between the bag-of-visual-words, Fisher Vectors and VLAD is discussed in (Jégou et al., 2012) . The discussion can be transferred to describe the relantionship of our work and the closely-related works of and #REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"While most of these approaches are based on deep learning (#REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The pre-trained GloVe model contains 300-dimensional vectors for 2.2 million words and phrases.",
                "Most of the steps required for building the VLAWE representation, such as the k-means clustering and the randomized forest of k-d trees, are implemented using the VLFeat library (#REF) .",
                "We set the number of clusters (size of the codebook) to k = 10, leading to a VLAWE representation of k · d = 10 · 300 = 3000 components.",
                "Similar to Jégou et al. (2012) , we set α = 0.5 for the power normalization step in Equation (4), which consistently leads to near-optimal results on all data sets.",
                "In the learning stage, we employ the Support Vector Machines (SVM) implementation provided by LibSVM (#REF Table 1 : Performance results (in %) of our approach (VLAWE) versus several state-of-the-art methods #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF on the #REF8, RT-2k, MR, TREC and Subj data sets."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The pre-trained GloVe model contains 300-dimensional vectors for 2.2 million words and phrases. Most of the steps required for building the VLAWE representation, such as the k-means clustering and the randomized forest of k-d trees, are implemented using the VLFeat library (#REF) . We set the number of clusters (size of the codebook) to k = 10, leading to a VLAWE representation of k · d = 10 · 300 = 3000 components. Similar to Jégou et al. (2012) , we set α = 0.5 for the power normalization step in Equation (4), which consistently leads to near-optimal results on all data sets. In the learning stage, we employ the Support Vector Machines (SVM) implementation provided by LibSVM (#REF Table 1 : Performance results (in %) of our approach (VLAWE) versus several state-of-the-art methods #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF on the #REF8, RT-2k, MR, TREC and Subj data sets.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In the learning stage, we employ the Support Vector Machines (SVM) implementation provided by LibSVM (#REF Table 1 : Performance results (in %) of our approach (VLAWE) versus several state-of-the-art methods #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF on the #REF8, RT-2k, MR, TREC and Subj data sets.\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare VLAWE with several state-of-theart methods #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF as well as two baseline methods, namely the average of word embeddings and the standard bag-of-words (BOW).",
                "The corresponding results are presented in Table 1 .",
                "First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (#REF; #REF) .",
                "In most cases, our improvements over the baselines are higher than 5%.",
                "On the #REF8 data set, we surpass the closely-related approach"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We compare VLAWE with several state-of-theart methods #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF as well as two baseline methods, namely the average of word embeddings and the standard bag-of-words (BOW). The corresponding results are presented in Table 1 . First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (#REF; #REF) . In most cases, our improvements over the baselines are higher than 5%. On the #REF8 data set, we surpass the closely-related approach",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"We compare VLAWE with several state-of-theart methods #REF; #REF; #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF #REF as well as two baseline methods, namely the average of word embeddings and the standard bag-of-words (BOW).\"]}"
    },
    {
        "gold": {
            "text": [
                "set the SVM regularization parameter to C = 1 in all our experiments.",
                "In the SVM, we use the linear kernel.",
                "For optimal results, the VLAWE representation is combined with the BOSWE representation , which is based on the PQ kernel #REF, 2015b) .",
                "We follow the same evaluation procedure as #REF and #TARGET_REF , using 10-fold cross-validation when a train and test split is not pre-defined for a given data set.",
                "As evaluation metrics, we employ the micro-averaged F 1 measure for the #REF8 data set and the standard classification accuracy for the RT-2k, the MR, the TREC and the Subj data sets, in order to fairly compare with the related art."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "set the SVM regularization parameter to C = 1 in all our experiments. In the SVM, we use the linear kernel. For optimal results, the VLAWE representation is combined with the BOSWE representation , which is based on the PQ kernel #REF, 2015b) . We follow the same evaluation procedure as #REF and #TARGET_REF , using 10-fold cross-validation when a train and test split is not pre-defined for a given data set. As evaluation metrics, we employ the micro-averaged F 1 measure for the #REF8 data set and the standard classification accuracy for the RT-2k, the MR, the TREC and the Subj data sets, in order to fairly compare with the related art.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"We follow the same evaluation procedure as #REF and #TARGET_REF , using 10-fold cross-validation when a train and test split is not pre-defined for a given data set.\"]}"
    },
    {
        "gold": {
            "text": [
                "The corresponding results are presented in Table 1 .",
                "First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (#REF; #TARGET_REF .",
                "In most cases, our improvements over the baselines are higher than 5%.",
                "On the #REF8 data set, we surpass the closely-related approach",
                "93.2 VLAWE (full, k = 10) 93.3 Table 2 : Performance results (in %) of the full VLAWE representation (with k = 10) versus two compact versions of VLAWE, obtained either by setting k = 2 or by applying PCA."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The corresponding results are presented in Table 1 . First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (#REF; #TARGET_REF . In most cases, our improvements over the baselines are higher than 5%. On the #REF8 data set, we surpass the closely-related approach 93.2 VLAWE (full, k = 10) 93.3 Table 2 : Performance results (in %) of the full VLAWE representation (with k = 10) versus two compact versions of VLAWE, obtained either by setting k = 2 or by applying PCA.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (#REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The title of our talk-an implicit reference to the English cliché like a spider weaving her webintends to attract one's attention to the metaphor that can be drawn between the dance of a spider weaving her web and a new lexicographic gesture that is gradually emerging from the work on Net-like lexical resources (#REF; #REF; #TARGET_REF .",
                "Our claim is that the inherent graph structure of natural language lexicons not only determine vocabulary acquisition and use (#REF) , but also lexicographic activity.",
                "In that respect, reflecting on new ways to implement the task of building lexical resources is essential for lexicographers themselves, but also for anyone interested is lexicons as mental structures.",
                "After all, lexicographers and language learners are those who have the most direct contact with lexical structures, through closely related activities: describing a natural phenomenon is a form of learning through explicit conceptualization.",
                "Lexicographers often experience the fact that by completing the description of a word they achieve a form of understanding and mastering of this word."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The title of our talk-an implicit reference to the English cliché like a spider weaving her webintends to attract one's attention to the metaphor that can be drawn between the dance of a spider weaving her web and a new lexicographic gesture that is gradually emerging from the work on Net-like lexical resources (#REF; #REF; #TARGET_REF . Our claim is that the inherent graph structure of natural language lexicons not only determine vocabulary acquisition and use (#REF) , but also lexicographic activity. In that respect, reflecting on new ways to implement the task of building lexical resources is essential for lexicographers themselves, but also for anyone interested is lexicons as mental structures. After all, lexicographers and language learners are those who have the most direct contact with lexical structures, through closely related activities: describing a natural phenomenon is a form of learning through explicit conceptualization. Lexicographers often experience the fact that by completing the description of a word they achieve a form of understanding and mastering of this word.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The title of our talk-an implicit reference to the English clich\\u00e9 like a spider weaving her webintends to attract one's attention to the metaphor that can be drawn between the dance of a spider weaving her web and a new lexicographic gesture that is gradually emerging from the work on Net-like lexical resources (#REF; #REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The title of our talk-an implicit reference to the English cliché like a spider weaving her webintends to attract one's attention to the metaphor that can be drawn between the dance of a spider weaving her web and a new lexicographic gesture that is gradually emerging from the work on Net-like lexical resources (#REF; #REF; #TARGET_REF .",
                "Our claim is that the inherent graph structure of natural language lexicons not only determine vocabulary acquisition and use (#REF) , but also lexicographic activity.",
                "In that respect, reflecting on new ways to implement the task of building lexical resources is essential for lexicographers themselves, but also for anyone interested is lexicons as mental structures.",
                "After all, lexicographers and language learners are those who have the most direct contact with lexical structures, through closely related activities: describing a natural phenomenon is a form of learning through explicit conceptualization.",
                "Lexicographers often experience the fact that by completing the description of a word they achieve a form of understanding and mastering of this word."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The title of our talk-an implicit reference to the English cliché like a spider weaving her webintends to attract one's attention to the metaphor that can be drawn between the dance of a spider weaving her web and a new lexicographic gesture that is gradually emerging from the work on Net-like lexical resources (#REF; #REF; #TARGET_REF . Our claim is that the inherent graph structure of natural language lexicons not only determine vocabulary acquisition and use (#REF) , but also lexicographic activity. In that respect, reflecting on new ways to implement the task of building lexical resources is essential for lexicographers themselves, but also for anyone interested is lexicons as mental structures. After all, lexicographers and language learners are those who have the most direct contact with lexical structures, through closely related activities: describing a natural phenomenon is a form of learning through explicit conceptualization. Lexicographers often experience the fact that by completing the description of a word they achieve a form of understanding and mastering of this word.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The title of our talk-an implicit reference to the English clich\\u00e9 like a spider weaving her webintends to attract one's attention to the metaphor that can be drawn between the dance of a spider weaving her web and a new lexicographic gesture that is gradually emerging from the work on Net-like lexical resources (#REF; #REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We illustrate how the lexicographic process of building graph-based lexical models can benefit from tools that allow lexicographers to wade through the lexical web, following paradigmatic and syntagmatic paths, while simultaneously weaving new links and incrementing the lexical description.",
                "Work performed on the French Lexical Network #TARGET_REF ) will serve to demonstrate how the lexicographic process can be made closer to actual navigation through lexical knowledge by the speaker.",
                "The main theoretical and descriptive tool that makes such navigation possible is the system of lexical functions proposed by the Meaning-Text linguistic approach (Mel'čuk, 1996) .",
                "It induces the multidimensional and non-hierarchical graph structure of the FLN that, we believe, is far better suited for designing lexical resources than hyperonymy-based structures.",
                "Computational aspects of the work on the French Lexical Network are dealt with in (#REF) ."
            ],
            "label": [
                "FUT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We illustrate how the lexicographic process of building graph-based lexical models can benefit from tools that allow lexicographers to wade through the lexical web, following paradigmatic and syntagmatic paths, while simultaneously weaving new links and incrementing the lexical description. Work performed on the French Lexical Network #TARGET_REF ) will serve to demonstrate how the lexicographic process can be made closer to actual navigation through lexical knowledge by the speaker. The main theoretical and descriptive tool that makes such navigation possible is the system of lexical functions proposed by the Meaning-Text linguistic approach (Mel'čuk, 1996) . It induces the multidimensional and non-hierarchical graph structure of the FLN that, we believe, is far better suited for designing lexical resources than hyperonymy-based structures. Computational aspects of the work on the French Lexical Network are dealt with in (#REF) .",
        "output": "{\"label\": [\"FUT\"], \"context\": [\"Work performed on the French Lexical Network #TARGET_REF ) will serve to demonstrate how the lexicographic process can be made closer to actual navigation through lexical knowledge by the speaker.\"]}"
    },
    {
        "gold": {
            "text": [
                "Work performed on the French Lexical Network (#REF ) will serve to demonstrate how the lexicographic process can be made closer to actual navigation through lexical knowledge by the speaker.",
                "The main theoretical and descriptive tool that makes such navigation possible is the system of lexical functions proposed by the Meaning-Text linguistic approach (Mel'čuk, 1996) .",
                "It induces the multidimensional and non-hierarchical graph structure of the FLN that, we believe, is far better suited for designing lexical resources than hyperonymy-based structures.",
                "Computational aspects of the work on the French Lexical Network are dealt with in #TARGET_REF .",
                "In our presentation, we focus on the actual process of weaving lexical relations."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Work performed on the French Lexical Network (#REF ) will serve to demonstrate how the lexicographic process can be made closer to actual navigation through lexical knowledge by the speaker. The main theoretical and descriptive tool that makes such navigation possible is the system of lexical functions proposed by the Meaning-Text linguistic approach (Mel'čuk, 1996) . It induces the multidimensional and non-hierarchical graph structure of the FLN that, we believe, is far better suited for designing lexical resources than hyperonymy-based structures. Computational aspects of the work on the French Lexical Network are dealt with in #TARGET_REF . In our presentation, we focus on the actual process of weaving lexical relations.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Computational aspects of the work on the French Lexical Network are dealt with in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We discuss this point further in Section 4.",
                "For these reasons, to obtain a more interesting comparison, we modified CoMiC-EN to perform scoring instead of meaning comparison.",
                "This means that the memory-based learning approach CoMiC-EN had employed so far was no longer applicable and had to be replaced with a regression-capable learning strategy.",
                "We chose Support Vector Regression (SVR) using libSVM 4 since that is one of the methods employed by #TARGET_REF .",
                "However, all other parts of CoMiC-EN such as the processing pipeline and the alignment approach and the extracted features remained the same."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "We discuss this point further in Section 4. For these reasons, to obtain a more interesting comparison, we modified CoMiC-EN to perform scoring instead of meaning comparison. This means that the memory-based learning approach CoMiC-EN had employed so far was no longer applicable and had to be replaced with a regression-capable learning strategy. We chose Support Vector Regression (SVR) using libSVM 4 since that is one of the methods employed by #TARGET_REF . However, all other parts of CoMiC-EN such as the processing pipeline and the alignment approach and the extracted features remained the same.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"For these reasons, to obtain a more interesting comparison, we modified CoMiC-EN to perform scoring instead of meaning comparison.\", \"This means that the memory-based learning approach CoMiC-EN had employed so far was no longer applicable and had to be replaced with a regression-capable learning strategy.\", \"We chose Support Vector Regression (SVR) using libSVM 4 since that is one of the methods employed by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Their results also reveal the differences between these methodologies in their assessment of topic coherence.",
                "A hyper-parameter in all these methodologies is the number of topic words, or its cardinality.",
                "These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for #REF , N = 5, whereas for #TARGET_REF , #REF and #REF , N = 10.",
                "The germ of this paper came when using the automatic word intrusion methodology (#REF) , and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction.",
                "This forms the kernel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evaluation of topic coherence."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Their results also reveal the differences between these methodologies in their assessment of topic coherence. A hyper-parameter in all these methodologies is the number of topic words, or its cardinality. These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for #REF , N = 5, whereas for #TARGET_REF , #REF and #REF , N = 10. The germ of this paper came when using the automatic word intrusion methodology (#REF) , and noticing that introducing one extra word to a given topic can dramatically change the accuracy of intruder word prediction. This forms the kernel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evaluation of topic coherence.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for #REF , N = 5, whereas for #TARGET_REF , #REF and #REF , N = 10.\"]}"
    },
    {
        "gold": {
            "text": [
                "To examine the relationship between topic cardinality and topic coherence, we require a dataset that has topics for a range of cardinality settings.",
                "Although there are existing datasets with human-annotated coherence scores #TARGET_REF; #REF; #REF; #REF) , these topics were annotated using a fixed cardinality setting (e.g. 5 or 10).",
                "We thus develop a new dataset for this experiment.",
                "Following #REF , we use two domains: (1) WIKI, a collection of 3.3 million English Wikipedia articles (retrieved November 28th 2009); and (2) NEWS, a collection of 1.2 million New York Times articles from 1994 to 2004 (English Gigaword).",
                "We sub-sample approximately 50M tokens (100K and 50K articles for WIKI and NEWS respectively) from both domains to create two smaller document collections."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "To examine the relationship between topic cardinality and topic coherence, we require a dataset that has topics for a range of cardinality settings. Although there are existing datasets with human-annotated coherence scores #TARGET_REF; #REF; #REF; #REF) , these topics were annotated using a fixed cardinality setting (e.g. 5 or 10). We thus develop a new dataset for this experiment. Following #REF , we use two domains: (1) WIKI, a collection of 3.3 million English Wikipedia articles (retrieved November 28th 2009); and (2) NEWS, a collection of 1.2 million New York Times articles from 1994 to 2004 (English Gigaword). We sub-sample approximately 50M tokens (100K and 50K articles for WIKI and NEWS respectively) from both domains to create two smaller document collections.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"Although there are existing datasets with human-annotated coherence scores #TARGET_REF; #REF; #REF; #REF) , these topics were annotated using a fixed cardinality setting (e.g. 5 or 10).\", \"We thus develop a new dataset for this experiment.\"]}"
    },
    {
        "gold": {
            "text": [
                "2 There are two primary approaches to assessing topic coherence: (1) via word intrusion (Chang et (2) by directly measuring observed coherence #TARGET_REF; #REF) .",
                "With the first method, #REF injects an intruder word into the top-5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words.",
                "In preliminary experiments, we found that the word intrusion task becomes unreasonably difficult for human annotators when the topic cardinality is high, e.g. when N = 20.",
                "As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities.",
                "3 To collect the coherence judgements, we used Amazon Mechanical Turk and asked Turkers to rate topics in terms of coherence using a 3-point ordinal scale, where 1 indicates incoherent and 3 very coherent (#REF) ."
            ],
            "label": [
                "BACK",
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                1,
                0
            ]
        },
        "input": "2 There are two primary approaches to assessing topic coherence: (1) via word intrusion (Chang et (2) by directly measuring observed coherence #TARGET_REF; #REF) . With the first method, #REF injects an intruder word into the top-5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words. In preliminary experiments, we found that the word intrusion task becomes unreasonably difficult for human annotators when the topic cardinality is high, e.g. when N = 20. As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities. 3 To collect the coherence judgements, we used Amazon Mechanical Turk and asked Turkers to rate topics in terms of coherence using a 3-point ordinal scale, where 1 indicates incoherent and 3 very coherent (#REF) .",
        "output": "{\"label\": [\"BACK\", \"USE\"], \"context\": [\"2 There are two primary approaches to assessing topic coherence: (1) via word intrusion (Chang et (2) by directly measuring observed coherence #TARGET_REF; #REF) .\", \"As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities.\"]}"
    },
    {
        "gold": {
            "text": [
                "To investigate this, we develop a new dataset with human-annotated coherence judgements for a range of cardinality settings (N = {5, 10, 15, 20}).",
                "We experiment with the automatic word intrusion (#REF) and discover that correlation with human ratings decreases systematically as cardinality increases.",
                "We also test the PMI methodology #TARGET_REF and make the same observation.",
                "To remedy this, we show that performance can be substantially improved if system scores and human ratings are aggregated over different cardinality settings before computing the correlation.",
                "This has broad implications for topic model evaluation."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "To investigate this, we develop a new dataset with human-annotated coherence judgements for a range of cardinality settings (N = {5, 10, 15, 20}). We experiment with the automatic word intrusion (#REF) and discover that correlation with human ratings decreases systematically as cardinality increases. We also test the PMI methodology #TARGET_REF and make the same observation. To remedy this, we show that performance can be substantially improved if system scores and human ratings are aggregated over different cardinality settings before computing the correlation. This has broad implications for topic model evaluation.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We also test the PMI methodology #TARGET_REF and make the same observation.\"]}"
    },
    {
        "gold": {
            "text": [
                "In preliminary experiments, we found that the word intrusion task becomes unreasonably difficult for human annotators when the topic cardinality is high, e.g. when N = 20.",
                "As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities.",
                "3 To collect the coherence judgements, we used Amazon Mechanical Turk and asked Turkers to rate topics in terms of coherence using a 3-point ordinal scale, where 1 indicates incoherent and 3 very coherent #TARGET_REF .",
                "For each topic (600 topics in total) we experiment with 4 cardinality settings: N = {5, 10, 15, 20}. For example, for N = 5, we display the top-5 topic words for coherence judgement.",
                "For annotation quality control, we embed a bad topic generated using random words into each HIT."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In preliminary experiments, we found that the word intrusion task becomes unreasonably difficult for human annotators when the topic cardinality is high, e.g. when N = 20. As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different topic cardinalities. 3 To collect the coherence judgements, we used Amazon Mechanical Turk and asked Turkers to rate topics in terms of coherence using a 3-point ordinal scale, where 1 indicates incoherent and 3 very coherent #TARGET_REF . For each topic (600 topics in total) we experiment with 4 cardinality settings: N = {5, 10, 15, 20}. For example, for N = 5, we display the top-5 topic words for coherence judgement. For annotation quality control, we embed a bad topic generated using random words into each HIT.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"3 To collect the coherence judgements, we used Amazon Mechanical Turk and asked Turkers to rate topics in terms of coherence using a 3-point ordinal scale, where 1 indicates incoherent and 3 very coherent #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Sample relations include people's titles, birth places, and marriage relationships.",
                "Traditional relation-extraction systems rely on manual annotations or domain-specific rules provided by experts, both of which are scarce resources that are not portable across domains.",
                "To remedy these problems, recent years have seen interest in the distant supervision approach for relation extraction (#REF; #TARGET_REF .",
                "The input to distant supervision is a set of seed facts for the target relation together with an (unlabeled) text corpus, and the output is a set of (noisy) annotations that can be used by any machine learning technique to train a statistical model for the target relation.",
                "For example, given the target relation birthPlace(person, place) and a seed fact birthPlace(John, Springfield), the sentence \"John and his wife were born in Springfield in 1946\" (S1) would qualify as a positive training example."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Sample relations include people's titles, birth places, and marriage relationships. Traditional relation-extraction systems rely on manual annotations or domain-specific rules provided by experts, both of which are scarce resources that are not portable across domains. To remedy these problems, recent years have seen interest in the distant supervision approach for relation extraction (#REF; #TARGET_REF . The input to distant supervision is a set of seed facts for the target relation together with an (unlabeled) text corpus, and the output is a set of (noisy) annotations that can be used by any machine learning technique to train a statistical model for the target relation. For example, given the target relation birthPlace(person, place) and a seed fact birthPlace(John, Springfield), the sentence \"John and his wife were born in Springfield in 1946\" (S1) would qualify as a positive training example.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Traditional relation-extraction systems rely on manual annotations or domain-specific rules provided by experts, both of which are scarce resources that are not portable across domains.\", \"To remedy these problems, recent years have seen interest in the distant supervision approach for relation extraction (#REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Our primary contribution is to empirically assess how scaling these inputs to distant supervision impacts its result quality.",
                "We study this question with input data sets that are orders of magnitude larger than those in prior work.",
                "While the largest corpus (Wikipedia and New York Times) employed by recent work on distant supervision #TARGET_REF; #REF) contain about 2M documents, we run experiments on a 100M-document (50X more) corpus drawn from ClueWeb.",
                "1 While prior work (#REF) on crowdsourcing for distant supervision used thousands of human feedback units, we acquire tens of thousands of human-provided labels.",
                "Despite the large scale, we follow state-of-the-art distant supervision approaches and use deep linguistic features, e.g., part-of-speech tags and dependency parsing."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Our primary contribution is to empirically assess how scaling these inputs to distant supervision impacts its result quality. We study this question with input data sets that are orders of magnitude larger than those in prior work. While the largest corpus (Wikipedia and New York Times) employed by recent work on distant supervision #TARGET_REF; #REF) contain about 2M documents, we run experiments on a 100M-document (50X more) corpus drawn from ClueWeb. 1 While prior work (#REF) on crowdsourcing for distant supervision used thousands of human feedback units, we acquire tens of thousands of human-provided labels. Despite the large scale, we follow state-of-the-art distant supervision approaches and use deep linguistic features, e.g., part-of-speech tags and dependency parsing.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"While the largest corpus (Wikipedia and New York Times) employed by recent work on distant supervision #TARGET_REF; #REF) contain about 2M documents, we run experiments on a 100M-document (50X more) corpus drawn from ClueWeb.\"]}"
    },
    {
        "gold": {
            "text": [
                "The idea of using entity-level structured data (e.g., facts in a database) to generate mention-level training data (e.g., in English text) is a classic one: researchers have used variants of this idea to extract entities of a certain type from webpages (#REF; #REF) .",
                "More closely related to relation extraction is the work of #REF that uses dependency paths to find answers that express the same relation as in a question.",
                "Since #TARGET_REF coined the name \"distant supervision,\" there has been growing interest in this technique.",
                "For example, distant supervision has been used for the TAC-KBP slot-filling tasks and other relation-extraction tasks (#REF; #REF; #REFa; #REFb) .",
                "In contrast, we study how increasing input size (and incorporating human feedback) improves the result quality of distant supervision."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The idea of using entity-level structured data (e.g., facts in a database) to generate mention-level training data (e.g., in English text) is a classic one: researchers have used variants of this idea to extract entities of a certain type from webpages (#REF; #REF) . More closely related to relation extraction is the work of #REF that uses dependency paths to find answers that express the same relation as in a question. Since #TARGET_REF coined the name \"distant supervision,\" there has been growing interest in this technique. For example, distant supervision has been used for the TAC-KBP slot-filling tasks and other relation-extraction tasks (#REF; #REF; #REFa; #REFb) . In contrast, we study how increasing input size (and incorporating human feedback) improves the result quality of distant supervision.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Since #TARGET_REF coined the name \\\"distant supervision,\\\" there has been growing interest in this technique.\"]}"
    },
    {
        "gold": {
            "text": [
                "3 taining mentions of named entities, our goal is to learn a classifier for R(x, y) using linguistic features of x and y, e.g., dependency-path information.",
                "The problem is that we lack the large amount of labeled examples that are typically required to apply supervised learning techniques.",
                "We describe an overview of these techniques and the methodological choices we made to implement our study.",
                "Figure 1 illustrates the overall workflow of a distant supervision system.",
                "At each step of the distant supervision process, we closely follow the recent literature #TARGET_REF; ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "3 taining mentions of named entities, our goal is to learn a classifier for R(x, y) using linguistic features of x and y, e.g., dependency-path information. The problem is that we lack the large amount of labeled examples that are typically required to apply supervised learning techniques. We describe an overview of these techniques and the methodological choices we made to implement our study. Figure 1 illustrates the overall workflow of a distant supervision system. At each step of the distant supervision process, we closely follow the recent literature #TARGET_REF; .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"At each step of the distant supervision process, we closely follow the recent literature #TARGET_REF; .\"]}"
    },
    {
        "gold": {
            "text": [
                "4 We use the facts in R i combined with C to generate examples.",
                "Following recent work #TARGET_REF; #REF) , we use Freebase 5 as the knowledge base for seed facts.",
                "We use two text corpora: (1) the TAC-KBP 6 2010 corpus that 4 We only consider binary predicates in this work.",
                "5 http://freebase.com 6 KBP stands for \"Knowledge-Base Population.\" consists of 1.8M newswire and blog articles 7 , and (2) the ClueWeb09 corpus that is a 2009 snapshot of 500M webpages.",
                "We use the TAC-KBP slot filling task and select those TAC-KBP relations that are present in the Freebase schema as targets (20 relations on people and organization)."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "4 We use the facts in R i combined with C to generate examples. Following recent work #TARGET_REF; #REF) , we use Freebase 5 as the knowledge base for seed facts. We use two text corpora: (1) the TAC-KBP 6 2010 corpus that 4 We only consider binary predicates in this work. 5 http://freebase.com 6 KBP stands for \"Knowledge-Base Population.\" consists of 1.8M newswire and blog articles 7 , and (2) the ClueWeb09 corpus that is a 2009 snapshot of 500M webpages. We use the TAC-KBP slot filling task and select those TAC-KBP relations that are present in the Freebase schema as targets (20 relations on people and organization).",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"Following recent work #TARGET_REF; #REF) , we use Freebase 5 as the knowledge base for seed facts.\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus, the pairs in such relations are not embedded in text, and so these pairs lack the linguistic context that we need to extract features, i.e., the features used to describe examples.",
                "In turn, this implies that these pairs cannot be used directly as training examples for our classifier.",
                "To generate training examples, we need to map the entities back to mentions in the corpus.",
                "We denote the relation that describes this mapping as the relation EL(e, m) where e ∈ E is an entity in the database D and m is a mention in the corpus C. For each relation R i , we generate a set of (noisy) positive examples denoted R",
                "As in previous work, we impose the constraint that both mentions (m 1 , m 2 ) ∈ R + i are contained in the same sentence #TARGET_REF; #REF) ."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Thus, the pairs in such relations are not embedded in text, and so these pairs lack the linguistic context that we need to extract features, i.e., the features used to describe examples. In turn, this implies that these pairs cannot be used directly as training examples for our classifier. To generate training examples, we need to map the entities back to mentions in the corpus. We denote the relation that describes this mapping as the relation EL(e, m) where e ∈ E is an entity in the database D and m is a mention in the corpus C. For each relation R i , we generate a set of (noisy) positive examples denoted R As in previous work, we impose the constraint that both mentions (m 1 , m 2 ) ∈ R + i are contained in the same sentence #TARGET_REF; #REF) .",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"As in previous work, we impose the constraint that both mentions (m 1 , m 2 ) \\u2208 R + i are contained in the same sentence #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "In turn, this implies that these pairs cannot be used directly as training examples for our classifier.",
                "To generate training examples, we need to map the entities back to mentions in the corpus.",
                "We denote the relation that describes this mapping as the relation EL(e, m) where e ∈ E is an entity in the database D and m is a mention in the corpus C. For each relation R i , we generate a set of (noisy) positive examples denoted R",
                "As in previous work, we impose the constraint that both mentions (m 1 , m 2 ) ∈ R + i are contained in the same sentence (#REF; #REF) .",
                "To generate negative examples for each relation, we follow the assumption in #TARGET_REF that relations are disjoint and sample from other relations, i.e., R"
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "In turn, this implies that these pairs cannot be used directly as training examples for our classifier. To generate training examples, we need to map the entities back to mentions in the corpus. We denote the relation that describes this mapping as the relation EL(e, m) where e ∈ E is an entity in the database D and m is a mention in the corpus C. For each relation R i , we generate a set of (noisy) positive examples denoted R As in previous work, we impose the constraint that both mentions (m 1 , m 2 ) ∈ R + i are contained in the same sentence (#REF; #REF) . To generate negative examples for each relation, we follow the assumption in #TARGET_REF that relations are disjoint and sample from other relations, i.e., R",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"To generate negative examples for each relation, we follow the assumption in #TARGET_REF that relations are disjoint and sample from other relations, i.e., R\"]}"
    },
    {
        "gold": {
            "text": [
                "Once we have constructed the set of possible mention pairs, the state-of-the-art technique to generate feature vectors uses linguistic tools such as partof-speech taggers, named-entity recognizers, dependency parsers, and string features.",
                "Following recent work on distant supervision #TARGET_REF; #REF) , we use both lexical and syntactic features.",
                "After this stage, we have a well-defined machine learning problem that is solvable using standard supervised techniques.",
                "We use sparse logistic regression ( 1 regularized) (#REF) , which is used in previous studies.",
                "Our feature extraction process consists of three steps:"
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Once we have constructed the set of possible mention pairs, the state-of-the-art technique to generate feature vectors uses linguistic tools such as partof-speech taggers, named-entity recognizers, dependency parsers, and string features. Following recent work on distant supervision #TARGET_REF; #REF) , we use both lexical and syntactic features. After this stage, we have a well-defined machine learning problem that is solvable using standard supervised techniques. We use sparse logistic regression ( 1 regularized) (#REF) , which is used in previous studies. Our feature extraction process consists of three steps:",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"Following recent work on distant supervision #TARGET_REF; #REF) , we use both lexical and syntactic features.\"]}"
    },
    {
        "gold": {
            "text": [
                "Just as direct training data are scarce, ground truth for relation extraction is scarce as well.",
                "As a result, prior work mainly considers two types of evaluation methods: (1) randomly sample a small portion of predictions (e.g., top-k) and manually evaluate precision/recall; and (2) use a held-out portion of seed facts (usually Freebase) as a kind of \"distant\" ground truth.",
                "We replace manual evaluation with a standardized relation-extraction benchmark: TAC-KBP 2010.",
                "TAC-KBP asks for extractions of 46 relations on a given set of 100 entities.",
                "Interestingly, the Freebase held-out metric #TARGET_REF; #REF ) turns out to be heavily biased toward distantly labeled data (e.g., increasing human feedback hurts precision; see Section 4.6)."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Just as direct training data are scarce, ground truth for relation extraction is scarce as well. As a result, prior work mainly considers two types of evaluation methods: (1) randomly sample a small portion of predictions (e.g., top-k) and manually evaluate precision/recall; and (2) use a held-out portion of seed facts (usually Freebase) as a kind of \"distant\" ground truth. We replace manual evaluation with a standardized relation-extraction benchmark: TAC-KBP 2010. TAC-KBP asks for extractions of 46 relations on a given set of 100 entities. Interestingly, the Freebase held-out metric #TARGET_REF; #REF ) turns out to be heavily biased toward distantly labeled data (e.g., increasing human feedback hurts precision; see Section 4.6).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Interestingly, the Freebase held-out metric #TARGET_REF; #REF ) turns out to be heavily biased toward distantly labeled data (e.g., increasing human feedback hurts precision; see Section 4.6).\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition to the TAC-KBP benchmark, we also follow prior work #TARGET_REF; #REF) and measure the quality using held-out data from Freebase.",
                "We randomly partition both Freebase and the corpus into two halves.",
                "One database-corpus pair is used for training and the other pair for testing.",
                "We evaluate the precision over the 10 3 highest-probability predictions on the test set.",
                "In Figure 5 , we vary the size of the corpus in the train pair and the number of human labels; the precision reaches a dramatic peak when we the corpus size is above 10 5 and uses little human feedback."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In addition to the TAC-KBP benchmark, we also follow prior work #TARGET_REF; #REF) and measure the quality using held-out data from Freebase. We randomly partition both Freebase and the corpus into two halves. One database-corpus pair is used for training and the other pair for testing. We evaluate the precision over the 10 3 highest-probability predictions on the test set. In Figure 5 , we vary the size of the corpus in the train pair and the number of human labels; the precision reaches a dramatic peak when we the corpus size is above 10 5 and uses little human feedback.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"In addition to the TAC-KBP benchmark, we also follow prior work #TARGET_REF; #REF) and measure the quality using held-out data from Freebase.\"]}"
    },
    {
        "gold": {
            "text": [
                "As shown in Figure 1 , one customer may write a question like \"Will it work with Surface Pro 3?\"; existing customer may reply with \"Yes.\".",
                "From those 4 QA pairs discussing a Microsoft mouse, we know that the Microsoft mouse is compatible with \"Microsoft Surface Pro 3\" and \"Windows 10\" but incompatible with \"iPad\".",
                "Furthermore, we have no idea whether \"Samsung Galaxy Tab 2 10.0\" is compatible or not with this mouse.",
                "Similar to our previous work in product reviews #TARGET_REF , we call the mouse target entity and those 4 products complementary entities of the target entity.",
                "Each complementary entity forms a complementary relation with the target entity."
            ],
            "label": [
                "SIM",
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "As shown in Figure 1 , one customer may write a question like \"Will it work with Surface Pro 3?\"; existing customer may reply with \"Yes.\". From those 4 QA pairs discussing a Microsoft mouse, we know that the Microsoft mouse is compatible with \"Microsoft Surface Pro 3\" and \"Windows 10\" but incompatible with \"iPad\". Furthermore, we have no idea whether \"Samsung Galaxy Tab 2 10.0\" is compatible or not with this mouse. Similar to our previous work in product reviews #TARGET_REF , we call the mouse target entity and those 4 products complementary entities of the target entity. Each complementary entity forms a complementary relation with the target entity.",
        "output": "{\"label\": [\"SIM\", \"BACK\"], \"context\": [\"Similar to our previous work in product reviews #TARGET_REF , we call the mouse target entity and those 4 products complementary entities of the target entity.\"]}"
    },
    {
        "gold": {
            "text": [
                "We leave the work of mining compatible/incompatible products on open questions to future work.",
                "Given the structure of a QA pair, our method naturally has a twostage framework: Complementary Entity Recognition (CER) (#REF) and yes/no answer classification.",
                "For the first stage, we employ a similar approach as in #TARGET_REF ; for the second stage, it is reduced to a yes/no answer classification problem (#REF) .",
                "We observe that the second stage provides further research opportunity since the polarities of many yes/no answers are implicit.",
                "For example, \"Will it work with Surface Pro 3? It works.\" has no explicit \"Yes\" but it is still a yes answer."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We leave the work of mining compatible/incompatible products on open questions to future work. Given the structure of a QA pair, our method naturally has a twostage framework: Complementary Entity Recognition (CER) (#REF) and yes/no answer classification. For the first stage, we employ a similar approach as in #TARGET_REF ; for the second stage, it is reduced to a yes/no answer classification problem (#REF) . We observe that the second stage provides further research opportunity since the polarities of many yes/no answers are implicit. For example, \"Will it work with Surface Pro 3? It works.\" has no explicit \"Yes\" but it is still a yes answer.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"For the first stage, we employ a similar approach as in #TARGET_REF ; for the second stage, it is reduced to a yes/no answer classification problem (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The problem of Complementary Entity Recognition (CER) is first proposed by Xu et.",
                "al. #TARGET_REF .",
                "However, our previous work focuses on product reviews and consider CER as a special kind of aspect extraction problem (#REF) .",
                "Determining the polarities of compatibility is reduced to a traditional sentiment classification problem.",
                "This paper focuses on yes/no QAs in PCQA and the polarities of compatibility is a yes/no answer classification problem."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "The problem of Complementary Entity Recognition (CER) is first proposed by Xu et. al. #TARGET_REF . However, our previous work focuses on product reviews and consider CER as a special kind of aspect extraction problem (#REF) . Determining the polarities of compatibility is reduced to a traditional sentiment classification problem. This paper focuses on yes/no QAs in PCQA and the polarities of compatibility is a yes/no answer classification problem.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"The problem of Complementary Entity Recognition (CER) is first proposed by Xu et.\", \"al. #TARGET_REF .\", \"However, our previous work focuses on product reviews and consider CER as a special kind of aspect extraction problem (#REF) .\", \"Determining the polarities of compatibility is reduced to a traditional sentiment classification problem.\", \"This paper focuses on yes/no QAs in PCQA and the polarities of compatibility is a yes/no answer classification problem.\"]}"
    },
    {
        "gold": {
            "text": [
                "CER is closely related to entity recognition (e.g., Named Entity Recognition (NER) (#REF; #REF ) problem).",
                "The major differences are that many complementary entities are not named entities and CER heavily relies on the context of an entity (e.g., \"iPhone\" in \"I like my iPhone\" is not a complementary entity).",
                "Complementary entities are also studied as a social network problem in recommender systems (#REF; #REF) .",
                "We discussed the benefit of CER over social network problem in #TARGET_REF ) so we omit here but keep a performance comparison in Section 5.",
                "Community Question and Answering (CQA) has been well studied in literature (#REF; #REF; #REF; #REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "CER is closely related to entity recognition (e.g., Named Entity Recognition (NER) (#REF; #REF ) problem). The major differences are that many complementary entities are not named entities and CER heavily relies on the context of an entity (e.g., \"iPhone\" in \"I like my iPhone\" is not a complementary entity). Complementary entities are also studied as a social network problem in recommender systems (#REF; #REF) . We discussed the benefit of CER over social network problem in #TARGET_REF ) so we omit here but keep a performance comparison in Section 5. Community Question and Answering (CQA) has been well studied in literature (#REF; #REF; #REF; #REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"We discussed the benefit of CER over social network problem in #TARGET_REF ) so we omit here but keep a performance comparison in Section 5.\"]}"
    },
    {
        "gold": {
            "text": [
                "This is because customers tend to ask specific questions in PCQA.",
                "We leave the work of mining compatible/incompatible products on open questions to future work.",
                "Given the structure of a QA pair, our method naturally has a twostage framework: Complementary Entity Recognition (CER) #TARGET_REF and yes/no answer classification.",
                "For the first stage, we employ a similar approach as in (#REF) ; for the second stage, it is reduced to a yes/no answer classification problem (#REF) .",
                "We observe that the second stage provides further research opportunity since the polarities of many yes/no answers are implicit."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "This is because customers tend to ask specific questions in PCQA. We leave the work of mining compatible/incompatible products on open questions to future work. Given the structure of a QA pair, our method naturally has a twostage framework: Complementary Entity Recognition (CER) #TARGET_REF and yes/no answer classification. For the first stage, we employ a similar approach as in (#REF) ; for the second stage, it is reduced to a yes/no answer classification problem (#REF) . We observe that the second stage provides further research opportunity since the polarities of many yes/no answers are implicit.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Given the structure of a QA pair, our method naturally has a twostage framework: Complementary Entity Recognition (CER) #TARGET_REF and yes/no answer classification.\"]}"
    },
    {
        "gold": {
            "text": [
                "(#REF) considers questions in PCQA as summaries of reviews to help customers to identify relevant reviews.",
                "Extracting compatible/incompatible products from PCQA is very important.",
                "Based on our experience of annotating PCQA, we notice that PCQA usually addresses compatibility issues that are not well addressed by product description.",
                "This is because the number of complementary products for a target product can be unlimited so it is impractical to cover all of them.",
                "We also bring out the test dataset used in #TARGET_REF for a comparison (Section 5)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "(#REF) considers questions in PCQA as summaries of reviews to help customers to identify relevant reviews. Extracting compatible/incompatible products from PCQA is very important. Based on our experience of annotating PCQA, we notice that PCQA usually addresses compatibility issues that are not well addressed by product description. This is because the number of complementary products for a target product can be unlimited so it is impractical to cover all of them. We also bring out the test dataset used in #TARGET_REF for a comparison (Section 5).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We also bring out the test dataset used in #TARGET_REF for a comparison (Section 5).\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section, we first introduce the two-stage framework of the proposed method.",
                "Then we briefly introduce the method for CER in #TARGET_REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "In this section, we first introduce the two-stage framework of the proposed method. Then we briefly introduce the method for CER in #TARGET_REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Then we briefly introduce the method for CER in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Since complementary entities are mentioned in yes/no questions and their polarities of compatibility information are in answers, the proposed method naturally has a two-stage framework: Complementary Entity Recognition: we extract complementary entities from questions using dependency paths almost the same as in #TARGET_REF .",
                "It utilizes a large amount of unlabeled reviews under the same category as the target entity to expand knowledge about domain-specific verbs.",
                "Identifying Polarities of Yes/No Answers: then we determine the polarity (yes, no or neutral) of yes/no answers for each question with complementary entity and assign a compatibility label (compatible, incompatible or unknown) to it.",
                "We form this 3-class classification via PU-learning and a binary SVM classifier in Section 4."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "Since complementary entities are mentioned in yes/no questions and their polarities of compatibility information are in answers, the proposed method naturally has a two-stage framework: Complementary Entity Recognition: we extract complementary entities from questions using dependency paths almost the same as in #TARGET_REF . It utilizes a large amount of unlabeled reviews under the same category as the target entity to expand knowledge about domain-specific verbs. Identifying Polarities of Yes/No Answers: then we determine the polarity (yes, no or neutral) of yes/no answers for each question with complementary entity and assign a compatibility label (compatible, incompatible or unknown) to it. We form this 3-class classification via PU-learning and a binary SVM classifier in Section 4.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Since complementary entities are mentioned in yes/no questions and their polarities of compatibility information are in answers, the proposed method naturally has a two-stage framework: Complementary Entity Recognition: we extract complementary entities from questions using dependency paths almost the same as in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We briefly introduce the method used in #TARGET_REF and how the dependency paths can be used in questions of PCQA (details of dependency paths can be found in the original paper).",
                "The basic idea is to use dependency paths to identify the context of complementary relations around complementary entities.",
                "Dependency paths can match dependency relations parsed through dependency parsing 1 , which parses a sentence into a set of dependency relations.",
                "In our previous work, we notice that the verbs used to indicate a complementary relation can be unlimited and product specific.",
                "So we utilize another novel set of dependency paths that are in high precision but low recall to expand knowledge about complementary entities on a large amount of unlabeled review."
            ],
            "label": [
                "USE",
                "EXT"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We briefly introduce the method used in #TARGET_REF and how the dependency paths can be used in questions of PCQA (details of dependency paths can be found in the original paper). The basic idea is to use dependency paths to identify the context of complementary relations around complementary entities. Dependency paths can match dependency relations parsed through dependency parsing 1 , which parses a sentence into a set of dependency relations. In our previous work, we notice that the verbs used to indicate a complementary relation can be unlimited and product specific. So we utilize another novel set of dependency paths that are in high precision but low recall to expand knowledge about complementary entities on a large amount of unlabeled review.",
        "output": "{\"label\": [\"USE\", \"EXT\"], \"context\": [\"We briefly introduce the method used in #TARGET_REF and how the dependency paths can be used in questions of PCQA (details of dependency paths can be found in the original paper).\", \"The basic idea is to use dependency paths to identify the context of complementary relations around complementary entities.\"]}"
    },
    {
        "gold": {
            "text": [
                "The whole test dataset is labeled by 3 annotators independently.",
                "The initial agreement is 93%.",
                "Then disagreements are discussed and final agreements are reached among all annotators.",
                "To obtain knowledge about domainspecific verbs, we use 6000 reviews for each product similar as in #TARGET_REF .",
                "We also select about 220 reviews for each product and label them in a similar way to show the difference between product QA community and reviews."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The whole test dataset is labeled by 3 annotators independently. The initial agreement is 93%. Then disagreements are discussed and final agreements are reached among all annotators. To obtain knowledge about domainspecific verbs, we use 6000 reviews for each product similar as in #TARGET_REF . We also select about 220 reviews for each product and label them in a similar way to show the difference between product QA community and reviews.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"To obtain knowledge about domainspecific verbs, we use 6000 reviews for each product similar as in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We mostly consider \"Windows\" as complementary products for \"Mouse\".",
                "CER6K: This method is the method proposed in #TARGET_REF .",
                "Specifically, it uses 6000 reviews to expand domain-specific verbs.",
                "Next, we perform a separate evaluation on yes/no answer classification.",
                "We assume the accuracies of complementary entities extraction are 100% and errors do not affect answer classification."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "We mostly consider \"Windows\" as complementary products for \"Mouse\". CER6K: This method is the method proposed in #TARGET_REF . Specifically, it uses 6000 reviews to expand domain-specific verbs. Next, we perform a separate evaluation on yes/no answer classification. We assume the accuracies of complementary entities extraction are 100% and errors do not affect answer classification.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"CER6K: This method is the method proposed in #TARGET_REF .\", \"Specifically, it uses 6000 reviews to expand domain-specific verbs.\", \"Next, we perform a separate evaluation on yes/no answer classification.\"]}"
    },
    {
        "gold": {
            "text": [
                "Twitter holds great potential for analyses in the social sciences both due to its explosive popularity, increasing accessibility to large amounts of data and its dynamic nature.",
                "For sentiment analysis on twitter the best performing approaches (#REF; #REF) have used a set of rich lexical features.",
                "However, the development of lexica can be time consuming and is not always suitable when shifting between domains, which examine new topics and user populations (#REF) .",
                "Excitingly, the state of the art has recently shifted toward novel semi-supervised techniques such as the incorporation of word embeddings to represent the context of words and concepts #TARGET_REF .",
                "Moreover, it is important to be able to identify sentiment in relation to particular entities, topics or events (aspect-based sentiment)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Twitter holds great potential for analyses in the social sciences both due to its explosive popularity, increasing accessibility to large amounts of data and its dynamic nature. For sentiment analysis on twitter the best performing approaches (#REF; #REF) have used a set of rich lexical features. However, the development of lexica can be time consuming and is not always suitable when shifting between domains, which examine new topics and user populations (#REF) . Excitingly, the state of the art has recently shifted toward novel semi-supervised techniques such as the incorporation of word embeddings to represent the context of words and concepts #TARGET_REF . Moreover, it is important to be able to identify sentiment in relation to particular entities, topics or events (aspect-based sentiment).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Excitingly, the state of the art has recently shifted toward novel semi-supervised techniques such as the incorporation of word embeddings to represent the context of words and concepts #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Here we employ a combination of lexical features and word embeddings to maximise our performance in task A. We build phrase-based classifiers both with an emphasis on the distinction between positive and negative sentiment, which conforms to the distribution of training data in task A, as well as phrasebased classifiers trained on a balanced set of positive, negative and neutral tweets.",
                "We use the latter to identify sentiment in the vicinity of topic words in task C, for targeted sentiment assignment.",
                "In previous work (#REFa; #TARGET_REF sentiment-specific word embeddings have been used as features for identification of tweet-level sentiment but not phrase-level sentiment.",
                "Other work which considered word embeddings for phrase level sentiment (dos #REF) did not focus on producing sentiment-specific representations and the embeddings learnt were a combination of character and word embeddings, where the relative contribution of the word embeddings is not clear.",
                "In this work we present two different strategies for learning phrase level sentiment specific word embeddings."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                1
            ]
        },
        "input": "Here we employ a combination of lexical features and word embeddings to maximise our performance in task A. We build phrase-based classifiers both with an emphasis on the distinction between positive and negative sentiment, which conforms to the distribution of training data in task A, as well as phrasebased classifiers trained on a balanced set of positive, negative and neutral tweets. We use the latter to identify sentiment in the vicinity of topic words in task C, for targeted sentiment assignment. In previous work (#REFa; #TARGET_REF sentiment-specific word embeddings have been used as features for identification of tweet-level sentiment but not phrase-level sentiment. Other work which considered word embeddings for phrase level sentiment (dos #REF) did not focus on producing sentiment-specific representations and the embeddings learnt were a combination of character and word embeddings, where the relative contribution of the word embeddings is not clear. In this work we present two different strategies for learning phrase level sentiment specific word embeddings.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"In previous work (#REFa; #TARGET_REF sentiment-specific word embeddings have been used as features for identification of tweet-level sentiment but not phrase-level sentiment.\", \"In this work we present two different strategies for learning phrase level sentiment specific word embeddings.\"]}"
    },
    {
        "gold": {
            "text": [
                "The embeddings were learnt by using Gensim (Řehůřek and #REF), a Python package that integrates word2vec 1 .",
                "In both cases, we created representations of length equal to 100 2 .",
                "For each strategy, class and dimension, we used the functions suggested by #TARGET_REF ) (average, maximum and minimum), resulting in 2,400 features.",
                "Extra Features: We used several features, potentially indicative of sentiment, a subset of those in (#REF) .",
                "These include: the total number of words of the target phrase, its position within the tweet (\"start\", \"end\", or \"other\"), the average word length of the target/context and the presence of elongated words, URLs and user mentions."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The embeddings were learnt by using Gensim (Řehůřek and #REF), a Python package that integrates word2vec 1 . In both cases, we created representations of length equal to 100 2 . For each strategy, class and dimension, we used the functions suggested by #TARGET_REF ) (average, maximum and minimum), resulting in 2,400 features. Extra Features: We used several features, potentially indicative of sentiment, a subset of those in (#REF) . These include: the total number of words of the target phrase, its position within the tweet (\"start\", \"end\", or \"other\"), the average word length of the target/context and the presence of elongated words, URLs and user mentions.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For each strategy, class and dimension, we used the functions suggested by #TARGET_REF ) (average, maximum and minimum), resulting in 2,400 features.\"]}"
    },
    {
        "gold": {
            "text": [
                "We learned positive and negative word embeddings separately by training on the HAPPY and NON-HAPPY tweets from Purver & Battersby's multi-class Twitter emoticon and hashtag corpus (#REF) , as with subtask A. The difference with subtask A is that here we used the whole tweet as our input (compared to the two-sided window around a polarised word in subtask A) in order to create tweet-level representations.",
                "We set the word embeddings dimension to 100 in order to gain enough semantic information whilst reducing training time.",
                "We also employed the word embeddings encoding sentiment information generated through the unified models in #TARGET_REF .",
                "Similar to Tang, we represent each tweet by the min, average, max and sum on each dimension of the word embeddings of all the words in the tweet.",
                "In the end, the number of our word embeddings features is 4 ⇥ 100 = 400."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We learned positive and negative word embeddings separately by training on the HAPPY and NON-HAPPY tweets from Purver & Battersby's multi-class Twitter emoticon and hashtag corpus (#REF) , as with subtask A. The difference with subtask A is that here we used the whole tweet as our input (compared to the two-sided window around a polarised word in subtask A) in order to create tweet-level representations. We set the word embeddings dimension to 100 in order to gain enough semantic information whilst reducing training time. We also employed the word embeddings encoding sentiment information generated through the unified models in #TARGET_REF . Similar to Tang, we represent each tweet by the min, average, max and sum on each dimension of the word embeddings of all the words in the tweet. In the end, the number of our word embeddings features is 4 ⇥ 100 = 400.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We also employed the word embeddings encoding sentiment information generated through the unified models in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The results we submitted were generated by the second classifier.",
                "Table 2 demonstrates that representing the tweet with positive and negative word embeddings is the most effective feature (performance is affected the most when we remove these) followed by the manually generated lexicon-based features.",
                "This combined with a 2% reduction in F1 score when the embeddings are removed, indicates that the embeddings improve sentiment analysis performance.",
                "Contrary to the approach by #TARGET_REF , we didn't integrate the sentiment information in the word embeddings training process, but rather the sentiment-specific nature of the embeddings was reflected in the choice of different training datasets, yielding different word embedding features for positive and negative tweets.",
                "To measure the contributions of our word embeddings and Tang's sentiment-specific word embeddings separately in the F1 score, we performed a further test."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The results we submitted were generated by the second classifier. Table 2 demonstrates that representing the tweet with positive and negative word embeddings is the most effective feature (performance is affected the most when we remove these) followed by the manually generated lexicon-based features. This combined with a 2% reduction in F1 score when the embeddings are removed, indicates that the embeddings improve sentiment analysis performance. Contrary to the approach by #TARGET_REF , we didn't integrate the sentiment information in the word embeddings training process, but rather the sentiment-specific nature of the embeddings was reflected in the choice of different training datasets, yielding different word embedding features for positive and negative tweets. To measure the contributions of our word embeddings and Tang's sentiment-specific word embeddings separately in the F1 score, we performed a further test.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Contrary to the approach by #TARGET_REF , we didn't integrate the sentiment information in the word embeddings training process, but rather the sentiment-specific nature of the embeddings was reflected in the choice of different training datasets, yielding different word embedding features for positive and negative tweets.\"]}"
    },
    {
        "gold": {
            "text": [
                "One of the tasks in aspect-based sentiment analysis is to extract aspect and opinion terms from review text.",
                "Our study focuses on evaluating transfer learning using BERT #TARGET_REF to classify tokens from hotel reviews in bahasa Indonesia.",
                "We show that the default BERT model failed to outperform a simple argmax method.",
                "However, changing the default BERT tokenizer to our custom one can improve the F 1 scores on our labels of interest by at least 5%.",
                "For I-ASPECT and B-SENTIMENT, it can even increased the F 1 scores by 11%."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "One of the tasks in aspect-based sentiment analysis is to extract aspect and opinion terms from review text. Our study focuses on evaluating transfer learning using BERT #TARGET_REF to classify tokens from hotel reviews in bahasa Indonesia. We show that the default BERT model failed to outperform a simple argmax method. However, changing the default BERT tokenizer to our custom one can improve the F 1 scores on our labels of interest by at least 5%. For I-ASPECT and B-SENTIMENT, it can even increased the F 1 scores by 11%.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Our study focuses on evaluating transfer learning using BERT #TARGET_REF to classify tokens from hotel reviews in bahasa Indonesia.\"]}"
    },
    {
        "gold": {
            "text": [
                "While some work has been done in this task #REF; #REF) , we have not seen a transfer learning approach (#REF) employed, which should need much less training effort.",
                "Using transfer learning is especially helpful for low-resource languages (#REF) , such as bahasa Indonesia.",
                "Our main contribution in this study is evaluating BERT #TARGET_REF as a pretrained transformer model on this token classification task on hotel reviews in bahasa Indonesia.",
                "We also found that the current pretrained BERT tokenizer has a poor encoder for bahasa Indonesia, thus we proposed our own custom tokenizer.",
                "We also provided simpler baselines, namely argmax and logistic regression on word embeddings as comparisons."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "While some work has been done in this task #REF; #REF) , we have not seen a transfer learning approach (#REF) employed, which should need much less training effort. Using transfer learning is especially helpful for low-resource languages (#REF) , such as bahasa Indonesia. Our main contribution in this study is evaluating BERT #TARGET_REF as a pretrained transformer model on this token classification task on hotel reviews in bahasa Indonesia. We also found that the current pretrained BERT tokenizer has a poor encoder for bahasa Indonesia, thus we proposed our own custom tokenizer. We also provided simpler baselines, namely argmax and logistic regression on word embeddings as comparisons.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Our main contribution in this study is evaluating BERT #TARGET_REF as a pretrained transformer model on this token classification task on hotel reviews in bahasa Indonesia.\"]}"
    },
    {
        "gold": {
            "text": [
                "In the argmax method, we classify a token as the most probable  Label   Train  Test   B-ASPECT  7005  1758  I-ASPECT  2292  584  B-SENTIMENT  9646  2384  I-SENTIMENT  4265  1067  OTHER  39897  9706 #REF5 15499 Table 1 : Label distribution label in the training set.",
                "For fastText implementation, we use the skip-gram model and produce 100-dimensional vectors.",
                "We proposed to use transfer learning from pretrained BERT-Base, Multilingual Cased #TARGET_REF for this token classification problem.",
                "We used the implementation in PyTorch by Hugging #REF 3 .",
                "We found out that the multilingual cased tokenizer of BERT does not recognize some common terms in our dataset, such as \"kamar\" (room), \"kendala\" (issue), \"wifi\", \"koneksi\" (connection), \"bagus\" (good), \"bersih\" (clean)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In the argmax method, we classify a token as the most probable  Label   Train  Test   B-ASPECT  7005  1758  I-ASPECT  2292  584  B-SENTIMENT  9646  2384  I-SENTIMENT  4265  1067  OTHER  39897  9706 #REF5 15499 Table 1 : Label distribution label in the training set. For fastText implementation, we use the skip-gram model and produce 100-dimensional vectors. We proposed to use transfer learning from pretrained BERT-Base, Multilingual Cased #TARGET_REF for this token classification problem. We used the implementation in PyTorch by Hugging #REF 3 . We found out that the multilingual cased tokenizer of BERT does not recognize some common terms in our dataset, such as \"kamar\" (room), \"kendala\" (issue), \"wifi\", \"koneksi\" (connection), \"bagus\" (good), \"bersih\" (clean).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We proposed to use transfer learning from pretrained BERT-Base, Multilingual Cased #TARGET_REF for this token classification problem.\"]}"
    },
    {
        "gold": {
            "text": [
                "The work by #REF itself is an improvement from what their prior work on the same task (#REF) .",
                "Thus, we only included the work by #REF because they show that we can get the best result by combining the latest work by and #REF .",
                "In their paper, #TARGET_REF show that they can achieve state-of-the-art performance not only on sentence-level, but also on token-level tasks, such as for named entity recognition (NER).",
                "This motivates us to explore BERT in our study.",
                "This way, we do not need to use dependency parsers or any feature engineering."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "The work by #REF itself is an improvement from what their prior work on the same task (#REF) . Thus, we only included the work by #REF because they show that we can get the best result by combining the latest work by and #REF . In their paper, #TARGET_REF show that they can achieve state-of-the-art performance not only on sentence-level, but also on token-level tasks, such as for named entity recognition (NER). This motivates us to explore BERT in our study. This way, we do not need to use dependency parsers or any feature engineering.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"In their paper, #TARGET_REF show that they can achieve state-of-the-art performance not only on sentence-level, but also on token-level tasks, such as for named entity recognition (NER).\", \"This motivates us to explore BERT in our study.\"]}"
    },
    {
        "gold": {
            "text": [
                "Scrambling is one well-known example (#REF) .",
                "In addition, in the semantics domain, the use of a new TAG operation, flexible composition, is used to perform certain semantic operations that seemingly cannot be modeled with TL-MCTAG alone #TARGET_REF and in work in synchronous TAG semantics, constructions such as nested quantifiers require a set-local MCTAG (SL-MCTAG) analysis (#REF) .",
                "In this paper we suggest a shift in focus from constraining locality and complexity through restrictions that all trees in a tree set must adjoin within a single tree or tree set to constraining locality and complexity through restrictions on the derivational distance between trees in the same tree set in a valid derivation.",
                "We examine three formalisms, two of them introduced in this work for the first time, that use derivational distance to constrain locality and demonstrate by construction of parsers their relationship to TL-MCTAG in both expressivity and complexity.",
                "In Section 2 we give a very brief introduction to TAG."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Scrambling is one well-known example (#REF) . In addition, in the semantics domain, the use of a new TAG operation, flexible composition, is used to perform certain semantic operations that seemingly cannot be modeled with TL-MCTAG alone #TARGET_REF and in work in synchronous TAG semantics, constructions such as nested quantifiers require a set-local MCTAG (SL-MCTAG) analysis (#REF) . In this paper we suggest a shift in focus from constraining locality and complexity through restrictions that all trees in a tree set must adjoin within a single tree or tree set to constraining locality and complexity through restrictions on the derivational distance between trees in the same tree set in a valid derivation. We examine three formalisms, two of them introduced in this work for the first time, that use derivational distance to constrain locality and demonstrate by construction of parsers their relationship to TL-MCTAG in both expressivity and complexity. In Section 2 we give a very brief introduction to TAG.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In addition, in the semantics domain, the use of a new TAG operation, flexible composition, is used to perform certain semantic operations that seemingly cannot be modeled with TL-MCTAG alone #TARGET_REF and in work in synchronous TAG semantics, constructions such as nested quantifiers require a set-local MCTAG (SL-MCTAG) analysis (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "In terms of well-formed derivation trees, this amounts to disallowing derivations in which a tree from a given set is the ancestor of a tree from the same tree set.",
                "For most linguistic applications of TAG, this requirement seems natural and is strictly obeyed.",
                "There are a few applications, including flexible composition and scrambling in free-word order languages that benefit from TAG-based grammars that drop the simultaneity requirement #TARGET_REF; #REF) .",
                "From a complexity perspective, however, checking the simultaneity requirement is expensive (#REF) .",
                "As a result, it can be advantageous to select a base formalism that does not require simultaneity even if the grammars implemented with it do not make use of that additional freedom."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In terms of well-formed derivation trees, this amounts to disallowing derivations in which a tree from a given set is the ancestor of a tree from the same tree set. For most linguistic applications of TAG, this requirement seems natural and is strictly obeyed. There are a few applications, including flexible composition and scrambling in free-word order languages that benefit from TAG-based grammars that drop the simultaneity requirement #TARGET_REF; #REF) . From a complexity perspective, however, checking the simultaneity requirement is expensive (#REF) . As a result, it can be advantageous to select a base formalism that does not require simultaneity even if the grammars implemented with it do not make use of that additional freedom.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"There are a few applications, including flexible composition and scrambling in free-word order languages that benefit from TAG-based grammars that drop the simultaneity requirement #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The Filter side condition accordingly expunges trees that are the top tree in the dominance chain of their tree vector.",
                "The side conditions for the Adjoin non-base rule enforce that the dominance constraints are satisfied and that the derivational distance from the base of a tree vector to its currently highest adjoined tree is maintained accurately.",
                "We note that in order to allow a non-total ordering of the trees in a vector we would simply have to record all trees in a tree vector in the histories as is done in the delayed TL-MCTAG parser.",
                "7 Delayed TL-MCTAG #TARGET_REF introduce the delayed TL-MCTAG formalism which makes use of a derivational distance restriction in a somewhat different way.",
                "Rather than restricting the absolute distance between the trees of a set and their nearest common ancestor, given a node α in a derivation tree, delayed TL-MCTAG restricts the number of tree sets that are not fully dominated by α."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The Filter side condition accordingly expunges trees that are the top tree in the dominance chain of their tree vector. The side conditions for the Adjoin non-base rule enforce that the dominance constraints are satisfied and that the derivational distance from the base of a tree vector to its currently highest adjoined tree is maintained accurately. We note that in order to allow a non-total ordering of the trees in a vector we would simply have to record all trees in a tree vector in the histories as is done in the delayed TL-MCTAG parser. 7 Delayed TL-MCTAG #TARGET_REF introduce the delayed TL-MCTAG formalism which makes use of a derivational distance restriction in a somewhat different way. Rather than restricting the absolute distance between the trees of a set and their nearest common ancestor, given a node α in a derivation tree, delayed TL-MCTAG restricts the number of tree sets that are not fully dominated by α.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"7 Delayed TL-MCTAG #TARGET_REF introduce the delayed TL-MCTAG formalism which makes use of a derivational distance restriction in a somewhat different way.\"]}"
    },
    {
        "gold": {
            "text": [
                "Section 4 briefly addresses the simultaneity requirement present in MCTAG formalisms but not in Vector- TAG formalisms and argues for dropping the requirement.",
                "In Sections 5 and 6 we introduce two novel formalisms, restricted non-simultaneous MC-TAG and restricted Vector-TAG, respectively, and define CKY-style parsers for them.",
                "In Section 7 we recall the delayed TL-MCTAG formalism introduced by #TARGET_REF and define a CKY-style parser for it as well.",
                "In Section 8 we explore the complexity of all three parsers and the relationship between the formalisms.",
                "In Section 9 we discuss the linguistic applications of these formalisms and show that they permit analyses of some of the hard cases that have led researchers to look beyond TL-MCTAG."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Section 4 briefly addresses the simultaneity requirement present in MCTAG formalisms but not in Vector- TAG formalisms and argues for dropping the requirement. In Sections 5 and 6 we introduce two novel formalisms, restricted non-simultaneous MC-TAG and restricted Vector-TAG, respectively, and define CKY-style parsers for them. In Section 7 we recall the delayed TL-MCTAG formalism introduced by #TARGET_REF and define a CKY-style parser for it as well. In Section 8 we explore the complexity of all three parsers and the relationship between the formalisms. In Section 9 we discuss the linguistic applications of these formalisms and show that they permit analyses of some of the hard cases that have led researchers to look beyond TL-MCTAG.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In Section 7 we recall the delayed TL-MCTAG formalism introduced by #TARGET_REF and define a CKY-style parser for it as well.\"]}"
    },
    {
        "gold": {
            "text": [
                "7 Delayed TL-MCTAG #REF introduce the delayed TL-MCTAG formalism which makes use of a derivational distance restriction in a somewhat different way.",
                "Rather than restricting the absolute distance between the trees of a set and their nearest common ancestor, given a node α in a derivation tree, delayed TL-MCTAG restricts the number of tree sets that are not fully dominated by α.",
                "Borrowing directly from #TARGET_REF , Figure 7 gives two examples.",
                "Parsing for delayed TL-MCTAG is not discussed by #REF but can be accomplished using a similar CKY-style strategy as in the two parsers above.",
                "We present a parser in Figure 6 ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "7 Delayed TL-MCTAG #REF introduce the delayed TL-MCTAG formalism which makes use of a derivational distance restriction in a somewhat different way. Rather than restricting the absolute distance between the trees of a set and their nearest common ancestor, given a node α in a derivation tree, delayed TL-MCTAG restricts the number of tree sets that are not fully dominated by α. Borrowing directly from #TARGET_REF , Figure 7 gives two examples. Parsing for delayed TL-MCTAG is not discussed by #REF but can be accomplished using a similar CKY-style strategy as in the two parsers above. We present a parser in Figure 6 .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Borrowing directly from #TARGET_REF , Figure 7 gives two examples.\"]}"
    },
    {
        "gold": {
            "text": [
                "Borrowing directly from #REF , Figure 7 gives two examples.",
                "Parsing for delayed TL-MCTAG is not discussed by #TARGET_REF but can be accomplished using a similar CKY-style strategy as in the two parsers above.",
                "We present a parser in Figure 6 .",
                "Rather than keeping histories that record derivational distance, we keep an active delay list for each item that records the delays that are active (by recording the identities of the trees that have adjoined) for the tree of which the current node is a part.",
                "At the root of each tree the active delay list is filtered using the Filter side condition to remove all tree sets that are fully dominated and the resulting list is checked using the Size to ensure that it contains no more than d distinct tree sets where d is the specified delay for the grammar."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Borrowing directly from #REF , Figure 7 gives two examples. Parsing for delayed TL-MCTAG is not discussed by #TARGET_REF but can be accomplished using a similar CKY-style strategy as in the two parsers above. We present a parser in Figure 6 . Rather than keeping histories that record derivational distance, we keep an active delay list for each item that records the delays that are active (by recording the identities of the trees that have adjoined) for the tree of which the current node is a part. At the root of each tree the active delay list is filtered using the Filter side condition to remove all tree sets that are fully dominated and the resulting list is checked using the Size to ensure that it contains no more than d distinct tree sets where d is the specified delay for the grammar.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Parsing for delayed TL-MCTAG is not discussed by #TARGET_REF but can be accomplished using a similar CKY-style strategy as in the two parsers above.\"]}"
    },
    {
        "gold": {
            "text": [
                "Recent work on neural constituency parsing #TARGET_REF; #REF) has found multiple cases where generative scoring models for which inference is complex outperform base models for which inference is simpler.",
                "Let A be a parser that we want to parse with (here one of the generative models), and let B be a base parser that we use to propose candidate parses which are then scored by the less-tractable parser A. We denote this cross-scoring setup by B → A. The papers above repeatedly saw that the cross-scoring setup B → A under which their generative models were applied outperformed the standard singleparser setup B → B. We term this a cross-scoring gain.",
                "This paper asks two questions.",
                "First, why do recent discriminative-to-generative cross-scoring se- * Equal contribution.",
                "tups B → A outperform their base parsers B?"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Recent work on neural constituency parsing #TARGET_REF; #REF) has found multiple cases where generative scoring models for which inference is complex outperform base models for which inference is simpler. Let A be a parser that we want to parse with (here one of the generative models), and let B be a base parser that we use to propose candidate parses which are then scored by the less-tractable parser A. We denote this cross-scoring setup by B → A. The papers above repeatedly saw that the cross-scoring setup B → A under which their generative models were applied outperformed the standard singleparser setup B → B. We term this a cross-scoring gain. This paper asks two questions. First, why do recent discriminative-to-generative cross-scoring se- * Equal contribution. tups B → A outperform their base parsers B?",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Recent work on neural constituency parsing #TARGET_REF; #REF) has found multiple cases where generative scoring models for which inference is complex outperform base models for which inference is simpler.\"]}"
    },
    {
        "gold": {
            "text": [
                "Past work on discriminative neural constituency parsers has shown the effectiveness of beam search with a small beam (#REF) or even greedy search, as in the case of RD #TARGET_REF .",
                "The standard beam search procedure, which we refer to as action-synchronous, maintains a beam of K partially-completed parses that all have the same number of actions taken.",
                "At each stage, a pool of successors is constructed by extending each candidate in the beam with each of its possible next actions.",
                "The K highest-probability successors are chosen as the next beam.",
                "Unfortunately, we find that action-synchronous beam search breaks down for both generative models we explore in this work, failing to find parses that are high scoring under the model."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Past work on discriminative neural constituency parsers has shown the effectiveness of beam search with a small beam (#REF) or even greedy search, as in the case of RD #TARGET_REF . The standard beam search procedure, which we refer to as action-synchronous, maintains a beam of K partially-completed parses that all have the same number of actions taken. At each stage, a pool of successors is constructed by extending each candidate in the beam with each of its possible next actions. The K highest-probability successors are chosen as the next beam. Unfortunately, we find that action-synchronous beam search breaks down for both generative models we explore in this work, failing to find parses that are high scoring under the model.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Past work on discriminative neural constituency parsers has shown the effectiveness of beam search with a small beam (#REF) or even greedy search, as in the case of RD #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "However, it's also possible that the hybrid system B → A shows gains merely from subtle model combination effects.",
                "If so, scoring candidates using some combined score A + B would be even better, which we would characterize as a model combination gain.",
                "It might even be the case that B is a better parser overall (i.e. B → B outperforms A → A).",
                "Of course, many real hybrids will exhibit both reranking and model combination gains.",
                "In this paper, we present experiments to isolate the degree to which each gain occurs for each of two state-of-the-art generative neural parsing models: the Recurrent Neural Network Grammar generative parser (RG) of #TARGET_REF , and the LSTM language modeling generative parser (LM) of #REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "However, it's also possible that the hybrid system B → A shows gains merely from subtle model combination effects. If so, scoring candidates using some combined score A + B would be even better, which we would characterize as a model combination gain. It might even be the case that B is a better parser overall (i.e. B → B outperforms A → A). Of course, many real hybrids will exhibit both reranking and model combination gains. In this paper, we present experiments to isolate the degree to which each gain occurs for each of two state-of-the-art generative neural parsing models: the Recurrent Neural Network Grammar generative parser (RG) of #TARGET_REF , and the LSTM language modeling generative parser (LM) of #REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In this paper, we present experiments to isolate the degree to which each gain occurs for each of two state-of-the-art generative neural parsing models: the Recurrent Neural Network Grammar generative parser (RG) of #TARGET_REF , and the LSTM language modeling generative parser (LM) of #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular, we present and use a beam-based search procedure with an augmented state space that can search directly in the generative models, allowing us to explore A → A for these generative parsers A independent of any base parsers.",
                "Our findings suggest the presence of model combination effects in both generative parsers: when parses found by searching directly in the generative parser are added to a list of candidates from a strong base parser (the RNNG discriminative parser, RD #TARGET_REF ), performance decreases when compared to using just candidates from the base parser, i.e., B ∪ A → A has lower evaluation performance than B → A (Section 3.1).",
                "This result suggests that both generative models benefit from fortuitous search errors in the rescoring setting -there are trees with higher probability under the generative model than any tree proposed by the base parser, but which would decrease evaluation performance if selected.",
                "Because of this, we hypothesize that model combination effects between the base and generative models are partially responsible for the high performance of the generative reranking systems, rather than the generative model being generally superior.",
                "Here we consider our second question: if crossscoring gains are at least partly due to implicit model combination, can we gain even more by combining the models explicitly? We find that this is indeed the case: simply taking a weighted average of the scores of both models when selecting a parse from the base parser's candidate list improves over using only the score of the generative model, in many cases substantially (Section 3.2)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In particular, we present and use a beam-based search procedure with an augmented state space that can search directly in the generative models, allowing us to explore A → A for these generative parsers A independent of any base parsers. Our findings suggest the presence of model combination effects in both generative parsers: when parses found by searching directly in the generative parser are added to a list of candidates from a strong base parser (the RNNG discriminative parser, RD #TARGET_REF ), performance decreases when compared to using just candidates from the base parser, i.e., B ∪ A → A has lower evaluation performance than B → A (Section 3.1). This result suggests that both generative models benefit from fortuitous search errors in the rescoring setting -there are trees with higher probability under the generative model than any tree proposed by the base parser, but which would decrease evaluation performance if selected. Because of this, we hypothesize that model combination effects between the base and generative models are partially responsible for the high performance of the generative reranking systems, rather than the generative model being generally superior. Here we consider our second question: if crossscoring gains are at least partly due to implicit model combination, can we gain even more by combining the models explicitly? We find that this is indeed the case: simply taking a weighted average of the scores of both models when selecting a parse from the base parser's candidate list improves over using only the score of the generative model, in many cases substantially (Section 3.2).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Our findings suggest the presence of model combination effects in both generative parsers: when parses found by searching directly in the generative parser are added to a list of candidates from a strong base parser (the RNNG discriminative parser, RD #TARGET_REF ), performance decreases when compared to using just candidates from the base parser, i.e., B \\u222a A \\u2192 A has lower evaluation performance than B \\u2192 A (Section 3.1).\"]}"
    },
    {
        "gold": {
            "text": [
                "All of the parsers we investigate in this work (the discriminative parser RD, and the two generative parsers RG and LM, see Section 1) produce parse trees in a depth-first, left-to-right traversal, using the same basic actions: NT(X), which opens a new constituent with the non-terminal symbol X; SHIFT / GEN(w), which adds a word; and RE-DUCE, which closes the current constituent.",
                "We refer to #TARGET_REF for a complete description of these actions, and the constraints on them necessary to ensure valid parse trees.",
                "1 The primary difference between the actions in the discriminative and generative models is that, whereas the discriminative model uses a SHIFT action which is fixed to produce the next word in the sentence, the generative models use GEN(w) to define a distribution over all possible words w in the lexicon.",
                "This stems from the generative model's definition of a joint probability p(x, y) over all possible sentences x and parses y. To use a generative model as a parser, we are interested in finding the maximum probability parse for a given sentence.",
                "This is made more complicated by not having an explicit representation for p(y|x), as we do in the discriminative setting."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "All of the parsers we investigate in this work (the discriminative parser RD, and the two generative parsers RG and LM, see Section 1) produce parse trees in a depth-first, left-to-right traversal, using the same basic actions: NT(X), which opens a new constituent with the non-terminal symbol X; SHIFT / GEN(w), which adds a word; and RE-DUCE, which closes the current constituent. We refer to #TARGET_REF for a complete description of these actions, and the constraints on them necessary to ensure valid parse trees. 1 The primary difference between the actions in the discriminative and generative models is that, whereas the discriminative model uses a SHIFT action which is fixed to produce the next word in the sentence, the generative models use GEN(w) to define a distribution over all possible words w in the lexicon. This stems from the generative model's definition of a joint probability p(x, y) over all possible sentences x and parses y. To use a generative model as a parser, we are interested in finding the maximum probability parse for a given sentence. This is made more complicated by not having an explicit representation for p(y|x), as we do in the discriminative setting.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We refer to #TARGET_REF for a complete description of these actions, and the constraints on them necessary to ensure valid parse trees.\"]}"
    },
    {
        "gold": {
            "text": [
                "Using the above decoding procedures, we attempt to separate reranking effects from model combination effects through a set of reranking experiments.",
                "Our base experiments are performed on the Penn Treebank (#REF) , using sections 2-21 for training, section 22 for development, and section 23 for testing.",
                "For the LSTM generative model (LM), we use the pre-trained model released by #REF .",
                "We train RNNG discriminative (RD) and generative (RG) models, following #TARGET_REF by using the same hyperparameter settings, and using pretrained word embeddings from #REF for the discriminative model.",
                "The automaticallypredicted part-of-speech tags we use as input for RD are the same as those used by #REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Using the above decoding procedures, we attempt to separate reranking effects from model combination effects through a set of reranking experiments. Our base experiments are performed on the Penn Treebank (#REF) , using sections 2-21 for training, section 22 for development, and section 23 for testing. For the LSTM generative model (LM), we use the pre-trained model released by #REF . We train RNNG discriminative (RD) and generative (RG) models, following #TARGET_REF by using the same hyperparameter settings, and using pretrained word embeddings from #REF for the discriminative model. The automaticallypredicted part-of-speech tags we use as input for RD are the same as those used by #REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We train RNNG discriminative (RD) and generative (RG) models, following #TARGET_REF by using the same hyperparameter settings, and using pretrained word embeddings from #REF for the discriminative model.\"]}"
    },
    {
        "gold": {
            "text": [
                "The e i,t and α i,t are calculated in each time step t of decoding.",
                "The t-th hidden state s t in dynamic attention-based decoder can be calculated as follows:",
                "The main difference between our proposed conversational response generation model and the above two state-of-the-art models is the two attention mechanisms for obtaining the contextual representation of a conversation.",
                "Rather than use a hierarchical attention neural network #TARGET_REF to obtain the contextual representation of a conversation, we propose two utterance-level attentions for weighting the importance of each utterance in the context, which is more simple in structure and has less number of parameters than the hierarchical attention approach.",
                "Meanwhile, rather than use a heuristic approach to weigh the importance of each utterance in the context (#REF) , in our proposed approach, the weights of utterance in the context are learned by two attention mechanisms from the data, which is more reasonable and flexible than the heuristic based approach."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The e i,t and α i,t are calculated in each time step t of decoding. The t-th hidden state s t in dynamic attention-based decoder can be calculated as follows: The main difference between our proposed conversational response generation model and the above two state-of-the-art models is the two attention mechanisms for obtaining the contextual representation of a conversation. Rather than use a hierarchical attention neural network #TARGET_REF to obtain the contextual representation of a conversation, we propose two utterance-level attentions for weighting the importance of each utterance in the context, which is more simple in structure and has less number of parameters than the hierarchical attention approach. Meanwhile, rather than use a heuristic approach to weigh the importance of each utterance in the context (#REF) , in our proposed approach, the weights of utterance in the context are learned by two attention mechanisms from the data, which is more reasonable and flexible than the heuristic based approach.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"Rather than use a hierarchical attention neural network #TARGET_REF to obtain the contextual representation of a conversation, we propose two utterance-level attentions for weighting the importance of each utterance in the context, which is more simple in structure and has less number of parameters than the hierarchical attention approach.\"]}"
    },
    {
        "gold": {
            "text": [
                "Topic segmentation likewise has multiple educational applications, such as question answering, detecting student initiative, and assessing student answers.",
                "There have been essentially two approaches to topic segmentation in the past.",
                "The first of these, lexical cohesion, may be used for either linear segmentation (#REF; #TARGET_REF or hierarchical segmentation (#REF; #REF) .",
                "The essential idea behind the lexical cohesion approaches is that different topics will have different vocabularies.",
                "Therefore the lexical cohesion within topics will be higher than the lexical cohesion between topics, and gaps in cohesion may mark topic boundaries."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Topic segmentation likewise has multiple educational applications, such as question answering, detecting student initiative, and assessing student answers. There have been essentially two approaches to topic segmentation in the past. The first of these, lexical cohesion, may be used for either linear segmentation (#REF; #TARGET_REF or hierarchical segmentation (#REF; #REF) . The essential idea behind the lexical cohesion approaches is that different topics will have different vocabularies. Therefore the lexical cohesion within topics will be higher than the lexical cohesion between topics, and gaps in cohesion may mark topic boundaries.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"There have been essentially two approaches to topic segmentation in the past.\", \"The first of these, lexical cohesion, may be used for either linear segmentation (#REF; #TARGET_REF or hierarchical segmentation (#REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Though the idea of using lexical cohesion to segment text has the advantages of simplicity and intuitive appeal, it lacks a unique implementation.",
                "An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment.",
                "Both Hearst (1994 #TARGET_REF and #REF use vector space methods discussed below to represent and compare units of text.",
                "The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.",
                "However, Hearst (1994 Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Though the idea of using lexical cohesion to segment text has the advantages of simplicity and intuitive appeal, it lacks a unique implementation. An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment. Both Hearst (1994 #TARGET_REF and #REF use vector space methods discussed below to represent and compare units of text. The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text. However, Hearst (1994 Hearst ( , 1997 and #REF differ on how text units are defined and on how to interpret the results of a comparison.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Both Hearst (1994 #TARGET_REF and #REF use vector space methods discussed below to represent and compare units of text.\"]}"
    },
    {
        "gold": {
            "text": [
                "An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment.",
                "Both Hearst (1994 Hearst ( , 1997 and #REF use vector space methods discussed below to represent and compare units of text.",
                "The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text.",
                "However, Hearst (1994 #TARGET_REF and #REF differ on how text units are defined and on how to interpret the results of a comparison.",
                "The text unit's definition in Hearst (1994 Hearst ( , 1997 and #REF is generally task dependent, depending on what size gives the best results."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "An implementation must define how to represent units of text, compare the cohesion between units, and determine whether the results of comparison indicate a new text segment. Both Hearst (1994 Hearst ( , 1997 and #REF use vector space methods discussed below to represent and compare units of text. The comparisons can be characterized by a moving window, where successive overlapping comparisons are advanced by one unit of text. However, Hearst (1994 #TARGET_REF and #REF differ on how text units are defined and on how to interpret the results of a comparison. The text unit's definition in Hearst (1994 Hearst ( , 1997 and #REF is generally task dependent, depending on what size gives the best results.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"However, Hearst (1994 #TARGET_REF and #REF differ on how text units are defined and on how to interpret the results of a comparison.\"]}"
    },
    {
        "gold": {
            "text": [
                "The text unit's definition in Hearst (1994 #TARGET_REF and #REF is generally task dependent, depending on what size gives the best results.",
                "For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level.",
                "However, when using LSA to segment text, #REF use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion.",
                "Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens (#REF) , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size.",
                "Under a vector space model, comparisons are performed by calculating the cosine of vectors representing text."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The text unit's definition in Hearst (1994 #TARGET_REF and #REF is generally task dependent, depending on what size gives the best results. For example, when measuring comprehension, use the unit of the sentence, as opposed to the more standard unit of the proposition, because LSA is most correlated with comprehension at that level. However, when using LSA to segment text, #REF use the paragraph as the unit, to \"smooth out\" the local changes in cohesion and become more sensitive to more global changes of cohesion. Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens (#REF) , but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size. Under a vector space model, comparisons are performed by calculating the cosine of vectors representing text.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The text unit's definition in Hearst (1994 #TARGET_REF and #REF is generally task dependent, depending on what size gives the best results.\"]}"
    },
    {
        "gold": {
            "text": [
                "Using LSA and this criterion, #REF detected chapter boundaries with an F-measure of .33 (see Manning and Schütze (1999) for a definition of Fmeasure).",
                "Hearst (1994 #TARGET_REF in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores.",
                "A depth score is computed as the difference between a given vector comparison and its surrounding peaks, i.e. the local maxima of vector comparisons on either side of the given vector comparison.",
                "The greater the difference between a given comparison and its surrounding peaks, the higher the depth score.",
                "Once all the depth scores are calculated for a text, those that are higher than one standard deviation below the mean are taken as topic boundaries."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Using LSA and this criterion, #REF detected chapter boundaries with an F-measure of .33 (see Manning and Schütze (1999) for a definition of Fmeasure). Hearst (1994 #TARGET_REF in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores. A depth score is computed as the difference between a given vector comparison and its surrounding peaks, i.e. the local maxima of vector comparisons on either side of the given vector comparison. The greater the difference between a given comparison and its surrounding peaks, the higher the depth score. Once all the depth scores are calculated for a text, those that are higher than one standard deviation below the mean are taken as topic boundaries.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"Hearst (1994 #TARGET_REF in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores.\"]}"
    },
    {
        "gold": {
            "text": [
                "The greater the difference between a given comparison and its surrounding peaks, the higher the depth score.",
                "Once all the depth scores are calculated for a text, those that are higher than one standard deviation below the mean are taken as topic boundaries.",
                "Using a vector space method without singular value decomposition, #TARGET_REF reports an F-measure of .70 when detecting topic shifts between paragraphs.",
                "Thus previous work suggests that the #REF method is superior to that of #REF , having roughly twice the accuracy indicated by F-measure.",
                "Although these two results used different data sets and are therefore not directly comparable, one would predict based on this limited evidence that the Hearst algorithm would outperform the Foltz algorithm on other topic segmentation tasks."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The greater the difference between a given comparison and its surrounding peaks, the higher the depth score. Once all the depth scores are calculated for a text, those that are higher than one standard deviation below the mean are taken as topic boundaries. Using a vector space method without singular value decomposition, #TARGET_REF reports an F-measure of .70 when detecting topic shifts between paragraphs. Thus previous work suggests that the #REF method is superior to that of #REF , having roughly twice the accuracy indicated by F-measure. Although these two results used different data sets and are therefore not directly comparable, one would predict based on this limited evidence that the Hearst algorithm would outperform the Foltz algorithm on other topic segmentation tasks.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Using a vector space method without singular value decomposition, #TARGET_REF reports an F-measure of .70 when detecting topic shifts between paragraphs.\"]}"
    },
    {
        "gold": {
            "text": [
                "The greater the difference between a given comparison and its surrounding peaks, the higher the depth score.",
                "Once all the depth scores are calculated for a text, those that are higher than one standard deviation below the mean are taken as topic boundaries.",
                "Using a vector space method without singular value decomposition, #REF reports an F-measure of .70 when detecting topic shifts between paragraphs.",
                "Thus previous work suggests that #TARGET_REF method is superior to that of #REF , having roughly twice the accuracy indicated by F-measure.",
                "Although these two results used different data sets and are therefore not directly comparable, one would predict based on this limited evidence that the Hearst algorithm would outperform the Foltz algorithm on other topic segmentation tasks."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The greater the difference between a given comparison and its surrounding peaks, the higher the depth score. Once all the depth scores are calculated for a text, those that are higher than one standard deviation below the mean are taken as topic boundaries. Using a vector space method without singular value decomposition, #REF reports an F-measure of .70 when detecting topic shifts between paragraphs. Thus previous work suggests that #TARGET_REF method is superior to that of #REF , having roughly twice the accuracy indicated by F-measure. Although these two results used different data sets and are therefore not directly comparable, one would predict based on this limited evidence that the Hearst algorithm would outperform the Foltz algorithm on other topic segmentation tasks.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Thus previous work suggests that #TARGET_REF method is superior to that of #REF , having roughly twice the accuracy indicated by F-measure.\"]}"
    },
    {
        "gold": {
            "text": [
                "Recall that in monologue, #TARGET_REF reports a much larger F-measure than #REF , .70 vs. .33, albeit on different data sets.",
                "In the present dialogue corpus, these roles are reversed, .14 vs. .52.",
                "Possible reasons for this reversal are the segmentation criterion, the vector space method, or the fact that Foltz has been trained on similar data via regression and Hearst has not.",
                "However, comparing the Hearst algorithm with the Hearst + LSA algorithm indicates that a 57% improvement stems from the addition of LSA, keeping all other factors constant.",
                "While this result is not statistically significant, the direction of the result supports the use of an \"inferencing\" vector space method for segmenting dialogue."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Recall that in monologue, #TARGET_REF reports a much larger F-measure than #REF , .70 vs. .33, albeit on different data sets. In the present dialogue corpus, these roles are reversed, .14 vs. .52. Possible reasons for this reversal are the segmentation criterion, the vector space method, or the fact that Foltz has been trained on similar data via regression and Hearst has not. However, comparing the Hearst algorithm with the Hearst + LSA algorithm indicates that a 57% improvement stems from the addition of LSA, keeping all other factors constant. While this result is not statistically significant, the direction of the result supports the use of an \"inferencing\" vector space method for segmenting dialogue.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"Recall that in monologue, #TARGET_REF reports a much larger F-measure than #REF , .70 vs. .33, albeit on different data sets.\", \"In the present dialogue corpus, these roles are reversed, .14 vs. .52.\"]}"
    },
    {
        "gold": {
            "text": [
                "While this result is not statistically significant, the direction of the result supports the use of an \"inferencing\" vector space method for segmenting dialogue.",
                "Unfortunately, the large difference in F-measure between the Foltz algorithm and the Hearst + LSA algorithm is more difficult to explain.",
                "These two methods differ by their segmentation criterion and by their training (Foltz is a regression model and Hearst is not).",
                "It may be that Hearst (1994 #TARGET_REF )'s segmentation criterion, i.e. depth scores, do not translate well to dialogue.",
                "Perhaps the assignment of segment boundaries based on the relative difference between a candidate score and its surrounding peaks is highly sensitive to cohesion gaps created by conversational implicatures."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "While this result is not statistically significant, the direction of the result supports the use of an \"inferencing\" vector space method for segmenting dialogue. Unfortunately, the large difference in F-measure between the Foltz algorithm and the Hearst + LSA algorithm is more difficult to explain. These two methods differ by their segmentation criterion and by their training (Foltz is a regression model and Hearst is not). It may be that Hearst (1994 #TARGET_REF )'s segmentation criterion, i.e. depth scores, do not translate well to dialogue. Perhaps the assignment of segment boundaries based on the relative difference between a candidate score and its surrounding peaks is highly sensitive to cohesion gaps created by conversational implicatures.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"It may be that Hearst (1994 #TARGET_REF )'s segmentation criterion, i.e. depth scores, do not translate well to dialogue.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF recommends using the average paragraph size as the window size.",
                "Using the development corpus's average topic length of 16 utterances as a reference point, F-measures were calculated for the combinations of window size and text unit size in Table 1 .",
                "The optimal combination of parameters (Fmeasure = .17) is a unit size of 16 words and a window size of 16 units.",
                "This combination matches #REF 's heuristic of choosing the window size to be the average paragraph length.",
                "On the test set, this combination of parameters yielded an F-measure of .14 as opposed to the Fmeasure for monologue reported by #TARGET_REF , .70."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                1
            ]
        },
        "input": "#REF recommends using the average paragraph size as the window size. Using the development corpus's average topic length of 16 utterances as a reference point, F-measures were calculated for the combinations of window size and text unit size in Table 1 . The optimal combination of parameters (Fmeasure = .17) is a unit size of 16 words and a window size of 16 units. This combination matches #REF 's heuristic of choosing the window size to be the average paragraph length. On the test set, this combination of parameters yielded an F-measure of .14 as opposed to the Fmeasure for monologue reported by #TARGET_REF , .70.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Using the development corpus's average topic length of 16 utterances as a reference point, F-measures were calculated for the combinations of window size and text unit size in Table 1 .\", \"On the test set, this combination of parameters yielded an F-measure of .14 as opposed to the Fmeasure for monologue reported by #TARGET_REF , .70.\"]}"
    },
    {
        "gold": {
            "text": [
                "Along with this, many authors have worked on producing datasets for eliminating bias, strengthening the performance of the model by robust question-answer pairs which try to cover the various types of questions, testing the visual and language understanding of the system.",
                "In this survey, first we cover major datasets published for validating the Visual Question Answering task, such as VQA dataset [1] , DAQUAR [8] , Visual7W [9] and most recent datasets up to 2019 include Tally-QA [10] and KVQA [11] .",
                "Next, we discuss the stateof-the-art architectures designed for the task of Visual Question Answering such as Vanilla VQA [1] , Stacked Attention #REF and Pythia v1.0 [13] .",
                "Next we present some of our computed results over the three architectures: vanilla VQA model [1] , Stacked Attention Network (SAN) [12] and Teney et al. model #TARGET_REF .",
                "Finally, we discuss the observations and future directions."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Along with this, many authors have worked on producing datasets for eliminating bias, strengthening the performance of the model by robust question-answer pairs which try to cover the various types of questions, testing the visual and language understanding of the system. In this survey, first we cover major datasets published for validating the Visual Question Answering task, such as VQA dataset [1] , DAQUAR [8] , Visual7W [9] and most recent datasets up to 2019 include Tally-QA [10] and KVQA [11] . Next, we discuss the stateof-the-art architectures designed for the task of Visual Question Answering such as Vanilla VQA [1] , Stacked Attention #REF and Pythia v1.0 [13] . Next we present some of our computed results over the three architectures: vanilla VQA model [1] , Stacked Attention Network (SAN) [12] and Teney et al. model #TARGET_REF . Finally, we discuss the observations and future directions.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Next we present some of our computed results over the three architectures: vanilla VQA model [1] , Stacked Attention Network (SAN) [12] and Teney et al. model #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The Differentail Network is the very recent method proposed for VQA task and shows very promising performance over different datasets.",
                "As part of this survey, we also implemented different methods over different datasets and performed the experiments.",
                "We considered the following three models for our experiments, 1) the baseline Vanilla VQA model [1] which uses the VGG16 CNN architecture [3] and LST#REF , 2) the Stacked Attention #REF architecture, and 3) the 2017 VQA challenge winner Teney et al. model #TARGET_REF .",
                "We considered the widely adapted datasets such as standard VQA dataset [1] and Visual7W dataset [9] for the experiments.",
                "We used the Adam Optimizer for all models with Cross-Entropy loss function."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The Differentail Network is the very recent method proposed for VQA task and shows very promising performance over different datasets. As part of this survey, we also implemented different methods over different datasets and performed the experiments. We considered the following three models for our experiments, 1) the baseline Vanilla VQA model [1] which uses the VGG16 CNN architecture [3] and LST#REF , 2) the Stacked Attention #REF architecture, and 3) the 2017 VQA challenge winner Teney et al. model #TARGET_REF . We considered the widely adapted datasets such as standard VQA dataset [1] and Visual7W dataset [9] for the experiments. We used the Adam Optimizer for all models with Cross-Entropy loss function.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We considered the following three models for our experiments, 1) the baseline Vanilla VQA model [1] which uses the VGG16 CNN architecture [3] and LST#REF , 2) the Stacked Attention #REF architecture, and 3) the 2017 VQA challenge winner Teney et al. model #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Each model is trained for 100 epochs for each dataset.",
                "The experimental results are presented in Table III in terms of the accuracy for three models over two datasets.",
                "In the experiments, we found that the Teney et al. #TARGET_REF is the best performing model on both VQA and Visual7W Dataset.",
                "The accuracies obtained over the Teney et al. model are 67.23% and 65.82% over VQA and Visual7W datasets for the openended question-answering task, respectively.",
                "The above results re-affirmed that the Teney et al. model is the best performing model till 2018 which has been pushed by Pythia v1.0 [13] , recently, where they have utilized the same model with more layers to boost the performance."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Each model is trained for 100 epochs for each dataset. The experimental results are presented in Table III in terms of the accuracy for three models over two datasets. In the experiments, we found that the Teney et al. #TARGET_REF is the best performing model on both VQA and Visual7W Dataset. The accuracies obtained over the Teney et al. model are 67.23% and 65.82% over VQA and Visual7W datasets for the openended question-answering task, respectively. The above results re-affirmed that the Teney et al. model is the best performing model till 2018 which has been pushed by Pythia v1.0 [13] , recently, where they have utilized the same model with more layers to boost the performance.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In the experiments, we found that the Teney et al. #TARGET_REF is the best performing model on both VQA and Visual7W Dataset.\"]}"
    },
    {
        "gold": {
            "text": [
                "This model is better suited for the VQA in videos which has more use cases than images.",
                "[20] VQA [1] , TDIUC [29] , COCO-QA [21] Faster-RCNN [22] , Differential #REF , GRU [31] 68.59 (VQA-v2), 86.73 (TDIUC), 69.36 (COCO-QA) AAAI 2019 Pythia v1.0 [28] : Pythia v1.0 is the award winning architecture for VQA #REF",
                "1 . The architecture is similar to Teney et al. #TARGET_REF with reduced computations with elementwise multiplication, use of GloVe vectors [23] , and ensemble of 30 models.",
                "Differential #REF : This model uses the differences between forward propagation steps to reduce the noise and to learn the interdependency between features.",
                "Image features are extracted using Faster-RCNN [22] ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "This model is better suited for the VQA in videos which has more use cases than images. [20] VQA [1] , TDIUC [29] , COCO-QA [21] Faster-RCNN [22] , Differential #REF , GRU [31] 68.59 (VQA-v2), 86.73 (TDIUC), 69.36 (COCO-QA) AAAI 2019 Pythia v1.0 [28] : Pythia v1.0 is the award winning architecture for VQA #REF 1 . The architecture is similar to Teney et al. #TARGET_REF with reduced computations with elementwise multiplication, use of GloVe vectors [23] , and ensemble of 30 models. Differential #REF : This model uses the differences between forward propagation steps to reduce the noise and to learn the interdependency between features. Image features are extracted using Faster-RCNN [22] .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"1 . The architecture is similar to Teney et al. #TARGET_REF with reduced computations with elementwise multiplication, use of GloVe vectors [23] , and ensemble of 30 models.\"]}"
    },
    {
        "gold": {
            "text": [
                "Decipherment of homophonic substitution ciphers using language models (LMs) is a wellstudied task in NLP.",
                "Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram LMs.",
                "The most widely used technique is the use of beam search with n-gram LMs proposed by #TARGET_REF .",
                "We propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural LM.",
                "We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural LM."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Decipherment of homophonic substitution ciphers using language models (LMs) is a wellstudied task in NLP. Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram LMs. The most widely used technique is the use of beam search with n-gram LMs proposed by #TARGET_REF . We propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural LM. We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural LM.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The most widely used technique is the use of beam search with n-gram LMs proposed by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic cipher key.",
                "Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (#REF; #TARGET_REF .",
                "Some methods use the ExpectationMaximization (EM) algorithm (#REF) while most state-of-the-art approaches for decipherment of 1:1 and homophonic substitution ciphers use beam search and rely on the clever use of n-gram LMs (#REF; #REF) .",
                "Neural LMs globally score the entire candidate plaintext sequence (#REF) .",
                "However, using a neural LM for decipherment is not trivial because scoring the entire candidate partially deciphered plaintext is computationally challenging."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic cipher key. Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (#REF; #TARGET_REF . Some methods use the ExpectationMaximization (EM) algorithm (#REF) while most state-of-the-art approaches for decipherment of 1:1 and homophonic substitution ciphers use beam search and rely on the clever use of n-gram LMs (#REF; #REF) . Neural LMs globally score the entire candidate plaintext sequence (#REF) . However, using a neural LM for decipherment is not trivial because scoring the entire candidate partially deciphered plaintext is computationally challenging.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (#REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The number of f s that are fixed in φ is given by its cardinality.",
                "φ is called an extension of φ, if f is fixed in φ such that δ(φ (f ), φ(f )) yields true ∀f ∈ V f which are already fixed in φ where δ is Kronecker delta.",
                "Decipherment is then the task of finding the φ for which the probability of the deciphered text is maximized.",
                "where p(.) is the language model (LM).",
                "Finding this argmax is solved using a beam search algorithm #TARGET_REF which incrementally finds the most likely substitutions using the language model scores as the ranking."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The number of f s that are fixed in φ is given by its cardinality. φ is called an extension of φ, if f is fixed in φ such that δ(φ (f ), φ(f )) yields true ∀f ∈ V f which are already fixed in φ where δ is Kronecker delta. Decipherment is then the task of finding the φ for which the probability of the deciphered text is maximized. where p(.) is the language model (LM). Finding this argmax is solved using a beam search algorithm #TARGET_REF which incrementally finds the most likely substitutions using the language model scores as the ranking.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Finding this argmax is solved using a beam search algorithm #TARGET_REF which incrementally finds the most likely substitutions using the language model scores as the ranking.\"]}"
    },
    {
        "gold": {
            "text": [
                "Algorithm 1 is the beam search algorithm #TARGET_REF (#REF Hs.",
                "ADD((∅,0)) 5:",
                "while",
                "for all φ ∈ Hs do 8:",
                "for all e ∈ Ve do 9:"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Algorithm 1 is the beam search algorithm #TARGET_REF (#REF Hs. ADD((∅,0)) 5: while for all φ ∈ Hs do 8: for all e ∈ Ve do 9:",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Algorithm 1 is the beam search algorithm #TARGET_REF (#REF Hs.\"]}"
    },
    {
        "gold": {
            "text": [
                "Zodiac-408, a homophonic cipher, is commonly used to evaluate decipherment algorithms.",
                "Our neural LM model with global rest cost estimation and frequency matching heuristic with a beam size of 1M has SER of 1.2% compared to the beam search algorithm #TARGET_REF with beam size of 10M with a 6-gram LM which gives an SER of 2%.",
                "The improved beam search (#REF) with an 8-gram LM, however, gets 52 out of 54 mappings correct on the Zodiac-408 cipher."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0
            ]
        },
        "input": "Zodiac-408, a homophonic cipher, is commonly used to evaluate decipherment algorithms. Our neural LM model with global rest cost estimation and frequency matching heuristic with a beam size of 1M has SER of 1.2% compared to the beam search algorithm #TARGET_REF with beam size of 10M with a 6-gram LM which gives an SER of 2%. The improved beam search (#REF) with an 8-gram LM, however, gets 52 out of 54 mappings correct on the Zodiac-408 cipher.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Our neural LM model with global rest cost estimation and frequency matching heuristic with a beam size of 1M has SER of 1.2% compared to the beam search algorithm #TARGET_REF with beam size of 10M with a 6-gram LM which gives an SER of 2%.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF produce better results in faster time compared to ILP and EM-based decipherment methods by employing a higher order language model and an iterative beam search algorithm.",
                "#REF present various improvements to the beam search algorithm in #TARGET_REF including improved rest cost estimation and an optimized strategy for ordering decipherment of the cipher symbols.",
                "#REF propose a novel approach for solving mono-alphabetic substitution ciphers which combines character-level and word-level language model.",
                "They formulate decipherment as a tree search problem, and use Monte Carlo Tree Search (MCTS) as an alternative to beam search.",
                "Their approach is the best for short ciphers."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "#REF produce better results in faster time compared to ILP and EM-based decipherment methods by employing a higher order language model and an iterative beam search algorithm. #REF present various improvements to the beam search algorithm in #TARGET_REF including improved rest cost estimation and an optimized strategy for ordering decipherment of the cipher symbols. #REF propose a novel approach for solving mono-alphabetic substitution ciphers which combines character-level and word-level language model. They formulate decipherment as a tree search problem, and use Monte Carlo Tree Search (MCTS) as an alternative to beam search. Their approach is the best for short ciphers.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#REF present various improvements to the beam search algorithm in #TARGET_REF including improved rest cost estimation and an optimized strategy for ordering decipherment of the cipher symbols.\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the notation from #TARGET_REF .",
                "Ciphertext f N 1 = f 1 ..f i ..f N and plaintext e N 1 = e 1 ..e i ..e N consist of vocabularies f i ∈ V f and e i ∈ V e respectively.",
                "The beginning tokens in the ciphertext (f 0 ) and plaintext (e 0 ) are set to \"$\" denoting the beginning of a sentence.",
                "The substitutions are represented by a function φ : V f → V e such that 1:1 substitutions are bijective while homophonic substitutions are general.",
                "A cipher function φ which does not have every φ(f ) fixed is called a partial cipher function (#REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We use the notation from #TARGET_REF . Ciphertext f N 1 = f 1 ..f i ..f N and plaintext e N 1 = e 1 ..e i ..e N consist of vocabularies f i ∈ V f and e i ∈ V e respectively. The beginning tokens in the ciphertext (f 0 ) and plaintext (e 0 ) are set to \"$\" denoting the beginning of a sentence. The substitutions are represented by a function φ : V f → V e such that 1:1 substitutions are bijective while homophonic substitutions are general. A cipher function φ which does not have every φ(f ) fixed is called a partial cipher function (#REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use the notation from #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In this experiment we use a synthetic 1:1 letter substitution cipher dataset following #REF , #TARGET_REF and #REF .",
                "The text is from English Wikipedia articles about history 3 , preprocessed by stripping the text of all images, tables, then lower-casing all characters, and removing all non-alphabetic and non-space characters.",
                "We create 50 cryptograms for each length 16, 32, 64, 128 and 256 using a random Caesar-cipher 1:1 substitution."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "In this experiment we use a synthetic 1:1 letter substitution cipher dataset following #REF , #TARGET_REF and #REF . The text is from English Wikipedia articles about history 3 , preprocessed by stripping the text of all images, tables, then lower-casing all characters, and removing all non-alphabetic and non-space characters. We create 50 cryptograms for each length 16, 32, 64, 128 and 256 using a random Caesar-cipher 1:1 substitution.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"In this experiment we use a synthetic 1:1 letter substitution cipher dataset following #REF , #TARGET_REF and #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper presents, to our knowledge, the first application of large pre-trained neural LMs to the decipherment problem.",
                "We modify the beam search algorithm for decipherment from #TARGET_REF; and extend it to use global scoring of the plaintext message using neural LMs.",
                "To enable full plaintext scoring we use the neural LM to sample plaintext characters which reduces the beam size required.",
                "For challenging ciphers such as Beale Pt 2 we obtain lower error rates with smaller beam sizes when compared to the state of the art in decipherment for such ciphers."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0
            ]
        },
        "input": "This paper presents, to our knowledge, the first application of large pre-trained neural LMs to the decipherment problem. We modify the beam search algorithm for decipherment from #TARGET_REF; and extend it to use global scoring of the plaintext message using neural LMs. To enable full plaintext scoring we use the neural LM to sample plaintext characters which reduces the beam size required. For challenging ciphers such as Beale Pt 2 we obtain lower error rates with smaller beam sizes when compared to the state of the art in decipherment for such ciphers.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"We modify the beam search algorithm for decipherment from #TARGET_REF; and extend it to use global scoring of the plaintext message using neural LMs.\"]}"
    },
    {
        "gold": {
            "text": [
                "Deep neural networks have been proven to be a powerful framework for natural language processing, and have demonstrated strong performance on a number of challenging tasks, ranging from machine translation (#REFb,a) , to text categorisation (#REF; #REF; #REFb) .",
                "Not only do such deep models outperform traditional machine learning methods, they also come with the benefit of not requiring difficult feature engineering.",
                "For instance, both #REF and #TARGET_REF propose end-to-end models for sequence labelling task and achieve state-of-the-art results.",
                "* https://github.com/minghao-wu/CRF-AE † Work carried out at The University of Melbourne",
                "Orthogonal to the advances in deep learning is the effort spent on feature engineering."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Deep neural networks have been proven to be a powerful framework for natural language processing, and have demonstrated strong performance on a number of challenging tasks, ranging from machine translation (#REFb,a) , to text categorisation (#REF; #REF; #REFb) . Not only do such deep models outperform traditional machine learning methods, they also come with the benefit of not requiring difficult feature engineering. For instance, both #REF and #TARGET_REF propose end-to-end models for sequence labelling task and achieve state-of-the-art results. * https://github.com/minghao-wu/CRF-AE † Work carried out at The University of Melbourne Orthogonal to the advances in deep learning is the effort spent on feature engineering.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"For instance, both #REF and #TARGET_REF propose end-to-end models for sequence labelling task and achieve state-of-the-art results.\"]}"
    },
    {
        "gold": {
            "text": [
                "Of particular interest to this paper is the work by #TARGET_REF introduce a strong end-to-end model combining a bi-directional Long Short-Term Memory (Bi-LSTM) network with Convolutional Neural Network (CNN) character encoding in a Conditional Random Field (CRF).",
                "Their model is highly capable of capturing not only word-but also characterlevel features.",
                "We extend this model by integrating an auto-encoder loss, allowing the model to take hand-crafted features as input and re-construct them as output, and show that, even with such a highly competitive model, incorporating linguistic features is still beneficial.",
                "Perhaps the closest to this study is the works by #REF and , who show how CRFs can be framed as auto-encoders in unsupervised or semisupervised settings.",
                "With our proposed model, we achieve strong performance on the CoNLL 2003 English NER shared task with an F 1 of 91.89, significantly outperforming an array of competitive baselines."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Of particular interest to this paper is the work by #TARGET_REF introduce a strong end-to-end model combining a bi-directional Long Short-Term Memory (Bi-LSTM) network with Convolutional Neural Network (CNN) character encoding in a Conditional Random Field (CRF). Their model is highly capable of capturing not only word-but also characterlevel features. We extend this model by integrating an auto-encoder loss, allowing the model to take hand-crafted features as input and re-construct them as output, and show that, even with such a highly competitive model, incorporating linguistic features is still beneficial. Perhaps the closest to this study is the works by #REF and , who show how CRFs can be framed as auto-encoders in unsupervised or semisupervised settings. With our proposed model, we achieve strong performance on the CoNLL 2003 English NER shared task with an F 1 of 91.89, significantly outperforming an array of competitive baselines.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Of particular interest to this paper is the work by #TARGET_REF introduce a strong end-to-end model combining a bi-directional Long Short-Term Memory (Bi-LSTM) network with Convolutional Neural Network (CNN) character encoding in a Conditional Random Field (CRF).\"]}"
    },
    {
        "gold": {
            "text": [
                "Specifically, our model, referred to as Neural-CRF+AE, consists of four major components: (1) a character-level CNN (char-CNN); (2) a word-level bi-directional LSTM (Bi-LSTM); (3) a conditional random field (CRF); and (4) an auto-encoder auxiliary loss.",
                "An illustration of the model architecture is presented in Figure 1 .",
                "#REF; #REF; #TARGET_REF have demonstrated that CNNs are highly capable of capturing character-level features.",
                "Here, our character-level CNN is similar to that used in #REF but differs in that we use a ReLU activation (#REF) .",
                "1"
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Specifically, our model, referred to as Neural-CRF+AE, consists of four major components: (1) a character-level CNN (char-CNN); (2) a word-level bi-directional LSTM (Bi-LSTM); (3) a conditional random field (CRF); and (4) an auto-encoder auxiliary loss. An illustration of the model architecture is presented in Figure 1 . #REF; #REF; #TARGET_REF have demonstrated that CNNs are highly capable of capturing character-level features. Here, our character-level CNN is similar to that used in #REF but differs in that we use a ReLU activation (#REF) . 1",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"#REF; #REF; #TARGET_REF have demonstrated that CNNs are highly capable of capturing character-level features.\"]}"
    },
    {
        "gold": {
            "text": [
                "We build on a highly competitive sequence labelling model, namely Bi-LSTM-CNN-CRF, first introduced by #TARGET_REF .",
                "Given an input sequence of x = {x 1 , x 2 , . . . , x T } of length T , the model is capable of tagging each input with a predicted labelŷ, resulting in a sequence ofŷ = {ŷ 1 ,ŷ 2 , . . . ,ŷ T } closely matching the gold label sequence y = {y 1 , y 2 , . . . , y T }.",
                "Here, we extend the model by incorporating an auto-encoder loss taking hand-crafted features as in/output, thereby forcing the model to preserve crucial information stored in such features and allowing us to evaluate the impacts of each feature on model performance.",
                "Specifically, our model, referred to as Neural-CRF+AE, consists of four major components: (1) a character-level CNN (char-CNN); (2) a word-level bi-directional LSTM (Bi-LSTM); (3) a conditional random field (CRF); and (4) an auto-encoder auxiliary loss.",
                "An illustration of the model architecture is presented in Figure 1 ."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We build on a highly competitive sequence labelling model, namely Bi-LSTM-CNN-CRF, first introduced by #TARGET_REF . Given an input sequence of x = {x 1 , x 2 , . . . , x T } of length T , the model is capable of tagging each input with a predicted labelŷ, resulting in a sequence ofŷ = {ŷ 1 ,ŷ 2 , . . . ,ŷ T } closely matching the gold label sequence y = {y 1 , y 2 , . . . , y T }. Here, we extend the model by incorporating an auto-encoder loss taking hand-crafted features as in/output, thereby forcing the model to preserve crucial information stored in such features and allowing us to evaluate the impacts of each feature on model performance. Specifically, our model, referred to as Neural-CRF+AE, consists of four major components: (1) a character-level CNN (char-CNN); (2) a word-level bi-directional LSTM (Bi-LSTM); (3) a conditional random field (CRF); and (4) an auto-encoder auxiliary loss. An illustration of the model architecture is presented in Figure 1 .",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"We build on a highly competitive sequence labelling model, namely Bi-LSTM-CNN-CRF, first introduced by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the CoNLL 2003 NER shared task dataset, consisting of 14,041/3,250/3,453 sentences in the training/development/test set respectively, all extracted from Reuters news articles during the period from 1996 to 1997.",
                "The dataset is annotated with four categories of name entities: PERSON, LOCATION, ORGANIZATION and MISC.",
                "We use the IOBES tagging scheme, as previous study have shown that this scheme provides a modest improvement to the model performance (#REF; #REF; #REF; #TARGET_REF .",
                "Model configuration.",
                "Following the work of #REF , we initialise word embeddings with GloVe (#REF ) (300-dimensional, trained on a 6B-token corpus)."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We use the CoNLL 2003 NER shared task dataset, consisting of 14,041/3,250/3,453 sentences in the training/development/test set respectively, all extracted from Reuters news articles during the period from 1996 to 1997. The dataset is annotated with four categories of name entities: PERSON, LOCATION, ORGANIZATION and MISC. We use the IOBES tagging scheme, as previous study have shown that this scheme provides a modest improvement to the model performance (#REF; #REF; #REF; #TARGET_REF . Model configuration. Following the work of #REF , we initialise word embeddings with GloVe (#REF ) (300-dimensional, trained on a 6B-token corpus).",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"We use the IOBES tagging scheme, as previous study have shown that this scheme provides a modest improvement to the model performance (#REF; #REF; #REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The dataset is annotated with four categories of name entities: PERSON, LOCATION, ORGANIZATION and MISC.",
                "We use the IOBES tagging scheme, as previous study have shown that this scheme provides a modest improvement to the model performance (#REF; #REF; #REF; #REF) .",
                "Model configuration.",
                "Following the work of #TARGET_REF , we initialise word embeddings with GloVe (#REF ) (300-dimensional, trained on a 6B-token corpus).",
                "Character embeddings are 30-dimensional and randomly initialised with a uniform distribution in"
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The dataset is annotated with four categories of name entities: PERSON, LOCATION, ORGANIZATION and MISC. We use the IOBES tagging scheme, as previous study have shown that this scheme provides a modest improvement to the model performance (#REF; #REF; #REF; #REF) . Model configuration. Following the work of #TARGET_REF , we initialise word embeddings with GloVe (#REF ) (300-dimensional, trained on a 6B-token corpus). Character embeddings are 30-dimensional and randomly initialised with a uniform distribution in",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Following the work of #TARGET_REF , we initialise word embeddings with GloVe (#REF ) (300-dimensional, trained on a 6B-token corpus).\"]}"
    },
    {
        "gold": {
            "text": [
                "Evaluation.",
                "We measure model performance with the official CoNLL evaluation script and report span-level named entity F-score on the test set using early stopping based on the performance on the validation set.",
                "We report average F-scores and standard deviation over 5 runs for our model.",
                "Baseline.",
                "In addition to reporting a number of prior results of competitive baseline models, as listed in Table 2 , we also re-implement the Bi-LSTM-CNN-CRF model by #TARGET_REF (referred to as Neural-CRF in Table 2 ) and report its average performance."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Evaluation. We measure model performance with the official CoNLL evaluation script and report span-level named entity F-score on the test set using early stopping based on the performance on the validation set. We report average F-scores and standard deviation over 5 runs for our model. Baseline. In addition to reporting a number of prior results of competitive baseline models, as listed in Table 2 , we also re-implement the Bi-LSTM-CNN-CRF model by #TARGET_REF (referred to as Neural-CRF in Table 2 ) and report its average performance.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"In addition to reporting a number of prior results of competitive baseline models, as listed in Table 2 , we also re-implement the Bi-LSTM-CNN-CRF model by #TARGET_REF (referred to as Neural-CRF in Table 2 ) and report its average performance.\"]}"
    },
    {
        "gold": {
            "text": [
                "Specifically, our model, referred to as Neural-CRF+AE, consists of four major components: (1) a character-level CNN (char-CNN); (2) a word-level bi-directional LSTM (Bi-LSTM); (3) a conditional random field (CRF); and (4) an auto-encoder auxiliary loss.",
                "An illustration of the model architecture is presented in Figure 1 .",
                "#REF; #REF; #REF) have demonstrated that CNNs are highly capable of capturing character-level features.",
                "Here, our character-level CNN is similar to that used in #TARGET_REF but differs in that we use a ReLU activation (#REF) .",
                "1"
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Specifically, our model, referred to as Neural-CRF+AE, consists of four major components: (1) a character-level CNN (char-CNN); (2) a word-level bi-directional LSTM (Bi-LSTM); (3) a conditional random field (CRF); and (4) an auto-encoder auxiliary loss. An illustration of the model architecture is presented in Figure 1 . #REF; #REF; #REF) have demonstrated that CNNs are highly capable of capturing character-level features. Here, our character-level CNN is similar to that used in #TARGET_REF but differs in that we use a ReLU activation (#REF) . 1",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"Here, our character-level CNN is similar to that used in #TARGET_REF but differs in that we use a ReLU activation (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Character-level models (#REF; #REF) learn relationship between similar word forms and have shown to be effective for parsing MRLs (#REF; #REF; #REF; Björkelund et al., 2017) .",
                "Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by #TARGET_REF Tsarfaty et al. ( , 2013 :",
                "• Can we represent words abstractly so as to reflect shared morphological aspects between them? • Which types of morphological information should we include in the parsing model?",
                "It is tempting to hypothesize that character-level models effectively solve the first problem.",
                "For the second, #REF and #REF reported that morphological case is beneficial across morphologically rich languages with extensive case systems, where case syncretism is pervasive and often hurts parsing performance. But these studies focus on vintage parsers; do neural parsers with character-level representations also solve this second problem?"
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Character-level models (#REF; #REF) learn relationship between similar word forms and have shown to be effective for parsing MRLs (#REF; #REF; #REF; Björkelund et al., 2017) . Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by #TARGET_REF Tsarfaty et al. ( , 2013 : • Can we represent words abstractly so as to reflect shared morphological aspects between them? • Which types of morphological information should we include in the parsing model? It is tempting to hypothesize that character-level models effectively solve the first problem. For the second, #REF and #REF reported that morphological case is beneficial across morphologically rich languages with extensive case systems, where case syncretism is pervasive and often hurts parsing performance. But these studies focus on vintage parsers; do neural parsers with character-level representations also solve this second problem?",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by #TARGET_REF Tsarfaty et al. ( , 2013 :\"]}"
    },
    {
        "gold": {
            "text": [
                "It is tempting to hypothesize that character-level models effectively solve the first problem.",
                "For the second, #TARGET_REF and #REF reported that morphological case is beneficial across morphologically rich languages with extensive case systems, where case syncretism is pervasive and often hurts parsing performance. But #TARGET_REF focus on vintage parsers; do neural parsers with character-level representations also solve this second problem?",
                "We attempt to answer this question by asking whether an explicit model of morphological case helps dependency parsing, and our results show that it does.",
                "Furthermore, a pipeline model in which we feed predicted case to the parser outperforms multi-task learning in which case prediction is an auxiliary task.",
                "These results suggest that neural dependency parsers do not adequately infer this crucial linguistic feature directly from the input text."
            ],
            "label": [
                "MOT",
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "It is tempting to hypothesize that character-level models effectively solve the first problem. For the second, #TARGET_REF and #REF reported that morphological case is beneficial across morphologically rich languages with extensive case systems, where case syncretism is pervasive and often hurts parsing performance. But #TARGET_REF focus on vintage parsers; do neural parsers with character-level representations also solve this second problem? We attempt to answer this question by asking whether an explicit model of morphological case helps dependency parsing, and our results show that it does. Furthermore, a pipeline model in which we feed predicted case to the parser outperforms multi-task learning in which case prediction is an auxiliary task. These results suggest that neural dependency parsers do not adequately infer this crucial linguistic feature directly from the input text.",
        "output": "{\"label\": [\"MOT\", \"BACK\"], \"context\": [\"For the second, #TARGET_REF and #REF reported that morphological case is beneficial across morphologically rich languages with extensive case systems, where case syncretism is pervasive and often hurts parsing performance. But #TARGET_REF focus on vintage parsers; do neural parsers with character-level representations also solve this second problem?\"]}"
    },
    {
        "gold": {
            "text": [
                "We train a morphological tagger to predict case information.",
                "The tagger has the same structure as the parser's encoder, with an additional feedforward neural network with one hidden layer followed by a softmax layer.",
                "We found that predicted case improves accuracy, although the effect is different across languages.",
                "These results are interesting, since in vintage parsers, predicted case usually harmed accuracy ( #TARGET_REF) .",
                "However, we note that our taggers use gold POS, which might help."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "We train a morphological tagger to predict case information. The tagger has the same structure as the parser's encoder, with an additional feedforward neural network with one hidden layer followed by a softmax layer. We found that predicted case improves accuracy, although the effect is different across languages. These results are interesting, since in vintage parsers, predicted case usually harmed accuracy ( #TARGET_REF) . However, we note that our taggers use gold POS, which might help.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"We found that predicted case improves accuracy, although the effect is different across languages.\", \"These results are interesting, since in vintage parsers, predicted case usually harmed accuracy ( #TARGET_REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, \"2/3\" can be verbalized as a date or fraction depending on the meaning of the sentence.",
                "Traditionally, the task of NSW normalization has been approached by manually authoring grammars in the form of finite-state transducers (#REF; #REF) such as integer grammars (e.g., \"26\" → \"twenty six\") or time grammars (e.g., \"5:26\" → \"five twenty six\").",
                "Constructing such grammars is time consuming and error-prone and requires extensive linguistic knowledge and programming proficiency.",
                "Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field #TARGET_REF .",
                "In this paper, we present our approach to nonstandard text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "For example, \"2/3\" can be verbalized as a date or fraction depending on the meaning of the sentence. Traditionally, the task of NSW normalization has been approached by manually authoring grammars in the form of finite-state transducers (#REF; #REF) such as integer grammars (e.g., \"26\" → \"twenty six\") or time grammars (e.g., \"5:26\" → \"five twenty six\"). Constructing such grammars is time consuming and error-prone and requires extensive linguistic knowledge and programming proficiency. Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field #TARGET_REF . In this paper, we present our approach to nonstandard text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Constructing such grammars is time consuming and error-prone and requires extensive linguistic knowledge and programming proficiency.\", \"Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Recently, methods based on neural networks have been applied to TN and ITN #TARGET_REF; #REF; #REF) .",
                "To overcome one of the biggest problems -a lack of supervision, WFSTs have been used to transform large amounts of written-form text to its spoken form.",
                "Researchers hope a vast amount of such data can counteract the errors inherited in WFST-based models.",
                "Recent data-driven approaches examine window-based sequence-to-sequence (seq2seq) models and convolutional neural networks (CNN) to normalize a central piece of text with the help of context (#REF; #REF) .",
                "Window-based methods have the advantage of limiting the output vocabulary size, as most tokens that do not need to be transformed are labeled with a special <self> token."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Recently, methods based on neural networks have been applied to TN and ITN #TARGET_REF; #REF; #REF) . To overcome one of the biggest problems -a lack of supervision, WFSTs have been used to transform large amounts of written-form text to its spoken form. Researchers hope a vast amount of such data can counteract the errors inherited in WFST-based models. Recent data-driven approaches examine window-based sequence-to-sequence (seq2seq) models and convolutional neural networks (CNN) to normalize a central piece of text with the help of context (#REF; #REF) . Window-based methods have the advantage of limiting the output vocabulary size, as most tokens that do not need to be transformed are labeled with a special <self> token.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Recently, methods based on neural networks have been applied to TN and ITN #TARGET_REF; #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "To overcome one of the biggest problems -a lack of supervision, WFSTs have been used to transform large amounts of written-form text to its spoken form.",
                "Researchers hope a vast amount of such data can counteract the errors inherited in WFST-based models.",
                "Recent data-driven approaches examine window-based sequence-to-sequence (seq2seq) models and convolutional neural networks (CNN) to normalize a central piece of text with the help of context #TARGET_REF; #REF) .",
                "Window-based methods have the advantage of limiting the output vocabulary size, as most tokens that do not need to be transformed are labeled with a special <self> token.",
                "Hybrid neural/WFST models have also been proposed and applied to the text normalization problem (#REF; #REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "To overcome one of the biggest problems -a lack of supervision, WFSTs have been used to transform large amounts of written-form text to its spoken form. Researchers hope a vast amount of such data can counteract the errors inherited in WFST-based models. Recent data-driven approaches examine window-based sequence-to-sequence (seq2seq) models and convolutional neural networks (CNN) to normalize a central piece of text with the help of context #TARGET_REF; #REF) . Window-based methods have the advantage of limiting the output vocabulary size, as most tokens that do not need to be transformed are labeled with a special <self> token. Hybrid neural/WFST models have also been proposed and applied to the text normalization problem (#REF; #REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Recent data-driven approaches examine window-based sequence-to-sequence (seq2seq) models and convolutional neural networks (CNN) to normalize a central piece of text with the help of context #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF, we implement a seq2seq model trained on window-based data.",
                "Table 1 illustrates the window-based model's training examples corresponding to one sentence \"wake me up at 8 AM .\" which is broken down into 6 pairs.",
                "<n> and </n> indicate the center of the window.",
                "A window center might contain 1 or more words (e.g., \"8 AM\") and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as TIME, DATE, ORDINAL (#REF) .",
                "The model outputs tokens which correspond to the center of the window."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Following #TARGET_REF, we implement a seq2seq model trained on window-based data. Table 1 illustrates the window-based model's training examples corresponding to one sentence \"wake me up at 8 AM .\" which is broken down into 6 pairs. <n> and </n> indicate the center of the window. A window center might contain 1 or more words (e.g., \"8 AM\") and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as TIME, DATE, ORDINAL (#REF) . The model outputs tokens which correspond to the center of the window.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following #TARGET_REF, we implement a seq2seq model trained on window-based data.\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #REF, we implement a seq2seq model trained on window-based data.",
                "Table 1 illustrates the window-based model's training examples corresponding to one sentence \"wake me up at 8 AM .\" which is broken down into 6 pairs.",
                "<n> and </n> indicate the center of the window.",
                "A window center might contain 1 or more words (e.g., \"8 AM\") and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as TIME, DATE, ORDINAL #TARGET_REF .",
                "The model outputs tokens which correspond to the center of the window."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "Following #REF, we implement a seq2seq model trained on window-based data. Table 1 illustrates the window-based model's training examples corresponding to one sentence \"wake me up at 8 AM .\" which is broken down into 6 pairs. <n> and </n> indicate the center of the window. A window center might contain 1 or more words (e.g., \"8 AM\") and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as TIME, DATE, ORDINAL #TARGET_REF . The model outputs tokens which correspond to the center of the window.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Table 1 illustrates the window-based model's training examples corresponding to one sentence \\\"wake me up at 8 AM .\\\" which is broken down into 6 pairs.\", \"<n> and </n> indicate the center of the window.\", \"A window center might contain 1 or more words (e.g., \\\"8 AM\\\") and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as TIME, DATE, ORDINAL #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The data for the window-based seq2seq model and full sentence seq2seq were generated from the publicly available release of parallel written/speech formatted text from #TARGET_REF .",
                "The set consists of Wikipedia text which was processed through Google TTS's Kestrel text normalization system relying primarily on handcrafted rules to produce speech-formatted text.",
                "Although a large parallel dataset is available for English, we consider the feasibility of developing neural models for other languages which may not have text normalization systems in place.",
                "Therefore, we choose to scale the training data size to a limited set of text which could be generated by annotators in a reasonable time frame.",
                "As summarized in Table 2 , both window-based and sentencebased models are trained with 500K training instances."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The data for the window-based seq2seq model and full sentence seq2seq were generated from the publicly available release of parallel written/speech formatted text from #TARGET_REF . The set consists of Wikipedia text which was processed through Google TTS's Kestrel text normalization system relying primarily on handcrafted rules to produce speech-formatted text. Although a large parallel dataset is available for English, we consider the feasibility of developing neural models for other languages which may not have text normalization systems in place. Therefore, we choose to scale the training data size to a limited set of text which could be generated by annotators in a reasonable time frame. As summarized in Table 2 , both window-based and sentencebased models are trained with 500K training instances.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The data for the window-based seq2seq model and full sentence seq2seq were generated from the publicly available release of parallel written/speech formatted text from #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "As summarized in Table 2 , both window-based and sentencebased models are trained with 500K training instances.",
                "Our datasets were randomly sampled from a set of 4.9M sentences in the training data portion of the #TARGET_REF data release and split into training, validation, and test data.",
                "However, the training data for window-based and sentencebased models are not identical due to differences in input configurations.",
                "While the window-based model uses 500K randomly sampled windows, the sentence-based models use 500K sentences.",
                "For testing, 62.5K identical test sentences are used across all models."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "As summarized in Table 2 , both window-based and sentencebased models are trained with 500K training instances. Our datasets were randomly sampled from a set of 4.9M sentences in the training data portion of the #TARGET_REF data release and split into training, validation, and test data. However, the training data for window-based and sentencebased models are not identical due to differences in input configurations. While the window-based model uses 500K randomly sampled windows, the sentence-based models use 500K sentences. For testing, 62.5K identical test sentences are used across all models.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Our datasets were randomly sampled from a set of 4.9M sentences in the training data portion of the #TARGET_REF data release and split into training, validation, and test data.\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, we exclude ELECTRONIC data in our experiments.",
                "There are large numbers of <self> tokens present in the dataset.",
                "We follow #TARGET_REF in down-sampling window-based training data to constrain the proportion of \"<self>\" tokens to 10% of the data.",
                "For training sentence-based models, the source sentence is segmented into characters while the target sentence is broken into tokens.",
                "For the subword model, both the source and target sentences are segmented into subword sequences."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Therefore, we exclude ELECTRONIC data in our experiments. There are large numbers of <self> tokens present in the dataset. We follow #TARGET_REF in down-sampling window-based training data to constrain the proportion of \"<self>\" tokens to 10% of the data. For training sentence-based models, the source sentence is segmented into characters while the target sentence is broken into tokens. For the subword model, both the source and target sentences are segmented into subword sequences.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We follow #TARGET_REF in down-sampling window-based training data to constrain the proportion of \\\"<self>\\\" tokens to 10% of the data.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our first approach replicates the window-based seq2seq model of #TARGET_REF .",
                "The model encodes the central piece of text (1 or more tokens) including its context of N previous and following tokens at the character level.",
                "The output is a target token or a sequence of tokens.",
                "The input vocabulary consists of 250 common characters including letters, digits and symbols (e.g., $).",
                "The decoder vocabulary consists of 1K tokens including <self> and <sil>, the latter of which is used to normalize punctuation."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Our first approach replicates the window-based seq2seq model of #TARGET_REF . The model encodes the central piece of text (1 or more tokens) including its context of N previous and following tokens at the character level. The output is a target token or a sequence of tokens. The input vocabulary consists of 250 common characters including letters, digits and symbols (e.g., $). The decoder vocabulary consists of 1K tokens including <self> and <sil>, the latter of which is used to normalize punctuation.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Our first approach replicates the window-based seq2seq model of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Edit labels are the most expensive to obtain in real life.",
                "Our labels are generated directly from the Google FST #TARGET_REF .",
                "Each type of feature is represented by a one-hot encoding.",
                "To combine linguistic features with subword units, one can add or concatenate each subword's embedding with its corresponding linguistic feature embedding and feed a combined embedding to the bi-LSTM encoder.",
                "Or, a multi-layer perceptron (MLP) can be applied to combine information in a non-linear way."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Edit labels are the most expensive to obtain in real life. Our labels are generated directly from the Google FST #TARGET_REF . Each type of feature is represented by a one-hot encoding. To combine linguistic features with subword units, one can add or concatenate each subword's embedding with its corresponding linguistic feature embedding and feed a combined embedding to the bi-LSTM encoder. Or, a multi-layer perceptron (MLP) can be applied to combine information in a non-linear way.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Our labels are generated directly from the Google FST #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 2: Evaluation of the window-based model.",
                "Categories are sorted by frequency.",
                "* TELEPHONE is not reported in #TARGET_REF but included in the dataset; ** we removed ELECTRONIC category.",
                "As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with #REF , considering our training set is much smaller.",
                "There are 16 different edit labels shown."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Figure 2: Evaluation of the window-based model. Categories are sorted by frequency. * TELEPHONE is not reported in #TARGET_REF but included in the dataset; ** we removed ELECTRONIC category. As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with #REF , considering our training set is much smaller. There are 16 different edit labels shown.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"* TELEPHONE is not reported in #TARGET_REF but included in the dataset; ** we removed ELECTRONIC category.\"]}"
    },
    {
        "gold": {
            "text": [
                "As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with #REF , considering our training set is much smaller.",
                "There are 16 different edit labels shown.",
                "Data with TELEPHONE labels were not included in the initial analysis of #TARGET_REF , but were made available in the dataset release.",
                "For our second baseline model which operates on whole sentences, on the input side, we still use 250 common characters.",
                "However, due to the removal of the <self> token, the output space is drastically extended from 1K tokens to 45K tokens."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with #REF , considering our training set is much smaller. There are 16 different edit labels shown. Data with TELEPHONE labels were not included in the initial analysis of #TARGET_REF , but were made available in the dataset release. For our second baseline model which operates on whole sentences, on the input side, we still use 250 common characters. However, due to the removal of the <self> token, the output space is drastically extended from 1K tokens to 45K tokens.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Data with TELEPHONE labels were not included in the initial analysis of #TARGET_REF , but were made available in the dataset release.\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 2: Evaluation of the window-based model.",
                "Categories are sorted by frequency.",
                "* TELEPHONE is not reported in #REF but included in the dataset; ** we removed ELECTRONIC category.",
                "As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with #TARGET_REF , considering our training set is much smaller.",
                "There are 16 different edit labels shown."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Figure 2: Evaluation of the window-based model. Categories are sorted by frequency. * TELEPHONE is not reported in #REF but included in the dataset; ** we removed ELECTRONIC category. As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with #TARGET_REF , considering our training set is much smaller. There are 16 different edit labels shown.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with #TARGET_REF , considering our training set is much smaller.\"]}"
    },
    {
        "gold": {
            "text": [
                "The introduction of pre-trained language models, such as BERT #TARGET_REF and Open-GPT [3] , among many others, has brought tremendous progress to the NLP research and industrial communities.",
                "The contribution of these models can be categorized into two aspects.",
                "First, pre-trained language models allow modelers to achieve reasonable accuracy without the need an excessive amount of manually labeled data.",
                "This strategy is in contrast with the classical deep learning methods, which requires a multitude more data to reach comparable results.",
                "Second, for many NLP tasks, including but not limited to, SQuAD [4] , CoQA [5] , named entity recognition [6] , #REF , machine translation [8] , pre-trained model allows the creation of new state-of-art, given a reasonable amount of labelled data."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The introduction of pre-trained language models, such as BERT #TARGET_REF and Open-GPT [3] , among many others, has brought tremendous progress to the NLP research and industrial communities. The contribution of these models can be categorized into two aspects. First, pre-trained language models allow modelers to achieve reasonable accuracy without the need an excessive amount of manually labeled data. This strategy is in contrast with the classical deep learning methods, which requires a multitude more data to reach comparable results. Second, for many NLP tasks, including but not limited to, SQuAD [4] , CoQA [5] , named entity recognition [6] , #REF , machine translation [8] , pre-trained model allows the creation of new state-of-art, given a reasonable amount of labelled data.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The introduction of pre-trained language models, such as BERT #TARGET_REF and Open-GPT [3] , among many others, has brought tremendous progress to the NLP research and industrial communities.\"]}"
    },
    {
        "gold": {
            "text": [
                "The aforementioned deficiencies prompt researchers to propose deep neural networks that are able to be trained in an unsupervised fashion while being able to capture the contextual meaning of the words presented in the texts.",
                "Some early attempts include pre-trained models includes, #REF , CVT [13, 14] , EL#REF and ULMFiT [16] .",
                "However, the most successful ones are BERT #TARGET_REF and Open-GPT [3] .",
                "Unlike standard NLP deep learning model, BERT and Open-GPT are built on top of transformer [17] structures, instead of LSTM [18] or GRU [19] .",
                "The difference between BERT and Open-GPT is that BERT uses bi-directional self-attentions while Open-GPT uses only unidirectional ones, as shown in Figure 2 ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "The aforementioned deficiencies prompt researchers to propose deep neural networks that are able to be trained in an unsupervised fashion while being able to capture the contextual meaning of the words presented in the texts. Some early attempts include pre-trained models includes, #REF , CVT [13, 14] , EL#REF and ULMFiT [16] . However, the most successful ones are BERT #TARGET_REF and Open-GPT [3] . Unlike standard NLP deep learning model, BERT and Open-GPT are built on top of transformer [17] structures, instead of LSTM [18] or GRU [19] . The difference between BERT and Open-GPT is that BERT uses bi-directional self-attentions while Open-GPT uses only unidirectional ones, as shown in Figure 2 .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Some early attempts include pre-trained models includes, #REF , CVT [13, 14] , EL#REF and ULMFiT [16] .\", \"However, the most successful ones are BERT #TARGET_REF and Open-GPT [3] .\"]}"
    },
    {
        "gold": {
            "text": [
                "After training on a large corpus, both BERT and Open-GPT are able to renew the SOTA of many important natural language tasks, such as such as SQuAD [4] , CoQA [5] , named entity recognition [6] , #REF , machine translation [8] .",
                "In the presence of the success of pre-trained language models, especially BERT [2] , it is natural to ask how to best utilize the pre-trained language models to achieve new state-of-the-art results.",
                "In this line of work, #REF investigated the linguistic knowledge and transferability of contextual representations by comparing BERT [2] with EL#REF , and concluded that while the higher levels of LSTM's are more task-specific, this trend does not exhibit in transformer based models.",
                "#REF invented projected attention layer for multi-task learning using BERT, which results in an improvement in various state-of-the-art results compared to the original work of Devlin et al. #TARGET_REF .",
                "#REF propose a \"post-training\" algorithms, which does not directly fine-tune BERT, but rather first \"post-train\" BERT on the task related corpus using the masked language prediction task next sentence prediction task, which helps to reduce the bias in the training corpus."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "After training on a large corpus, both BERT and Open-GPT are able to renew the SOTA of many important natural language tasks, such as such as SQuAD [4] , CoQA [5] , named entity recognition [6] , #REF , machine translation [8] . In the presence of the success of pre-trained language models, especially BERT [2] , it is natural to ask how to best utilize the pre-trained language models to achieve new state-of-the-art results. In this line of work, #REF investigated the linguistic knowledge and transferability of contextual representations by comparing BERT [2] with EL#REF , and concluded that while the higher levels of LSTM's are more task-specific, this trend does not exhibit in transformer based models. #REF invented projected attention layer for multi-task learning using BERT, which results in an improvement in various state-of-the-art results compared to the original work of Devlin et al. #TARGET_REF . #REF propose a \"post-training\" algorithms, which does not directly fine-tune BERT, but rather first \"post-train\" BERT on the task related corpus using the masked language prediction task next sentence prediction task, which helps to reduce the bias in the training corpus.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"#REF invented projected attention layer for multi-task learning using BERT, which results in an improvement in various state-of-the-art results compared to the original work of Devlin et al. #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Second, the core computational unit is matrix multiplications, which allows researchers to utilize the full computational potential of TPU [20] .",
                "After training on a large corpus, both BERT and Open-GPT are able to renew the SOTA of many important natural language tasks, such as such as SQuAD [4] , CoQA [5] , named entity recognition [6] , #REF , machine translation [8] .",
                "In the presence of the success of pre-trained language models, especially BERT [2] , it is natural to ask how to best utilize the pre-trained language models to achieve new state-of-the-art results.",
                "In this line of work, #REF investigated the linguistic knowledge and transferability of contextual representations by comparing BERT #TARGET_REF with EL#REF , and concluded that while the higher levels of LSTM's are more task-specific, this trend does not exhibit in transformer based models.",
                "#REF invented projected attention layer for multi-task learning using BERT, which results in an improvement in various state-of-the-art results compared to the original work of #REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Second, the core computational unit is matrix multiplications, which allows researchers to utilize the full computational potential of TPU [20] . After training on a large corpus, both BERT and Open-GPT are able to renew the SOTA of many important natural language tasks, such as such as SQuAD [4] , CoQA [5] , named entity recognition [6] , #REF , machine translation [8] . In the presence of the success of pre-trained language models, especially BERT [2] , it is natural to ask how to best utilize the pre-trained language models to achieve new state-of-the-art results. In this line of work, #REF investigated the linguistic knowledge and transferability of contextual representations by comparing BERT #TARGET_REF with EL#REF , and concluded that while the higher levels of LSTM's are more task-specific, this trend does not exhibit in transformer based models. #REF invented projected attention layer for multi-task learning using BERT, which results in an improvement in various state-of-the-art results compared to the original work of #REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In this line of work, #REF investigated the linguistic knowledge and transferability of contextual representations by comparing BERT #TARGET_REF with EL#REF , and concluded that while the higher levels of LSTM's are more task-specific, this trend does not exhibit in transformer based models.\"]}"
    },
    {
        "gold": {
            "text": [
                "We choose this strategy for the following reasons.",
                "Pre-training models have been used to obtain more effective word representations through the study of a large number of corpora.",
                "In the paradigm proposed in the original work by Devlin et al. #TARGET_REF , the author directly trained BERT along with with a light-weighted task-specific head.",
                "In our case though, we top BERT with a more complex network structure, using Kaiming initialization [29] .",
                "If one would fine-tune directly the top models along with the weights in BERT, one is faced with the following dilemma: on the one hand, if the learning rate is too large, it is likely to disturb the structure innate to the pre-trained language models; on the other hand, if the learning rate is too small, since we top BERT with relatively complex models, the convergence of the top models might be impeded."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We choose this strategy for the following reasons. Pre-training models have been used to obtain more effective word representations through the study of a large number of corpora. In the paradigm proposed in the original work by Devlin et al. #TARGET_REF , the author directly trained BERT along with with a light-weighted task-specific head. In our case though, we top BERT with a more complex network structure, using Kaiming initialization [29] . If one would fine-tune directly the top models along with the weights in BERT, one is faced with the following dilemma: on the one hand, if the learning rate is too large, it is likely to disturb the structure innate to the pre-trained language models; on the other hand, if the learning rate is too small, since we top BERT with relatively complex models, the convergence of the top models might be impeded.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In the paradigm proposed in the original work by Devlin et al. #TARGET_REF , the author directly trained BERT along with with a light-weighted task-specific head.\"]}"
    },
    {
        "gold": {
            "text": [
                "In the presence of the success of pre-trained language models, especially BERT #TARGET_REF , it is natural to ask how to best utilize the pre-trained language models to achieve new state-of-the-art results.",
                "In this line of work, #REF investigated the linguistic knowledge and transferability of contextual representations by comparing BERT [2] with EL#REF , and concluded that while the higher levels of LSTM's are more task-specific, this trend does not exhibit in transformer based models.",
                "#REF invented projected attention layer for multi-task learning using BERT, which results in an improvement in various state-of-the-art results compared to the original work of #REF .",
                "#REF propose a \"post-training\" algorithms, which does not directly fine-tune BERT, but rather first \"post-train\" BERT on the task related corpus using the masked language prediction task next sentence prediction task, which helps to reduce the bias in the training corpus.",
                "Finally, #REF added additional fine-tuning tasks based on multi-task training, which further improves the prediction power of BERT in the tasks of text classification."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In the presence of the success of pre-trained language models, especially BERT #TARGET_REF , it is natural to ask how to best utilize the pre-trained language models to achieve new state-of-the-art results. In this line of work, #REF investigated the linguistic knowledge and transferability of contextual representations by comparing BERT [2] with EL#REF , and concluded that while the higher levels of LSTM's are more task-specific, this trend does not exhibit in transformer based models. #REF invented projected attention layer for multi-task learning using BERT, which results in an improvement in various state-of-the-art results compared to the original work of #REF . #REF propose a \"post-training\" algorithms, which does not directly fine-tune BERT, but rather first \"post-train\" BERT on the task related corpus using the masked language prediction task next sentence prediction task, which helps to reduce the bias in the training corpus. Finally, #REF added additional fine-tuning tasks based on multi-task training, which further improves the prediction power of BERT in the tasks of text classification.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"In the presence of the success of pre-trained language models, especially BERT #TARGET_REF , it is natural to ask how to best utilize the pre-trained language models to achieve new state-of-the-art results.\"]}"
    },
    {
        "gold": {
            "text": [
                "In the next two verification tasks, we use \"[CLS]\" for prediction and add two fully connected layers subsequently.",
                "Under our strategy stackand-finetune, we set different learning rates for the two phases.",
                "We tried to set the learning rate of the first stage to 1e −1 ,1e −2 ,5e −3 ,1e −3 and 5e −4 , and set it to a smaller number in the latter stage, such as 1e −3 ,1e −4 ,5e −5 and 1e −5 .",
                "After our experiments, we found that it gets better results while the learning rate is set to 0.001 in the stage of training only the upper model and set to 5e −5 in the later stage.",
                "Since BERT-Adam #TARGET_REF has excellent performance, in our experiments, we use it as an optimizer with β 1 = 0.9, β 2 = 0.999,L 2 -weight decay of 0.01.We apply a dropout trick on all layers and set the dropout probability as 0.1."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "In the next two verification tasks, we use \"[CLS]\" for prediction and add two fully connected layers subsequently. Under our strategy stackand-finetune, we set different learning rates for the two phases. We tried to set the learning rate of the first stage to 1e −1 ,1e −2 ,5e −3 ,1e −3 and 5e −4 , and set it to a smaller number in the latter stage, such as 1e −3 ,1e −4 ,5e −5 and 1e −5 . After our experiments, we found that it gets better results while the learning rate is set to 0.001 in the stage of training only the upper model and set to 5e −5 in the later stage. Since BERT-Adam #TARGET_REF has excellent performance, in our experiments, we use it as an optimizer with β 1 = 0.9, β 2 = 0.999,L 2 -weight decay of 0.01.We apply a dropout trick on all layers and set the dropout probability as 0.1.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"Since BERT-Adam #TARGET_REF has excellent performance, in our experiments, we use it as an optimizer with \\u03b2 1 = 0.9, \\u03b2 2 = 0.999,L 2 -weight decay of 0.01.We apply a dropout trick on all layers and set the dropout probability as 0.1.\"]}"
    },
    {
        "gold": {
            "text": [
                "In the sequence labeling task,we explore sub-task named entity recognition using CoNLL03 dataset [6] , which is a public available used in many studies to test the accuracy of their proposed methods [30, 31, 32, 33, #TARGET_REF .",
                "For strategy finetune-only and strategy stack-and-finetune, we implemented two models: one with BERT and the other with BERT adding a Bi-LSTM on top.",
                "Eval measure is accuracy and F1 score."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "In the sequence labeling task,we explore sub-task named entity recognition using CoNLL03 dataset [6] , which is a public available used in many studies to test the accuracy of their proposed methods [30, 31, 32, 33, #TARGET_REF . For strategy finetune-only and strategy stack-and-finetune, we implemented two models: one with BERT and the other with BERT adding a Bi-LSTM on top. Eval measure is accuracy and F1 score.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"In the sequence labeling task,we explore sub-task named entity recognition using CoNLL03 dataset [6] , which is a public available used in many studies to test the accuracy of their proposed methods [30, 31, 32, 33, #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Recent successes in statistical syntactic parsing based on supervised learning techniques trained on a large corpus of syntactic trees (#REF; #REF; #TARGET_REF have brought forth the hope that the same approaches could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence.",
                "Moving towards a shallow semantic level of representation is a first initial step towards the distant goal of natural language understanding and has immediate applications in question-answering and information extraction.",
                "For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to Trento will need to know that from Geneva denotes the origin of the flight and to Trento denotes its destination. Knowing that these two phrases are prepositional phrases, the information provided by a syntactic parser, is only moderately useful.",
                "The growing interest in learning deeper information is to a large extent supported and due to the recent development of semantically annotated databases such as FrameNet (#REF) or the Proposition Bank , that can be used as training resources for a number of supervised learning paradigms.",
                "We focus here on the Proposition Bank (PropBank)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Recent successes in statistical syntactic parsing based on supervised learning techniques trained on a large corpus of syntactic trees (#REF; #REF; #TARGET_REF have brought forth the hope that the same approaches could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence. Moving towards a shallow semantic level of representation is a first initial step towards the distant goal of natural language understanding and has immediate applications in question-answering and information extraction. For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to Trento will need to know that from Geneva denotes the origin of the flight and to Trento denotes its destination. Knowing that these two phrases are prepositional phrases, the information provided by a syntactic parser, is only moderately useful. The growing interest in learning deeper information is to a large extent supported and due to the recent development of semantically annotated databases such as FrameNet (#REF) or the Proposition Bank , that can be used as training resources for a number of supervised learning paradigms. We focus here on the Proposition Bank (PropBank).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Recent successes in statistical syntactic parsing based on supervised learning techniques trained on a large corpus of syntactic trees (#REF; #REF; #TARGET_REF have brought forth the hope that the same approaches could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence.\"]}"
    },
    {
        "gold": {
            "text": [
                "Because the history representation computed for the move i − 1 is included in the inputs to the computation of the representation for the next move i, virtually any information about the derivation history could flow from history representation to history representation and be used to estimate the probability of a derivation move.",
                "However, the recency preference exhibited by recursively defined neural networks biases learning towards information which flows through fewer history representations.",
                "#TARGET_REF exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step.",
                "In addition to history representations, the inputs to h(d 1 , . . . , d i−1 ) include handcrafted features of the derivation history that are meant to be relevant to the move to be chosen at step i. For each of the experiments reported here, the set D that is input to the computation of the history representation of the derivation moves d 1 , . . . , d i−1 includes the most recent history representation of the following nodes: top i , the node on top of the pushdown stack before the ith move; the left-corner ancestor of top i (that is, the second top-most node on the parser's stack); the leftmost child of top i ; and the most recent child of top i , if any.",
                "The set of features f includes the last move in the derivation, the label or tag of top i , the tag-word pair of the most recently shifted word, and the leftmost tag-word pair that top i dominates."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Because the history representation computed for the move i − 1 is included in the inputs to the computation of the representation for the next move i, virtually any information about the derivation history could flow from history representation to history representation and be used to estimate the probability of a derivation move. However, the recency preference exhibited by recursively defined neural networks biases learning towards information which flows through fewer history representations. #TARGET_REF exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step. In addition to history representations, the inputs to h(d 1 , . . . , d i−1 ) include handcrafted features of the derivation history that are meant to be relevant to the move to be chosen at step i. For each of the experiments reported here, the set D that is input to the computation of the history representation of the derivation moves d 1 , . . . , d i−1 includes the most recent history representation of the following nodes: top i , the node on top of the pushdown stack before the ith move; the left-corner ancestor of top i (that is, the second top-most node on the parser's stack); the leftmost child of top i ; and the most recent child of top i , if any. The set of features f includes the last move in the derivation, the label or tag of top i , the tag-word pair of the most recently shifted word, and the leftmost tag-word pair that top i dominates.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"However, the recency preference exhibited by recursively defined neural networks biases learning towards information which flows through fewer history representations.\", \"#TARGET_REF exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step.\"]}"
    },
    {
        "gold": {
            "text": [
                "The VP node is assumed to be on the top of the parser's stack, and the S one is supposed to be its leftcorner ancestor.",
                "The directed arcs represent the information that flows from one node to another.",
                "According to the original SSN model in #TARGET_REF , only the information carried over by the leftmost child and the most recent child of a constituent directly flows to that constituent.",
                "In the figure above, only the information conveyed by the nodes α and δ is directly input to the node S. Similarly, the only bottom-up information directly input to the VP node is conveyed by the child nodes ǫ and θ.",
                "In the original SSN models, nodes bearing a function label such as φ 1 and φ 2 are not directly input to their respective parents."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The VP node is assumed to be on the top of the parser's stack, and the S one is supposed to be its leftcorner ancestor. The directed arcs represent the information that flows from one node to another. According to the original SSN model in #TARGET_REF , only the information carried over by the leftmost child and the most recent child of a constituent directly flows to that constituent. In the figure above, only the information conveyed by the nodes α and δ is directly input to the node S. Similarly, the only bottom-up information directly input to the VP node is conveyed by the child nodes ǫ and θ. In the original SSN models, nodes bearing a function label such as φ 1 and φ 2 are not directly input to their respective parents.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"According to the original SSN model in #TARGET_REF , only the information carried over by the leftmost child and the most recent child of a constituent directly flows to that constituent.\"]}"
    },
    {
        "gold": {
            "text": [
                "The first stage selects the elements to be labelled, while the second determines the labels to be assigned to the selected elements.",
                "While some of these models are based on full parse trees (#REF; #REF) , other methods have been proposed that eschew the need for a full parse (CoNNL, 2004; CoNLL, 2005) .",
                "Because of the way the problem has been formulated -as a pipeline of parsing (or chunking) feeding into labelling -specific investigations of integrated approaches that solve both the parsing and the semantic role labelling problems at the same time have not been studied.",
                "We present work to test the hypothesis that a current statistical parser #TARGET_REF can output richer information robustly, that is without any significant degradation of the parser's accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics.",
                "We achieve promising results both on the simple parsing task, where the accuracy of the parser is measured on the standard Parseval measures, and also on the parsing task where the more complex labels of PropBank are taken into account."
            ],
            "label": [
                "USE",
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The first stage selects the elements to be labelled, while the second determines the labels to be assigned to the selected elements. While some of these models are based on full parse trees (#REF; #REF) , other methods have been proposed that eschew the need for a full parse (CoNNL, 2004; CoNLL, 2005) . Because of the way the problem has been formulated -as a pipeline of parsing (or chunking) feeding into labelling -specific investigations of integrated approaches that solve both the parsing and the semantic role labelling problems at the same time have not been studied. We present work to test the hypothesis that a current statistical parser #TARGET_REF can output richer information robustly, that is without any significant degradation of the parser's accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics. We achieve promising results both on the simple parsing task, where the accuracy of the parser is measured on the standard Parseval measures, and also on the parsing task where the more complex labels of PropBank are taken into account.",
        "output": "{\"label\": [\"USE\", \"EXT\"], \"context\": [\"We present work to test the hypothesis that a current statistical parser #TARGET_REF can output richer information robustly, that is without any significant degradation of the parser's accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics.\"]}"
    },
    {
        "gold": {
            "text": [
                "To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers #TARGET_REF , which do not make any explicit independence assumptions, and are therefore likely to adapt without much modification to the current problem.",
                "This architecture has shown state-of-the-art performance.",
                "SSN parsers comprise two components, one which estimates the parameters of a stochastic model for syntactic trees, and one which searches for the most probable syntactic tree given the parameter estimates.",
                "As with many other statistical parsers (#REF; #REF) , SSN parsers use a history-based model of parsing.",
                "Events in such a model are derivation moves."
            ],
            "label": [
                "USE",
                "MOT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers #TARGET_REF , which do not make any explicit independence assumptions, and are therefore likely to adapt without much modification to the current problem. This architecture has shown state-of-the-art performance. SSN parsers comprise two components, one which estimates the parameters of a stochastic model for syntactic trees, and one which searches for the most probable syntactic tree given the parameter estimates. As with many other statistical parsers (#REF; #REF) , SSN parsers use a history-based model of parsing. Events in such a model are derivation moves.",
        "output": "{\"label\": [\"USE\", \"MOT\"], \"context\": [\"To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers #TARGET_REF , which do not make any explicit independence assumptions, and are therefore likely to adapt without much modification to the current problem.\"]}"
    },
    {
        "gold": {
            "text": [
                "1 According to these criteria, we came up with the following four data sets covering three typologically diverse languages (exemplary entries in Table 1) .",
                "SE07: The test set of #REF Task 14 #TARGET_REF comprises 1000 English news headlines which are annotated according to six Basic Emotions, joy, anger, sadness, fear, disgust, and surprise on a [0; 100]-scale (BE6 annotation format).",
                "ANET: The Affective Norms for English Text (#REF) are an adaptation of the popular lexical database ANEW (#REF ) to short texts.",
                "The corpus comprises 120 situation description which are annotated according to Valence, Arousal, and Dominance on a 9-point scale (VAD annotation format).",
                "ANPST and MAS: The Affective Norms of Polish Short Texts (#REF) ) and the Minho Affective Sentences (#REF) can be seen as loose adaptations of ANET, very similar in methodology, but different in size and linguistic characteristics (see Table 1 )."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "1 According to these criteria, we came up with the following four data sets covering three typologically diverse languages (exemplary entries in Table 1) . SE07: The test set of #REF Task 14 #TARGET_REF comprises 1000 English news headlines which are annotated according to six Basic Emotions, joy, anger, sadness, fear, disgust, and surprise on a [0; 100]-scale (BE6 annotation format). ANET: The Affective Norms for English Text (#REF) are an adaptation of the popular lexical database ANEW (#REF ) to short texts. The corpus comprises 120 situation description which are annotated according to Valence, Arousal, and Dominance on a 9-point scale (VAD annotation format). ANPST and MAS: The Affective Norms of Polish Short Texts (#REF) ) and the Minho Affective Sentences (#REF) can be seen as loose adaptations of ANET, very similar in methodology, but different in size and linguistic characteristics (see Table 1 ).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"SE07: The test set of #REF Task 14 #TARGET_REF comprises 1000 English news headlines which are annotated according to six Basic Emotions, joy, anger, sadness, fear, disgust, and surprise on a [0; 100]-scale (BE6 annotation format).\"]}"
    },
    {
        "gold": {
            "text": [
                "It also suggests that the high quality of the pretrained embedding models may be one of the keyfactors for our generally very strong results because Ridge BV heavily relies on lexical signals.",
                "In line with that, we found in a supplemental experiment that not using pre-trained embeddings but instead learning them during training significantly reduces performance, e.g., by over 15%-points for the GRU on SE07.",
                "We now compare our best performing model against previously reported results for the SE07 corpus.",
                "Table 5 provides the performance of the winning system of the original shared task (WIN-NER; #REF ), the IAA as reported by the organizers #TARGET_REF , the performance by #REF , the highest one reported for this data set so far (BECK), as well as the results of our GRU from the 10×10-CV.",
                "As can be seen, the GRU established a new state-of-the-art result and even achieves superhuman performance."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "It also suggests that the high quality of the pretrained embedding models may be one of the keyfactors for our generally very strong results because Ridge BV heavily relies on lexical signals. In line with that, we found in a supplemental experiment that not using pre-trained embeddings but instead learning them during training significantly reduces performance, e.g., by over 15%-points for the GRU on SE07. We now compare our best performing model against previously reported results for the SE07 corpus. Table 5 provides the performance of the winning system of the original shared task (WIN-NER; #REF ), the IAA as reported by the organizers #TARGET_REF , the performance by #REF , the highest one reported for this data set so far (BECK), as well as the results of our GRU from the 10×10-CV. As can be seen, the GRU established a new state-of-the-art result and even achieves superhuman performance.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We now compare our best performing model against previously reported results for the SE07 corpus.\", \"Table 5 provides the performance of the winning system of the original shared task (WIN-NER; #REF ), the IAA as reported by the organizers #TARGET_REF , the performance by #REF , the highest one reported for this data set so far (BECK), as well as the results of our GRU from the 10\\u00d710-CV.\"]}"
    },
    {
        "gold": {
            "text": [
                "Counterintuitively, we found that all DL approaches performed well under every experimental condition.",
                "Our proposed GRU model even established a novel state-of-the-art result on the #REF test set #TARGET_REF outperforming human reliability.",
                "Moreover, it has been frequently argued that pre-trained word embeddings do not comprise sufficient affective information to be used verbatim in emotion analysis.",
                "We here provided evidence that in actuality the opposite holds-high-quality pre-trained word embeddings are instrumental in achieving strong results in low-resource scenarios and largely boost performance independent of model type.",
                "Hence, this contribution pointed out two obstructive misconceptions thus opening up DL for applications in low-resource scenarios."
            ],
            "label": [
                "USE",
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Counterintuitively, we found that all DL approaches performed well under every experimental condition. Our proposed GRU model even established a novel state-of-the-art result on the #REF test set #TARGET_REF outperforming human reliability. Moreover, it has been frequently argued that pre-trained word embeddings do not comprise sufficient affective information to be used verbatim in emotion analysis. We here provided evidence that in actuality the opposite holds-high-quality pre-trained word embeddings are instrumental in achieving strong results in low-resource scenarios and largely boost performance independent of model type. Hence, this contribution pointed out two obstructive misconceptions thus opening up DL for applications in low-resource scenarios.",
        "output": "{\"label\": [\"USE\", \"DIFFER\"], \"context\": [\"Our proposed GRU model even established a novel state-of-the-art result on the #REF test set #TARGET_REF outperforming human reliability.\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 5 provides the performance of the winning system of the original shared task (WIN-NER; #REF ), the IAA as reported by the organizers (#REF) , the performance by #REF , the highest one reported for this data set so far (BECK), as well as the results of our GRU from the 10×10-CV.",
                "As can be seen, the GRU established a new state-of-the-art result and even achieves superhuman performance.",
                "This may sound improbable at first glance.",
                "However, #TARGET_REF employ a rather weak notion of human performance which is-broadly speaking-based on the reliability of a single human rater.",
                "5 Interestingly, the GRU shows particularly large improvements over human performance for categories where the IAA is low (anger, disgust, and surprise) which might be an effect of the additional supervision introduced by multi-task learning."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "Table 5 provides the performance of the winning system of the original shared task (WIN-NER; #REF ), the IAA as reported by the organizers (#REF) , the performance by #REF , the highest one reported for this data set so far (BECK), as well as the results of our GRU from the 10×10-CV. As can be seen, the GRU established a new state-of-the-art result and even achieves superhuman performance. This may sound improbable at first glance. However, #TARGET_REF employ a rather weak notion of human performance which is-broadly speaking-based on the reliability of a single human rater. 5 Interestingly, the GRU shows particularly large improvements over human performance for categories where the IAA is low (anger, disgust, and surprise) which might be an effect of the additional supervision introduced by multi-task learning.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"As can be seen, the GRU established a new state-of-the-art result and even achieves superhuman performance.\", \"This may sound improbable at first glance.\", \"However, #TARGET_REF employ a rather weak notion of human performance which is-broadly speaking-based on the reliability of a single human rater.\"]}"
    },
    {
        "gold": {
            "text": [
                "Words and phrases that may directly mark the structure of a discourse have been termed CUE PttR.ASES, CLUE WORDS, DISCOURSE MAI:tKERS~ arid DISCOURSE PARTICLES [3, #TARGET_REF 14, 17, 19] .",
                "Some exarnpies are 'now', which marks the introduction of a new subtopic or return to a previous one, 'incidentally' and 'by the way', which indicate the beginning of a digression, and 'anyway' and 'in any case', which indicate return from a digression.",
                "In a previous study [8] , we noted that such terms are potentially ambiguous between DISCOURSE and SENTENTIAL uses [18] .",
                "So, 'now' may be used as a temporal adverbial as well as a discourse marker, 'incidentally' may also function as an adverbial, and other cue phrases similarly have one or more senses in addition to their function as markers of discourse structure.",
                "Based upon an empiricM study of 'now' in recorded speech, we proposed that such discourse and sentential uses of cue phrases can be disambiguated intonationally."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Words and phrases that may directly mark the structure of a discourse have been termed CUE PttR.ASES, CLUE WORDS, DISCOURSE MAI:tKERS~ arid DISCOURSE PARTICLES [3, #TARGET_REF 14, 17, 19] . Some exarnpies are 'now', which marks the introduction of a new subtopic or return to a previous one, 'incidentally' and 'by the way', which indicate the beginning of a digression, and 'anyway' and 'in any case', which indicate return from a digression. In a previous study [8] , we noted that such terms are potentially ambiguous between DISCOURSE and SENTENTIAL uses [18] . So, 'now' may be used as a temporal adverbial as well as a discourse marker, 'incidentally' may also function as an adverbial, and other cue phrases similarly have one or more senses in addition to their function as markers of discourse structure. Based upon an empiricM study of 'now' in recorded speech, we proposed that such discourse and sentential uses of cue phrases can be disambiguated intonationally.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Words and phrases that may directly mark the structure of a discourse have been termed CUE PttR.ASES, CLUE WORDS, DISCOURSE MAI:tKERS~ arid DISCOURSE PARTICLES [3, #TARGET_REF 14, 17, 19] .\"]}"
    },
    {
        "gold": {
            "text": [
                "The important role that cue phrases play in understanding and generating discourse has been well documented in the computational linguistics literature.",
                "For example, by indicating the presence of a structural boundary or a relationship between parts of a discourse, cue phrases caa assist in the resolution of anaphora [5, #TARGET_REF 17] and in the identification of rhetorical relations [10, 12, 17] .",
                "Cue phrases have also been used to reduce the complexity of discourse processing and to increase textual coherence [3, 11, 21] .",
                "In Example (1) 1, interpretation of the anaphor 'it' as (correctly) co-indexed with THE SYSTEM is facilitated by the presence of the cue phrases 'say' and 'then', marking potential antecedents in '... as an EXPERT DATABASE for AN EXPERT SYSTEM ...' as structurally unavailable.",
                "2"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The important role that cue phrases play in understanding and generating discourse has been well documented in the computational linguistics literature. For example, by indicating the presence of a structural boundary or a relationship between parts of a discourse, cue phrases caa assist in the resolution of anaphora [5, #TARGET_REF 17] and in the identification of rhetorical relations [10, 12, 17] . Cue phrases have also been used to reduce the complexity of discourse processing and to increase textual coherence [3, 11, 21] . In Example (1) 1, interpretation of the anaphor 'it' as (correctly) co-indexed with THE SYSTEM is facilitated by the presence of the cue phrases 'say' and 'then', marking potential antecedents in '... as an EXPERT DATABASE for AN EXPERT SYSTEM ...' as structurally unavailable. 2",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"For example, by indicating the presence of a structural boundary or a relationship between parts of a discourse, cue phrases caa assist in the resolution of anaphora [5, #TARGET_REF 17] and in the identification of rhetorical relations [10, 12, 17] .\"]}"
    },
    {
        "gold": {
            "text": [
                "2InformMly, 'say' indicates the beginning of a discourse subtopic and 'then' signals a return from that subtopic.",
                "Previous attempts to define the set of cue phrases have typically been extensional, 3 with such lists of cue phrases then further classified as to their discourse function.",
                "For example, #REF uses a taxonomy of connectives based on that of #REF to associate with each class of cue phrases a semantic function with respect to a model of argument understanding.",
                "Grosz and Sidner #TARGET_REF classify cue phrases based on changes to the attentional stack and intentional structure found in their theory of discourse.",
                "#REF classifies cue phrases into groups based on their sentential usage (e.g. conjunctive, adverbial, and clausal markers), while #REF and #REF associate groups of cue phrases with the rhetorical relationships they signal."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "2InformMly, 'say' indicates the beginning of a discourse subtopic and 'then' signals a return from that subtopic. Previous attempts to define the set of cue phrases have typically been extensional, 3 with such lists of cue phrases then further classified as to their discourse function. For example, #REF uses a taxonomy of connectives based on that of #REF to associate with each class of cue phrases a semantic function with respect to a model of argument understanding. Grosz and Sidner #TARGET_REF classify cue phrases based on changes to the attentional stack and intentional structure found in their theory of discourse. #REF classifies cue phrases into groups based on their sentential usage (e.g. conjunctive, adverbial, and clausal markers), while #REF and #REF associate groups of cue phrases with the rhetorical relationships they signal.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Grosz and Sidner #TARGET_REF classify cue phrases based on changes to the attentional stack and intentional structure found in their theory of discourse.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF classifies cue phrases into groups based on their sentential usage (e.g. conjunctive, adverbial, and clausal markers), while #REF and #REF associate groups of cue phrases with the rhetorical relationships they signal.",
                "Finally, #REF presents a taxonomy of cue phrases based on three functions relevant to her work in language generation: knowledge organization, knowledge acquisition, and affect maintenance.",
                "Once a cue phrase has been identified, however, it is not always clear whether to interpret it as a discourse marker or not [6, #TARGET_REF 8, 18] .",
                "The texts in Exampie (2) are potentially ambiguous between a temporal reading of 'now' and a discourse interpretation:",
                "\"Now in AI our approach is to look at a knowledge base as a set of symbolic items that represent something.\""
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "#REF classifies cue phrases into groups based on their sentential usage (e.g. conjunctive, adverbial, and clausal markers), while #REF and #REF associate groups of cue phrases with the rhetorical relationships they signal. Finally, #REF presents a taxonomy of cue phrases based on three functions relevant to her work in language generation: knowledge organization, knowledge acquisition, and affect maintenance. Once a cue phrase has been identified, however, it is not always clear whether to interpret it as a discourse marker or not [6, #TARGET_REF 8, 18] . The texts in Exampie (2) are potentially ambiguous between a temporal reading of 'now' and a discourse interpretation: \"Now in AI our approach is to look at a knowledge base as a set of symbolic items that represent something.\"",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Once a cue phrase has been identified, however, it is not always clear whether to interpret it as a discourse marker or not [6, #TARGET_REF 8, 18] .\"]}"
    },
    {
        "gold": {
            "text": [
                "In future work we plan to develop additional models for discourse a~nL(l aententiel uses of multl-word cue phrases, e.g 'that reminds me', 'first o] all', 'speaking off and so on.",
                "8Our set of cue phrases was derived from extensional definitions provided by ourselves and othel~ [3, #TARGET_REF 17, 18, 21] .",
                "Tim following lexicel items, although also cue phrases, are not present in the portion of the axlch-ess examined to date: 9The address was transcribed independently of our study by a meraber of the text processing pool at AT&T Bell Laboratories.",
                "We found that 20 cite phrases had been omitted by the traalscriber: 'and', 'now', 'ok', 'so', and 'well'.",
                "Significantly, ell but two of these were termed 'discourse' uses by In comparing our judgments, we were interested in areas of disagreement as well as agreement."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In future work we plan to develop additional models for discourse a~nL(l aententiel uses of multl-word cue phrases, e.g 'that reminds me', 'first o] all', 'speaking off and so on. 8Our set of cue phrases was derived from extensional definitions provided by ourselves and othel~ [3, #TARGET_REF 17, 18, 21] . Tim following lexicel items, although also cue phrases, are not present in the portion of the axlch-ess examined to date: 9The address was transcribed independently of our study by a meraber of the text processing pool at AT&T Bell Laboratories. We found that 20 cite phrases had been omitted by the traalscriber: 'and', 'now', 'ok', 'so', and 'well'. Significantly, ell but two of these were termed 'discourse' uses by In comparing our judgments, we were interested in areas of disagreement as well as agreement.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"8Our set of cue phrases was derived from extensional definitions provided by ourselves and othel~ [3, #TARGET_REF 17, 18, 21] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Many services such as web search (#REF) , recommender systems (#REF) , targeted advertising (#REF) , and rapid disaster response (#REF) rely on the location of users to personalise information and extract actionable knowledge.",
                "Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (#REF) or some combination of these (#REFb,a) .",
                "The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users #TARGET_REF; #REF; #REF; #REF) or dialectology #REF) .",
                "In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (#REF) .",
                "Lexical dialectology is (in part) the converse of user geolocation (#REF) : given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Many services such as web search (#REF) , recommender systems (#REF) , targeted advertising (#REF) , and rapid disaster response (#REF) rely on the location of users to personalise information and extract actionable knowledge. Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (#REF) or some combination of these (#REFb,a) . The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users #TARGET_REF; #REF; #REF; #REF) or dialectology #REF) . In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (#REF) . Lexical dialectology is (in part) the converse of user geolocation (#REF) : given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users #TARGET_REF; #REF; #REF; #REF) or dialectology #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Related work on Twitter user geolocation falls into two categories: text-based and network-based methods.",
                "Text-based methods make use of the geographical biases of language use, and networkbased methods rely on the geospatial homophily of user-user interactions.",
                "In both cases, the assumption is that users who live in the same geographic area share similar features (linguistic or interactional).",
                "Three main text-based approaches are: (1) the use of gazetteers #REF) ; (2) unsupervised text clustering based on topic models or similar #TARGET_REF; #REF; #REF) ; and (3) supervised classification (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) , which unlike gazetteers can be applied to informal text and compared to topic models, scales better.",
                "The classification models often rely on less than 1% of geotagged tweets for supervision and discretise real-valued coordinates into equalsized grids (#REF ), administrative regions (#REF; #REF; #REF; #REF , or flat (#REF) or hierarchical k-d tree clusters (#REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Related work on Twitter user geolocation falls into two categories: text-based and network-based methods. Text-based methods make use of the geographical biases of language use, and networkbased methods rely on the geospatial homophily of user-user interactions. In both cases, the assumption is that users who live in the same geographic area share similar features (linguistic or interactional). Three main text-based approaches are: (1) the use of gazetteers #REF) ; (2) unsupervised text clustering based on topic models or similar #TARGET_REF; #REF; #REF) ; and (3) supervised classification (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) , which unlike gazetteers can be applied to informal text and compared to topic models, scales better. The classification models often rely on less than 1% of geotagged tweets for supervision and discretise real-valued coordinates into equalsized grids (#REF ), administrative regions (#REF; #REF; #REF; #REF , or flat (#REF) or hierarchical k-d tree clusters (#REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Three main text-based approaches are: (1) the use of gazetteers #REF) ; (2) unsupervised text clustering based on topic models or similar #TARGET_REF; #REF; #REF) ; and (3) supervised classification (#REF; #REF; #REF; #REF; #REF; #REF; #REF; #REF) , which unlike gazetteers can be applied to informal text and compared to topic models, scales better.\"]}"
    },
    {
        "gold": {
            "text": [
                "The traditional approach to dialectology is to find the geographical distribution of known lexical alternatives (e.g. you, yall and yinz: (#REF; #REF; Gonçalves and Sánchez, 2014; #REF; #REF; #REF) ), the shortcoming of which is that the alternative lexical variables must be known beforehand.",
                "There have also been attempts to automatically identify such words from geotagged documents #TARGET_REF; #REF; #REF) .",
                "The main idea is to find lexical variables that are disproportionately distributed in different locations either via model-based or statistical methods (#REF) .",
                "There is a research gap in evaluating the geolocation models in terms of their usability in retrieving dialect terms given a geographic region.",
                "We use a text-based neural approach trained on geotagged Twitter messages that: (a) given a geographical region, identifies the associated lexical terms; and (b) given a text, predicts its location."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The traditional approach to dialectology is to find the geographical distribution of known lexical alternatives (e.g. you, yall and yinz: (#REF; #REF; Gonçalves and Sánchez, 2014; #REF; #REF; #REF) ), the shortcoming of which is that the alternative lexical variables must be known beforehand. There have also been attempts to automatically identify such words from geotagged documents #TARGET_REF; #REF; #REF) . The main idea is to find lexical variables that are disproportionately distributed in different locations either via model-based or statistical methods (#REF) . There is a research gap in evaluating the geolocation models in terms of their usability in retrieving dialect terms given a geographic region. We use a text-based neural approach trained on geotagged Twitter messages that: (a) given a geographical region, identifies the associated lexical terms; and (b) given a text, predicts its location.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"There have also been attempts to automatically identify such words from geotagged documents #TARGET_REF; #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We use three existing Twitter user geolocation datasets: (1) GEOTEXT #TARGET_REF , (2) TWITTER-US (#REF) , and (3) TWITTER-WORLD (#REF) .",
                "These datasets have been used widely for training and evaluation of geolocation models.",
                "They are all prepartitioned into training, development and test sets.",
                "Each user is represented by the concatenation of their tweets, and labeled with the latitude/longitude of the first collected geotagged tweet in the case of GEOTEXT and TWITTER-US, and the centre of the closest city in the case of TWITTER-WORLD.",
                "1 GEOTEXT and TWITTER-US cover the continental US, and TWITTER-WORLD covers the whole world, with 9k, 449k and 1.3m users, respectively as shown in Figure 1 ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We use three existing Twitter user geolocation datasets: (1) GEOTEXT #TARGET_REF , (2) TWITTER-US (#REF) , and (3) TWITTER-WORLD (#REF) . These datasets have been used widely for training and evaluation of geolocation models. They are all prepartitioned into training, development and test sets. Each user is represented by the concatenation of their tweets, and labeled with the latitude/longitude of the first collected geotagged tweet in the case of GEOTEXT and TWITTER-US, and the centre of the closest city in the case of TWITTER-WORLD. 1 GEOTEXT and TWITTER-US cover the continental US, and TWITTER-WORLD covers the whole world, with 9k, 449k and 1.3m users, respectively as shown in Figure 1 .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use three existing Twitter user geolocation datasets: (1) GEOTEXT #TARGET_REF , (2) TWITTER-US (#REF) , and (3) TWITTER-WORLD (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The parameters are optimised using Adamx (#REF) using Lasagne/Theano (Theano Development #REF) .",
                "Following #REF and #TARGET_REF , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\").",
                "Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161.",
                "4 The results reported in Rahimi et al. (2015b; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset.",
                "While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user (#REFa) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The parameters are optimised using Adamx (#REF) using Lasagne/Theano (Theano Development #REF) . Following #REF and #TARGET_REF , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\"). Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161. 4 The results reported in Rahimi et al. (2015b; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset. While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user (#REFa) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following #REF and #TARGET_REF , we evaluated the geolocation model using mean and median error in km (\\\"Mean\\\" and \\\"Median\\\" resp.) and accuracy within 161km of the actual location (\\\"Acc@161\\\").\"]}"
    },
    {
        "gold": {
            "text": [
                "In such a framework, the generator is not directly exposed to the ground truth data, but instead learns to imitate it using global feedback from the discriminator.",
                "This has led to several attempts to use GANs for text generation, with a generator using either a recurrent neural network (RNN) #REF; #TARGET_REF; #REF) , or a Convolutional Neural Network (CNN) (#REF; #REF) .",
                "However, evaluating GANs is more difficult than evaluating LMs.",
                "While in language modeling, evaluation is based on the log-probability of a model on held-out text, this cannot be straightforwardly extended to GAN-based text generation, because the generator outputs discrete tokens, rather than a probability distribution.",
                "Currently, there is no single evaluation metric for GAN-based text generation, and existing metrics that are based on n-gram overlap are known to lack robustness and have low correlation with semantic coherence (#REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In such a framework, the generator is not directly exposed to the ground truth data, but instead learns to imitate it using global feedback from the discriminator. This has led to several attempts to use GANs for text generation, with a generator using either a recurrent neural network (RNN) #REF; #TARGET_REF; #REF) , or a Convolutional Neural Network (CNN) (#REF; #REF) . However, evaluating GANs is more difficult than evaluating LMs. While in language modeling, evaluation is based on the log-probability of a model on held-out text, this cannot be straightforwardly extended to GAN-based text generation, because the generator outputs discrete tokens, rather than a probability distribution. Currently, there is no single evaluation metric for GAN-based text generation, and existing metrics that are based on n-gram overlap are known to lack robustness and have low correlation with semantic coherence (#REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"This has led to several attempts to use GANs for text generation, with a generator using either a recurrent neural network (RNN) #REF; #TARGET_REF; #REF) , or a Convolutional Neural Network (CNN) (#REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "One solution is to perform a continuous relaxation of the GAN output, which leads to generators that emit a nearly discrete continuous distribution #TARGET_REF .",
                "This keeps the model differentiable and enables end-to-end training through the discriminator.",
                "Alternatively, SeqGAN and Leak-GAN (#REF) used policy gradient methods to overcome the differentiablity requirement.",
                "We apply our approximation to both model types.",
                "3 Evaluating GANs and LMs LM Evaluation."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "One solution is to perform a continuous relaxation of the GAN output, which leads to generators that emit a nearly discrete continuous distribution #TARGET_REF . This keeps the model differentiable and enables end-to-end training through the discriminator. Alternatively, SeqGAN and Leak-GAN (#REF) used policy gradient methods to overcome the differentiablity requirement. We apply our approximation to both model types. 3 Evaluating GANs and LMs LM Evaluation.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"One solution is to perform a continuous relaxation of the GAN output, which leads to generators that emit a nearly discrete continuous distribution #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "• N-gram overlap: #TARGET_REF : Inspired by BLEU (#REF) , this measures whether n-grams generated by the model appear in a held-out corpus.",
                "A major drawback is that this metric favors conservative models that always generate very common text (e.g., \"it is\").",
                "To mitigate this, self-BLEU has been proposed (#REF) as an additional metric, where overlap is measured between two independently sampled texts from the model.",
                "• LM score: The probability of generated text according to a pre-trained LM.",
                "This has the same problem of favoring conservative models."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "• N-gram overlap: #TARGET_REF : Inspired by BLEU (#REF) , this measures whether n-grams generated by the model appear in a held-out corpus. A major drawback is that this metric favors conservative models that always generate very common text (e.g., \"it is\"). To mitigate this, self-BLEU has been proposed (#REF) as an additional metric, where overlap is measured between two independently sampled texts from the model. • LM score: The probability of generated text according to a pre-trained LM. This has the same problem of favoring conservative models.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"\\u2022 N-gram overlap: #TARGET_REF : Inspired by BLEU (#REF) , this measures whether n-grams generated by the model appear in a held-out corpus.\"]}"
    },
    {
        "gold": {
            "text": [
                "The inputs to an RNN at time step t, are the state vector h t and the current input token x t .",
                "The output token (one-hot) is denoted by o t .",
                "In RNNbased GANs, the previous output token is used at inference time as the input x t #REF; #TARGET_REF; #REF) .",
                "In contrast, when evaluating with BPC or perplexity, the gold token x t is given as input.",
                "Hence, LM-based evaluation neutralizes the problem of exposure bias addressed by GANs."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The inputs to an RNN at time step t, are the state vector h t and the current input token x t . The output token (one-hot) is denoted by o t . In RNNbased GANs, the previous output token is used at inference time as the input x t #REF; #TARGET_REF; #REF) . In contrast, when evaluating with BPC or perplexity, the gold token x t is given as input. Hence, LM-based evaluation neutralizes the problem of exposure bias addressed by GANs.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In RNNbased GANs, the previous output token is used at inference time as the input x t #REF; #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The rest of the corpus was automatically validated synthetic material using general data from Leipzig (#REF Engine customization The data was cleaned using the Bicleaner tool (Sánchez-#REF) .",
                "The data was lowercased and extra embeddings were added in order to keep the case information.",
                "The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE #TARGET_REF approach.",
                "The models were trained with multi-domain data and we improved performance following a domainmixing approach (#REF) .",
                "The domain information was prepended with special tokens for each target sequence."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The rest of the corpus was automatically validated synthetic material using general data from Leipzig (#REF Engine customization The data was cleaned using the Bicleaner tool (Sánchez-#REF) . The data was lowercased and extra embeddings were added in order to keep the case information. The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE #TARGET_REF approach. The models were trained with multi-domain data and we improved performance following a domainmixing approach (#REF) . The domain information was prepended with special tokens for each target sequence.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE #TARGET_REF approach.\"]}"
    },
    {
        "gold": {
            "text": [
                "The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE #TARGET_REF approach.",
                "The models were trained with multi-domain data and we improved performance following a domainmixing approach (#REF) .",
                "The domain information was prepended with special tokens for each target sequence.",
                "The domain prediction was based only on the source as the extra token was added at target-side and there was no need for apriori domain information.",
                "This approach allowed the model to improve the quality for each domain."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE #TARGET_REF approach. The models were trained with multi-domain data and we improved performance following a domainmixing approach (#REF) . The domain information was prepended with special tokens for each target sequence. The domain prediction was based only on the source as the extra token was added at target-side and there was no need for apriori domain information. This approach allowed the model to improve the quality for each domain.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The tokenization used was the one provided by OpenNMT 3 and words were divided in subwords according to the BPE #TARGET_REF approach.\"]}"
    },
    {
        "gold": {
            "text": [
                "Dependency parsing is a topic that has engendered increasing interest in recent years.",
                "One promising approach is based on exact search and structural learning #TARGET_REF; #REF) .",
                "In this work we also pursue this approach.",
                "Our system makes no provisions for non-projective edges.",
                "In contrast to previous work, we aim to learn labelled dependency trees at one fell swoop."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Dependency parsing is a topic that has engendered increasing interest in recent years. One promising approach is based on exact search and structural learning #TARGET_REF; #REF) . In this work we also pursue this approach. Our system makes no provisions for non-projective edges. In contrast to previous work, we aim to learn labelled dependency trees at one fell swoop.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Dependency parsing is a topic that has engendered increasing interest in recent years.\", \"One promising approach is based on exact search and structural learning #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "In our approach, we adopt #REF 's bottomup chart-parsing algorithm in #TARGET_REF 's formulation, which finds the best projective dependency tree for an input string",
                "We assume that every possible headdependent pair ¨ is described by a feature vector",
                "Eisner's algorithm achieves optimal tree packing by storing partial structures in two matrices and .",
                "First the diagonals of the matrices are initiated with 0; then all other cells are filled according to eqs. (1) and (2) and their symmetric variants."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "In our approach, we adopt #REF 's bottomup chart-parsing algorithm in #TARGET_REF 's formulation, which finds the best projective dependency tree for an input string We assume that every possible headdependent pair ¨ is described by a feature vector Eisner's algorithm achieves optimal tree packing by storing partial structures in two matrices and . First the diagonals of the matrices are initiated with 0; then all other cells are filled according to eqs. (1) and (2) and their symmetric variants.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"In our approach, we adopt #REF 's bottomup chart-parsing algorithm in #TARGET_REF 's formulation, which finds the best projective dependency tree for an input string\"]}"
    },
    {
        "gold": {
            "text": [
                "In deriving features, we used all information given in the treebanks, i.e. words (w), fine-grained POS tags (fp), combinations of lemmas and coarsegrained POS tags (lcp), and whether two tokens agree 1 (agr = yes, no, don't know).",
                "We essentially employ the same set of features as #TARGET_REF :"
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "In deriving features, we used all information given in the treebanks, i.e. words (w), fine-grained POS tags (fp), combinations of lemmas and coarsegrained POS tags (lcp), and whether two tokens agree 1 (agr = yes, no, don't know). We essentially employ the same set of features as #TARGET_REF :",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We essentially employ the same set of features as #TARGET_REF :\"]}"
    },
    {
        "gold": {
            "text": [
                "It tries to change weights as little as possible (passiveness), while ensuring that (1)",
                "1 Agreement was computed from morphological features, viz. gender, number and person, and case.",
                "In languages with subject-verb agreement, we added a nominative case feature to finite verbs.",
                "In Basque, agreement is case-specific (absolutive, dative, ergative, other case).",
                "Having a closed-form solution, OPAL is easier to implement and more efficient than the MIRA algorithm used by #TARGET_REF , although it achieves a performance comparable to MIRA's on many problems (#REF) ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "It tries to change weights as little as possible (passiveness), while ensuring that (1) 1 Agreement was computed from morphological features, viz. gender, number and person, and case. In languages with subject-verb agreement, we added a nominative case feature to finite verbs. In Basque, agreement is case-specific (absolutive, dative, ergative, other case). Having a closed-form solution, OPAL is easier to implement and more efficient than the MIRA algorithm used by #TARGET_REF , although it achieves a performance comparable to MIRA's on many problems (#REF) .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Having a closed-form solution, OPAL is easier to implement and more efficient than the MIRA algorithm used by #TARGET_REF , although it achieves a performance comparable to MIRA's on many problems (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "So far, the presented system, which follows closely the approach of #TARGET_REF , only predicts unlabelled dependency trees.",
                "To derive a labeling, we departed from their approach: We split each feature along the deprel label dimension, so that each deprel U is associated with its own feature vector (cf.",
                "eq. (4), where V is the tensor product and",
                "In parsing, we only consider the best deprel label.",
                "On its own, this simple approach led to a severe degradation of performance, so we took a step back by re-introducing features for unlabelled trees."
            ],
            "label": [
                "DIFFER",
                "MOT"
            ],
            "context": [
                1,
                1,
                1,
                1,
                0
            ]
        },
        "input": "So far, the presented system, which follows closely the approach of #TARGET_REF , only predicts unlabelled dependency trees. To derive a labeling, we departed from their approach: We split each feature along the deprel label dimension, so that each deprel U is associated with its own feature vector (cf. eq. (4), where V is the tensor product and In parsing, we only consider the best deprel label. On its own, this simple approach led to a severe degradation of performance, so we took a step back by re-introducing features for unlabelled trees.",
        "output": "{\"label\": [\"DIFFER\", \"MOT\"], \"context\": [\"So far, the presented system, which follows closely the approach of #TARGET_REF , only predicts unlabelled dependency trees.\", \"To derive a labeling, we departed from their approach: We split each feature along the deprel label dimension, so that each deprel U is associated with its own feature vector (cf.\", \"eq. (4), where V is the tensor product and\", \"In parsing, we only consider the best deprel label.\"]}"
    },
    {
        "gold": {
            "text": [
                "As an alternative, we contrast this design with a direct classification approach that relies only on textual examples and effectively combines the dialogue policy with NLU.",
                "In our case study evaluation, we find that this approach offers superior performance, owing to the high frequency of NLU errors in the two step pipeline.",
                "The research presented in this paper extends our previous work.",
                "As we summarize in Section 2, this paper relies on the same data set and evaluation metric as #TARGET_REF , which reports results for learned policies based on maximum entropy models.",
                "In this paper, we add a comparison to a hand-authored policy (Rules) and a new policy based on relevance models (RM)."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "As an alternative, we contrast this design with a direct classification approach that relies only on textual examples and effectively combines the dialogue policy with NLU. In our case study evaluation, we find that this approach offers superior performance, owing to the high frequency of NLU errors in the two step pipeline. The research presented in this paper extends our previous work. As we summarize in Section 2, this paper relies on the same data set and evaluation metric as #TARGET_REF , which reports results for learned policies based on maximum entropy models. In this paper, we add a comparison to a hand-authored policy (Rules) and a new policy based on relevance models (RM).",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"As we summarize in Section 2, this paper relies on the same data set and evaluation metric as #TARGET_REF , which reports results for learned policies based on maximum entropy models.\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, the label elicit-whq-tellmemoreabouttheincident is assigned to the user's utterance of can you tell me what you know of the incident? in Figure 1 .",
                "The system also defines a different set of 96 unique SAs (responses) for the Amani character.",
                "We perform our experiments and evaluation using an existing set of 19 annotated Amani dialogues #TARGET_REF .",
                "The dialogues were collected through teletype-based role play.",
                "Each dialogue turn includes a single user utterance followed by the response chosen by a human role player in the role of Amani."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "For example, the label elicit-whq-tellmemoreabouttheincident is assigned to the user's utterance of can you tell me what you know of the incident? in Figure 1 . The system also defines a different set of 96 unique SAs (responses) for the Amani character. We perform our experiments and evaluation using an existing set of 19 annotated Amani dialogues #TARGET_REF . The dialogues were collected through teletype-based role play. Each dialogue turn includes a single user utterance followed by the response chosen by a human role player in the role of Amani.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"We perform our experiments and evaluation using an existing set of 19 annotated Amani dialogues #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate the dialogue policies learned in each of our experimental conditions through 19-fold cross-validation of our set of 19 dialogues.",
                "In each fold, we hold out one dialogue and use the remaining 18 dialogues as training data.",
                "To measure the performance of the dialogue policy, we follow the approach of #TARGET_REF , which counts an automatically produced system SA as correct if that SA was chosen by at least one referee for that dialogue turn in the data set.",
                "We then count the proportion of the correct SAs among all the SAs produced across all 19 dialogues, and use this measure of weak accuracy to score competing dialogue policies.",
                "We can use the weak accuracy of one referee, measured against all the others, to establish a performance ceiling for this metric."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We evaluate the dialogue policies learned in each of our experimental conditions through 19-fold cross-validation of our set of 19 dialogues. In each fold, we hold out one dialogue and use the remaining 18 dialogues as training data. To measure the performance of the dialogue policy, we follow the approach of #TARGET_REF , which counts an automatically produced system SA as correct if that SA was chosen by at least one referee for that dialogue turn in the data set. We then count the proportion of the correct SAs among all the SAs produced across all 19 dialogues, and use this measure of weak accuracy to score competing dialogue policies. We can use the weak accuracy of one referee, measured against all the others, to establish a performance ceiling for this metric.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"To measure the performance of the dialogue policy, we follow the approach of #TARGET_REF , which counts an automatically produced system SA as correct if that SA was chosen by at least one referee for that dialogue turn in the data set.\"]}"
    },
    {
        "gold": {
            "text": [
                "We begin by summarizing our research setting, data set, and evaluation metric.",
                "We refer the reader to #TARGET_REF for additional details.",
                "We use an existing virtual human scenario designed for Tactical Questioning (TACQ) (#REF) , where military personnel interview individuals for information of military value.",
                "TACQ characters are designed to be non-cooperative at times.",
                "They may answer some of the interviewer's questions, but either lie or refuse to answer others until certain conditions are met ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We begin by summarizing our research setting, data set, and evaluation metric. We refer the reader to #TARGET_REF for additional details. We use an existing virtual human scenario designed for Tactical Questioning (TACQ) (#REF) , where military personnel interview individuals for information of military value. TACQ characters are designed to be non-cooperative at times. They may answer some of the interviewer's questions, but either lie or refuse to answer others until certain conditions are met .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"We refer the reader to #TARGET_REF for additional details.\"]}"
    },
    {
        "gold": {
            "text": [
                "In the experiments reported here, the user's utterance may be provided to the DM either directly as text or using a SA label.",
                "We call the DM's decision process a dialogue policy.",
                "The system builders' intended policy for Amani is detailed in #TARGET_REF .",
                "Because Amani has only a fixed set of system responses, the policy problem looks like a traditional classification task.",
                "However, there are two sources of uncertainty that complicate the task."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In the experiments reported here, the user's utterance may be provided to the DM either directly as text or using a SA label. We call the DM's decision process a dialogue policy. The system builders' intended policy for Amani is detailed in #TARGET_REF . Because Amani has only a fixed set of system responses, the policy problem looks like a traditional classification task. However, there are two sources of uncertainty that complicate the task.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The system builders' intended policy for Amani is detailed in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Firstly, the mapping between the user's utterance and an appropriate system SA is often one-tomany.",
                "In our data set, 6 referees independently linked each user utterance to the best system SA response.",
                "In Figure 1 , we provide an example in which three different system SAs were selected by the 6 referees.",
                "In other cases, up to 6 different system SAs were selected #TARGET_REF .",
                "Our first experimental question is therefore: how well can a dialogue policy select an appropriate system SA, if it is provided with an accurate user SA?"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Firstly, the mapping between the user's utterance and an appropriate system SA is often one-tomany. In our data set, 6 referees independently linked each user utterance to the best system SA response. In Figure 1 , we provide an example in which three different system SAs were selected by the 6 referees. In other cases, up to 6 different system SAs were selected #TARGET_REF . Our first experimental question is therefore: how well can a dialogue policy select an appropriate system SA, if it is provided with an accurate user SA?",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In other cases, up to 6 different system SAs were selected #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In each fold, we hold out one dialogue and use the remaining 18 dialogues as training data.",
                "To measure the performance of the dialogue policy, we follow the approach of #REF , which counts an automatically produced system SA as correct if that SA was chosen by at least one referee for that dialogue turn in the data set.",
                "We then count the proportion of the correct SAs among all the SAs produced across all 19 dialogues, and use this measure of weak accuracy to score competing dialogue policies.",
                "We can use the weak accuracy of one referee, measured against all the others, to establish a performance ceiling for this metric.",
                "(We do not expect that an automatic system would outperform a human referee.) This score is .79; see #TARGET_REF for discussion."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "In each fold, we hold out one dialogue and use the remaining 18 dialogues as training data. To measure the performance of the dialogue policy, we follow the approach of #REF , which counts an automatically produced system SA as correct if that SA was chosen by at least one referee for that dialogue turn in the data set. We then count the proportion of the correct SAs among all the SAs produced across all 19 dialogues, and use this measure of weak accuracy to score competing dialogue policies. We can use the weak accuracy of one referee, measured against all the others, to establish a performance ceiling for this metric. (We do not expect that an automatic system would outperform a human referee.) This score is .79; see #TARGET_REF for discussion.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"(We do not expect that an automatic system would outperform a human referee.) This score is .79; see #TARGET_REF for discussion.\"]}"
    },
    {
        "gold": {
            "text": [
                "For the MaxEnt policy, a score of .71 is achieved with \"gold\" SAs, and a lower .57 with run-time SAs.",
                "Note that .71 is an inferior performance to the .79 achieved with G-SA/Rules, indicating that MaxEnt does not learn a policy as effective as the hand-authored Rules, even if it is trained and evaluated on gold SA labels.",
                "As previously reported in #TARGET_REF , a performance of .66 is achieved with the MaxEnt policy when trained on text-based features.",
                "It is interesting to see here, however, that this .66 performance is significantly higher than the .58 that is achieved using Rules together with run-time SAs.",
                "In fact, the accuracy of the NLU-SA labels in this data set, with respect to the gold SAs, is 53%."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "For the MaxEnt policy, a score of .71 is achieved with \"gold\" SAs, and a lower .57 with run-time SAs. Note that .71 is an inferior performance to the .79 achieved with G-SA/Rules, indicating that MaxEnt does not learn a policy as effective as the hand-authored Rules, even if it is trained and evaluated on gold SA labels. As previously reported in #TARGET_REF , a performance of .66 is achieved with the MaxEnt policy when trained on text-based features. It is interesting to see here, however, that this .66 performance is significantly higher than the .58 that is achieved using Rules together with run-time SAs. In fact, the accuracy of the NLU-SA labels in this data set, with respect to the gold SAs, is 53%.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"As previously reported in #TARGET_REF , a performance of .66 is achieved with the MaxEnt policy when trained on text-based features.\"]}"
    },
    {
        "gold": {
            "text": [
                "In fact, the reward function is one of the most handcoded aspects in RL (#REF) .",
                "In this paper we propose a new method for meta-evaluation of the objective function.",
                "We bring together two strands of research: one strand uses Reinforcement Learning to automatically optimise dialogue strategies, e.g. (#REF) , (#REF) , (#REFa; #TARGET_REF ; the other other focuses on automatic evaluation of dialogue strategies, e.g. the PARADISE framework (#REF) , and meta-evaluation of dialogue metrics, e.g. (Engelbrecht and Möller, 2007; #REF) .",
                "Clearly, automatic optimisation and evaluation of dialogue policies, as well as quality control of the objective function, are closely inter-related problems: how can we make sure that we optimise a system according to real users' preferences?",
                "In particular, we construct a data-driven objective function using the PARADISE framework, and use it for automatic dialogue strategy optimisation following pioneering work by (#REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In fact, the reward function is one of the most handcoded aspects in RL (#REF) . In this paper we propose a new method for meta-evaluation of the objective function. We bring together two strands of research: one strand uses Reinforcement Learning to automatically optimise dialogue strategies, e.g. (#REF) , (#REF) , (#REFa; #TARGET_REF ; the other other focuses on automatic evaluation of dialogue strategies, e.g. the PARADISE framework (#REF) , and meta-evaluation of dialogue metrics, e.g. (Engelbrecht and Möller, 2007; #REF) . Clearly, automatic optimisation and evaluation of dialogue policies, as well as quality control of the objective function, are closely inter-related problems: how can we make sure that we optimise a system according to real users' preferences? In particular, we construct a data-driven objective function using the PARADISE framework, and use it for automatic dialogue strategy optimisation following pioneering work by (#REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We bring together two strands of research: one strand uses Reinforcement Learning to automatically optimise dialogue strategies, e.g. (#REF) , (#REF) , (#REFa; #TARGET_REF ; the other other focuses on automatic evaluation of dialogue strategies, e.g. the PARADISE framework (#REF) , and meta-evaluation of dialogue metrics, e.g. (Engelbrecht and M\\u00f6ller, 2007; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Furthermore, it is not clear how they perform when being used for automatic strategy optimisation within the RL framework.",
                "In the following we evaluate different aspects of an objective function obtained from Wizard-of-Oz (WOZ) data #TARGET_REF .",
                "We proceed as follows: The next Section shortly summarises the overall dialogue system design.",
                "In Section 3. we test the model stability in a test-retest comparison across different user populations and data sets.",
                "In Section 4. we measure prediction accuracy."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Furthermore, it is not clear how they perform when being used for automatic strategy optimisation within the RL framework. In the following we evaluate different aspects of an objective function obtained from Wizard-of-Oz (WOZ) data #TARGET_REF . We proceed as follows: The next Section shortly summarises the overall dialogue system design. In Section 3. we test the model stability in a test-retest comparison across different user populations and data sets. In Section 4. we measure prediction accuracy.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In the following we evaluate different aspects of an objective function obtained from Wizard-of-Oz (WOZ) data #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The structure of information seeking dialogues consists of an information acquisition dialogue and an information presentation sub-dialogue (see Figure 1) .",
                "For information acquisition the task of the dialogue policy is to gather 'enough' search constraints from the user, and then, 'at the right time', to start the information presentation phase where the task is to present 'the right amount' of information -either on the screen or listing the items verbally. What this actually means depends on the dialogue context and the preferences of our users as reflected in the objective function.",
                "We therefore formulate dialogue learning as a hierarchical optimisation problem #TARGET_REF .",
                "The applied objective function follows this structure as well.",
                "Figure 1 : Hierarchical dialogue structure for information seeking multimodal systems."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The structure of information seeking dialogues consists of an information acquisition dialogue and an information presentation sub-dialogue (see Figure 1) . For information acquisition the task of the dialogue policy is to gather 'enough' search constraints from the user, and then, 'at the right time', to start the information presentation phase where the task is to present 'the right amount' of information -either on the screen or listing the items verbally. What this actually means depends on the dialogue context and the preferences of our users as reflected in the objective function. We therefore formulate dialogue learning as a hierarchical optimisation problem #TARGET_REF . The applied objective function follows this structure as well. Figure 1 : Hierarchical dialogue structure for information seeking multimodal systems.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We therefore formulate dialogue learning as a hierarchical optimisation problem #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "2. I had no problems finding the information I wanted.",
                "We choose Task Ease as the ultimate measure to be optimised following (#REF) 's principle of the least effort which says: \"All things being equal, agents try to minimize their effort in doing what they intend to do\".",
                "The PARADISE regression model is constructed from 3 different corpora: the SAMMIE WOZ experiment (#REF) , and the iTalk system used for the user tests #TARGET_REF running the supervised baseline policy and the RL-based policy.",
                "By replicating the regression model on different data sets we test whether the automatic estimate of Task Ease generalises beyond the conditions and assumptions of a particular experimental design.",
                "The resulting models are shown in Equations 1-3 , where T askEase W OZ is the regression model obtained from the WOZ data, T askEase SL is obtained from the user test data running the supervised policy, and T askEase RL is obtained from the user test data running the RL-based policy."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "2. I had no problems finding the information I wanted. We choose Task Ease as the ultimate measure to be optimised following (#REF) 's principle of the least effort which says: \"All things being equal, agents try to minimize their effort in doing what they intend to do\". The PARADISE regression model is constructed from 3 different corpora: the SAMMIE WOZ experiment (#REF) , and the iTalk system used for the user tests #TARGET_REF running the supervised baseline policy and the RL-based policy. By replicating the regression model on different data sets we test whether the automatic estimate of Task Ease generalises beyond the conditions and assumptions of a particular experimental design. The resulting models are shown in Equations 1-3 , where T askEase W OZ is the regression model obtained from the WOZ data, T askEase SL is obtained from the user test data running the supervised policy, and T askEase RL is obtained from the user test data running the RL-based policy.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The PARADISE regression model is constructed from 3 different corpora: the SAMMIE WOZ experiment (#REF) , and the iTalk system used for the user tests #TARGET_REF running the supervised baseline policy and the RL-based policy.\"]}"
    },
    {
        "gold": {
            "text": [
                "In the following the overall method is shortly summarised. Please see #TARGET_REF; #REF) for details.",
                "1. We obtain an objective function from the WOZ data of (#REF) according to the PARADISE framework.",
                "In PARADISE multivariate linear regression is applied to experimental dialogue data in order to develop predictive models of user preferences (obtained from questionnaires) as a linear weighted function of dialogue performance measures (such as dialogue length).",
                "This predictive model is used to automatically evaluate dialogues.",
                "For RL this function is used as the \"reward\" for training."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In the following the overall method is shortly summarised. Please see #TARGET_REF; #REF) for details. 1. We obtain an objective function from the WOZ data of (#REF) according to the PARADISE framework. In PARADISE multivariate linear regression is applied to experimental dialogue data in order to develop predictive models of user preferences (obtained from questionnaires) as a linear weighted function of dialogue performance measures (such as dialogue length). This predictive model is used to automatically evaluate dialogues. For RL this function is used as the \"reward\" for training.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In the following the overall method is shortly summarised. Please see #TARGET_REF; #REF) for details.\"]}"
    },
    {
        "gold": {
            "text": [
                "In previous work we showed that the RL-based policy significantly outperforms the supervised policy in terms of improved user ratings and dialogue performance measures #TARGET_REF .",
                "Here, we test the relationship between improved user ratings and dialogue behaviour, i.e. we investigate which factors lead the users to give higher scores, and whether this was correctly reflected in the original reward function.",
                "We concentrate on the information presentation phase, since there is a simple two-way relationship between user scores and the number of presented items.",
                "To estimate this relationship we use curve fitting, which is used as an alternative model to linear regression in cases where the relationship between two variables can also be non-linear.",
                "For each presentation mode (verbal vs. multimodal) we select the (simplest) model with the closest fit to the data (R 2 )."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In previous work we showed that the RL-based policy significantly outperforms the supervised policy in terms of improved user ratings and dialogue performance measures #TARGET_REF . Here, we test the relationship between improved user ratings and dialogue behaviour, i.e. we investigate which factors lead the users to give higher scores, and whether this was correctly reflected in the original reward function. We concentrate on the information presentation phase, since there is a simple two-way relationship between user scores and the number of presented items. To estimate this relationship we use curve fitting, which is used as an alternative model to linear regression in cases where the relationship between two variables can also be non-linear. For each presentation mode (verbal vs. multimodal) we select the (simplest) model with the closest fit to the data (R 2 ).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In previous work we showed that the RL-based policy significantly outperforms the supervised policy in terms of improved user ratings and dialogue performance measures #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "For verbal presentation both learning schemes (RL and SL) were able to learn a policy from the WOZ data which received consistently good ratings from the users (between 6-5 for RL, and 5-4 for SL on a 7-point Likert scale).",
                "For multimodal presentation the WOZ objective function has a turning point at 14.8 (see Figure 3) .",
                "The RL-based policy learned to maximise the returned reward by displaying no more than 15 items.",
                "The SL policy, in contrast, did not learn an upper boundary for when to show items on the screen (since the wizards did not follow a specific pattern, #TARGET_REF ).",
                "When relating number of items to user scores, the RL policy produces a linear (slightly declining) line between 7 and 6 (Table 3 , bottom right), indicating that the applied policy reflected the users' preferences."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "For verbal presentation both learning schemes (RL and SL) were able to learn a policy from the WOZ data which received consistently good ratings from the users (between 6-5 for RL, and 5-4 for SL on a 7-point Likert scale). For multimodal presentation the WOZ objective function has a turning point at 14.8 (see Figure 3) . The RL-based policy learned to maximise the returned reward by displaying no more than 15 items. The SL policy, in contrast, did not learn an upper boundary for when to show items on the screen (since the wizards did not follow a specific pattern, #TARGET_REF ). When relating number of items to user scores, the RL policy produces a linear (slightly declining) line between 7 and 6 (Table 3 , bottom right), indicating that the applied policy reflected the users' preferences.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The SL policy, in contrast, did not learn an upper boundary for when to show items on the screen (since the wizards did not follow a specific pattern, #TARGET_REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "We use XLNet #TARGET_REF to attempt to capture long range language dependencies.",
                "At the time of this writing, XLNet provides the best accuracy for many downstream tasks that require language modeling pre-training, including question-answering, text classification, and other natural language understanding tasks.",
                "We also attempt to take advantage of a transformer's parallel properties to make some performance optimizations when re-scoring our lattices."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "We use XLNet #TARGET_REF to attempt to capture long range language dependencies. At the time of this writing, XLNet provides the best accuracy for many downstream tasks that require language modeling pre-training, including question-answering, text classification, and other natural language understanding tasks. We also attempt to take advantage of a transformer's parallel properties to make some performance optimizations when re-scoring our lattices.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use XLNet #TARGET_REF to attempt to capture long range language dependencies.\"]}"
    },
    {
        "gold": {
            "text": [
                "An acoustic model and n-gram language model are trained to provide a baseline word-error rate.",
                "We use a library that contains a pre-trained version of XLNet, an implementation of the transformer-XL architecture [14] .",
                "The model is fairly large with 110M parameters.",
                "It was previously trained on #REF and English Wikipedia which have 13GB of plain text combined #TARGET_REF .",
                "We run a transfer learning step using PyTorch on the TED-LIUM dataset."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                1,
                0
            ]
        },
        "input": "An acoustic model and n-gram language model are trained to provide a baseline word-error rate. We use a library that contains a pre-trained version of XLNet, an implementation of the transformer-XL architecture [14] . The model is fairly large with 110M parameters. It was previously trained on #REF and English Wikipedia which have 13GB of plain text combined #TARGET_REF . We run a transfer learning step using PyTorch on the TED-LIUM dataset.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use a library that contains a pre-trained version of XLNet, an implementation of the transformer-XL architecture [14] .\", \"It was previously trained on #REF and English Wikipedia which have 13GB of plain text combined #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In the lattice some very good answers exist.",
                "However, our results in table 1 show how difficult it is to make a dent in the WER with such a large XLNet model.",
                "The RNNLM still gives a much better score.",
                "We suspect that this is due to a few things: firstly, the XLNet is 110M parameters and was trained on approximately 13GB of text compared to 25MB worth of text for TED-LIUM.",
                "Given the size of the model, and the fact that it was pre-trained on 512 TPUs #TARGET_REF , we expect that training for 20 epochs on TED-LIUM's text is not enough to overcome the differences between written text and conversational speech."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "In the lattice some very good answers exist. However, our results in table 1 show how difficult it is to make a dent in the WER with such a large XLNet model. The RNNLM still gives a much better score. We suspect that this is due to a few things: firstly, the XLNet is 110M parameters and was trained on approximately 13GB of text compared to 25MB worth of text for TED-LIUM. Given the size of the model, and the fact that it was pre-trained on 512 TPUs #TARGET_REF , we expect that training for 20 epochs on TED-LIUM's text is not enough to overcome the differences between written text and conversational speech.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Given the size of the model, and the fact that it was pre-trained on 512 TPUs #TARGET_REF , we expect that training for 20 epochs on TED-LIUM's text is not enough to overcome the differences between written text and conversational speech.\"]}"
    },
    {
        "gold": {
            "text": [
                "XLNet is a generalized auto-regressive model that can be used for language modeling based on the transformer-XL architecture #TARGET_REF .",
                "This means that the outputs of XLNet depend strictly on the previous outputs.",
                "This is different from other state-of-the-art language models like BERT (Bidirectional Encoder Representations from Transformers) which rely on conditioning the probabilities given surrounding words.",
                "In BERT, the model tries to predict a masked word by looking at all surrounding unmasked words ( figure 3 ).",
                "The concept of Fig. 3 ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "XLNet is a generalized auto-regressive model that can be used for language modeling based on the transformer-XL architecture #TARGET_REF . This means that the outputs of XLNet depend strictly on the previous outputs. This is different from other state-of-the-art language models like BERT (Bidirectional Encoder Representations from Transformers) which rely on conditioning the probabilities given surrounding words. In BERT, the model tries to predict a masked word by looking at all surrounding unmasked words ( figure 3 ). The concept of Fig. 3 .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"XLNet is a generalized auto-regressive model that can be used for language modeling based on the transformer-XL architecture #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "BERT attempts to predict the masked word use both left and right contexts.",
                "During training, a certain percentage of words are masked for use in prediction.",
                "If both \"San\" and \"Francisco\" were masked, BERT would not be able to use information when decoding one of the words to help in decoding the other.",
                "masking the input introduces a few disadvantages mentioned in #TARGET_REF .",
                "Firstly, a masked token is rarely seen for most subsequent language modeling tasks, so there tends to be a discrepancy between the \"pre-taining\" step and the \"fine-tuning\" step."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                1,
                0,
                1,
                0
            ]
        },
        "input": "BERT attempts to predict the masked word use both left and right contexts. During training, a certain percentage of words are masked for use in prediction. If both \"San\" and \"Francisco\" were masked, BERT would not be able to use information when decoding one of the words to help in decoding the other. masking the input introduces a few disadvantages mentioned in #TARGET_REF . Firstly, a masked token is rarely seen for most subsequent language modeling tasks, so there tends to be a discrepancy between the \"pre-taining\" step and the \"fine-tuning\" step.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"During training, a certain percentage of words are masked for use in prediction.\", \"masking the input introduces a few disadvantages mentioned in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Here, we describe briefly the underlying framework, called RNN Encoder-Decoder, proposed by #TARGET_REF and (#REF) upon which we build a machine transliteration model that learns to transliterate end-to-end.",
                "The enoder is a character-based recurrent neural network that learns a highly nonlinear mapping from a spelling to the phonetic of the input sequence.",
                "This network reads the source name x = (x 1 , . . . , x T ) and encodes it into a sequence of hidden states h = (h 1 , · · · , h T ):",
                "Each hidden state h i is a bidirectional recurrent representation with forward and backward sequence information around the ith character.",
                "The representation of a forward sequence and a backward sequence of the input character sequence is estimated and concatenated to form a context set C = {h 1 , h 2 , ..., h T } (#REF; #REF) ."
            ],
            "label": [
                "EXT",
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Here, we describe briefly the underlying framework, called RNN Encoder-Decoder, proposed by #TARGET_REF and (#REF) upon which we build a machine transliteration model that learns to transliterate end-to-end. The enoder is a character-based recurrent neural network that learns a highly nonlinear mapping from a spelling to the phonetic of the input sequence. This network reads the source name x = (x 1 , . . . , x T ) and encodes it into a sequence of hidden states h = (h 1 , · · · , h T ): Each hidden state h i is a bidirectional recurrent representation with forward and backward sequence information around the ith character. The representation of a forward sequence and a backward sequence of the input character sequence is estimated and concatenated to form a context set C = {h 1 , h 2 , ..., h T } (#REF; #REF) .",
        "output": "{\"label\": [\"EXT\", \"BACK\"], \"context\": [\"Here, we describe briefly the underlying framework, called RNN Encoder-Decoder, proposed by #TARGET_REF and (#REF) upon which we build a machine transliteration model that learns to transliterate end-to-end.\"]}"
    },
    {
        "gold": {
            "text": [
                "Using characters instead of words leads to longer sequences, so Gated Recurrent Units (#REFa) have been used for the encoder network to model long term dependencies.",
                "The encoder has 128 hidden units for each direction (forward and backward), and the decoder has 128 hidden units with soft attention mechanism #TARGET_REF .",
                "We train the model using stochastic gradient descent with Adam (#REF).",
                "Each update is computed using a minibatch of 128 sequence pairs.",
                "The norm of the gradient is clipped with a threshold 1 (#REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Using characters instead of words leads to longer sequences, so Gated Recurrent Units (#REFa) have been used for the encoder network to model long term dependencies. The encoder has 128 hidden units for each direction (forward and backward), and the decoder has 128 hidden units with soft attention mechanism #TARGET_REF . We train the model using stochastic gradient descent with Adam (#REF). Each update is computed using a minibatch of 128 sequence pairs. The norm of the gradient is clipped with a threshold 1 (#REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The encoder has 128 hidden units for each direction (forward and backward), and the decoder has 128 hidden units with soft attention mechanism #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We conducted a set of experiments to show the effectiveness of RNN Encoder-Decoder model #TARGET_REF; #REF) in the task of machine transliteration using standard benchmark datasets provided by NEWS 2015-16 shared task .",
                "Table 1 shows different datasets in our experiments.",
                "Each dataset covers different levels of difficulty and training set size.",
                "The proposed model has been applied on .",
                "each dataset without tuning the algorithm for each specific language pairs."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We conducted a set of experiments to show the effectiveness of RNN Encoder-Decoder model #TARGET_REF; #REF) in the task of machine transliteration using standard benchmark datasets provided by NEWS 2015-16 shared task . Table 1 shows different datasets in our experiments. Each dataset covers different levels of difficulty and training set size. The proposed model has been applied on . each dataset without tuning the algorithm for each specific language pairs.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We conducted a set of experiments to show the effectiveness of RNN Encoder-Decoder model #TARGET_REF; #REF) in the task of machine transliteration using standard benchmark datasets provided by NEWS 2015-16 shared task .\"]}"
    },
    {
        "gold": {
            "text": [
                "The proposed model has been applied on .",
                "each dataset without tuning the algorithm for each specific language pairs.",
                "Also, we don't apply any preprocessing on the source or target language in order to evaluate the effectiveness of the proposed model in a fair situation.",
                "'TaskID' is a unique identifier in the following experiments.",
                "We leveraged a character-based encoderdecoder model (#REF; #REF) with soft attention mechanism #TARGET_REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The proposed model has been applied on . each dataset without tuning the algorithm for each specific language pairs. Also, we don't apply any preprocessing on the source or target language in order to evaluate the effectiveness of the proposed model in a fair situation. 'TaskID' is a unique identifier in the following experiments. We leveraged a character-based encoderdecoder model (#REF; #REF) with soft attention mechanism #TARGET_REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We leveraged a character-based encoderdecoder model (#REF; #REF) with soft attention mechanism #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Spellings change over time, but also vary within a single time period and even within a single author, since orthography only became standardized in many languages fairly recently.",
                "Over the years, researchers have proposed normalization methods based on rules and/or edit distances (#REF; #REF; #REF; #REF; #REFa; #REF; #TARGET_REF , statistical machine translation (#REFb; #REF) , and most recently neural network models (Bollmann and Søgaard, 2016; #REF; #REF) .",
                "However, most of these systems have been developed and tested on a single language (or even a single corpus), and many have not been compared to the naïve but strong baseline that only changes words seen in the training data, normalizing each to its most frequent modern form observed during training.",
                "1 These issues make it hard to tell which methods generalize across languages and corpora, and how they compare to each other.",
                "Moreover, researchers have rarely examined whether their systems actually improve performance on downstream tasks."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "Spellings change over time, but also vary within a single time period and even within a single author, since orthography only became standardized in many languages fairly recently. Over the years, researchers have proposed normalization methods based on rules and/or edit distances (#REF; #REF; #REF; #REF; #REFa; #REF; #TARGET_REF , statistical machine translation (#REFb; #REF) , and most recently neural network models (Bollmann and Søgaard, 2016; #REF; #REF) . However, most of these systems have been developed and tested on a single language (or even a single corpus), and many have not been compared to the naïve but strong baseline that only changes words seen in the training data, normalizing each to its most frequent modern form observed during training. 1 These issues make it hard to tell which methods generalize across languages and corpora, and how they compare to each other. Moreover, researchers have rarely examined whether their systems actually improve performance on downstream tasks.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Over the years, researchers have proposed normalization methods based on rules and/or edit distances (#REF; #REF; #REF; #REF; #REFa; #REF; #TARGET_REF , statistical machine translation (#REFb; #REF) , and most recently neural network models (Bollmann and S\\u00f8gaard, 2016; #REF; #REF) .\", \"However, most of these systems have been developed and tested on a single language (or even a single corpus), and many have not been compared to the na\\u00efve but strong baseline that only changes words seen in the training data, normalizing each to its most frequent modern form observed during training.\", \"1 These issues make it hard to tell which methods generalize across languages and corpora, and how they compare to each other.\"]}"
    },
    {
        "gold": {
            "text": [
                "We present results on five languages, for both seen and unseen words and for various amounts of training data.",
                "The soft attention model performs surprisingly poorly on seen words, so that its overall performance is worse than the naïve baseline and several earlier models #TARGET_REF .",
                "However, on unseen words (which we argue are what matters), both neural models do well.",
                "Unfortunately, these positive results did not",
                "translate into improvements when we tested the English-trained models on a downstream POS tagging task using a different historical collection spanning a similar time range."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We present results on five languages, for both seen and unseen words and for various amounts of training data. The soft attention model performs surprisingly poorly on seen words, so that its overall performance is worse than the naïve baseline and several earlier models #TARGET_REF . However, on unseen words (which we argue are what matters), both neural models do well. Unfortunately, these positive results did not translate into improvements when we tested the English-trained models on a downstream POS tagging task using a different historical collection spanning a similar time range.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"The soft attention model performs surprisingly poorly on seen words, so that its overall performance is worse than the na\\u00efve baseline and several earlier models #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In describing the training and test sets, researchers should not only report the number of types and tokens, but also the per-centage of unseen tokens in the test (or dev) set and the percentage of training items (h, m) where h = m. This last statistic measures the degree of spelling variation, which varies considerably between corpora.",
                "As for reporting results, we have argued that accuracy should be reported separately for seen vs unseen tokens, and overall results compared to the naïve memorization baseline.",
                "Since historical spelling normalization is typically a low-resource task, systems should also ideally be tested with varying amounts of training data to assess how much annotation might be required for a new corpus #TARGET_REF; Bollmann and Søgaard, 2016; #REF) .",
                "Finally, since these systems may be deployed on corpora other than those they were trained on, and used as preprocessing for other tasks, we advocate reporting performance on a downstream task and/or different corpus.",
                "To our knowledge the only previous supervised learning system to do so is Pettersson et al. (2013b) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In describing the training and test sets, researchers should not only report the number of types and tokens, but also the per-centage of unseen tokens in the test (or dev) set and the percentage of training items (h, m) where h = m. This last statistic measures the degree of spelling variation, which varies considerably between corpora. As for reporting results, we have argued that accuracy should be reported separately for seen vs unseen tokens, and overall results compared to the naïve memorization baseline. Since historical spelling normalization is typically a low-resource task, systems should also ideally be tested with varying amounts of training data to assess how much annotation might be required for a new corpus #TARGET_REF; Bollmann and Søgaard, 2016; #REF) . Finally, since these systems may be deployed on corpora other than those they were trained on, and used as preprocessing for other tasks, we advocate reporting performance on a downstream task and/or different corpus. To our knowledge the only previous supervised learning system to do so is Pettersson et al. (2013b) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Since historical spelling normalization is typically a low-resource task, systems should also ideally be tested with varying amounts of training data to assess how much annotation might be required for a new corpus #TARGET_REF; Bollmann and S\\u00f8gaard, 2016; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the same datasets as #TARGET_REF , with data from five languages over a range of historical periods.",
                "6 We use the same train/dev/test splits as Pettersson; dataset statistics are shown in Table  1 . Because we do no hyperparameter tuning, we do not use the development sets, and all results are reported on the test sets.",
                "Each system was tested as recommended above, with accuracy reported separately on seen and unseen items, and for different training data sizes.",
                "To evaluate the downstream effects of normalization, we applied the models to a collection of unseen documents and then tagged them with the Stan-ford POS tagger, which comes pre-trained on modern English.",
                "The documents are from the Parsed Corpus of Early English Correspondence (PCEEC) (#REF) , comprised of 84 letter collections from the 15th-17th centuries."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We use the same datasets as #TARGET_REF , with data from five languages over a range of historical periods. 6 We use the same train/dev/test splits as Pettersson; dataset statistics are shown in Table  1 . Because we do no hyperparameter tuning, we do not use the development sets, and all results are reported on the test sets. Each system was tested as recommended above, with accuracy reported separately on seen and unseen items, and for different training data sizes. To evaluate the downstream effects of normalization, we applied the models to a collection of unseen documents and then tagged them with the Stan-ford POS tagger, which comes pre-trained on modern English. The documents are from the Parsed Corpus of Early English Correspondence (PCEEC) (#REF) , comprised of 84 letter collections from the 15th-17th centuries.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use the same datasets as #TARGET_REF , with data from five languages over a range of historical periods.\"]}"
    },
    {
        "gold": {
            "text": [
                "8 When we varied the training data sizes, we found that the soft attention model actually gets worse on seen tokens in all languages as the training data increases beyond a relatively small size.",
                "We have no good explanation for this, and it's possible that tuning the parameters would help.",
                "Table 2 : Tokens normalized correctly (%) for each dataset.",
                "Upper half: results on (A)ll tokens reported by #TARGET_REF for a hybrid model (apply memorization baseline to seen tokens and an edit-distance-based model to unseen tokens) and two SMT models (which align character unigrams and bigrams, respectively).",
                "Lower half: results from our experiments, including accuracy reported separately on (S)een and (U)nseen tokens. and presumably more difficult as training data size increases, so the baseline gets worse."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "8 When we varied the training data sizes, we found that the soft attention model actually gets worse on seen tokens in all languages as the training data increases beyond a relatively small size. We have no good explanation for this, and it's possible that tuning the parameters would help. Table 2 : Tokens normalized correctly (%) for each dataset. Upper half: results on (A)ll tokens reported by #TARGET_REF for a hybrid model (apply memorization baseline to seen tokens and an edit-distance-based model to unseen tokens) and two SMT models (which align character unigrams and bigrams, respectively). Lower half: results from our experiments, including accuracy reported separately on (S)een and (U)nseen tokens. and presumably more difficult as training data size increases, so the baseline gets worse.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Upper half: results on (A)ll tokens reported by #TARGET_REF for a hybrid model (apply memorization baseline to seen tokens and an edit-distance-based model to unseen tokens) and two SMT models (which align character unigrams and bigrams, respectively).\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus, in recent years there has been growing interest in multimodal sentiment analysis.",
                "The three most widely studied modalities in current multimodal sentiment analysis research are: vocal (e.g., speech acoustics), visual (e.g., facial expressions), and verbal (e.g., lexical content).",
                "These are sometimes referred to as \"the three Vs\" of communication (#REF) .",
                "Multimodal sentiment analysis research focuses on understanding how an individual modality conveys sentiment information (intra-modal dynamics), and how they interact with each other (intermodal dynamics).",
                "It is a challenging research area and state-of-the-art performance of automatic sentiment prediction has room for improvement compared to human performance #TARGET_REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "Thus, in recent years there has been growing interest in multimodal sentiment analysis. The three most widely studied modalities in current multimodal sentiment analysis research are: vocal (e.g., speech acoustics), visual (e.g., facial expressions), and verbal (e.g., lexical content). These are sometimes referred to as \"the three Vs\" of communication (#REF) . Multimodal sentiment analysis research focuses on understanding how an individual modality conveys sentiment information (intra-modal dynamics), and how they interact with each other (intermodal dynamics). It is a challenging research area and state-of-the-art performance of automatic sentiment prediction has room for improvement compared to human performance #TARGET_REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Multimodal sentiment analysis research focuses on understanding how an individual modality conveys sentiment information (intra-modal dynamics), and how they interact with each other (intermodal dynamics).\", \"It is a challenging research area and state-of-the-art performance of automatic sentiment prediction has room for improvement compared to human performance #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In the LF model, as shown in Figure 5 , we concatenate the unimodal model top layers as the multimodal inputs.",
                "In the HF model, unimodal information is used in a hierarchy where the top layer of the lower unimodal model is concatenated with the input layer of the higher unimodal model, as shown in Figure 6 .",
                "We use the vocal modality at the bottom of the hierarchy while using the verbal modality at the top in HF fusion.",
                "This is because in previous studies (e.g., #TARGET_REF ) the verbal modality was shown to be the most effective for unimodal sentiment analysis, while the vocal modality was shown to be the least effective.",
                "4"
            ],
            "label": [
                "BACK",
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "In the LF model, as shown in Figure 5 , we concatenate the unimodal model top layers as the multimodal inputs. In the HF model, unimodal information is used in a hierarchy where the top layer of the lower unimodal model is concatenated with the input layer of the higher unimodal model, as shown in Figure 6 . We use the vocal modality at the bottom of the hierarchy while using the verbal modality at the top in HF fusion. This is because in previous studies (e.g., #TARGET_REF ) the verbal modality was shown to be the most effective for unimodal sentiment analysis, while the vocal modality was shown to be the least effective. 4",
        "output": "{\"label\": [\"BACK\", \"SIM\"], \"context\": [\"We use the vocal modality at the bottom of the hierarchy while using the verbal modality at the top in HF fusion.\", \"This is because in previous studies (e.g., #TARGET_REF ) the verbal modality was shown to be the most effective for unimodal sentiment analysis, while the vocal modality was shown to be the least effective.\"]}"
    },
    {
        "gold": {
            "text": [
                "Structures of the single-task and multi-task learning models only differ at the output layer: for sentiment score regression the output is a single node with tanh activation; for polarity classification the output is a single node with sigmoid activation; for intensity classification the output is 4 nodes with softmax activation.",
                "The main task uses mean absolute error as the loss function, while polarity classification uses binary cross-entropy as the loss function, and intensity classification uses categorical crossentropy as the loss function.",
                "Following state-ofthe-art on the CMU-MOSI database , during training we used Adam as the optimization function with a learning rate of 0.0005.",
                "We use the CMU Multimodal Data Software Development Kit (SDK) #TARGET_REF to load and pre-process the CMU-MOSI database, which splits the 2199 opinion segments into training (1283 segments), validation (229 segments), and test (686 segments) sets.",
                "1 We implement the sentiment analysis models using the Keras deep learning library (#REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Structures of the single-task and multi-task learning models only differ at the output layer: for sentiment score regression the output is a single node with tanh activation; for polarity classification the output is a single node with sigmoid activation; for intensity classification the output is 4 nodes with softmax activation. The main task uses mean absolute error as the loss function, while polarity classification uses binary cross-entropy as the loss function, and intensity classification uses categorical crossentropy as the loss function. Following state-ofthe-art on the CMU-MOSI database , during training we used Adam as the optimization function with a learning rate of 0.0005. We use the CMU Multimodal Data Software Development Kit (SDK) #TARGET_REF to load and pre-process the CMU-MOSI database, which splits the 2199 opinion segments into training (1283 segments), validation (229 segments), and test (686 segments) sets. 1 We implement the sentiment analysis models using the Keras deep learning library (#REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use the CMU Multimodal Data Software Development Kit (SDK) #TARGET_REF to load and pre-process the CMU-MOSI database, which splits the 2199 opinion segments into training (1283 segments), validation (229 segments), and test (686 segments) sets.\"]}"
    },
    {
        "gold": {
            "text": [
                "Here we report our sentiment score prediction experiments.",
                "2 In Tables 2 and 3 , \"S\" is the singletask learning model; \"S+P\" is the bi-task learning model with polarity classification as the auxillary task; \"S+I\" is the bi-task learning model with intensity classification as the auxillary task; \"S+P+I\" is the tri-task learning model.",
                "To evaluate the performance of sentiment score prediction, following previous work #TARGET_REF , we Tables 2 and 3 , the numbers in bold are the best performance for each modality or fusion strategy.",
                "To identify the significant differences in results, we perform a two-sample Wilcoxon test on the sentiment score predictions given by each pair of models being compared and consider p < 0.05 as significant.",
                "We also include random prediction as a baseline and the human performance reported by ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Here we report our sentiment score prediction experiments. 2 In Tables 2 and 3 , \"S\" is the singletask learning model; \"S+P\" is the bi-task learning model with polarity classification as the auxillary task; \"S+I\" is the bi-task learning model with intensity classification as the auxillary task; \"S+P+I\" is the tri-task learning model. To evaluate the performance of sentiment score prediction, following previous work #TARGET_REF , we Tables 2 and 3 , the numbers in bold are the best performance for each modality or fusion strategy. To identify the significant differences in results, we perform a two-sample Wilcoxon test on the sentiment score predictions given by each pair of models being compared and consider p < 0.05 as significant. We also include random prediction as a baseline and the human performance reported by .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"To evaluate the performance of sentiment score prediction, following previous work #TARGET_REF , we Tables 2 and 3 , the numbers in bold are the best performance for each modality or fusion strategy.\"]}"
    },
    {
        "gold": {
            "text": [
                "The results of unimodal sentiment prediction experiments are shown in Table 2 .",
                "3 The verbal models have the best performance here, which is consistent with previous sentiment analysis studies on multiple databases (e.g., #TARGET_REF ).",
                "This suggests that lexical information remains the most effective for sentiment analysis.",
                "On each modality, the best performance is achieved by a multi-task learning model.",
                "This answers our first research question and suggests that sentiment analysis can benefit from multi-task learning."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "The results of unimodal sentiment prediction experiments are shown in Table 2 . 3 The verbal models have the best performance here, which is consistent with previous sentiment analysis studies on multiple databases (e.g., #TARGET_REF ). This suggests that lexical information remains the most effective for sentiment analysis. On each modality, the best performance is achieved by a multi-task learning model. This answers our first research question and suggests that sentiment analysis can benefit from multi-task learning.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"3 The verbal models have the best performance here, which is consistent with previous sentiment analysis studies on multiple databases (e.g., #TARGET_REF ).\", \"This suggests that lexical information remains the most effective for sentiment analysis.\"]}"
    },
    {
        "gold": {
            "text": [
                "Cognitively motivated methods take into account the cohesion and coherence of text, its latent topic structure, Kintsch's propositions, etc [1] , [8] , [13] .",
                "Finally, machine learning methods utilize sophisticated structures such as language models [3] , [4] , [18] , query logs [15] , and several other features to predict the readability of open-domain text data.",
                "There are very few studies on readability assessment in Bengali texts.",
                "We found only three lines of work that specifically looked into Bengali readability [6] , [11] , #TARGET_REF .",
                "Das and Roychoudhury worked with a miniature model of two parameters in their pioneering study [6] ."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Cognitively motivated methods take into account the cohesion and coherence of text, its latent topic structure, Kintsch's propositions, etc [1] , [8] , [13] . Finally, machine learning methods utilize sophisticated structures such as language models [3] , [4] , [18] , query logs [15] , and several other features to predict the readability of open-domain text data. There are very few studies on readability assessment in Bengali texts. We found only three lines of work that specifically looked into Bengali readability [6] , [11] , #TARGET_REF . Das and Roychoudhury worked with a miniature model of two parameters in their pioneering study [6] .",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"We found only three lines of work that specifically looked into Bengali readability [6] , [11] , #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We are working on readability modeling in Bengali, and this dataset will be very helpful.",
                "An important limitation of our study is the small corpus size.",
                "We only have 30 annotated passages at our disposal, whereas #REF had around 300. But Islam et al.'s dataset is not annotated in as fine-grained a fashion as ours.",
                "Note also that our dataset is larger than both #TARGET_REF 16document dataset #TARGET_REF , and Das and Roychoudhury's seven document dataset [6] .",
                "We plan to increase the size of our dataset in future."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "We are working on readability modeling in Bengali, and this dataset will be very helpful. An important limitation of our study is the small corpus size. We only have 30 annotated passages at our disposal, whereas #REF had around 300. But Islam et al.'s dataset is not annotated in as fine-grained a fashion as ours. Note also that our dataset is larger than both #TARGET_REF 16document dataset #TARGET_REF , and Das and Roychoudhury's seven document dataset [6] . We plan to increase the size of our dataset in future.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We only have 30 annotated passages at our disposal, whereas #REF had around 300. But Islam et al.'s dataset is not annotated in as fine-grained a fashion as ours.\", \"Note also that our dataset is larger than both #TARGET_REF 16document dataset #TARGET_REF , and Das and Roychoudhury's seven document dataset [6] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Low-resource automatic speech-to-text translation (AST) has recently gained traction as a way to bring NLP tools to under-represented languages.",
                "An end-to-end approach [1] [2] [3] [4] #TARGET_REF [6] [7] is particularly appealing for source languages with no written form, or for endangered languages where translations into a high-resource language may be easier to collect than transcriptions [8] .",
                "However, building high-quality endto-end AST with little parallel data is challenging, and has led researchers to explore how other sources of data could be used to help.",
                "A number of methods have been investigated.",
                "Several of these use transcribed source language audio and/or translated source language text in a multitask learning scenario [4, 6, 9] or to pre-train parts of the model before fine-tuning on the end-to-end AST task [4] ."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Low-resource automatic speech-to-text translation (AST) has recently gained traction as a way to bring NLP tools to under-represented languages. An end-to-end approach [1] [2] [3] [4] #TARGET_REF [6] [7] is particularly appealing for source languages with no written form, or for endangered languages where translations into a high-resource language may be easier to collect than transcriptions [8] . However, building high-quality endto-end AST with little parallel data is challenging, and has led researchers to explore how other sources of data could be used to help. A number of methods have been investigated. Several of these use transcribed source language audio and/or translated source language text in a multitask learning scenario [4, 6, 9] or to pre-train parts of the model before fine-tuning on the end-to-end AST task [4] .",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"An end-to-end approach [1] [2] [3] [4] #TARGET_REF [6] [7] is particularly appealing for source languages with no written form, or for endangered languages where translations into a high-resource language may be easier to collect than transcriptions [8] .\", \"However, building high-quality endto-end AST with little parallel data is challenging, and has led researchers to explore how other sources of data could be used to help.\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular, several researchers have shown that low-resource AST can be improved by pretraining on an ASR task in some other language, then transferring the encoder parameters to initialize the AST model.",
                "For example, Bansal et al. #TARGET_REF showed that pre-training on either English or French ASR improved their Spanish-English AST system (trained on 20 hours of parallel data) and #REF got improvements on an 8-hour Swahili-English AST dataset using English ASR pretraining.",
                "Overall these results show that pretraining helps, but leave open the question of what factors affect the degree of improvement.",
                "For example, does language relatedness play a role, or simply the amount of pretraining data?",
                "Bansal et al. showed bigger AST gains as the amount of English pretraining data increased from 20 to 300 hours, and also found a slightly larger improvement when pretraining on 20 hours of English versus 20 hours of French, but they pointed out that the Spanish data contains many English code-switched words, which could explain the latter result."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In particular, several researchers have shown that low-resource AST can be improved by pretraining on an ASR task in some other language, then transferring the encoder parameters to initialize the AST model. For example, Bansal et al. #TARGET_REF showed that pre-training on either English or French ASR improved their Spanish-English AST system (trained on 20 hours of parallel data) and #REF got improvements on an 8-hour Swahili-English AST dataset using English ASR pretraining. Overall these results show that pretraining helps, but leave open the question of what factors affect the degree of improvement. For example, does language relatedness play a role, or simply the amount of pretraining data? Bansal et al. showed bigger AST gains as the amount of English pretraining data increased from 20 to 300 hours, and also found a slightly larger improvement when pretraining on 20 hours of English versus 20 hours of French, but they pointed out that the Spanish data contains many English code-switched words, which could explain the latter result.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"For example, Bansal et al. #TARGET_REF showed that pre-training on either English or French ASR improved their Spanish-English AST system (trained on 20 hours of parallel data) and #REF got improvements on an 8-hour Swahili-English AST dataset using English ASR pretraining.\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous experiments #TARGET_REF showed that the encoder accounts for most of the benefits of transferring the parameters.",
                "Transferring also the decoder and attention mechanism does bring some improvements, but is only feasible when the ASR pretraining language is the same as the AST target language, which is not true in most of our experiments.",
                "In addition to pretraining, we experimented with data augmentation.",
                "Specifically, we augmented the AST data using Kaldi's [13] 3-way speed perturbation, adding versions of the AST data where the audio is sped down and up by a factor of 0.9 and 1.1, respectively.",
                "1 To evaluate ASR performance we compute the word error rate (WER)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Previous experiments #TARGET_REF showed that the encoder accounts for most of the benefits of transferring the parameters. Transferring also the decoder and attention mechanism does bring some improvements, but is only feasible when the ASR pretraining language is the same as the AST target language, which is not true in most of our experiments. In addition to pretraining, we experimented with data augmentation. Specifically, we augmented the AST data using Kaldi's [13] 3-way speed perturbation, adding versions of the AST data where the audio is sped down and up by a factor of 0.9 and 1.1, respectively. 1 To evaluate ASR performance we compute the word error rate (WER).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Previous experiments #TARGET_REF showed that the encoder accounts for most of the benefits of transferring the parameters.\"]}"
    },
    {
        "gold": {
            "text": [
                "Transcriptions with annotated word boundaries are available in both Hanzi (Chinese characters) and Romanized versions, and we built models with each.",
                "To compare to the GlobalPhone data, we also created a 20-hour subset of the Romanized AISHELL (zh-ai-small) by randomly selecting utterances from a subset of the speakers (81, roughly the number present in most of the GlobalPhone datasets).",
                "Finally, to reproduce one of the experiments from [5] , we pretrained one model using 300 hours of Switchboard #REF .",
                "This data is the most similar to the AST speech data in terms of style and channel (both are conversational telephone speech).",
                "However, as noted by #TARGET_REF , the Fisher Spanish speech contains many words that are actually in English (code-switching), so pretraining on English may provide an unfair advantage relative to other languages."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Transcriptions with annotated word boundaries are available in both Hanzi (Chinese characters) and Romanized versions, and we built models with each. To compare to the GlobalPhone data, we also created a 20-hour subset of the Romanized AISHELL (zh-ai-small) by randomly selecting utterances from a subset of the speakers (81, roughly the number present in most of the GlobalPhone datasets). Finally, to reproduce one of the experiments from [5] , we pretrained one model using 300 hours of Switchboard #REF . This data is the most similar to the AST speech data in terms of style and channel (both are conversational telephone speech). However, as noted by #TARGET_REF , the Fisher Spanish speech contains many words that are actually in English (code-switching), so pretraining on English may provide an unfair advantage relative to other languages.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"However, as noted by #TARGET_REF , the Fisher Spanish speech contains many words that are actually in English (code-switching), so pretraining on English may provide an unfair advantage relative to other languages.\"]}"
    },
    {
        "gold": {
            "text": [
                "To begin to tease apart these issues, we focus here on monolingual pretraining for low-resource AST, and investigate two questions.",
                "First, can we predict what sort of pretraining data is best for a particular AST task? Does it matter if the pretraining language is related to the AST source language (defined here as part of the same language family, since phonetic similarity is difficult to measure), or is the amount of pretraining data (or some other factor) more important?",
                "Second, can pretraining be effectively combined with other methods, such as data augmentation, in order to further improve AST results?",
                "To answer these questions, we use the same AST architecture and Spanish-English parallel data as Bansal et al. #TARGET_REF , but pretrain the encoder using a number of different ASR datasets: the 150hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data.",
                "We find that pretraining on a larger amount of data from an unrelated language is much better than pretraining on a smaller amount of data from a related language."
            ],
            "label": [
                "EXT",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "To begin to tease apart these issues, we focus here on monolingual pretraining for low-resource AST, and investigate two questions. First, can we predict what sort of pretraining data is best for a particular AST task? Does it matter if the pretraining language is related to the AST source language (defined here as part of the same language family, since phonetic similarity is difficult to measure), or is the amount of pretraining data (or some other factor) more important? Second, can pretraining be effectively combined with other methods, such as data augmentation, in order to further improve AST results? To answer these questions, we use the same AST architecture and Spanish-English parallel data as Bansal et al. #TARGET_REF , but pretrain the encoder using a number of different ASR datasets: the 150hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data. We find that pretraining on a larger amount of data from an unrelated language is much better than pretraining on a smaller amount of data from a related language.",
        "output": "{\"label\": [\"EXT\", \"USE\"], \"context\": [\"To answer these questions, we use the same AST architecture and Spanish-English parallel data as Bansal et al. #TARGET_REF , but pretrain the encoder using a number of different ASR datasets: the 150hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data.\"]}"
    },
    {
        "gold": {
            "text": [
                "For both ASR and AST tasks we use the same end-to-end system architecture shown in Figure 1 : the encoder-decoder model from #TARGET_REF , which itself is adapted from [2] , [4] and [3] .",
                "Details of the architecture and training parameters are described in Section 3.4.",
                "After pretraining an ASR model, we transfer only its encoder parameters to the AST task.",
                "Previous experiments [5] showed that the encoder accounts for most of the benefits of transferring the parameters.",
                "Transferring also the decoder and attention mechanism does bring some improvements, but is only feasible when the ASR pretraining language is the same as the AST target language, which is not true in most of our experiments."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "For both ASR and AST tasks we use the same end-to-end system architecture shown in Figure 1 : the encoder-decoder model from #TARGET_REF , which itself is adapted from [2] , [4] and [3] . Details of the architecture and training parameters are described in Section 3.4. After pretraining an ASR model, we transfer only its encoder parameters to the AST task. Previous experiments [5] showed that the encoder accounts for most of the benefits of transferring the parameters. Transferring also the decoder and attention mechanism does bring some improvements, but is only feasible when the ASR pretraining language is the same as the AST target language, which is not true in most of our experiments.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For both ASR and AST tasks we use the same end-to-end system architecture shown in Figure 1 : the encoder-decoder model from #TARGET_REF , which itself is adapted from [2] , [4] and [3] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Transcriptions with annotated word boundaries are available in both Hanzi (Chinese characters) and Romanized versions, and we built models with each.",
                "To compare to the GlobalPhone data, we also created a 20-hour subset of the Romanized AISHELL (zh-ai-small) by randomly selecting utterances from a subset of the speakers (81, roughly the number present in most of the GlobalPhone datasets).",
                "Finally, to reproduce one of the experiments from #TARGET_REF , we pretrained one model using 300 hours of Switchboard #REF .",
                "This data is the most similar to the AST speech data in terms of style and channel (both are conversational telephone speech).",
                "However, as noted by [5] , the Fisher Spanish speech contains many words that are actually in English (code-switching), so pretraining on English may provide an unfair advantage relative to other languages."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Transcriptions with annotated word boundaries are available in both Hanzi (Chinese characters) and Romanized versions, and we built models with each. To compare to the GlobalPhone data, we also created a 20-hour subset of the Romanized AISHELL (zh-ai-small) by randomly selecting utterances from a subset of the speakers (81, roughly the number present in most of the GlobalPhone datasets). Finally, to reproduce one of the experiments from #TARGET_REF , we pretrained one model using 300 hours of Switchboard #REF . This data is the most similar to the AST speech data in terms of style and channel (both are conversational telephone speech). However, as noted by [5] , the Fisher Spanish speech contains many words that are actually in English (code-switching), so pretraining on English may provide an unfair advantage relative to other languages.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Finally, to reproduce one of the experiments from #TARGET_REF , we pretrained one model using 300 hours of Switchboard #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Following the architecture and training procedure described in #TARGET_REF , input speech features are fed into a stack of two CNN layers.",
                "In each CNN layer we stride the input with a factor of 2 along time, apply The points in the circled group come from different runs on the same dataset but with different BPE or learning rate schedules.",
                "The Spearman rank correlation of these points is -0.97; the correlation is -0.92 when using test sets to compute both ASR and BLEU.",
                "ReLU activation [20] followed by batch normalization [21] .",
                "The CNN output is fed into a three-layer bi-directional long short-term memory network (LSTM) [22] , with 512 hidden layer dimensions."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Following the architecture and training procedure described in #TARGET_REF , input speech features are fed into a stack of two CNN layers. In each CNN layer we stride the input with a factor of 2 along time, apply The points in the circled group come from different runs on the same dataset but with different BPE or learning rate schedules. The Spearman rank correlation of these points is -0.97; the correlation is -0.92 when using test sets to compute both ASR and BLEU. ReLU activation [20] followed by batch normalization [21] . The CNN output is fed into a three-layer bi-directional long short-term memory network (LSTM) [22] , with 512 hidden layer dimensions.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following the architecture and training procedure described in #TARGET_REF , input speech features are fed into a stack of two CNN layers.\"]}"
    },
    {
        "gold": {
            "text": [
                "The Spearman rank correlation of these points is -0.97; the correlation is -0.92 when using test sets to compute both ASR and BLEU.",
                "ReLU activation [20] followed by batch normalization [21] .",
                "The CNN output is fed into a three-layer bi-directional long short-term memory network (LSTM) [22] , with 512 hidden layer dimensions.",
                "For decoding, we use the predicted token 20% of the time and the training token 80% of the time [23] as input to a 128-dimensional embedding layer followed by a three-layer LSTM, with 256 hidden layer dimensions, and combine this with the output from the attention mechanism [24] to predict the word at the current time step.",
                "We use code and hyperparameter settings from #TARGET_REF 4 : the Adam optimizer [25] with an initial learning rate of 0.001 and decay it by a factor of 0.5 based on the dev set BLEU score."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The Spearman rank correlation of these points is -0.97; the correlation is -0.92 when using test sets to compute both ASR and BLEU. ReLU activation [20] followed by batch normalization [21] . The CNN output is fed into a three-layer bi-directional long short-term memory network (LSTM) [22] , with 512 hidden layer dimensions. For decoding, we use the predicted token 20% of the time and the training token 80% of the time [23] as input to a 128-dimensional embedding layer followed by a three-layer LSTM, with 256 hidden layer dimensions, and combine this with the output from the attention mechanism [24] to predict the word at the current time step. We use code and hyperparameter settings from #TARGET_REF 4 : the Adam optimizer [25] with an initial learning rate of 0.001 and decay it by a factor of 0.5 based on the dev set BLEU score.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use code and hyperparameter settings from #TARGET_REF 4 : the Adam optimizer [25] with an initial learning rate of 0.001 and decay it by a factor of 0.5 based on the dev set BLEU score.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our baseline 20-hour AST system obtains a BLEU score of 10.3 ( Table 1 , first row), 0.5 BLEU point lower than that reported by #TARGET_REF .",
                "This discrepancy might be due to differences in subsampling from the 160-hour AST dataset to create the 20-hour subset, or from Kaldi parameters when computing the MFCCs.",
                "WERs for our pre-trained models ( Table 1 ) vary from 22.5 for the large AISHELL dataset with Romanized transcript to 80.5 for Portuguese GlobalPhone.",
                "These are considerably worse than stateof-the-art ASR systems (e.g., Kaldi recipes can achieve WER of 7.5 on AISHELL and 26.5 on Portuguese GlobalPhone), but we did not optimize our architecture or hyperparameters for the ASR task since our main goal is to analyze the relationship between pretraining and AST performance (and in order to use pretraining, we must use a seq2seq model with the architecture as for AST)."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "Our baseline 20-hour AST system obtains a BLEU score of 10.3 ( Table 1 , first row), 0.5 BLEU point lower than that reported by #TARGET_REF . This discrepancy might be due to differences in subsampling from the 160-hour AST dataset to create the 20-hour subset, or from Kaldi parameters when computing the MFCCs. WERs for our pre-trained models ( Table 1 ) vary from 22.5 for the large AISHELL dataset with Romanized transcript to 80.5 for Portuguese GlobalPhone. These are considerably worse than stateof-the-art ASR systems (e.g., Kaldi recipes can achieve WER of 7.5 on AISHELL and 26.5 on Portuguese GlobalPhone), but we did not optimize our architecture or hyperparameters for the ASR task since our main goal is to analyze the relationship between pretraining and AST performance (and in order to use pretraining, we must use a seq2seq model with the architecture as for AST).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Our baseline 20-hour AST system obtains a BLEU score of 10.3 ( Table 1 , first row), 0.5 BLEU point lower than that reported by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "ranging from 0.2 (pt-gp) to 4.3 (zh-ai-large).",
                "These results make it clear that language relatedness does not play a strong role in predicting AST improvements, since on the similar-sized GlobalPhone datasets, the two languages most related to Spanish (French and Portuguese) yield the highest and lowest improvements, respectively.",
                "Moreover, pretraining on the large Chinese dataset yields a bigger improvement than either of these-4.3 BLEU points.",
                "This is nearly as much as the 6 point improvement reported by #TARGET_REF when pretraining on 100 hours of English data, which is especially surprising given not only that Chinese is very different from Spanish, but also that the Spanish data contains some English words.",
                "This finding seems to suggest that data size is more important than language relatedness for predicting the effects of pretraining."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "ranging from 0.2 (pt-gp) to 4.3 (zh-ai-large). These results make it clear that language relatedness does not play a strong role in predicting AST improvements, since on the similar-sized GlobalPhone datasets, the two languages most related to Spanish (French and Portuguese) yield the highest and lowest improvements, respectively. Moreover, pretraining on the large Chinese dataset yields a bigger improvement than either of these-4.3 BLEU points. This is nearly as much as the 6 point improvement reported by #TARGET_REF when pretraining on 100 hours of English data, which is especially surprising given not only that Chinese is very different from Spanish, but also that the Spanish data contains some English words. This finding seems to suggest that data size is more important than language relatedness for predicting the effects of pretraining.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"This is nearly as much as the 6 point improvement reported by #TARGET_REF when pretraining on 100 hours of English data, which is especially surprising given not only that Chinese is very different from Spanish, but also that the Spanish data contains some English words.\"]}"
    },
    {
        "gold": {
            "text": [
                "Morphological segmentation is the task of dividing words into morphemes, the smallest meaning-bearing units in the word (#REF) .",
                "For example the morpheme over occurs in words like hold+over, lay+over, and skip+over.",
                "1 Roots combine with derivational (e.g. refut+able) and inflectional affixes (e.g. hold+ing).",
                "Computational segmentation approaches can be divided into rule-based (#REF) , supervised (#REF) , semi-supervised (Grönroos et al., 2014) , and unsupervised (#REF) .",
                "#TARGET_REF observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Morphological segmentation is the task of dividing words into morphemes, the smallest meaning-bearing units in the word (#REF) . For example the morpheme over occurs in words like hold+over, lay+over, and skip+over. 1 Roots combine with derivational (e.g. refut+able) and inflectional affixes (e.g. hold+ing). Computational segmentation approaches can be divided into rule-based (#REF) , supervised (#REF) , semi-supervised (Grönroos et al., 2014) , and unsupervised (#REF) . #TARGET_REF observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section, we describe the original syllabification method of #REF , which serves as our baseline system, and discuss various approaches to incorporating morphological information.",
                "#TARGET_REF present a discriminative approach to automatic syllabification.",
                "They formulate syllabification as a tagging problem, and learn a Structured SVM tagger from labeled data (#REF) .",
                "Under the Markov assumption that each tag is dependent on its previous n tags, the tagger predicts the optimal tag sequence (#REF) .",
                "A large-margin training objective is applied to learn a weight vector to separate the correct tag sequence from other possible sequences for each training instance."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In this section, we describe the original syllabification method of #REF , which serves as our baseline system, and discuss various approaches to incorporating morphological information. #TARGET_REF present a discriminative approach to automatic syllabification. They formulate syllabification as a tagging problem, and learn a Structured SVM tagger from labeled data (#REF) . Under the Markov assumption that each tag is dependent on its previous n tags, the tagger predicts the optimal tag sequence (#REF) . A large-margin training objective is applied to learn a weight vector to separate the correct tag sequence from other possible sequences for each training instance.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF present a discriminative approach to automatic syllabification.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification.",
                "In this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology.",
                "We augment the syllabification approach of #TARGET_REF , with features encoding morphological segmentation of words.",
                "We investigate the degree of overlap between the morphological and syllable boundaries.",
                "The results of our experiments on English and German show that the incorporation of expert-annotated (gold) morphological boundaries extracted from lexical databases substantially reduces the syllabification error rate, particularly in low-resource settings."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "#REF observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification. In this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology. We augment the syllabification approach of #TARGET_REF , with features encoding morphological segmentation of words. We investigate the degree of overlap between the morphological and syllable boundaries. The results of our experiments on English and German show that the incorporation of expert-annotated (gold) morphological boundaries extracted from lexical databases substantially reduces the syllabification error rate, particularly in low-resource settings.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"We augment the syllabification approach of #TARGET_REF , with features encoding morphological segmentation of words.\"]}"
    },
    {
        "gold": {
            "text": [
                "As a baseline, we replicate the experiments of #TARGET_REF , and extend them to lowresource settings.",
                "Since the training sets are of slightly different sizes, we label each training size point as specified in Table 3 .",
                "We see that correct syllabification of approximately half of the words is achieved with as few as 100 English and 50 German training examples."
            ],
            "label": [
                "EXT",
                "USE"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "As a baseline, we replicate the experiments of #TARGET_REF , and extend them to lowresource settings. Since the training sets are of slightly different sizes, we label each training size point as specified in Table 3 . We see that correct syllabification of approximately half of the words is achieved with as few as 100 English and 50 German training examples.",
        "output": "{\"label\": [\"EXT\", \"USE\"], \"context\": [\"As a baseline, we replicate the experiments of #TARGET_REF , and extend them to lowresource settings.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section, we describe the original syllabification method of #TARGET_REF , which serves as our baseline system, and discuss various approaches to incorporating morphological information.",
                "#REF present a discriminative approach to automatic syllabification.",
                "They formulate syllabification as a tagging problem, and learn a Structured SVM tagger from labeled data (#REF) .",
                "Under the Markov assumption that each tag is dependent on its previous n tags, the tagger predicts the optimal tag sequence (#REF) .",
                "A large-margin training objective is applied to learn a weight vector to separate the correct tag sequence from other possible sequences for each training instance."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In this section, we describe the original syllabification method of #TARGET_REF , which serves as our baseline system, and discuss various approaches to incorporating morphological information. #REF present a discriminative approach to automatic syllabification. They formulate syllabification as a tagging problem, and learn a Structured SVM tagger from labeled data (#REF) . Under the Markov assumption that each tag is dependent on its previous n tags, the tagger predicts the optimal tag sequence (#REF) . A large-margin training objective is applied to learn a weight vector to separate the correct tag sequence from other possible sequences for each training instance.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In this section, we describe the original syllabification method of #TARGET_REF , which serves as our baseline system, and discuss various approaches to incorporating morphological information.\"]}"
    },
    {
        "gold": {
            "text": [
                "Contextualized word embeddings have played an essential role in many NLP tasks.",
                "One could expect considerable performance boosts by simply substituting distributional word embeddings with Flair (#REF) , ELMo #TARGET_REF , and BERT (#REF) embeddings.",
                "The unique thing about contextualized word embeddings is that different representations are generated for the same word type with different topical senses.",
                "This work focuses on interpreting embedding representations for word senses.",
                "We propose an algorithm (Section 3) that learns the dimension importance in representing sense information and then mask unessential dimensions that are deemed less meaningful in word sense representations to 0."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Contextualized word embeddings have played an essential role in many NLP tasks. One could expect considerable performance boosts by simply substituting distributional word embeddings with Flair (#REF) , ELMo #TARGET_REF , and BERT (#REF) embeddings. The unique thing about contextualized word embeddings is that different representations are generated for the same word type with different topical senses. This work focuses on interpreting embedding representations for word senses. We propose an algorithm (Section 3) that learns the dimension importance in representing sense information and then mask unessential dimensions that are deemed less meaningful in word sense representations to 0.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"One could expect considerable performance boosts by simply substituting distributional word embeddings with Flair (#REF) , ELMo #TARGET_REF , and BERT (#REF) embeddings.\"]}"
    },
    {
        "gold": {
            "text": [
                "Three popular word embedding algorithms are used for our experiments with various dimensions: ELMo, Flair, and BERT.",
                "ELMo is a deep word-level bidirectional LSTM language model with character level convolution networks along with a final linear projection output layer #TARGET_REF .",
                "Flair is a character-level bidirectional LSTM language model on sequences of characters (#REF) .",
                "BERT has an architecture of a multi-layer bidirectional transformer encoder (#REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0
            ]
        },
        "input": "Three popular word embedding algorithms are used for our experiments with various dimensions: ELMo, Flair, and BERT. ELMo is a deep word-level bidirectional LSTM language model with character level convolution networks along with a final linear projection output layer #TARGET_REF . Flair is a character-level bidirectional LSTM language model on sequences of characters (#REF) . BERT has an architecture of a multi-layer bidirectional transformer encoder (#REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"ELMo is a deep word-level bidirectional LSTM language model with character level convolution networks along with a final linear projection output layer #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The Most Frequent Sense (MFS) heuristic is the most common baseline, which selects the most frequent sense in the training data for the target word (#REFa) .",
                "Depending on the evaluation dataset, the state-of-art in WSD varies.",
                "Raganato et al. (2017b) utilize bi-LSTM networks with attention mechanism and a softmax layer.",
                "#REF and #TARGET_REF also adopt bi-LSTM networks with KNN classifiers.",
                "Later work incorporates word features such as gloss and POS information into memory networks (#REF; #REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The Most Frequent Sense (MFS) heuristic is the most common baseline, which selects the most frequent sense in the training data for the target word (#REFa) . Depending on the evaluation dataset, the state-of-art in WSD varies. Raganato et al. (2017b) utilize bi-LSTM networks with attention mechanism and a softmax layer. #REF and #TARGET_REF also adopt bi-LSTM networks with KNN classifiers. Later work incorporates word features such as gloss and POS information into memory networks (#REF; #REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#REF and #TARGET_REF also adopt bi-LSTM networks with KNN classifiers.\"]}"
    },
    {
        "gold": {
            "text": [
                "K-Nearest Neighbor (KNN) approach is adopted from both ELMo #TARGET_REF and con-text2vec (#REF) to establish strong baseline approaches.",
                "Sense-based KNN Adapted from ELMo (#REF) with k = 1, words that have the same senses are clustered together, and the average of that cluster is used as the sense vector, which is then fitted using a one KNN classifier.",
                "Unseen words from the test corpus fall back using the first sense from WordNet (#REF) .",
                "Word-based KNN Following context2vec (#REF), a cluster of each lemma occurrences in the training set is formed.",
                "Each word has a distinct classifier, which will assign labels based on k, where k = min(# of occurrences, 5)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "K-Nearest Neighbor (KNN) approach is adopted from both ELMo #TARGET_REF and con-text2vec (#REF) to establish strong baseline approaches. Sense-based KNN Adapted from ELMo (#REF) with k = 1, words that have the same senses are clustered together, and the average of that cluster is used as the sense vector, which is then fitted using a one KNN classifier. Unseen words from the test corpus fall back using the first sense from WordNet (#REF) . Word-based KNN Following context2vec (#REF), a cluster of each lemma occurrences in the training set is formed. Each word has a distinct classifier, which will assign labels based on k, where k = min(# of occurrences, 5).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"K-Nearest Neighbor (KNN) approach is adopted from both ELMo #TARGET_REF and con-text2vec (#REF) to establish strong baseline approaches.\"]}"
    },
    {
        "gold": {
            "text": [
                "K-Nearest Neighbor (KNN) approach is adopted from both ELMo (#REF) and con-text2vec (#REF) to establish strong baseline approaches.",
                "Sense-based KNN Adapted from ELMo #TARGET_REF with k = 1, words that have the same senses are clustered together, and the average of that cluster is used as the sense vector, which is then fitted using a one KNN classifier.",
                "Unseen words from the test corpus fall back using the first sense from WordNet (#REF) .",
                "Word-based KNN Following context2vec (#REF), a cluster of each lemma occurrences in the training set is formed.",
                "Each word has a distinct classifier, which will assign labels based on k, where k = min(# of occurrences, 5)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "K-Nearest Neighbor (KNN) approach is adopted from both ELMo (#REF) and con-text2vec (#REF) to establish strong baseline approaches. Sense-based KNN Adapted from ELMo #TARGET_REF with k = 1, words that have the same senses are clustered together, and the average of that cluster is used as the sense vector, which is then fitted using a one KNN classifier. Unseen words from the test corpus fall back using the first sense from WordNet (#REF) . Word-based KNN Following context2vec (#REF), a cluster of each lemma occurrences in the training set is formed. Each word has a distinct classifier, which will assign labels based on k, where k = min(# of occurrences, 5).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Sense-based KNN Adapted from ELMo #TARGET_REF with k = 1, words that have the same senses are clustered together, and the average of that cluster is used as the sense vector, which is then fitted using a one KNN classifier.\"]}"
    },
    {
        "gold": {
            "text": [
                "The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (#REF; #TARGET_REF; #REF) .",
                "This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models.",
                "The rule-based approach was rejected with the claim that rules are bound to overgenerate (#REF) .",
                "In this paper, we present a hybrid model that combines the strengths of a rule-based model with those of two statistical models for this task.",
                "The three models make use of different sources of information."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (#REF; #TARGET_REF; #REF) . This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models. The rule-based approach was rejected with the claim that rules are bound to overgenerate (#REF) . In this paper, we present a hybrid model that combines the strengths of a rule-based model with those of two statistical models for this task. The three models make use of different sources of information.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (#REF; #TARGET_REF; #REF) .\", \"This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models.\"]}"
    },
    {
        "gold": {
            "text": [
                "Unknown words constitute a major source of difficulty for Chinese part-of-speech (POS) tagging, yet relatively little work has been done on POS guessing of Chinese unknown words.",
                "The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (#REF; #REF; #REF) .",
                "This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models.",
                "The rule-based approach was rejected with the claim that rules are bound to overgenerate #TARGET_REF .",
                "In this paper, we present a hybrid model that combines the strengths of a rule-based model with those of two statistical models for this task."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Unknown words constitute a major source of difficulty for Chinese part-of-speech (POS) tagging, yet relatively little work has been done on POS guessing of Chinese unknown words. The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (#REF; #REF; #REF) . This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models. The rule-based approach was rejected with the claim that rules are bound to overgenerate #TARGET_REF . In this paper, we present a hybrid model that combines the strengths of a rule-based model with those of two statistical models for this task.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The rule-based approach was rejected with the claim that rules are bound to overgenerate #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Several reasons were suggested for rejecting the rule-based approach.",
                "First, #REF claimed that it does not work because the syntactic and semantic information for each character or morpheme is unavailable.",
                "This claim does not fully hold, as the POS information about the component words or morphemes of many unknown words is available in the training lexicon.",
                "Second, #TARGET_REF argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will \"result in massive overgeneration\" (p. 48).",
                "We will show that overgeneration can be controlled by additional constraints."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "Several reasons were suggested for rejecting the rule-based approach. First, #REF claimed that it does not work because the syntactic and semantic information for each character or morpheme is unavailable. This claim does not fully hold, as the POS information about the component words or morphemes of many unknown words is available in the training lexicon. Second, #TARGET_REF argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will \"result in massive overgeneration\" (p. 48). We will show that overgeneration can be controlled by additional constraints.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"Second, #TARGET_REF argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will \\\"result in massive overgeneration\\\" (p. 48).\", \"We will show that overgeneration can be controlled by additional constraints.\"]}"
    },
    {
        "gold": {
            "text": [
                "We propose a hybrid model that combines the strengths of different models to arrive at better results for this task.",
                "The models we will consider are a rule-based model, the trigram model, and the statistical model developed by #TARGET_REF .",
                "Combination of the three models will be based on the evaluation of their individual performances on the training data."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0
            ]
        },
        "input": "We propose a hybrid model that combines the strengths of different models to arrive at better results for this task. The models we will consider are a rule-based model, the trigram model, and the statistical model developed by #TARGET_REF . Combination of the three models will be based on the evaluation of their individual performances on the training data.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The models we will consider are a rule-based model, the trigram model, and the statistical model developed by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "where x and y are the source and target sentences, and P(y j |y <j , x) is the probability of generating the j-th word y j given the previously-generated words y <j and the source sentence x. However, the straightforward implementation of this model suffers from many problems, the most obvious one being the bias that the system tends to choose shorter translations because the log-probability is added over time steps.",
                "The situation is worse when we use beam search where the shorter translations have more chances to beat the longer ones.",
                "It is in general to normalize the model score by translation length (say length normalization) to eliminate this system bias #TARGET_REF .",
                "Though widely used, length normalization is not a perfect solution.",
                "NMT systems still have under-translation and over-translation problem even with a normalized model."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "where x and y are the source and target sentences, and P(y j |y <j , x) is the probability of generating the j-th word y j given the previously-generated words y <j and the source sentence x. However, the straightforward implementation of this model suffers from many problems, the most obvious one being the bias that the system tends to choose shorter translations because the log-probability is added over time steps. The situation is worse when we use beam search where the shorter translations have more chances to beat the longer ones. It is in general to normalize the model score by translation length (say length normalization) to eliminate this system bias #TARGET_REF . Though widely used, length normalization is not a perfect solution. NMT systems still have under-translation and over-translation problem even with a normalized model.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"It is in general to normalize the model score by translation length (say length normalization) to eliminate this system bias #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "E.g., #REF developed a coveragebased model to measure the fractional count that a source word is translated during decoding.",
                "It can be jointly learned with the NMT model.",
                "Alternatively, one can rerank the n-best outputs by coverage-sensitive models, but this method just affects the final output list which has a very limited scope #TARGET_REF .",
                "In this paper we present a simple and effective approach by introducing a coverage-based feature into NMT.",
                "Unlike previous studies, we do not resort to developing extra models nor reranking the limited n-best translations."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "E.g., #REF developed a coveragebased model to measure the fractional count that a source word is translated during decoding. It can be jointly learned with the NMT model. Alternatively, one can rerank the n-best outputs by coverage-sensitive models, but this method just affects the final output list which has a very limited scope #TARGET_REF . In this paper we present a simple and effective approach by introducing a coverage-based feature into NMT. Unlike previous studies, we do not resort to developing extra models nor reranking the limited n-best translations.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Alternatively, one can rerank the n-best outputs by coverage-sensitive models, but this method just affects the final output list which has a very limited scope #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In NMT, several good methods have been developed.",
                "The simplest of these is length normalization which penalizes short translations in decoding #TARGET_REF .",
                "More sophisticated methods focus on modeling the coverage problem with extra sub-modules in NMT and require a training process (#REF; #REF) .",
                "Perhaps the most related work to this paper is #REF .",
                "In their work, the coverage problem can be interpreted in a probability story."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In NMT, several good methods have been developed. The simplest of these is length normalization which penalizes short translations in decoding #TARGET_REF . More sophisticated methods focus on modeling the coverage problem with extra sub-modules in NMT and require a training process (#REF; #REF) . Perhaps the most related work to this paper is #REF . In their work, the coverage problem can be interpreted in a probability story.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The simplest of these is length normalization which penalizes short translations in decoding #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The simplest of these is length normalization which penalizes short translations in decoding (#REF) .",
                "More sophisticated methods focus on modeling the coverage problem with extra sub-modules in NMT and require a training process (#REF; #REF) .",
                "Perhaps the most related work to this paper is #TARGET_REF .",
                "In their work, the coverage problem can be interpreted in a probability story.",
                "However, it fails to account for the cases that one source word is translated into multiple target words and is thus of a total attention score > 1."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The simplest of these is length normalization which penalizes short translations in decoding (#REF) . More sophisticated methods focus on modeling the coverage problem with extra sub-modules in NMT and require a training process (#REF; #REF) . Perhaps the most related work to this paper is #TARGET_REF . In their work, the coverage problem can be interpreted in a probability story. However, it fails to account for the cases that one source word is translated into multiple target words and is thus of a total attention score > 1.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Perhaps the most related work to this paper is #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "For a given target position j, the attention-based NMT computes attention score a ij for each source position i. a ij can be regarded as the measure of the correspondent strength between i and j, and is normalized over all source positions (i.e.,",
                "Here, we present a coverage score (CS) to describe to what extent the source words are translated.",
                "In principle, the coverage score should be high if the translation covers most words in source sentence, and low if it covers only a few of them.",
                "Given a source position i, we define its coverage as the sum of the past attention probabilities c i = |y| j a ij #TARGET_REF; #REF) .",
                "Then, the coverage score of the sentence pair (x, y) is defined as the sum of the truncated coverage over all positions (See Figure 1 for an 1 As the discussion of the attention mechanism is out of the scope of this work, we refer the reader to #REF ; #REF for more details."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "For a given target position j, the attention-based NMT computes attention score a ij for each source position i. a ij can be regarded as the measure of the correspondent strength between i and j, and is normalized over all source positions (i.e., Here, we present a coverage score (CS) to describe to what extent the source words are translated. In principle, the coverage score should be high if the translation covers most words in source sentence, and low if it covers only a few of them. Given a source position i, we define its coverage as the sum of the past attention probabilities c i = |y| j a ij #TARGET_REF; #REF) . Then, the coverage score of the sentence pair (x, y) is defined as the sum of the truncated coverage over all positions (See Figure 1 for an 1 As the discussion of the attention mechanism is out of the scope of this work, we refer the reader to #REF ; #REF for more details.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Given a source position i, we define its coverage as the sum of the past attention probabilities c i = |y| j a ij #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "For the English-German task, BPE (#REF) was used for better performance.",
                "For comparison, we re-implemented the length normalization (LN) and coverage penalty (CP) methods #TARGET_REF .",
                "We used grid search to tune all hyperparameters on the development set as #REF .",
                "Specifically, weights for both CP and our CS are evaluated in interval [0, 1] with step 0.1, while the weight for LN is in interval [0.5, 1.5].",
                "We found that the settings determined with beam size 10 can be reliably applied to larger beam sizes in the preliminary experiments and thus we tuned all systems with beam size 10."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "For the English-German task, BPE (#REF) was used for better performance. For comparison, we re-implemented the length normalization (LN) and coverage penalty (CP) methods #TARGET_REF . We used grid search to tune all hyperparameters on the development set as #REF . Specifically, weights for both CP and our CS are evaluated in interval [0, 1] with step 0.1, while the weight for LN is in interval [0.5, 1.5]. We found that the settings determined with beam size 10 can be reliably applied to larger beam sizes in the preliminary experiments and thus we tuned all systems with beam size 10.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"For comparison, we re-implemented the length normalization (LN) and coverage penalty (CP) methods #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "For the English-German task, BPE (#REF) was used for better performance.",
                "For comparison, we re-implemented the length normalization (LN) and coverage penalty (CP) methods (#REF) .",
                "We used grid search to tune all hyperparameters on the development set as #TARGET_REF .",
                "Specifically, weights for both CP and our CS are evaluated in interval [0, 1] with step 0.1, while the weight for LN is in interval [0.5, 1.5].",
                "We found that the settings determined with beam size 10 can be reliably applied to larger beam sizes in the preliminary experiments and thus we tuned all systems with beam size 10."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "For the English-German task, BPE (#REF) was used for better performance. For comparison, we re-implemented the length normalization (LN) and coverage penalty (CP) methods (#REF) . We used grid search to tune all hyperparameters on the development set as #TARGET_REF . Specifically, weights for both CP and our CS are evaluated in interval [0, 1] with step 0.1, while the weight for LN is in interval [0.5, 1.5]. We found that the settings determined with beam size 10 can be reliably applied to larger beam sizes in the preliminary experiments and thus we tuned all systems with beam size 10.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"We used grid search to tune all hyperparameters on the development set as #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The truncation with the lowest value β can ensure that the coverage score has a reasonable value.",
                "Here β is similar to model warm-up, which makes the model easy to run in the first few decoding steps.",
                "Note that our way of truncation is different from #TARGET_REF 's, where they clip the coverage into [0, 1] and ignore the fact that a source word may be translated into multiple target words and its coverage should be of a value larger than 1.",
                "For decoding, we incorporate the coverage score into beam search via linear combination with the NMT model score as below,",
                "where y is a partial translation generated during decoding, log P(y|x) is the model score, and α is the coefficient for linear interpolation."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The truncation with the lowest value β can ensure that the coverage score has a reasonable value. Here β is similar to model warm-up, which makes the model easy to run in the first few decoding steps. Note that our way of truncation is different from #TARGET_REF 's, where they clip the coverage into [0, 1] and ignore the fact that a source word may be translated into multiple target words and its coverage should be of a value larger than 1. For decoding, we incorporate the coverage score into beam search via linear combination with the NMT model score as below, where y is a partial translation generated during decoding, log P(y|x) is the model score, and α is the coefficient for linear interpolation.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Note that our way of truncation is different from #TARGET_REF 's, where they clip the coverage into [0, 1] and ignore the fact that a source word may be translated into multiple target words and its coverage should be of a value larger than 1.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, it fails to account for the cases that one source word is translated into multiple target words and is thus of a total attention score > 1.",
                "To address this issue, we remove the probability constraint and make the coverage score interpretable for different cases.",
                "Another difference lies in that our coverage model is applied to every beam search step, while #TARGET_REF 's model affects only a small number of translation outputs.",
                "Previous work have pointed out that BLEU scores of NMT systems drop as beam size increases (#REF; #REF; #REF) , and the existing length normalization and coverage models can alleviate this problem to some extent.",
                "In this work we show that our method can do this much better."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "However, it fails to account for the cases that one source word is translated into multiple target words and is thus of a total attention score > 1. To address this issue, we remove the probability constraint and make the coverage score interpretable for different cases. Another difference lies in that our coverage model is applied to every beam search step, while #TARGET_REF 's model affects only a small number of translation outputs. Previous work have pointed out that BLEU scores of NMT systems drop as beam size increases (#REF; #REF; #REF) , and the existing length normalization and coverage models can alleviate this problem to some extent. In this work we show that our method can do this much better.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Another difference lies in that our coverage model is applied to every beam search step, while #TARGET_REF 's model affects only a small number of translation outputs.\"]}"
    },
    {
        "gold": {
            "text": [
                "The automatic identification of information status (#REF; 1992) , i.e. categorizing discourse entities into different classes on the given-new scale, has recently been identified as an important issue in natural language processing (#REF; #TARGET_REF; .",
                "It is widely acknowledged that information status and, more generally, information structure, 1 is reflected in word order, in the form of referring expressions as well as in prosody.",
                "In computational linguistics, the ability to automatically label text with information status, therefore, could be of great benefit to many applications, including surface realization, text-to-speech synthesis, anaphora resolution, summarization, etc.",
                "The task of automatically labeling text with information status, however, is a difficult one.",
                "Part of the difficulty arises from the fact that, to a certain degree, such labeling requires world knowledge and semantic comprehension of the text, but another obstacle is simply that theoretical notions of information status are not used consistently in the literature."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The automatic identification of information status (#REF; 1992) , i.e. categorizing discourse entities into different classes on the given-new scale, has recently been identified as an important issue in natural language processing (#REF; #TARGET_REF; . It is widely acknowledged that information status and, more generally, information structure, 1 is reflected in word order, in the form of referring expressions as well as in prosody. In computational linguistics, the ability to automatically label text with information status, therefore, could be of great benefit to many applications, including surface realization, text-to-speech synthesis, anaphora resolution, summarization, etc. The task of automatically labeling text with information status, however, is a difficult one. Part of the difficulty arises from the fact that, to a certain degree, such labeling requires world knowledge and semantic comprehension of the text, but another obstacle is simply that theoretical notions of information status are not used consistently in the literature.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"The automatic identification of information status (#REF; 1992) , i.e. categorizing discourse entities into different classes on the given-new scale, has recently been identified as an important issue in natural language processing (#REF; #TARGET_REF; .\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, Wolfgang Bosbach, deputy chairman of the CDU parliamentary group may be known to parts of a German audience but not to other people.",
                "We address this by collecting both hearer-known and hearer-unknown definite expressions into one class UNUSED.",
                "This does not rule out further subclassification (known/unknown) or the possibility of using machine learning techniques to identify this distinction, see #REF .",
                "The fact that #TARGET_REF report the highest confusion rate between NEW and MEDIATED entities may have its roots in this issue.",
                "New."
            ],
            "label": [
                "MOT",
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "For example, Wolfgang Bosbach, deputy chairman of the CDU parliamentary group may be known to parts of a German audience but not to other people. We address this by collecting both hearer-known and hearer-unknown definite expressions into one class UNUSED. This does not rule out further subclassification (known/unknown) or the possibility of using machine learning techniques to identify this distinction, see #REF . The fact that #TARGET_REF report the highest confusion rate between NEW and MEDIATED entities may have its roots in this issue. New.",
        "output": "{\"label\": [\"MOT\", \"BACK\"], \"context\": [\"The fact that #TARGET_REF report the highest confusion rate between NEW and MEDIATED entities may have its roots in this issue.\"]}"
    },
    {
        "gold": {
            "text": [
                "A simpler variant of the task is anaphoricity detection (discourse-new detection) (#REF; #REF; #REF; #REF; #REF) , which divides discourse entities into anaphoric (given) and new.",
                "Identifying discourse-new expressions in texts is helpful as a precursor to coreference resolution, since, by definition, there is no need to identify antecedents for new entities.",
                "In the linguistic literature, referring expressions have been distinguished in much more detail, and there is reason to believe that this could also provide useful information for NLP applications.",
                "#REF and #TARGET_REF developed methods to automatically identify three different classes: OLD, MEDIATED and NEW expressions.",
                "This classification, which is described in #REF , has been used for annotating the Switchboard dialog corpus (#REF) , on which both studies are based."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "A simpler variant of the task is anaphoricity detection (discourse-new detection) (#REF; #REF; #REF; #REF; #REF) , which divides discourse entities into anaphoric (given) and new. Identifying discourse-new expressions in texts is helpful as a precursor to coreference resolution, since, by definition, there is no need to identify antecedents for new entities. In the linguistic literature, referring expressions have been distinguished in much more detail, and there is reason to believe that this could also provide useful information for NLP applications. #REF and #TARGET_REF developed methods to automatically identify three different classes: OLD, MEDIATED and NEW expressions. This classification, which is described in #REF , has been used for annotating the Switchboard dialog corpus (#REF) , on which both studies are based.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#REF and #TARGET_REF developed methods to automatically identify three different classes: OLD, MEDIATED and NEW expressions.\"]}"
    },
    {
        "gold": {
            "text": [
                "\" 2 Two kinds of expressions which fall into this category are unfamiliar definites (3a) and (specific) indefinites (3b).",
                "(3) a. The man who shot a policeman yesterday has not been caught yet.",
                "b. Klose scored a penalty in the 80th minute.",
                "Based on work described in #REF , #TARGET_REF develop a machine learning approach to information-status determination.",
                "They develop a support vector machine (SVM) model from the annotated Switchboard dialogs in order to predict the three possible classes."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "\" 2 Two kinds of expressions which fall into this category are unfamiliar definites (3a) and (specific) indefinites (3b). (3) a. The man who shot a policeman yesterday has not been caught yet. b. Klose scored a penalty in the 80th minute. Based on work described in #REF , #TARGET_REF develop a machine learning approach to information-status determination. They develop a support vector machine (SVM) model from the annotated Switchboard dialogs in order to predict the three possible classes.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Based on work described in #REF , #TARGET_REF develop a machine learning approach to information-status determination.\"]}"
    },
    {
        "gold": {
            "text": [
                "We parse each sentence with the German Lexical Functional Grammar of #REF using the XLE parser in order to automati- #REF show that there are asymmetries between pairs of information status labels contained in sentences, i.e. certain classes of expressions tend to precede certain other classes.",
                "We therefore treat the prediction of IS labels as a sequence labeling task.",
                "4 We train a CRF using wapiti (#REF) , with the features outlined in Table 1 .",
                "We also include a basic \"coreference\" feature, similar to the lexical features of #TARGET_REF , that fires if there is some lexical overlap of nouns (or compound nouns) in the preceding 10 sentences.",
                "The original label set described in contains 21 labels."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We parse each sentence with the German Lexical Functional Grammar of #REF using the XLE parser in order to automati- #REF show that there are asymmetries between pairs of information status labels contained in sentences, i.e. certain classes of expressions tend to precede certain other classes. We therefore treat the prediction of IS labels as a sequence labeling task. 4 We train a CRF using wapiti (#REF) , with the features outlined in Table 1 . We also include a basic \"coreference\" feature, similar to the lexical features of #TARGET_REF , that fires if there is some lexical overlap of nouns (or compound nouns) in the preceding 10 sentences. The original label set described in contains 21 labels.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We also include a basic \\\"coreference\\\" feature, similar to the lexical features of #TARGET_REF , that fires if there is some lexical overlap of nouns (or compound nouns) in the preceding 10 sentences.\"]}"
    },
    {
        "gold": {
            "text": [
                "The aim of these models is to identify abusive language that directly targets certain individuals or groups, particularly people belonging to protected categories (#REF) .",
                "Bias may reduce the accuracy of these models, and at worst, will mean that the models actively discriminate against the same groups they are designed to protect.",
                "Our study focuses on racial bias in hate speech and abusive language detection datasets (#REF; #TARGET_REF; #REF; #REF) , all of which use data collected from Twitter.",
                "We train classifiers using each of the datasets and use a corpus of tweets with demographic information to compare how each classifier performs on tweets written in African-American English (AAE) versus Standard American English (SAE) (#REF) .",
                "We use bootstrap sampling (#REF) to estimate the proportion of tweets in each group that each classifier assigns to each class."
            ],
            "label": [
                "MOT",
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The aim of these models is to identify abusive language that directly targets certain individuals or groups, particularly people belonging to protected categories (#REF) . Bias may reduce the accuracy of these models, and at worst, will mean that the models actively discriminate against the same groups they are designed to protect. Our study focuses on racial bias in hate speech and abusive language detection datasets (#REF; #TARGET_REF; #REF; #REF) , all of which use data collected from Twitter. We train classifiers using each of the datasets and use a corpus of tweets with demographic information to compare how each classifier performs on tweets written in African-American English (AAE) versus Standard American English (SAE) (#REF) . We use bootstrap sampling (#REF) to estimate the proportion of tweets in each group that each classifier assigns to each class.",
        "output": "{\"label\": [\"MOT\", \"BACK\"], \"context\": [\"Our study focuses on racial bias in hate speech and abusive language detection datasets (#REF; #TARGET_REF; #REF; #REF) , all of which use data collected from Twitter.\"]}"
    },
    {
        "gold": {
            "text": [
                "We use all available datasets where tweets are labeled as various types of abuse and are written in English.",
                "We now briefly describe each of these datasets in chronological order.",
                "#TARGET_REF collected 130k tweets containing one of seventeen different terms or phrases they considered to be hateful.",
                "They then annotated a sample of these tweets themselves, using guidelines inspired by critical race theory.",
                "These annotators were then reviewed by \"a 25 year old woman studying gender studies and a nonactivist feminist\" to check for bias."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We use all available datasets where tweets are labeled as various types of abuse and are written in English. We now briefly describe each of these datasets in chronological order. #TARGET_REF collected 130k tweets containing one of seventeen different terms or phrases they considered to be hateful. They then annotated a sample of these tweets themselves, using guidelines inspired by critical race theory. These annotators were then reviewed by \"a 25 year old woman studying gender studies and a nonactivist feminist\" to check for bias.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF collected 130k tweets containing one of seventeen different terms or phrases they considered to be hateful.\"]}"
    },
    {
        "gold": {
            "text": [
                "For the #REF classifier we see that black-aligned tweets are slightly less frequently considered to be hate speech but are much more frequently classified as abusive.",
                "The results for the second variation of Experiment 2 where we conditioned on the word \"b*tch\" are shown in Table 4 .",
                "We see similar results for #REF and #REF .",
                "In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism.",
                "The #TARGET_REF classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "For the #REF classifier we see that black-aligned tweets are slightly less frequently considered to be hate speech but are much more frequently classified as abusive. The results for the second variation of Experiment 2 where we conditioned on the word \"b*tch\" are shown in Table 4 . We see similar results for #REF and #REF . In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism. The #TARGET_REF classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The #TARGET_REF classifier is particularly sensitive to the word \\\"b*tch\\\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class.\"]}"
    },
    {
        "gold": {
            "text": [
                "First, we expect that some biases emerge at the point of data collection.",
                "Some studies sampled tweets using small, ad hoc sets of keywords created by the authors #TARGET_REF; #REF; #REF) , an approach demonstrated to produce poor results (#REF) .",
                "Others start with large crowdsourced dictionaries of keywords, which tend to include many irrelevant terms, resulting in high rates of false positives #REF) .",
                "In both cases, by using keywords to identify relevant tweets we are likely to get non-representative samples of training data that may over-or under-represent certain communities.",
                "In particular, we need to consider whether the linguistic markers we use to identify potentially abusive language may be associated with language used by members of protected categories."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "First, we expect that some biases emerge at the point of data collection. Some studies sampled tweets using small, ad hoc sets of keywords created by the authors #TARGET_REF; #REF; #REF) , an approach demonstrated to produce poor results (#REF) . Others start with large crowdsourced dictionaries of keywords, which tend to include many irrelevant terms, resulting in high rates of false positives #REF) . In both cases, by using keywords to identify relevant tweets we are likely to get non-representative samples of training data that may over-or under-represent certain communities. In particular, we need to consider whether the linguistic markers we use to identify potentially abusive language may be associated with language used by members of protected categories.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Some studies sampled tweets using small, ad hoc sets of keywords created by the authors #TARGET_REF; #REF; #REF) , an approach demonstrated to produce poor results (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Second, we expect that the people who annotate data have their own biases.",
                "Since individual biases in reflect societal prejudices, they aggregate into systematic biases in training data.",
                "The datasets considered here relied upon a range of different annotators, from the authors (#REF; #TARGET_REF and crowdworkers #REF) to activists (#REF) .",
                "Even the classifier trained on expert-labeled data (#REF) flags black-aligned tweets as sexist at almost twice the rate of white-aligned tweets.",
                "While we agree that there is value in working with domain-experts to annotate data, these results suggest that activists may be prone to similar biases as academics and crowdworkers."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Second, we expect that the people who annotate data have their own biases. Since individual biases in reflect societal prejudices, they aggregate into systematic biases in training data. The datasets considered here relied upon a range of different annotators, from the authors (#REF; #TARGET_REF and crowdworkers #REF) to activists (#REF) . Even the classifier trained on expert-labeled data (#REF) flags black-aligned tweets as sexist at almost twice the rate of white-aligned tweets. While we agree that there is value in working with domain-experts to annotate data, these results suggest that activists may be prone to similar biases as academics and crowdworkers.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The datasets considered here relied upon a range of different annotators, from the authors (#REF; #TARGET_REF and crowdworkers #REF) to activists (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The results of Experiment 1 are shown in Table 2.",
                "We observe substantial racial disparities in the performance of all classifiers.",
                "In all but one of the comparisons, there are statistically significant (p < 0.001) differences and in all but one of these we see that tweets in the black-aligned corpus are assigned negative labels more frequently than those by whites.",
                "The only case where blackaligned tweets are classified into a negative class less frequently than white-aligned tweets is the racism class in the #TARGET_REF classifier.",
                "Note, however, the extremely low rate at which tweets are predicted to belong to this class for both groups."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "The results of Experiment 1 are shown in Table 2. We observe substantial racial disparities in the performance of all classifiers. In all but one of the comparisons, there are statistically significant (p < 0.001) differences and in all but one of these we see that tweets in the black-aligned corpus are assigned negative labels more frequently than those by whites. The only case where blackaligned tweets are classified into a negative class less frequently than white-aligned tweets is the racism class in the #TARGET_REF classifier. Note, however, the extremely low rate at which tweets are predicted to belong to this class for both groups.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"In all but one of the comparisons, there are statistically significant (p < 0.001) differences and in all but one of these we see that tweets in the black-aligned corpus are assigned negative labels more frequently than those by whites.\", \"The only case where blackaligned tweets are classified into a negative class less frequently than white-aligned tweets is the racism class in the #TARGET_REF classifier.\"]}"
    },
    {
        "gold": {
            "text": [
                "While some of the remaining disparities are likely due to differences in the distributions of other keywords we did not condition on, we expect that other more innocuous aspects of black-aligned language may be associated with negative labels in the training data, leading classifiers to disproportionately predict that tweets by African-Americans belong to negative classes.",
                "We now discuss the results as they pertain to each of the datasets used.",
                "Classifiers trained on data from #TARGET_REF and #REF only predicted a small fraction of the tweets to be racism.",
                "We suspect that this is due to the composition of their dataset, since the majority of the racist training examples consist of anti-Muslim rather than anti- Table 4 : Experiment 2, t = \"b*tch\" black language.",
                "Across both datasets the words \"n*gger\" and \"n*gga\" appear in 4 and 10 tweets respectively."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "While some of the remaining disparities are likely due to differences in the distributions of other keywords we did not condition on, we expect that other more innocuous aspects of black-aligned language may be associated with negative labels in the training data, leading classifiers to disproportionately predict that tweets by African-Americans belong to negative classes. We now discuss the results as they pertain to each of the datasets used. Classifiers trained on data from #TARGET_REF and #REF only predicted a small fraction of the tweets to be racism. We suspect that this is due to the composition of their dataset, since the majority of the racist training examples consist of anti-Muslim rather than anti- Table 4 : Experiment 2, t = \"b*tch\" black language. Across both datasets the words \"n*gger\" and \"n*gga\" appear in 4 and 10 tweets respectively.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Classifiers trained on data from #TARGET_REF and #REF only predicted a small fraction of the tweets to be racism.\"]}"
    },
    {
        "gold": {
            "text": [
                "The results of Experiment 2 are consistent with the previous results, although there are some notable differences.",
                "In most cases the racial disparities persist, although they are generally smaller in magnitude and in some cases the direction even changes.",
                "Table 3 shows that for tweets containing the word \"n*gga\", classifiers trained on #TARGET_REF and #REF are both predict black-aligned tweets to be instances of sexism approximately 1.5 times as often as white-aligned tweets.",
                "The classifier trained on the data is significantly less likely to classify black-aligned tweets as hate speech, although it is more likely to classify them as offensive.",
                "#REF classifies black-aligned tweets as harassment at a higher rate for both groups than in the previous experiment, although the disparity is narrower."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The results of Experiment 2 are consistent with the previous results, although there are some notable differences. In most cases the racial disparities persist, although they are generally smaller in magnitude and in some cases the direction even changes. Table 3 shows that for tweets containing the word \"n*gga\", classifiers trained on #TARGET_REF and #REF are both predict black-aligned tweets to be instances of sexism approximately 1.5 times as often as white-aligned tweets. The classifier trained on the data is significantly less likely to classify black-aligned tweets as hate speech, although it is more likely to classify them as offensive. #REF classifies black-aligned tweets as harassment at a higher rate for both groups than in the previous experiment, although the disparity is narrower.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"Table 3 shows that for tweets containing the word \\\"n*gga\\\", classifiers trained on #TARGET_REF and #REF are both predict black-aligned tweets to be instances of sexism approximately 1.5 times as often as white-aligned tweets.\"]}"
    },
    {
        "gold": {
            "text": [
                "The results for the second variation of Experiment 2 where we conditioned on the word \"b*tch\" are shown in Table 4 .",
                "We see similar results for #TARGET_REF and #REF .",
                "In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism.",
                "The #REF classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class.",
                "For almost all of these tweets are classified as offensive, however those in the blackaligned corpus are 1.15 times as frequently classified as hate speech."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The results for the second variation of Experiment 2 where we conditioned on the word \"b*tch\" are shown in Table 4 . We see similar results for #TARGET_REF and #REF . In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism. The #REF classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class. For almost all of these tweets are classified as offensive, however those in the blackaligned corpus are 1.15 times as frequently classified as hate speech.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We see similar results for #TARGET_REF and #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Named entity recognition (NER) continues to be an active area of research.",
                "NER research presented here involves development of new statistical and hybrid approaches to identification and disambiguation of gene [1] , protein [2] , chemical names [3] , and clinical entities.",
                "Overwhelmingly, researchers chose statistical or hybrid approaches to the tasks at hand.",
                "This is probably the reason for growing interest in creation of annotated corpora [4] , development of methods for augmenting the existing annotation [5] , speeding up the annotation process [5] , and reducing its cost; evaluating the comparability of results obtained applying the same methods to different collections #TARGET_REF , And increasing compatibility of different annotations [7] .",
                "Increasingly sophisticated relation extraction methods [6, 8] are being applied to a broader set of iii relations [9] ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Named entity recognition (NER) continues to be an active area of research. NER research presented here involves development of new statistical and hybrid approaches to identification and disambiguation of gene [1] , protein [2] , chemical names [3] , and clinical entities. Overwhelmingly, researchers chose statistical or hybrid approaches to the tasks at hand. This is probably the reason for growing interest in creation of annotated corpora [4] , development of methods for augmenting the existing annotation [5] , speeding up the annotation process [5] , and reducing its cost; evaluating the comparability of results obtained applying the same methods to different collections #TARGET_REF , And increasing compatibility of different annotations [7] . Increasingly sophisticated relation extraction methods [6, 8] are being applied to a broader set of iii relations [9] .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"This is probably the reason for growing interest in creation of annotated corpora [4] , development of methods for augmenting the existing annotation [5] , speeding up the annotation process [5] , and reducing its cost; evaluating the comparability of results obtained applying the same methods to different collections #TARGET_REF , And increasing compatibility of different annotations [7] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Overwhelmingly, researchers chose statistical or hybrid approaches to the tasks at hand.",
                "This is probably the reason for growing interest in creation of annotated corpora [4] , development of methods for augmenting the existing annotation [5] , speeding up the annotation process [5] , and reducing its cost; evaluating the comparability of results obtained applying the same methods to different collections [6] , And increasing compatibility of different annotations [7] .",
                "Increasingly sophisticated relation extraction methods #TARGET_REF 8] are being applied to a broader set of iii relations [9] .",
                "Other steps towards deeper understanding of the text include methods for creation of gene profiles [10] , identification of uncertainty [11] , discourse connectivity [12] , and temporal features of clinical conditions [13] .",
                "The applicability of NLP methods to clinical tasks is explored in the work on identification of language impairments [14] and seriousness of suicidal attempts [15] ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Overwhelmingly, researchers chose statistical or hybrid approaches to the tasks at hand. This is probably the reason for growing interest in creation of annotated corpora [4] , development of methods for augmenting the existing annotation [5] , speeding up the annotation process [5] , and reducing its cost; evaluating the comparability of results obtained applying the same methods to different collections [6] , And increasing compatibility of different annotations [7] . Increasingly sophisticated relation extraction methods #TARGET_REF 8] are being applied to a broader set of iii relations [9] . Other steps towards deeper understanding of the text include methods for creation of gene profiles [10] , identification of uncertainty [11] , discourse connectivity [12] , and temporal features of clinical conditions [13] . The applicability of NLP methods to clinical tasks is explored in the work on identification of language impairments [14] and seriousness of suicidal attempts [15] .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Increasingly sophisticated relation extraction methods #TARGET_REF 8] are being applied to a broader set of iii relations [9] .\"]}"
    },
    {
        "gold": {
            "text": [
                "There are also uses of KWS where a view simple speech commands (e.g. \"on\", \"off\") are enough to interact with a device such as a voice-controlled light bulb.",
                "Conventional hybrid approaches to KWS first divide their audio signal in time frames to extract features, e.g., Mel Frequency Cepstral Coefficients (MFCC).",
                "A neural net then estimates phoneme or state posteriors of the keyword Hidden Markov Model in order to calculate the keyword probability using a Viterbi search.",
                "In recent years, end-to-end architectures gained traction that directly classify keyword posterior probabilites based on the previously extracted features, e.g., [1, #TARGET_REF 3, 4, 5] .",
                "Typical application scenarios imply that the device is powered by a battery, and possesses restricted hardware resources to reduce costs."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "There are also uses of KWS where a view simple speech commands (e.g. \"on\", \"off\") are enough to interact with a device such as a voice-controlled light bulb. Conventional hybrid approaches to KWS first divide their audio signal in time frames to extract features, e.g., Mel Frequency Cepstral Coefficients (MFCC). A neural net then estimates phoneme or state posteriors of the keyword Hidden Markov Model in order to calculate the keyword probability using a Viterbi search. In recent years, end-to-end architectures gained traction that directly classify keyword posterior probabilites based on the previously extracted features, e.g., [1, #TARGET_REF 3, 4, 5] . Typical application scenarios imply that the device is powered by a battery, and possesses restricted hardware resources to reduce costs.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In recent years, end-to-end architectures gained traction that directly classify keyword posterior probabilites based on the previously extracted features, e.g., [1, #TARGET_REF 3, 4, 5] .\"]}"
    },
    {
        "gold": {
            "text": [
                "The first convolutional layer of our model is inspired by SincNet and we combine it with DSCconv.",
                "DSCconvs have first been introduced in the domain of Image Processing [8, 13] and have been applied to other domains since: Zhang et al. applied DSCconv to KWS #TARGET_REF .",
                "Kaiser et al. used DSConv for neural machine translation [7] .",
                "They also introduce the \"super-separable\" convolution, a DSConv that also uses grouping and thus reduces the already small number of parameters of DSConv even further.",
                "A similar method is used by ShuffleNet where they combine DSConv with grouping and channel shuffling [14] ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The first convolutional layer of our model is inspired by SincNet and we combine it with DSCconv. DSCconvs have first been introduced in the domain of Image Processing [8, 13] and have been applied to other domains since: Zhang et al. applied DSCconv to KWS #TARGET_REF . Kaiser et al. used DSConv for neural machine translation [7] . They also introduce the \"super-separable\" convolution, a DSConv that also uses grouping and thus reduces the already small number of parameters of DSConv even further. A similar method is used by ShuffleNet where they combine DSConv with grouping and channel shuffling [14] .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"DSCconvs have first been introduced in the domain of Image Processing [8, 13] and have been applied to other domains since: Zhang et al. applied DSCconv to KWS #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "DSConv have been successfully applied to the domain of computer vision [8, 13] , neural translation [7] and KWS #TARGET_REF .",
                "Fig. 3 provides an overview of the steps from a regular convolution to the GDSConv.",
                "The number of parameters of one DSConv layer amounts to N DSConv = k · c in + c in · c out with the kernel size k and the number of input and output channels c in and c out respectively; the first summand is determined by the depthwise convolution, the second summand by the pointwise convolution [7] .",
                "In our model configuration, the depthwise convolution only accounts for roughly 5% of parameters in this layer, the pointwise for 95%.",
                "We therefore reduced the parameters of the pointwise convolution using grouping by a factor g to N GDSConv = k · c in + cin·cout g , rather than the parameters in the depthwise convolution."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "DSConv have been successfully applied to the domain of computer vision [8, 13] , neural translation [7] and KWS #TARGET_REF . Fig. 3 provides an overview of the steps from a regular convolution to the GDSConv. The number of parameters of one DSConv layer amounts to N DSConv = k · c in + c in · c out with the kernel size k and the number of input and output channels c in and c out respectively; the first summand is determined by the depthwise convolution, the second summand by the pointwise convolution [7] . In our model configuration, the depthwise convolution only accounts for roughly 5% of parameters in this layer, the pointwise for 95%. We therefore reduced the parameters of the pointwise convolution using grouping by a factor g to N GDSConv = k · c in + cin·cout g , rather than the parameters in the depthwise convolution.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"DSConv have been successfully applied to the domain of computer vision [8, 13] , neural translation [7] and KWS #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Our key contributions are:",
                "• We propose a neural network architecture tuned towards energy efficiency in microcontrollers grounded on the observation that memory access is costly, while computation is cheap [9] .",
                "• Our keyword-spotting network classifies on raw audio employing SincConvs while at the same time reducing the number of parameters using (G)DSConvs.",
                "[3] , while keeping the number of parameters comparable to #TARGET_REF .",
                "Choi et al. build on this work as they also use a ResNet-inspired architecture."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Our key contributions are: • We propose a neural network architecture tuned towards energy efficiency in microcontrollers grounded on the observation that memory access is costly, while computation is cheap [9] . • Our keyword-spotting network classifies on raw audio employing SincConvs while at the same time reducing the number of parameters using (G)DSConvs. [3] , while keeping the number of parameters comparable to #TARGET_REF . Choi et al. build on this work as they also use a ResNet-inspired architecture.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"\\u2022 Our keyword-spotting network classifies on raw audio employing SincConvs while at the same time reducing the number of parameters using (G)DSConvs.\", \"[3] , while keeping the number of parameters comparable to #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 1 lists these results in comparison with related work.",
                "Compared to the DSConv network in #TARGET_REF , our network is more efficient in terms of accuracy for a given parameter count.",
                "Their biggest model has a 1.2% lower accuracy than our base model while having about 4 times the parameters.",
                "#REF has the most competitive results while we are still able to improve upon their accuracy for a given number of parameters.",
                "They are using 1D convolution along the time dimension as well which may be evidence that this yields better performance for audio processing or at least KWS."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Table 1 lists these results in comparison with related work. Compared to the DSConv network in #TARGET_REF , our network is more efficient in terms of accuracy for a given parameter count. Their biggest model has a 1.2% lower accuracy than our base model while having about 4 times the parameters. #REF has the most competitive results while we are still able to improve upon their accuracy for a given number of parameters. They are using 1D convolution along the time dimension as well which may be evidence that this yields better performance for audio processing or at least KWS.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Compared to the DSConv network in #TARGET_REF , our network is more efficient in terms of accuracy for a given parameter count.\"]}"
    },
    {
        "gold": {
            "text": [
                "Large-scale distributional thesauri created automatically from corpora (#REF; #TARGET_REF; #REF; #REF) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (#REF ) are unavailable or lack coverage.",
                "To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words.",
                "That is, two words are similar if they share a large proportion of contexts.",
                "Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (#REF; #REF; #REF) , identifying and demoting bad neighbors (#REF) , or using more relevant contexts (#REF; #REF) .",
                "For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Large-scale distributional thesauri created automatically from corpora (#REF; #TARGET_REF; #REF; #REF) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (#REF ) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (#REF; #REF; #REF) , identifying and demoting bad neighbors (#REF) , or using more relevant contexts (#REF; #REF) . For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Large-scale distributional thesauri created automatically from corpora (#REF; #TARGET_REF; #REF; #REF) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (#REF ) are unavailable or lack coverage.\"]}"
    },
    {
        "gold": {
            "text": [
                "Large-scale distributional thesauri created automatically from corpora (#REF; #REF; #REF; #REF) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (#REF ) are unavailable or lack coverage.",
                "To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words.",
                "That is, two words are similar if they share a large proportion of contexts.",
                "Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures #TARGET_REF; #REF; #REF) , identifying and demoting bad neighbors (#REF) , or using more relevant contexts (#REF; #REF) .",
                "For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Large-scale distributional thesauri created automatically from corpora (#REF; #REF; #REF; #REF) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (#REF ) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures #TARGET_REF; #REF; #REF) , identifying and demoting bad neighbors (#REF) , or using more relevant contexts (#REF; #REF) . For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures #TARGET_REF; #REF; #REF) , identifying and demoting bad neighbors (#REF) , or using more relevant contexts (#REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts.",
                "The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (#REF; #REF; #REF) or in terms of the syntactic dependencies in which the target appears #TARGET_REF; #REF; #REF) .",
                "The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin's (1998) , cosine, Jensen-Shannon divergence, Dice or Jaccard.",
                "Evaluation of the quality of distributional thesauri is a well know problem in the area (#REF; #REF) .",
                "For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri (#REF) , and at the overlap and rank agreement between the thesauri for target words like nouns (#REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (#REF; #REF; #REF) or in terms of the syntactic dependencies in which the target appears #TARGET_REF; #REF; #REF) . The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin's (1998) , cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area (#REF; #REF) . For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri (#REF) , and at the overlap and rank agreement between the thesauri for target words like nouns (#REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (#REF; #REF; #REF) or in terms of the syntactic dependencies in which the target appears #TARGET_REF; #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (#REF; #REF; #REF) or in terms of the syntactic dependencies in which the target appears (#REF; #REF; #REF) .",
                "The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like #TARGET_REF , cosine, Jensen-Shannon divergence, Dice or Jaccard.",
                "Evaluation of the quality of distributional thesauri is a well know problem in the area (#REF; #REF) .",
                "For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri (#REF) , and at the overlap and rank agreement between the thesauri for target words like nouns (#REF) .",
                "Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (#REF; #REF; #REF) or in terms of the syntactic dependencies in which the target appears (#REF; #REF; #REF) . The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like #TARGET_REF , cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area (#REF; #REF) . For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri (#REF) , and at the overlap and rank agreement between the thesauri for target words like nouns (#REF) . Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like #TARGET_REF , cosine, Jensen-Shannon divergence, Dice or Jaccard.\"]}"
    },
    {
        "gold": {
            "text": [
                "The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (#REF; #REF; #REF) or in terms of the syntactic dependencies in which the target appears (#REF; #REF; #REF) .",
                "The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin's (1998) , cosine, Jensen-Shannon divergence, Dice or Jaccard.",
                "Evaluation of the quality of distributional thesauri is a well know problem in the area #TARGET_REF; #REF) .",
                "For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri (#REF) , and at the overlap and rank agreement between the thesauri for target words like nouns (#REF) .",
                "Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (#REF; #REF; #REF) or in terms of the syntactic dependencies in which the target appears (#REF; #REF; #REF) . The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin's (1998) , cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area #TARGET_REF; #REF) . For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri (#REF) , and at the overlap and rank agreement between the thesauri for target words like nouns (#REF) . Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Evaluation of the quality of distributional thesauri is a well know problem in the area #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Contexts are extracted from syntactic dependencies generated by RASP (#REF) , using nouns (heads of NPs) which have subject and direct object relations with the target verb.",
                "Thus, each target verb is represented by a set of triples containing (i) the verb itself, (ii) a context noun and (iii) a syntactic relation (object, subject).",
                "The thesauri were constructed using #TARGET_REF method.",
                "Lin's version of the distributional hypothesis states that two words (verbs v 1 and v 2 in our case) are similar if they share a large proportion of contexts weighted by their information content, assessed with PMI (#REF; #REF) .",
                "In the literature, little attention is paid to context filters."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Contexts are extracted from syntactic dependencies generated by RASP (#REF) , using nouns (heads of NPs) which have subject and direct object relations with the target verb. Thus, each target verb is represented by a set of triples containing (i) the verb itself, (ii) a context noun and (iii) a syntactic relation (object, subject). The thesauri were constructed using #TARGET_REF method. Lin's version of the distributional hypothesis states that two words (verbs v 1 and v 2 in our case) are similar if they share a large proportion of contexts weighted by their information content, assessed with PMI (#REF; #REF) . In the literature, little attention is paid to context filters.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The thesauri were constructed using #TARGET_REF method.\"]}"
    },
    {
        "gold": {
            "text": [
                "Models tackling vision-to-language (V2L) tasks, for example Image Captioning (IC) and Visual Question Answering (VQA), have demonstrated impressive results in recent years in terms of automatic metric scores.",
                "However, whether or not these models are actually learning to address the tasks they are designed for is questionable.",
                "For example, #REF showed that IC models do not understand images sufficiently, as reflected by the generated captions.",
                "As a consequence, in the last few years many diagnostic tasks and datasets have been proposed aiming at investigating the capabilities of such models in more detail to determine whether and how these models are capable of exploiting visual and/or linguistic information #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF) .",
                "FOIL (#REFb ) is one such dataset."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Models tackling vision-to-language (V2L) tasks, for example Image Captioning (IC) and Visual Question Answering (VQA), have demonstrated impressive results in recent years in terms of automatic metric scores. However, whether or not these models are actually learning to address the tasks they are designed for is questionable. For example, #REF showed that IC models do not understand images sufficiently, as reflected by the generated captions. As a consequence, in the last few years many diagnostic tasks and datasets have been proposed aiming at investigating the capabilities of such models in more detail to determine whether and how these models are capable of exploiting visual and/or linguistic information #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF) . FOIL (#REFb ) is one such dataset.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"As a consequence, in the last few years many diagnostic tasks and datasets have been proposed aiming at investigating the capabilities of such models in more detail to determine whether and how these models are capable of exploiting visual and/or linguistic information #TARGET_REF; #REF; #REF; #REF; #REF; #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, #REF showed that IC models do not understand images sufficiently, as reflected by the generated captions.",
                "As a consequence, in the last few years many diagnostic tasks and datasets have been proposed aiming at investigating the capabilities of such models in more detail to determine whether and how these models are capable of exploiting visual and/or linguistic information (#REFb; #REF; #REF; #REF; #REF; #REF; #REF) .",
                "FOIL #TARGET_REF ) is one such dataset.",
                "It was proposed to evaluate the ability of V2L models in understanding the interplay of objects and their attributes in the images and their relations in an image captioning framework.",
                "This is done by replacing a word in MSCOCO (#REF) captions with a 'foiled' word that is semantically similar or related to the original word (substituting dog with cat), thus rendering the image caption unfaithful to the image content, while yet linguistically valid."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "For example, #REF showed that IC models do not understand images sufficiently, as reflected by the generated captions. As a consequence, in the last few years many diagnostic tasks and datasets have been proposed aiming at investigating the capabilities of such models in more detail to determine whether and how these models are capable of exploiting visual and/or linguistic information (#REFb; #REF; #REF; #REF; #REF; #REF; #REF) . FOIL #TARGET_REF ) is one such dataset. It was proposed to evaluate the ability of V2L models in understanding the interplay of objects and their attributes in the images and their relations in an image captioning framework. This is done by replacing a word in MSCOCO (#REF) captions with a 'foiled' word that is semantically similar or related to the original word (substituting dog with cat), thus rendering the image caption unfaithful to the image content, while yet linguistically valid.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"FOIL #TARGET_REF ) is one such dataset.\"]}"
    },
    {
        "gold": {
            "text": [
                "• Foiled Adjective and Adverb: Adjectives and adverbs are replaced with similar adjectives and adverbs.",
                "Here, the notion of similarity again is obtained from external resources;",
                "• Foiled Preposition: Prepositions are directly replaced with functionally similar prepositions.",
                "The Verb, Adjective, Adverb and Preposition subsets were obtained using a slightly different methodology (see Shekhar et al. (2017a) ) than that used for Nouns #TARGET_REF .",
                "Therefore, we evaluate these two groups separately."
            ],
            "label": [
                "BACK",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "• Foiled Adjective and Adverb: Adjectives and adverbs are replaced with similar adjectives and adverbs. Here, the notion of similarity again is obtained from external resources; • Foiled Preposition: Prepositions are directly replaced with functionally similar prepositions. The Verb, Adjective, Adverb and Preposition subsets were obtained using a slightly different methodology (see Shekhar et al. (2017a) ) than that used for Nouns #TARGET_REF . Therefore, we evaluate these two groups separately.",
        "output": "{\"label\": [\"BACK\", \"USE\"], \"context\": [\"The Verb, Adjective, Adverb and Preposition subsets were obtained using a slightly different methodology (see Shekhar et al. (2017a) ) than that used for Nouns #TARGET_REF .\", \"Therefore, we evaluate these two groups separately.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section we describe the foiled caption classification task and dataset.",
                "We combine the tasks and data from #TARGET_REF and Shekhar et al. (2017a) .",
                "Given an image and a caption, in both cases the task is to learn a model that can distinguish between a REAL caption that describes the image, and a FOILed caption where a word from the original caption is swapped such that it no longer describes the image accurately.",
                "There are several sets of 'foiled captions' where words from specific parts of speech are swapped:",
                "• Foiled Noun: In this case a noun word in the original caption is replaced with another similar noun, such that the resultant caption is not the correct description for the image."
            ],
            "label": [
                "USE",
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In this section we describe the foiled caption classification task and dataset. We combine the tasks and data from #TARGET_REF and Shekhar et al. (2017a) . Given an image and a caption, in both cases the task is to learn a model that can distinguish between a REAL caption that describes the image, and a FOILed caption where a word from the original caption is swapped such that it no longer describes the image accurately. There are several sets of 'foiled captions' where words from specific parts of speech are swapped: • Foiled Noun: In this case a noun word in the original caption is replaced with another similar noun, such that the resultant caption is not the correct description for the image.",
        "output": "{\"label\": [\"USE\", \"EXT\"], \"context\": [\"We combine the tasks and data from #TARGET_REF and Shekhar et al. (2017a) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Data: We use the dataset for nouns from #TARGET_REF 1 and the datasets for other parts of speech from Shekhar et al. (2017a) 2 .",
                "Statistics about the dataset are given in Table 1 .",
                "The evaluation metric is accuracy per class and the average (overall) accuracy over the two classes."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "Data: We use the dataset for nouns from #TARGET_REF 1 and the datasets for other parts of speech from Shekhar et al. (2017a) 2 . Statistics about the dataset are given in Table 1 . The evaluation metric is accuracy per class and the average (overall) accuracy over the two classes.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Data: We use the dataset for nouns from #TARGET_REF 1 and the datasets for other parts of speech from Shekhar et al. (2017a) 2 .\"]}"
    },
    {
        "gold": {
            "text": [
                "† are taken directly from Shekhar et al. (2017b) .",
                "HieCoAtt is the state of the art reported in the paper.",
                "bag of objects information are the best performing models across classifiers.",
                "We also note that the performance is better than human performance.",
                "We hypothesize the following reasons for this: (a) human responses were crowd-sourced, which could have resulted in some noisy annotations; (b) our gold object-based features closely resembles the information used for data-generation as described in #TARGET_REF for the foil noun dataset."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "† are taken directly from Shekhar et al. (2017b) . HieCoAtt is the state of the art reported in the paper. bag of objects information are the best performing models across classifiers. We also note that the performance is better than human performance. We hypothesize the following reasons for this: (a) human responses were crowd-sourced, which could have resulted in some noisy annotations; (b) our gold object-based features closely resembles the information used for data-generation as described in #TARGET_REF for the foil noun dataset.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"We also note that the performance is better than human performance.\", \"We hypothesize the following reasons for this: (a) human responses were crowd-sourced, which could have resulted in some noisy annotations; (b) our gold object-based features closely resembles the information used for data-generation as described in #TARGET_REF for the foil noun dataset.\"]}"
    },
    {
        "gold": {
            "text": [
                "The Multimodal LSTM (MM-LSTM) has a slightly better performance than LSTM classifiers.",
                "In all cases, we observe that the performance is on par with human-level accuracy.",
                "Our overall accuracy is substantially higher than that reported in #TARGET_REF .",
                "Interestingly, our implementation of CNN+LSTM produced better results than their equivalent model (they reported 61.07% vs. our 87.45%).",
                "We investigate this further in Section 5."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The Multimodal LSTM (MM-LSTM) has a slightly better performance than LSTM classifiers. In all cases, we observe that the performance is on par with human-level accuracy. Our overall accuracy is substantially higher than that reported in #TARGET_REF . Interestingly, our implementation of CNN+LSTM produced better results than their equivalent model (they reported 61.07% vs. our 87.45%). We investigate this further in Section 5.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Our overall accuracy is substantially higher than that reported in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In the case of LSTMs, adding either image information helps slightly.",
                "The accuracy of our models is substantially higher than that reported in #TARGET_REF , even for equivalent models.",
                "We note, however, that while the trends of image information is similar for other parts of speech datasets, the performance of BOW based models are lower than the performance of LSTM based models.",
                "The anomaly of improved performance of BOW based models seems heavily pronounced in the nouns dataset.",
                "Thus, we further analyze our model in the next section to shed light on whether the high performance is due to the models or the dataset itself."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In the case of LSTMs, adding either image information helps slightly. The accuracy of our models is substantially higher than that reported in #TARGET_REF , even for equivalent models. We note, however, that while the trends of image information is similar for other parts of speech datasets, the performance of BOW based models are lower than the performance of LSTM based models. The anomaly of improved performance of BOW based models seems heavily pronounced in the nouns dataset. Thus, we further analyze our model in the next section to shed light on whether the high performance is due to the models or the dataset itself.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The accuracy of our models is substantially higher than that reported in #TARGET_REF , even for equivalent models.\"]}"
    },
    {
        "gold": {
            "text": [
                "The Vision-and-Language Navigation (VLN) task (#REF) requires an agent to navigate to a particular location in a real-world environment, following complex, context-dependent instructions written by humans (e.g. go down the second hallway on the left, enter the bedroom and stop by the mirror).",
                "The agent must navigate through the environment, conditioning on the instruction as well as the visual imagery that it observes along the route, to stop at the location specified by the instruction (e.g. the mirror).",
                "Recent state-of-the-art models (#REF; #TARGET_REF; #REF) have demonstrated large gains in accuracy on the VLN task.",
                "However, it is unclear which modality these go past the couch … Figure 1 : We factor the grounding of language instructions into visual appearance, route structure, and object detections using a mixture-of-experts approach.",
                "substantial increases in task metrics can be attributed to, and, in particular, whether the gains in performance are due to stronger grounding into visual context or e.g. simply into the discrete, geometric structure of possible routes, such as turning left or moving forward (see Fig. 1"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The Vision-and-Language Navigation (VLN) task (#REF) requires an agent to navigate to a particular location in a real-world environment, following complex, context-dependent instructions written by humans (e.g. go down the second hallway on the left, enter the bedroom and stop by the mirror). The agent must navigate through the environment, conditioning on the instruction as well as the visual imagery that it observes along the route, to stop at the location specified by the instruction (e.g. the mirror). Recent state-of-the-art models (#REF; #TARGET_REF; #REF) have demonstrated large gains in accuracy on the VLN task. However, it is unclear which modality these go past the couch … Figure 1 : We factor the grounding of language instructions into visual appearance, route structure, and object detections using a mixture-of-experts approach. substantial increases in task metrics can be attributed to, and, in particular, whether the gains in performance are due to stronger grounding into visual context or e.g. simply into the discrete, geometric structure of possible routes, such as turning left or moving forward (see Fig. 1",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Recent state-of-the-art models (#REF; #TARGET_REF; #REF) have demonstrated large gains in accuracy on the VLN task.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, it is unclear where the high performance comes from.",
                "In this paper, we find that agents without any visual input can achieve competitive performance, matching or even outperforming their vision-based counterparts under two state-of-theart model models #TARGET_REF; #REF) .",
                "We also explore two approaches to make the agents better utilize their visual inputs.",
                "The role of vision in vision-and-language tasks.",
                "In several vision-and-language tasks, high performance can be achieved without effective modeling of the visual modality."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "However, it is unclear where the high performance comes from. In this paper, we find that agents without any visual input can achieve competitive performance, matching or even outperforming their vision-based counterparts under two state-of-theart model models #TARGET_REF; #REF) . We also explore two approaches to make the agents better utilize their visual inputs. The role of vision in vision-and-language tasks. In several vision-and-language tasks, high performance can be achieved without effective modeling of the visual modality.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"In this paper, we find that agents without any visual input can achieve competitive performance, matching or even outperforming their vision-based counterparts under two state-of-theart model models #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF find that stateof-the-art results can be achieved on the EmbodiedQA task (#REF ) using an agent without visual inputs.",
                "Work concurrent to ours evaluates the performance of single-modality models for several embodied tasks including VLN (#REF) , finding that high performance can be achieved on the R2R dataset using a non-visual version of the baseline model (#REF) .",
                "In this paper, we show that the same trends hold for two recent state-of-the-art architectures (#REF; #TARGET_REF for the VLN task; we also analyze to what extent object-based representations and mixture-ofexperts methods can address these issues.",
                "in a connectivity graph determined by line-of-sight in the physical environment.",
                "See the top row of Fig. 1 for a top-down environment illustration."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "#REF find that stateof-the-art results can be achieved on the EmbodiedQA task (#REF ) using an agent without visual inputs. Work concurrent to ours evaluates the performance of single-modality models for several embodied tasks including VLN (#REF) , finding that high performance can be achieved on the R2R dataset using a non-visual version of the baseline model (#REF) . In this paper, we show that the same trends hold for two recent state-of-the-art architectures (#REF; #TARGET_REF for the VLN task; we also analyze to what extent object-based representations and mixture-ofexperts methods can address these issues. in a connectivity graph determined by line-of-sight in the physical environment. See the top row of Fig. 1 for a top-down environment illustration.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"In this paper, we show that the same trends hold for two recent state-of-the-art architectures (#REF; #TARGET_REF for the VLN task; we also analyze to what extent object-based representations and mixture-ofexperts methods can address these issues.\"]}"
    },
    {
        "gold": {
            "text": [
                "At each timestep, the agent receives the panoramic image for the viewpoint it is currently located at, and either predicts to move to one of the adjacent connected viewpoints, or to stop.",
                "When the agent predicts the stop action, it is evaluated on whether it has correctly reached the end of the route that the human annotator was asked to describe.",
                "In this work, we analyze two recent VLN models, which typify the visual grounding approaches of VLN work: the panoramic \"follower\" model from the Speaker-Follower (SF) system of #TARGET_REF and the Self-Monitoring (SM) model of #REF .",
                "These models obtained stateof-the-art results on the R2R dataset.",
                "Both models are based on the encoder-decoder approach (#REF ) and map an instruction to a sequence of actions in context by encoding the instruction with an LSTM, and outputting actions using an LSTM decoder that conditions on the encoded instruction and visual features summarizing the agent's environmental context."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "At each timestep, the agent receives the panoramic image for the viewpoint it is currently located at, and either predicts to move to one of the adjacent connected viewpoints, or to stop. When the agent predicts the stop action, it is evaluated on whether it has correctly reached the end of the route that the human annotator was asked to describe. In this work, we analyze two recent VLN models, which typify the visual grounding approaches of VLN work: the panoramic \"follower\" model from the Speaker-Follower (SF) system of #TARGET_REF and the Self-Monitoring (SM) model of #REF . These models obtained stateof-the-art results on the R2R dataset. Both models are based on the encoder-decoder approach (#REF ) and map an instruction to a sequence of actions in context by encoding the instruction with an LSTM, and outputting actions using an LSTM decoder that conditions on the encoded instruction and visual features summarizing the agent's environmental context.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"In this work, we analyze two recent VLN models, which typify the visual grounding approaches of VLN work: the panoramic \\\"follower\\\" model from the Speaker-Follower (SF) system of #TARGET_REF and the Self-Monitoring (SM) model of #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We construct a set of vectors {x obj,j } representing detected objects and their attributes.",
                "Each vector x obj,j (j-th detected object in the scene) is a concatenation of summed GloVe vectors (#REF) for the detected object label (e.g. door) and attribute labels (e.g. white) and a location vector from the object's bounding box coordinates.",
                "We then use the same visual attention mechanism as in #TARGET_REF and #REF to obtain an attended object representation x obj,att over these {x obj,j } vectors.",
                "We either substitute the ResNet CNN features x img,att (\"RN\") with our object representation x obj,att (\"Obj\"), or concatenate x img,att and x obj,att (\"RN+Obj\").",
                "Then we train the SF model or the SM model using this object representation, with results shown in Table 2 . 3 For SF (lines 1-4), object representations substantially improve generalization ability: using either the object representation (\"Obj\") or the combined representation (\"RN+Obj\") obtains higher success rate on unseen environments than using only the ResNet features (\"RN\"), and the combined representation (\"RN+Obj\") obtains the highest overall performance."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We construct a set of vectors {x obj,j } representing detected objects and their attributes. Each vector x obj,j (j-th detected object in the scene) is a concatenation of summed GloVe vectors (#REF) for the detected object label (e.g. door) and attribute labels (e.g. white) and a location vector from the object's bounding box coordinates. We then use the same visual attention mechanism as in #TARGET_REF and #REF to obtain an attended object representation x obj,att over these {x obj,j } vectors. We either substitute the ResNet CNN features x img,att (\"RN\") with our object representation x obj,att (\"Obj\"), or concatenate x img,att and x obj,att (\"RN+Obj\"). Then we train the SF model or the SM model using this object representation, with results shown in Table 2 . 3 For SF (lines 1-4), object representations substantially improve generalization ability: using either the object representation (\"Obj\") or the combined representation (\"RN+Obj\") obtains higher success rate on unseen environments than using only the ResNet features (\"RN\"), and the combined representation (\"RN+Obj\") obtains the highest overall performance.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"We then use the same visual attention mechanism as in #TARGET_REF and #REF to obtain an attended object representation x obj,att over these {x obj,j } vectors.\"]}"
    },
    {
        "gold": {
            "text": [
                "The Speaker-Follower (SF) model #TARGET_REF ) and the Self-Monitoring (SM) model (#REF) which we analyze both use sequenceto-sequence model (#REF) with attention (#REF) as their base instruction-following agent.",
                "Both use an encoder LSTM (#REF ) to represent the instruction text, and a decoder LSTM to predict actions sequentially.",
                "At each timestep, the decoder LSTM conditions on the action previously taken, a representation of the visual context at the agent's current location, and an attended representation of the encoded instruction.",
                "While at a high level these models are similar (at least in terms of the base sequence-tosequence models -both papers additionally develop techniques to select routes from these base models during search-based inference techniques, either using a separate language generation model in SF, or a progress-monitor in SM), they differ in the mechanism by which they combine representations of the text instruction and visual input.",
                "The SM uses a co-grounded attention mechanism, where both the visual attention on image features and the textual attention on the instruction words are generated based on previous decoder LSTM hidden state h t−1 , and then the attended visual and textual features are used as LSTM inputs to produce h t ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The Speaker-Follower (SF) model #TARGET_REF ) and the Self-Monitoring (SM) model (#REF) which we analyze both use sequenceto-sequence model (#REF) with attention (#REF) as their base instruction-following agent. Both use an encoder LSTM (#REF ) to represent the instruction text, and a decoder LSTM to predict actions sequentially. At each timestep, the decoder LSTM conditions on the action previously taken, a representation of the visual context at the agent's current location, and an attended representation of the encoded instruction. While at a high level these models are similar (at least in terms of the base sequence-tosequence models -both papers additionally develop techniques to select routes from these base models during search-based inference techniques, either using a separate language generation model in SF, or a progress-monitor in SM), they differ in the mechanism by which they combine representations of the text instruction and visual input. The SM uses a co-grounded attention mechanism, where both the visual attention on image features and the textual attention on the instruction words are generated based on previous decoder LSTM hidden state h t−1 , and then the attended visual and textual features are used as LSTM inputs to produce h t .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"The Speaker-Follower (SF) model #TARGET_REF ) and the Self-Monitoring (SM) model (#REF) which we analyze both use sequenceto-sequence model (#REF) with attention (#REF) as their base instruction-following agent.\"]}"
    },
    {
        "gold": {
            "text": [
                "For SM, we use a reimplementation without the progress monitor, which was shown to be most important for search in inference (#REF) .",
                "We investigate how well these models ground instructions into visual features of the environment, by training and evaluating them without access to the visual context: setting their visual feature vectors to zeroes during training and testing.",
                "We compare performance on the validation sets of the R2R dataset: the val-seen split, consisting of the same environments as in training, and the val- Table 1 : Success rate (SR) of the vision-based full agent (\"RN\", using ResNet) and the non-visual agent (\"no vis.\", setting all visual features to zero) on the R2R dataset under different model architectures (SpeakerFollower (SF) #TARGET_REF and Self-Monitoring (SM) (#REF) ) and training schemes.",
                "unseen split of novel environments.",
                "Since we aim to evaluate how well the agents generalize to the unseen environments, we focus on the val-unseen split."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "For SM, we use a reimplementation without the progress monitor, which was shown to be most important for search in inference (#REF) . We investigate how well these models ground instructions into visual features of the environment, by training and evaluating them without access to the visual context: setting their visual feature vectors to zeroes during training and testing. We compare performance on the validation sets of the R2R dataset: the val-seen split, consisting of the same environments as in training, and the val- Table 1 : Success rate (SR) of the vision-based full agent (\"RN\", using ResNet) and the non-visual agent (\"no vis.\", setting all visual features to zero) on the R2R dataset under different model architectures (SpeakerFollower (SF) #TARGET_REF and Self-Monitoring (SM) (#REF) ) and training schemes. unseen split of novel environments. Since we aim to evaluate how well the agents generalize to the unseen environments, we focus on the val-unseen split.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"We compare performance on the validation sets of the R2R dataset: the val-seen split, consisting of the same environments as in training, and the val- Table 1 : Success rate (SR) of the vision-based full agent (\\\"RN\\\", using ResNet) and the non-visual agent (\\\"no vis.\\\", setting all visual features to zero) on the R2R dataset under different model architectures (SpeakerFollower (SF) #TARGET_REF and Self-Monitoring (SM) (#REF) ) and training schemes.\"]}"
    },
    {
        "gold": {
            "text": [
                "Confusion network decoding has been applied in combining outputs from multiple machine translation systems.",
                "The earliest approach in (#REF ) used edit distance based multiple string alignment (MSA) (#REF) to build the confusion networks.",
                "The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (#REF) or edit distance alignments allowing shifts #TARGET_REF .",
                "The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment.",
                "The confusion networks are built around a \"skeleton\" hypothesis."
            ],
            "label": [
                "BACK",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Confusion network decoding has been applied in combining outputs from multiple machine translation systems. The earliest approach in (#REF ) used edit distance based multiple string alignment (MSA) (#REF) to build the confusion networks. The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (#REF) or edit distance alignments allowing shifts #TARGET_REF . The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment. The confusion networks are built around a \"skeleton\" hypothesis.",
        "output": "{\"label\": [\"BACK\", \"EXT\"], \"context\": [\"The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (#REF) or edit distance alignments allowing shifts #TARGET_REF .\", \"The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment.\"]}"
    },
    {
        "gold": {
            "text": [
                "The incremental hypothesis alignment algorithm combines these two steps.",
                "All words from the previously aligned hypotheses are available, even if not present in the skeleton hypothesis, when aligning the following hypotheses.",
                "As in #TARGET_REF , confusion networks built around all skeletons are joined into a lattice which is expanded and rescored with language models.",
                "System weights and language model weights are tuned to optimize the quality of the decoding output on a development set.",
                "This paper is organized as follows."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The incremental hypothesis alignment algorithm combines these two steps. All words from the previously aligned hypotheses are available, even if not present in the skeleton hypothesis, when aligning the following hypotheses. As in #TARGET_REF , confusion networks built around all skeletons are joined into a lattice which is expanded and rescored with language models. System weights and language model weights are tuned to optimize the quality of the decoding output on a development set. This paper is organized as follows.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"As in #TARGET_REF , confusion networks built around all skeletons are joined into a lattice which is expanded and rescored with language models.\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally, each insertion will generate a new node and two word arcs at the corresponding position in the network.",
                "The first word arc will have the inserted word with the confidence set as in the case of a substitution and the second word arc will have a NULL word with confidences set by assuming all previously aligned hypotheses and the skeleton generated the NULL word arc.",
                "After all hypotheses have been added into the confusion network, the system specific word arc confidences are scaled to sum to one over all arcs between 1 2 3 4 5 6 I (3) like (3) kites (1) NULL (2) NULL (1) big (1) blue (2) balloons (2) Figure 2: Network after incremental TER alignment.",
                "each set of two consecutive nodes.",
                "Other scores for the word arc are set as in #TARGET_REF ."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Finally, each insertion will generate a new node and two word arcs at the corresponding position in the network. The first word arc will have the inserted word with the confidence set as in the case of a substitution and the second word arc will have a NULL word with confidences set by assuming all previously aligned hypotheses and the skeleton generated the NULL word arc. After all hypotheses have been added into the confusion network, the system specific word arc confidences are scaled to sum to one over all arcs between 1 2 3 4 5 6 I (3) like (3) kites (1) NULL (2) NULL (1) big (1) blue (2) balloons (2) Figure 2: Network after incremental TER alignment. each set of two consecutive nodes. Other scores for the word arc are set as in #TARGET_REF .",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"Other scores for the word arc are set as in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The example is taken from the Simple English Wikipedia corpus #TARGET_REF connectives do not belong to any linguistic class and except for a few discourse connectives such as oh and well, most carry meaning.",
                "#REF revised this definition; even though she agreed that discourse connectives have meaning by themselves, she argued that they should contribute to the semantic interpretations of the discourse.",
                "Apart from research efforts aiming at defining discourse connectives, another line of research has focused on providing a list of discourse connectives in English (#REF; #REF; #REF; #REF; #REF; #REF) and other languages (#REF; #REF) .",
                "While most of these inventories have been built by hand, some work has attempted to identify them automatically.",
                "#REF used the Europal parallel corpus and collocation techniques to induce French connectives from their English counterparts."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The example is taken from the Simple English Wikipedia corpus #TARGET_REF connectives do not belong to any linguistic class and except for a few discourse connectives such as oh and well, most carry meaning. #REF revised this definition; even though she agreed that discourse connectives have meaning by themselves, she argued that they should contribute to the semantic interpretations of the discourse. Apart from research efforts aiming at defining discourse connectives, another line of research has focused on providing a list of discourse connectives in English (#REF; #REF; #REF; #REF; #REF; #REF) and other languages (#REF; #REF) . While most of these inventories have been built by hand, some work has attempted to identify them automatically. #REF used the Europal parallel corpus and collocation techniques to induce French connectives from their English counterparts.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The example is taken from the Simple English Wikipedia corpus #TARGET_REF connectives do not belong to any linguistic class and except for a few discourse connectives such as oh and well, most carry meaning.\"]}"
    },
    {
        "gold": {
            "text": [
                "To discover AltLexes automatically, we created two sentence-aligned data sets using standard corpora in text simplification.",
                "The first data set was created from the Simple English Wikipedia corpus #TARGET_REF ; the other was created from the Newsela corpus (#REF) .",
                "The Simple English Wikipedia (SEW) corpus (#REF) contains two sections: 1) article-aligned and 2) sentence-aligned.",
                "Here, we used the sentence-aligned section, which contains 167,686 pairs of aligned sentences.",
                "In order not to overfit to a specific corpus, we also used the Newsela (News) corpus (#REF) ."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "To discover AltLexes automatically, we created two sentence-aligned data sets using standard corpora in text simplification. The first data set was created from the Simple English Wikipedia corpus #TARGET_REF ; the other was created from the Newsela corpus (#REF) . The Simple English Wikipedia (SEW) corpus (#REF) contains two sections: 1) article-aligned and 2) sentence-aligned. Here, we used the sentence-aligned section, which contains 167,686 pairs of aligned sentences. In order not to overfit to a specific corpus, we also used the Newsela (News) corpus (#REF) .",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"The first data set was created from the Simple English Wikipedia corpus #TARGET_REF ; the other was created from the Newsela corpus (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "To discover AltLexes automatically, we created two sentence-aligned data sets using standard corpora in text simplification.",
                "The first data set was created from the Simple English Wikipedia corpus (#REF) ; the other was created from the Newsela corpus (#REF) .",
                "The Simple English Wikipedia (SEW) corpus #TARGET_REF contains two sections: 1) article-aligned and 2) sentence-aligned.",
                "Here, we used the sentence-aligned section, which contains 167,686 pairs of aligned sentences.",
                "In order not to overfit to a specific corpus, we also used the Newsela (News) corpus (#REF) ."
            ],
            "label": [
                "EXT",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "To discover AltLexes automatically, we created two sentence-aligned data sets using standard corpora in text simplification. The first data set was created from the Simple English Wikipedia corpus (#REF) ; the other was created from the Newsela corpus (#REF) . The Simple English Wikipedia (SEW) corpus #TARGET_REF contains two sections: 1) article-aligned and 2) sentence-aligned. Here, we used the sentence-aligned section, which contains 167,686 pairs of aligned sentences. In order not to overfit to a specific corpus, we also used the Newsela (News) corpus (#REF) .",
        "output": "{\"label\": [\"EXT\", \"USE\"], \"context\": [\"The Simple English Wikipedia (SEW) corpus #TARGET_REF contains two sections: 1) article-aligned and 2) sentence-aligned.\", \"Here, we used the sentence-aligned section, which contains 167,686 pairs of aligned sentences.\"]}"
    },
    {
        "gold": {
            "text": [
                "In order not to overfit to a specific corpus, we also used the Newsela (News) corpus (#REF) .",
                "This corpus contains 1,911 English news articles which have been manually re-written at most 5 times, each time with decreasing complexity level.",
                "We used this article-aligned corpus to align it at the sentence-level using an approach similar to #TARGET_REF .",
                "Then, two native English speakers evaluated the alignments.",
                "The Kappa inter-annotation agreement was 0.898 computed on 100 randomly chosen alignments."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In order not to overfit to a specific corpus, we also used the Newsela (News) corpus (#REF) . This corpus contains 1,911 English news articles which have been manually re-written at most 5 times, each time with decreasing complexity level. We used this article-aligned corpus to align it at the sentence-level using an approach similar to #TARGET_REF . Then, two native English speakers evaluated the alignments. The Kappa inter-annotation agreement was 0.898 computed on 100 randomly chosen alignments.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We used this article-aligned corpus to align it at the sentence-level using an approach similar to #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, an automatic way to derive technical paraphrases specific to software engineering is desired.",
                "Paraphrases can be extracted from non-parallel corpora using contextual similarity (#REF) .",
                "They can also be obtained from parallel corpora if such data is available ( #TARGET_REF; #REF) .",
                "Recently, there are also a number of studies that extract paraphrases from multilingual corpora (#REF; #REF) .",
                "The approach in (#REF) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Therefore, an automatic way to derive technical paraphrases specific to software engineering is desired. Paraphrases can be extracted from non-parallel corpora using contextual similarity (#REF) . They can also be obtained from parallel corpora if such data is available ( #TARGET_REF; #REF) . Recently, there are also a number of studies that extract paraphrases from multilingual corpora (#REF; #REF) . The approach in (#REF) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Paraphrases can be extracted from non-parallel corpora using contextual similarity (#REF) .\", \"They can also be obtained from parallel corpora if such data is available ( #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "They can also be obtained from parallel corpora if such data is available (#REF; #REF) .",
                "Recently, there are also a number of studies that extract paraphrases from multilingual corpora (#REF; #REF) .",
                "The approach in ( #TARGET_REF) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours.",
                "Due to this reason, we build our technique on top of theirs.",
                "The following provides a summary of their technique."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "They can also be obtained from parallel corpora if such data is available (#REF; #REF) . Recently, there are also a number of studies that extract paraphrases from multilingual corpora (#REF; #REF) . The approach in ( #TARGET_REF) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. Due to this reason, we build our technique on top of theirs. The following provides a summary of their technique.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The approach in ( #TARGET_REF) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours.\"]}"
    },
    {
        "gold": {
            "text": [
                "In ( #TARGET_REF ), a paraphrase is reported as long as there is a single good supporting pair of sentences.",
                "Although this works well for a relatively clean parallel corpus considered in their work, i.e., novels, this does not work well for bug reports.",
                "Consider the context-peculiar example in Table 1 (bottom).",
                "For a context-peculiar para-phrase, there can be many sentences containing the pair of phrases but very few support them to be a paraphrase.",
                "We develop a technique to offset this noise by computing a global context-based score for two phrases being a paraphrase over all their parallel occurrences."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In ( #TARGET_REF ), a paraphrase is reported as long as there is a single good supporting pair of sentences. Although this works well for a relatively clean parallel corpus considered in their work, i.e., novels, this does not work well for bug reports. Consider the context-peculiar example in Table 1 (bottom). For a context-peculiar para-phrase, there can be many sentences containing the pair of phrases but very few support them to be a paraphrase. We develop a technique to offset this noise by computing a global context-based score for two phrases being a paraphrase over all their parallel occurrences.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In ( #TARGET_REF ), a paraphrase is reported as long as there is a single good supporting pair of sentences.\"]}"
    },
    {
        "gold": {
            "text": [
                "Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor.",
                "In most of the previous studies (#REF; #REF; #TARGET_REF , humor recognition was modeled as a binary classification task",
                "In the seminal work (#REF) , a corpus of 16,000 \"one-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances.",
                "Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers.",
                "In a recent work (#REF) , a new corpus was constructed from a Pun of the Day website."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies (#REF; #REF; #TARGET_REF , humor recognition was modeled as a binary classification task In the seminal work (#REF) , a corpus of 16,000 \"one-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work (#REF) , a new corpus was constructed from a Pun of the Day website.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In most of the previous studies (#REF; #REF; #TARGET_REF , humor recognition was modeled as a binary classification task\"]}"
    },
    {
        "gold": {
            "text": [
                "In the seminal work (#REF) , a corpus of 16,000 \"one-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances.",
                "Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers.",
                "In a recent work #TARGET_REF , a new corpus was constructed from a Pun of the Day website.",
                "It systematically explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style.",
                "In addition, Word2Vec (#REF) distributed representations were utilized in the model building."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In the seminal work (#REF) , a corpus of 16,000 \"one-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work #TARGET_REF , a new corpus was constructed from a Pun of the Day website. It systematically explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style. In addition, Word2Vec (#REF) distributed representations were utilized in the model building.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In a recent work #TARGET_REF , a new corpus was constructed from a Pun of the Day website.\"]}"
    },
    {
        "gold": {
            "text": [
                "We collected 1,192 TED Talk transcripts 4 .",
                "An example transcription is given in Figure 1 .",
                "The collected transcripts were split into sentences using the Stanford CoreNLP tool (#REF) .",
                "In this study, sentences containing or immediately followed by '(Laughter)' were used as humorous sentences, as shown in Figure 1 ; all other sentences were defined as non-humorous sentences.",
                "Following (#REF; #TARGET_REF , we selected the same sizes (n = 4726) of humorous and non-humorous sentences."
            ],
            "label": [
                "BACK",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "We collected 1,192 TED Talk transcripts 4 . An example transcription is given in Figure 1 . The collected transcripts were split into sentences using the Stanford CoreNLP tool (#REF) . In this study, sentences containing or immediately followed by '(Laughter)' were used as humorous sentences, as shown in Figure 1 ; all other sentences were defined as non-humorous sentences. Following (#REF; #TARGET_REF , we selected the same sizes (n = 4726) of humorous and non-humorous sentences.",
        "output": "{\"label\": [\"BACK\", \"USE\"], \"context\": [\"Following (#REF; #TARGET_REF , we selected the same sizes (n = 4726) of humorous and non-humorous sentences.\"]}"
    },
    {
        "gold": {
            "text": [
                "After running 200 iterations of tweaking, we ended up with the following selection: f w is 6 (entailing that the various filter sizes are (5, 6, 7)), f n is 100, dropout 1 is 0.7 and dropout 2 is 0.35, optimization uses Adam (#REF) .",
                "When training the CNN model, we randomly selected 10% of the training data as the validation set for using early stopping to avoid overfitting.",
                "7 https://github.com/ EducationalTestingService/skll 8 https://github.com/fchollet/keras 9 The implementation will be released with the paper On the Pun data, the CNN model shows consistent improved performance over the conventional model, as suggested in #TARGET_REF .",
                "In particular, precision has been greatly increased from 0.762 to 0.864.",
                "On the TED data, we also observed that the CNN model helps to increase precision (from 0.515 to 0.582) and accuracy (from 52.0% to 58.9%)."
            ],
            "label": [
                "BACK",
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "After running 200 iterations of tweaking, we ended up with the following selection: f w is 6 (entailing that the various filter sizes are (5, 6, 7)), f n is 100, dropout 1 is 0.7 and dropout 2 is 0.35, optimization uses Adam (#REF) . When training the CNN model, we randomly selected 10% of the training data as the validation set for using early stopping to avoid overfitting. 7 https://github.com/ EducationalTestingService/skll 8 https://github.com/fchollet/keras 9 The implementation will be released with the paper On the Pun data, the CNN model shows consistent improved performance over the conventional model, as suggested in #TARGET_REF . In particular, precision has been greatly increased from 0.762 to 0.864. On the TED data, we also observed that the CNN model helps to increase precision (from 0.515 to 0.582) and accuracy (from 52.0% to 58.9%).",
        "output": "{\"label\": [\"BACK\", \"SIM\"], \"context\": [\"7 https://github.com/ EducationalTestingService/skll 8 https://github.com/fchollet/keras 9 The implementation will be released with the paper On the Pun data, the CNN model shows consistent improved performance over the conventional model, as suggested in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "By using Long Short Time Memory (LSTM) cells (#REF), Bertero and Fung (2016a) showed that Recurrent Neural Networks (RNNs) perform better on modeling sequential information than Conditional Random Fields (CRFs) (#REF) .",
                "From the brief review, we can find that the limited number of previously created corpora only cover one-line puns or jokes and conversations from TV comedy shows.",
                "There is a great need for an open corpus that can support investigating humor in presentations.",
                "1 CNN-based text categorization methods have been applied for humor recognition (e.g., in (#REFb) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in #TARGET_REF is missing; (b) CNN's performance in the previous research is not quite clear 2 ; and (c) some important techniques that can improve CNN performance (e.g., using variedsized filters and dropout regularization (#REF) ) were missing.",
                "Therefore, the present study is meant to address these limitations."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "By using Long Short Time Memory (LSTM) cells (#REF), Bertero and Fung (2016a) showed that Recurrent Neural Networks (RNNs) perform better on modeling sequential information than Conditional Random Fields (CRFs) (#REF) . From the brief review, we can find that the limited number of previously created corpora only cover one-line puns or jokes and conversations from TV comedy shows. There is a great need for an open corpus that can support investigating humor in presentations. 1 CNN-based text categorization methods have been applied for humor recognition (e.g., in (#REFb) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in #TARGET_REF is missing; (b) CNN's performance in the previous research is not quite clear 2 ; and (c) some important techniques that can improve CNN performance (e.g., using variedsized filters and dropout regularization (#REF) ) were missing. Therefore, the present study is meant to address these limitations.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"1 CNN-based text categorization methods have been applied for humor recognition (e.g., in (#REFb) ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in #TARGET_REF is missing; (b) CNN's performance in the previous research is not quite clear 2 ; and (c) some important techniques that can improve CNN performance (e.g., using variedsized filters and dropout regularization (#REF) ) were missing.\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF , we applied Random Forest (#REF ) to do humor recognition by using the following two groups of features.",
                "The first group are latent semantic structural features covering the following 4 categories 5 : Incongruity (2), Ambiguity (6), Interpersonal Effect (4), and Phonetic Pattern (4).",
                "The second group are semantic distance features, including the humor label classes from 5 sentences in the training set that are closest to this sentence (found by using a k-Nearest Neighbors (kNN) method), and each sentence's averaged Word2Vec representations (n = 300)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "Following #TARGET_REF , we applied Random Forest (#REF ) to do humor recognition by using the following two groups of features. The first group are latent semantic structural features covering the following 4 categories 5 : Incongruity (2), Ambiguity (6), Interpersonal Effect (4), and Phonetic Pattern (4). The second group are semantic distance features, including the humor label classes from 5 sentences in the training set that are closest to this sentence (found by using a k-Nearest Neighbors (kNN) method), and each sentence's averaged Word2Vec representations (n = 300).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following #TARGET_REF , we applied Random Forest (#REF ) to do humor recognition by using the following two groups of features.\"]}"
    },
    {
        "gold": {
            "text": [
                "We used two corpora: the TED Talk corpus (denoted as TED) and the Pun of the Day corpus 6 (denoted as Pun).",
                "Note that we normalized words in the Pun data to lowercase to avoid avoid a possibly elevated result caused by a special pattern: in the original format, all negative instances started with capital letters.",
                "The Pun data allows us to verify that our implementation is consistent with the work reported in #TARGET_REF .",
                "In our experiment, we firstly divided each corpus into two parts.",
                "The smaller part (the Held-Out Partition) was used for tweaking various hyper- Table 1 : Humor recognition on both Pun and TED data sets by using (a) random prediction (Chance), conventional method (Base) and CNN method; the sizes of the dev and CV partitions are provided for each data set."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We used two corpora: the TED Talk corpus (denoted as TED) and the Pun of the Day corpus 6 (denoted as Pun). Note that we normalized words in the Pun data to lowercase to avoid avoid a possibly elevated result caused by a special pattern: in the original format, all negative instances started with capital letters. The Pun data allows us to verify that our implementation is consistent with the work reported in #TARGET_REF . In our experiment, we firstly divided each corpus into two parts. The smaller part (the Held-Out Partition) was used for tweaking various hyper- Table 1 : Humor recognition on both Pun and TED data sets by using (a) random prediction (Chance), conventional method (Base) and CNN method; the sizes of the dev and CV partitions are provided for each data set.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"The Pun data allows us to verify that our implementation is consistent with the work reported in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) #TARGET_REF subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena.",
                "The BERT model performs remarkably well on all cases."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                0
            ]
        },
        "input": "I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) #TARGET_REF subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) #TARGET_REF subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena.\"]}"
    },
    {
        "gold": {
            "text": [
                "[CLS] the game that the guard hates [MASK] bad .",
                "and compare the scores predicted for is and are.",
                "This differs from #REF and #TARGET_REF by considering the entire sentence (excluding the verb) and not just its prefix leading to the verb, and differs from #REF by conditioning the focus verb on bidirectional context.",
                "I use the PyTorch implementation of BERT, with the pre-trained models supplied by Google.",
                "4 I experiment with the bert-large-uncased and bert-base-uncased models."
            ],
            "label": [
                "MOT",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "[CLS] the game that the guard hates [MASK] bad . and compare the scores predicted for is and are. This differs from #REF and #TARGET_REF by considering the entire sentence (excluding the verb) and not just its prefix leading to the verb, and differs from #REF by conditioning the focus verb on bidirectional context. I use the PyTorch implementation of BERT, with the pre-trained models supplied by Google. 4 I experiment with the bert-large-uncased and bert-base-uncased models.",
        "output": "{\"label\": [\"MOT\", \"EXT\"], \"context\": [\"This differs from #REF and #TARGET_REF by considering the entire sentence (excluding the verb) and not just its prefix leading to the verb, and differs from #REF by conditioning the focus verb on bidirectional context.\", \"I use the PyTorch implementation of BERT, with the pre-trained models supplied by Google.\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular, in (#REF) we assess the ability of LSTMs to learn subject-verb agreement patterns in English, and evaluate on naturally occurring wikipedia sentences.",
                "#TARGET_REF also consider subject-verb agreement, but in a #TARGET_REF setting in which content words in naturally occurring sentences are replaced with random words with the same partof-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues.",
                "#REF consider a wider range of syntactic phenomena (subjectverb agreement, reflexive anaphora, negative polarity items) using manually constructed stimuli, allowing for greater coverage and control than in the naturally occurring setting.",
                "The BERT model is based on the \"Transformer\" architecture (#REF) , which-in contrast to RNNs-relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute-position embedding.",
                "This reliance on attention may lead one 1 to expect decreased performance on syntax-sensitive tasks compared to RNN (LSTM) models that do model word order directly, and explicitly track states across the sentence."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In particular, in (#REF) we assess the ability of LSTMs to learn subject-verb agreement patterns in English, and evaluate on naturally occurring wikipedia sentences. #TARGET_REF also consider subject-verb agreement, but in a #TARGET_REF setting in which content words in naturally occurring sentences are replaced with random words with the same partof-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues. #REF consider a wider range of syntactic phenomena (subjectverb agreement, reflexive anaphora, negative polarity items) using manually constructed stimuli, allowing for greater coverage and control than in the naturally occurring setting. The BERT model is based on the \"Transformer\" architecture (#REF) , which-in contrast to RNNs-relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute-position embedding. This reliance on attention may lead one 1 to expect decreased performance on syntax-sensitive tasks compared to RNN (LSTM) models that do model word order directly, and explicitly track states across the sentence.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"In particular, in (#REF) we assess the ability of LSTMs to learn subject-verb agreement patterns in English, and evaluate on naturally occurring wikipedia sentences.\", \"#TARGET_REF also consider subject-verb agreement, but in a #TARGET_REF setting in which content words in naturally occurring sentences are replaced with random words with the same partof-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues.\"]}"
    },
    {
        "gold": {
            "text": [
                "I use the PyTorch implementation of BERT, with the pre-trained models supplied by Google.",
                "4 I experiment with the bert-large-uncased and bert-base-uncased models.",
                "Discarded Material The bi-directional setup precludes using using the NPI stimuli of #REF , in which the minimal pair differs in two words position, which I discard from the evaluation.",
                "I also discard the agreement cases involving the verbs is or are in #REF and in #TARGET_REF , because some of them are copular construction, in which strong agreement hints can be found also on the object following the verb.",
                "5 This is not an issue in the manually constructed (#REF ) stimuli due to the patterns they chose."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "I use the PyTorch implementation of BERT, with the pre-trained models supplied by Google. 4 I experiment with the bert-large-uncased and bert-base-uncased models. Discarded Material The bi-directional setup precludes using using the NPI stimuli of #REF , in which the minimal pair differs in two words position, which I discard from the evaluation. I also discard the agreement cases involving the verbs is or are in #REF and in #TARGET_REF , because some of them are copular construction, in which strong agreement hints can be found also on the object following the verb. 5 This is not an issue in the manually constructed (#REF ) stimuli due to the patterns they chose.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"I also discard the agreement cases involving the verbs is or are in #REF and in #TARGET_REF , because some of them are copular construction, in which strong agreement hints can be found also on the object following the verb.\"]}"
    },
    {
        "gold": {
            "text": [
                "This reliance on attention may lead one 1 to expect decreased performance on syntax-sensitive tasks compared to RNN (LSTM) models that do model word order directly, and explicitly track states across the sentence.",
                "Indeed, #REF finds that transformerbased models perform worse than LSTM models on the #REF agreement prediction dataset.",
                "In contrast, (#REF) find that self-attention performs on par with LSTM for syntax sensitive dependencies in the context of machine-translation, and performance on syntactic tasks is correlated with the number of attention heads in multi-head attention.",
                "I adapt the evaluation protocol and stimuli of #REF , #TARGET_REF and #REF to the bidirectional setting required by BERT, and evaluate the pretrained BERT models (both the LARGE and the BASE models).",
                "Surprisingly (at least to me), the out-of-the-box models (without any task-specific fine-tuning) perform very well on all the syntactic tasks."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "This reliance on attention may lead one 1 to expect decreased performance on syntax-sensitive tasks compared to RNN (LSTM) models that do model word order directly, and explicitly track states across the sentence. Indeed, #REF finds that transformerbased models perform worse than LSTM models on the #REF agreement prediction dataset. In contrast, (#REF) find that self-attention performs on par with LSTM for syntax sensitive dependencies in the context of machine-translation, and performance on syntactic tasks is correlated with the number of attention heads in multi-head attention. I adapt the evaluation protocol and stimuli of #REF , #TARGET_REF and #REF to the bidirectional setting required by BERT, and evaluate the pretrained BERT models (both the LARGE and the BASE models). Surprisingly (at least to me), the out-of-the-box models (without any task-specific fine-tuning) perform very well on all the syntactic tasks.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"I adapt the evaluation protocol and stimuli of #REF , #TARGET_REF and #REF to the bidirectional setting required by BERT, and evaluate the pretrained BERT models (both the LARGE and the BASE models).\"]}"
    },
    {
        "gold": {
            "text": [
                "I use the stimuli provided by (#REF; #TARGET_REF; #REF) , but change the experimental protocol to adapt it to the bidirectional nature of the BERT model.",
                "This requires discarding some of the stimuli, as described below.",
                "Thus, the numbers are not strictly comparable to those reported in previous work."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "I use the stimuli provided by (#REF; #TARGET_REF; #REF) , but change the experimental protocol to adapt it to the bidirectional nature of the BERT model. This requires discarding some of the stimuli, as described below. Thus, the numbers are not strictly comparable to those reported in previous work.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"I use the stimuli provided by (#REF; #TARGET_REF; #REF) , but change the experimental protocol to adapt it to the bidirectional nature of the BERT model.\"]}"
    },
    {
        "gold": {
            "text": [
                "5 This is not an issue in the manually constructed (#REF ) stimuli due to the patterns they chose.",
                "Finally, I discard stimuli in which the focus verb or its plural/singular inflection does not appear as a single word in the BERT wordpiece-based vocabulary (and hence cannot be predicted by the model).",
                "This include discarding #REF stimuli involving the words swims or admires, resulting in 23,368 discarded pairs (out of 152,300).",
                "I similarly discard 680 sentences from (#REF) where the focus verb or its inflection were one of 108 out-ofvocabulary tokens, 6 and 28 sentence-pairs (8 tokens 7 ) from #TARGET_REF .",
                "Limitations The BERT results are not directly comparable to the numbers reported in previous work."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "5 This is not an issue in the manually constructed (#REF ) stimuli due to the patterns they chose. Finally, I discard stimuli in which the focus verb or its plural/singular inflection does not appear as a single word in the BERT wordpiece-based vocabulary (and hence cannot be predicted by the model). This include discarding #REF stimuli involving the words swims or admires, resulting in 23,368 discarded pairs (out of 152,300). I similarly discard 680 sentences from (#REF) where the focus verb or its inflection were one of 108 out-ofvocabulary tokens, 6 and 28 sentence-pairs (8 tokens 7 ) from #TARGET_REF . Limitations The BERT results are not directly comparable to the numbers reported in previous work.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"I similarly discard 680 sentences from (#REF) where the focus verb or its inflection were one of 108 out-ofvocabulary tokens, 6 and 28 sentence-pairs (8 tokens 7 ) from #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Limitations The BERT results are not directly comparable to the numbers reported in previous work.",
                "Beyond the differences due to bidirectionality and the discarded stimuli, the BERT models are also trained on a different and larger corpus (covering both wikipedia and books).",
                "4 https://github.com/huggingface/pytorch-pretrained-BERT 5 Results are generally a bit higher when not discarding the is/are cases.",
                "6 blames, dislike, inhabit, exclude, revolves, governs, delete, composes, overlap, edits, embrace, compose, undertakes, disagrees, redirect, persist, recognise, rotates, accompanies, attach, undertake, earn, communicates, imagine, contradicts, specialize, accuses, obtain, caters, welcomes, interprets, await, communicate, templates, qualify, reverts, achieve, achieves, govern, restricts, violate, behave, emit, contend, adopt, overlaps, reproduces, rotate, defends, submit, revolve, lend, pertain, disagree, concentrate, detects, endorses, detect, predate, persists, consume, locates, earns, predict, interact, merge, consumes, behaves, locate, predates, enhances, predicts, integrates, inhabits, satisfy, contradict, swear, activate, restrict, satisfies, redirects, excludes, violates, interacts, admires, speculate, blame, drag, qualifies, activates, criticize, assures, welcome, depart, characterizes, defend, obtains, lends, strives, accuse, recognises, characterize, contends, perceive, complain, awaits 7 toss, spills, tosses, affirms, spill, melt, approves, affirm Table 2 : Results on the EN NONCE (#REF) stimuli.",
                "While not strictly comparable, the numbers reported by #TARGET_REF for the LSTM in this condition (on All) is 74.1 ± 1.6."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Limitations The BERT results are not directly comparable to the numbers reported in previous work. Beyond the differences due to bidirectionality and the discarded stimuli, the BERT models are also trained on a different and larger corpus (covering both wikipedia and books). 4 https://github.com/huggingface/pytorch-pretrained-BERT 5 Results are generally a bit higher when not discarding the is/are cases. 6 blames, dislike, inhabit, exclude, revolves, governs, delete, composes, overlap, edits, embrace, compose, undertakes, disagrees, redirect, persist, recognise, rotates, accompanies, attach, undertake, earn, communicates, imagine, contradicts, specialize, accuses, obtain, caters, welcomes, interprets, await, communicate, templates, qualify, reverts, achieve, achieves, govern, restricts, violate, behave, emit, contend, adopt, overlaps, reproduces, rotate, defends, submit, revolve, lend, pertain, disagree, concentrate, detects, endorses, detect, predate, persists, consume, locates, earns, predict, interact, merge, consumes, behaves, locate, predates, enhances, predicts, integrates, inhabits, satisfy, contradict, swear, activate, restrict, satisfies, redirects, excludes, violates, interacts, admires, speculate, blame, drag, qualifies, activates, criticize, assures, welcome, depart, characterizes, defend, obtains, lends, strives, accuse, recognises, characterize, contends, perceive, complain, awaits 7 toss, spills, tosses, affirms, spill, melt, approves, affirm Table 2 : Results on the EN NONCE (#REF) stimuli. While not strictly comparable, the numbers reported by #TARGET_REF for the LSTM in this condition (on All) is 74.1 ± 1.6.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"While not strictly comparable, the numbers reported by #TARGET_REF for the LSTM in this condition (on All) is 74.1 \\u00b1 1.6.\"]}"
    },
    {
        "gold": {
            "text": [
                "The BERT models perform remarkably well on all the syntactic test cases.",
                "I expected the attentionbased mechanism to fail on these (compared to the LSTM-based models), and am surprised by these results.",
                "The #TARGET_REF and #REF conditions rule out the possibility of overly relying on selectional preference cues or memorizing the wikipedia training data, and suggest real syntactic generalization is taking place.",
                "Exploring the extent to which deep purely-attention-based architectures such as BERT are capable of capturing hierarchy-sensitive and syntactic dependencies-as well as the mechanisms by which this is achieved-is a fascinating area for future research."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                1,
                0
            ]
        },
        "input": "The BERT models perform remarkably well on all the syntactic test cases. I expected the attentionbased mechanism to fail on these (compared to the LSTM-based models), and am surprised by these results. The #TARGET_REF and #REF conditions rule out the possibility of overly relying on selectional preference cues or memorizing the wikipedia training data, and suggest real syntactic generalization is taking place. Exploring the extent to which deep purely-attention-based architectures such as BERT are capable of capturing hierarchy-sensitive and syntactic dependencies-as well as the mechanisms by which this is achieved-is a fascinating area for future research.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"I expected the attentionbased mechanism to fail on these (compared to the LSTM-based models), and am surprised by these results.\", \"The #TARGET_REF and #REF conditions rule out the possibility of overly relying on selectional preference cues or memorizing the wikipedia training data, and suggest real syntactic generalization is taking place.\"]}"
    },
    {
        "gold": {
            "text": [
                "Qualia Structures have been originally introduced by #TARGET_REF and are used for a variety of purposes in Natural Language processing such as the analysis of compounds (#REF) , co-composition and coercion #TARGET_REF as well as for bridging reference resolution (#REF) .",
                "Further, it has also been argued that qualia structures and lexical semantic relations in general have applications in information retrieval (#REF; #REF) .",
                "One major bottleneck however is that currently Qualia Structures need to be created by hand, which is probably also the reason why there are no practical system using qualia structures, but a lot of systems using globally available resources such as WordNet (#REF) or FrameNet 1 1 http://framenet.icsi.berkeley.edu/ as source of lexical/world knowledge.",
                "The work described in this paper addresses this issue and presents an approach to automatically learning qualia structures for nominals from the Web.",
                "The approach is inspired in recent work on using the Web to identify instances of a relation of interest such as in (#REF) and (#REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Qualia Structures have been originally introduced by #TARGET_REF and are used for a variety of purposes in Natural Language processing such as the analysis of compounds (#REF) , co-composition and coercion #TARGET_REF as well as for bridging reference resolution (#REF) . Further, it has also been argued that qualia structures and lexical semantic relations in general have applications in information retrieval (#REF; #REF) . One major bottleneck however is that currently Qualia Structures need to be created by hand, which is probably also the reason why there are no practical system using qualia structures, but a lot of systems using globally available resources such as WordNet (#REF) or FrameNet 1 1 http://framenet.icsi.berkeley.edu/ as source of lexical/world knowledge. The work described in this paper addresses this issue and presents an approach to automatically learning qualia structures for nominals from the Web. The approach is inspired in recent work on using the Web to identify instances of a relation of interest such as in (#REF) and (#REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Qualia Structures have been originally introduced by #TARGET_REF and are used for a variety of purposes in Natural Language processing such as the analysis of compounds (#REF) , co-composition and coercion #TARGET_REF as well as for bridging reference resolution (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "According to Aristotle, there are four basic factors or causes by which the nature of an object can be described (cf. (#REF) ): the material cause, i.e. the material an object is made of the agentive cause, i.e. the source of movement, creation or change the formal cause, i.e. its form or type the final cause, i.e. its purpose, intention or aim In his Generative Lexicon (GL) framework #TARGET_REF reused Aristotle's basic factors for the description of the meaning of lexical elements.",
                "In fact he introduced so called Qualia Structures by which the meaning of a lexical element is described in terms of four roles:",
                "Constitutive: describing physical properties of an object, i.e. its weight, material as well as parts and components",
                "Agentive: describing factors involved in the bringing about of an object, i.e. its creator or the causal chain leading to its creation Formal: describing that properties which distinguish an object in a larger domain, i.e. orientation, magnitude, shape and dimensionality Telic: describing the purpose or function of an object Most of the qualia structures used in (#REF) however seem to have a more restricted interpretation.",
                "In fact, in most examples the Constitutive role seems to describe the parts or components of an object, while the Agentive role is typically described by a verb denoting an action which typically brings the object in question into existence."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "According to Aristotle, there are four basic factors or causes by which the nature of an object can be described (cf. (#REF) ): the material cause, i.e. the material an object is made of the agentive cause, i.e. the source of movement, creation or change the formal cause, i.e. its form or type the final cause, i.e. its purpose, intention or aim In his Generative Lexicon (GL) framework #TARGET_REF reused Aristotle's basic factors for the description of the meaning of lexical elements. In fact he introduced so called Qualia Structures by which the meaning of a lexical element is described in terms of four roles: Constitutive: describing physical properties of an object, i.e. its weight, material as well as parts and components Agentive: describing factors involved in the bringing about of an object, i.e. its creator or the causal chain leading to its creation Formal: describing that properties which distinguish an object in a larger domain, i.e. orientation, magnitude, shape and dimensionality Telic: describing the purpose or function of an object Most of the qualia structures used in (#REF) however seem to have a more restricted interpretation. In fact, in most examples the Constitutive role seems to describe the parts or components of an object, while the Agentive role is typically described by a verb denoting an action which typically brings the object in question into existence.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"According to Aristotle, there are four basic factors or causes by which the nature of an object can be described (cf. (#REF) ): the material cause, i.e. the material an object is made of the agentive cause, i.e. the source of movement, creation or change the formal cause, i.e. its form or type the final cause, i.e. its purpose, intention or aim In his Generative Lexicon (GL) framework #TARGET_REF reused Aristotle's basic factors for the description of the meaning of lexical elements.\"]}"
    },
    {
        "gold": {
            "text": [
                "In fact he introduced so called Qualia Structures by which the meaning of a lexical element is described in terms of four roles:",
                "Constitutive: describing physical properties of an object, i.e. its weight, material as well as parts and components",
                "Agentive: describing factors involved in the bringing about of an object, i.e. its creator or the causal chain leading to its creation Formal: describing that properties which distinguish an object in a larger domain, i.e. orientation, magnitude, shape and dimensionality Telic: describing the purpose or function of an object Most of the qualia structures used in #TARGET_REF however seem to have a more restricted interpretation.",
                "In fact, in most examples the Constitutive role seems to describe the parts or components of an object, while the Agentive role is typically described by a verb denoting an action which typically brings the object in question into existence.",
                "The Formal role normally consists in typing information about the object, i.e. its hypernym or superconcept."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In fact he introduced so called Qualia Structures by which the meaning of a lexical element is described in terms of four roles: Constitutive: describing physical properties of an object, i.e. its weight, material as well as parts and components Agentive: describing factors involved in the bringing about of an object, i.e. its creator or the causal chain leading to its creation Formal: describing that properties which distinguish an object in a larger domain, i.e. orientation, magnitude, shape and dimensionality Telic: describing the purpose or function of an object Most of the qualia structures used in #TARGET_REF however seem to have a more restricted interpretation. In fact, in most examples the Constitutive role seems to describe the parts or components of an object, while the Agentive role is typically described by a verb denoting an action which typically brings the object in question into existence. The Formal role normally consists in typing information about the object, i.e. its hypernym or superconcept.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Agentive: describing factors involved in the bringing about of an object, i.e. its creator or the causal chain leading to its creation Formal: describing that properties which distinguish an object in a larger domain, i.e. orientation, magnitude, shape and dimensionality Telic: describing the purpose or function of an object Most of the qualia structures used in #TARGET_REF however seem to have a more restricted interpretation.\"]}"
    },
    {
        "gold": {
            "text": [
                "If this value is over a threshold (0.0005 in our case), we assume that it is a valid filler of the Agentive qualia role.",
                "#REF) or #TARGET_REF , as well as computer, an abstract noun, i.e. conversation, as well as two very specific multi-term words, i.e. natural language processing and data mining.",
                "We give the automatically learned weighted Qualia Structures for these entries in Figures 3,  4 , 5 and 6.",
                "The evaluation of our approach consists on the one hand of a discussion of the weighted qualia structures, in particular comparing them to the ideal structures form the literature.",
                "On the other hand, we also asked a student at our institute to assign credits to each of the qualia elements from 0 (incorrect) to 3 (totally correct) whereby 1 credit meaning 'not totally wrong' and 2 meaning 'still acceptable'."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "If this value is over a threshold (0.0005 in our case), we assume that it is a valid filler of the Agentive qualia role. #REF) or #TARGET_REF , as well as computer, an abstract noun, i.e. conversation, as well as two very specific multi-term words, i.e. natural language processing and data mining. We give the automatically learned weighted Qualia Structures for these entries in Figures 3,  4 , 5 and 6. The evaluation of our approach consists on the one hand of a discussion of the weighted qualia structures, in particular comparing them to the ideal structures form the literature. On the other hand, we also asked a student at our institute to assign credits to each of the qualia elements from 0 (incorrect) to 3 (totally correct) whereby 1 credit meaning 'not totally wrong' and 2 meaning 'still acceptable'.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#REF) or #TARGET_REF , as well as computer, an abstract noun, i.e. conversation, as well as two very specific multi-term words, i.e. natural language processing and data mining.\"]}"
    },
    {
        "gold": {
            "text": [
                "The result is then a Weighted Qualia Structure (WQS) in which for each role the qualia elements are weighted according to this Jaccard coefficient.",
                "In what follows we describe in detail the procedure for acquiring qualia elements for each qualia role.",
                "In particular, we describe in detail the clues and lexico-syntactic patterns used.",
                "In general, the patterns have been crafted by hand, testing and refining them in an iterative process, paying attention to maximize their coverage but also accuracy.",
                "In general it is important to mention that by this approach we are not able to detect and separate multiple meanings of words, i.e. to handle polysemy, which is appropriately accounted for in the framework of the Generative Lexicon #TARGET_REF ."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The result is then a Weighted Qualia Structure (WQS) in which for each role the qualia elements are weighted according to this Jaccard coefficient. In what follows we describe in detail the procedure for acquiring qualia elements for each qualia role. In particular, we describe in detail the clues and lexico-syntactic patterns used. In general, the patterns have been crafted by hand, testing and refining them in an iterative process, paying attention to maximize their coverage but also accuracy. In general it is important to mention that by this approach we are not able to detect and separate multiple meanings of words, i.e. to handle polysemy, which is appropriately accounted for in the framework of the Generative Lexicon #TARGET_REF .",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"In general it is important to mention that by this approach we are not able to detect and separate multiple meanings of words, i.e. to handle polysemy, which is appropriately accounted for in the framework of the Generative Lexicon #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section we provide a more subjective evaluation of the automatically learned qualia structures by comparing them to ideal qualia structures discussed in the literature wherever possible.",
                "In particular, we discuss more in detail the qualia structure for book, knife and beer and leave the detailed assessment of the qualia structures for computer, natural language processing, data mining and conversation to the interested reader.",
                "For book, the first four candidates of the Formal role, i.e. product, item, publication and document are very appropriate, but alluding to the physical object meaning of book as opposed to the meaning in the sense of information container (compare #TARGET_REF .",
                "As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to (#REF) .",
                "For the Constitutive role of book we get -besides it at the first position which could be easily filtered out -sign (2nd position), letter (3rd position) and page (6th position), which are quite appropriate."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In this section we provide a more subjective evaluation of the automatically learned qualia structures by comparing them to ideal qualia structures discussed in the literature wherever possible. In particular, we discuss more in detail the qualia structure for book, knife and beer and leave the detailed assessment of the qualia structures for computer, natural language processing, data mining and conversation to the interested reader. For book, the first four candidates of the Formal role, i.e. product, item, publication and document are very appropriate, but alluding to the physical object meaning of book as opposed to the meaning in the sense of information container (compare #TARGET_REF . As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to (#REF) . For the Constitutive role of book we get -besides it at the first position which could be easily filtered out -sign (2nd position), letter (3rd position) and page (6th position), which are quite appropriate.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"For book, the first four candidates of the Formal role, i.e. product, item, publication and document are very appropriate, but alluding to the physical object meaning of book as opposed to the meaning in the sense of information container (compare #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular, we discuss more in detail the qualia structure for book, knife and beer and leave the detailed assessment of the qualia structures for computer, natural language processing, data mining and conversation to the interested reader.",
                "For book, the first four candidates of the Formal role, i.e. product, item, publication and document are very appropriate, but alluding to the physical object meaning of book as opposed to the meaning in the sense of information container (compare (#REF) .",
                "As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to #TARGET_REF .",
                "For the Constitutive role of book we get -besides it at the first position which could be easily filtered out -sign (2nd position), letter (3rd position) and page (6th position), which are quite appropriate.",
                "The top four candidates for the Telic role are give, select, read and purchase."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In particular, we discuss more in detail the qualia structure for book, knife and beer and leave the detailed assessment of the qualia structures for computer, natural language processing, data mining and conversation to the interested reader. For book, the first four candidates of the Formal role, i.e. product, item, publication and document are very appropriate, but alluding to the physical object meaning of book as opposed to the meaning in the sense of information container (compare (#REF) . As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to #TARGET_REF . For the Constitutive role of book we get -besides it at the first position which could be easily filtered out -sign (2nd position), letter (3rd position) and page (6th position), which are quite appropriate. The top four candidates for the Telic role are give, select, read and purchase.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to (#REF) .",
                "For the Constitutive role of book we get -besides it at the first position which could be easily filtered out -sign (2nd position), letter (3rd position) and page (6th position), which are quite appropriate.",
                "The top four candidates for the Telic role are give, select, read and purchase.",
                "It seems that give is emphasizing the role of a book as a gift, read is referring to the most obvious purpose of a book as specified in the ideal qualia structures of #TARGET_REF as well as (#REF) and purchase denotes the more general purpose of a book, i.e. to be bought.",
                "The first element of the Formal role of knife unfortunately denotes the material it is typically made of, i.e. steel, but the next 5 elements are definitely appropriate: weapon, item, kitchenware, object and instrument."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "As candidates for the Agentive role we have make, write and create which are appropriate, write being the ideal filler of the Agentive role according to (#REF) . For the Constitutive role of book we get -besides it at the first position which could be easily filtered out -sign (2nd position), letter (3rd position) and page (6th position), which are quite appropriate. The top four candidates for the Telic role are give, select, read and purchase. It seems that give is emphasizing the role of a book as a gift, read is referring to the most obvious purpose of a book as specified in the ideal qualia structures of #TARGET_REF as well as (#REF) and purchase denotes the more general purpose of a book, i.e. to be bought. The first element of the Formal role of knife unfortunately denotes the material it is typically made of, i.e. steel, but the next 5 elements are definitely appropriate: weapon, item, kitchenware, object and instrument.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"It seems that give is emphasizing the role of a book as a gift, read is referring to the most obvious purpose of a book as specified in the ideal qualia structures of #TARGET_REF as well as (#REF) and purchase denotes the more general purpose of a book, i.e. to be bought.\"]}"
    },
    {
        "gold": {
            "text": [
                "Considering the qualia structure for beer, it is surprising that no purpose has been found.",
                "The reason is that currently no results are returned by Google for the clue a beer is used to and the four snippets returned for the purpose of a beer contain expressions of the form the purpose of a beer is to drink it which is not matched by our patterns as it is a pronoun and not matched by our NP pattern (unless it is matched by an error as in the Qualia Structure for book in Figure 4 ).",
                "Considering the results for the Formal role, the elements drink (1st), alcohol (2nd) and beverage (4th) are much more specific than liquid as given in #TARGET_REF , while thing at the 3rd position is certainly too general.",
                "Furthermore, according to the automatically learned qualia structure, beer is made of rice, malt and hop, which are perfectly reasonable results.",
                "Very interesting are the results concoction and libation for the Formal role of beer, which unfortunately were rated low by our evaluator (compare Figure 3) ."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Considering the qualia structure for beer, it is surprising that no purpose has been found. The reason is that currently no results are returned by Google for the clue a beer is used to and the four snippets returned for the purpose of a beer contain expressions of the form the purpose of a beer is to drink it which is not matched by our patterns as it is a pronoun and not matched by our NP pattern (unless it is matched by an error as in the Qualia Structure for book in Figure 4 ). Considering the results for the Formal role, the elements drink (1st), alcohol (2nd) and beverage (4th) are much more specific than liquid as given in #TARGET_REF , while thing at the 3rd position is certainly too general. Furthermore, according to the automatically learned qualia structure, beer is made of rice, malt and hop, which are perfectly reasonable results. Very interesting are the results concoction and libation for the Formal role of beer, which unfortunately were rated low by our evaluator (compare Figure 3) .",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Considering the results for the Formal role, the elements drink (1st), alcohol (2nd) and beverage (4th) are much more specific than liquid as given in #TARGET_REF , while thing at the 3rd position is certainly too general.\", \"Furthermore, according to the automatically learned qualia structure, beer is made of rice, malt and hop, which are perfectly reasonable results.\"]}"
    },
    {
        "gold": {
            "text": [
                "Such classes can be identified across the entire lexicon, and they may also apply across languages, since their meaning components are said to be cross-linguistically applicable (#REF) .",
                "Offering a powerful tool for generalization, abstraction and prediction, VerbNet classes have been used to support many important NLP tasks, including e.g. computational lexicography, parsing, word sense disambiguation, semantic role labeling, information extraction, questionanswering, and machine translation (#REF; #REF; #REF; #REF) .",
                "However, to date their exploitation has been limited because for most languages, no Levin style classification is available.",
                "Since manual classification is costly (#REF) automatic approaches have been proposed recently which could be used to learn novel classifications in a cost-effective manner (#REF; #REF; Ó Séaghdha and #REF; #REF; #TARGET_REF ).",
                "However, most work on Levin type classification has focussed on English."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Such classes can be identified across the entire lexicon, and they may also apply across languages, since their meaning components are said to be cross-linguistically applicable (#REF) . Offering a powerful tool for generalization, abstraction and prediction, VerbNet classes have been used to support many important NLP tasks, including e.g. computational lexicography, parsing, word sense disambiguation, semantic role labeling, information extraction, questionanswering, and machine translation (#REF; #REF; #REF; #REF) . However, to date their exploitation has been limited because for most languages, no Levin style classification is available. Since manual classification is costly (#REF) automatic approaches have been proposed recently which could be used to learn novel classifications in a cost-effective manner (#REF; #REF; Ó Séaghdha and #REF; #REF; #TARGET_REF ). However, most work on Levin type classification has focussed on English.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Since manual classification is costly (#REF) automatic approaches have been proposed recently which could be used to learn novel classifications in a cost-effective manner (#REF; #REF; \\u00d3 S\\u00e9aghdha and #REF; #REF; #TARGET_REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "Spectral clustering (SPEC) has proved promising in previous verb clustering experiments (Brew and Schulte im #REF; #TARGET_REF ) and other similar NLP tasks involving high dimensional feature space (#REF) .",
                "Following #REF we used the MNCut spectral clustering (#REF ) which has a wide applicability and a clear probabilistic interpretation (von #REF; #REF) .",
                "However, we extended the method to determine the optimal number of clusters automatically using the technique proposed by (#REF) .",
                "Clustering groups a given set of verbs V = {v n } N n=1 into a disjoint partition of K classes.",
                "SPEC takes a similarity matrix as input."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Spectral clustering (SPEC) has proved promising in previous verb clustering experiments (Brew and Schulte im #REF; #TARGET_REF ) and other similar NLP tasks involving high dimensional feature space (#REF) . Following #REF we used the MNCut spectral clustering (#REF ) which has a wide applicability and a clear probabilistic interpretation (von #REF; #REF) . However, we extended the method to determine the optimal number of clusters automatically using the technique proposed by (#REF) . Clustering groups a given set of verbs V = {v n } N n=1 into a disjoint partition of K classes. SPEC takes a similarity matrix as input.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Spectral clustering (SPEC) has proved promising in previous verb clustering experiments (Brew and Schulte im #REF; #TARGET_REF ) and other similar NLP tasks involving high dimensional feature space (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "This experiment involving 514 verbs and 31 classes produced results only slightly better than the random baseline.",
                "In this paper, we investigate the cross-linguistic potential of Levin style classification further.",
                "In past years, verb classification techniques -in particular unsupervised ones -have improved considerably, making investigations for a new language more feasible.",
                "We take a recent verb clustering approach developed for English #TARGET_REF ) and apply it to French -a major language for which no such experiment has been conducted yet.",
                "Basic NLP resources (corpora, taggers, parsers and subcategorization acquisition systems) are now sufficiently developed for this language for the application of a state-ofthe-art verb clustering approach to be realistic."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "This experiment involving 514 verbs and 31 classes produced results only slightly better than the random baseline. In this paper, we investigate the cross-linguistic potential of Levin style classification further. In past years, verb classification techniques -in particular unsupervised ones -have improved considerably, making investigations for a new language more feasible. We take a recent verb clustering approach developed for English #TARGET_REF ) and apply it to French -a major language for which no such experiment has been conducted yet. Basic NLP resources (corpora, taggers, parsers and subcategorization acquisition systems) are now sufficiently developed for this language for the application of a state-ofthe-art verb clustering approach to be realistic.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We take a recent verb clustering approach developed for English #TARGET_REF ) and apply it to French -a major language for which no such experiment has been conducted yet.\"]}"
    },
    {
        "gold": {
            "text": [
                "Following #TARGET_REF we used the MNCut spectral clustering (#REF ) which has a wide applicability and a clear probabilistic interpretation (von #REF; #REF) .",
                "However, we extended the method to determine the optimal number of clusters automatically using the technique proposed by (#REF) .",
                "Clustering groups a given set of verbs V = {v n } N n=1 into a disjoint partition of K classes.",
                "SPEC takes a similarity matrix as input.",
                "All our features can be viewed as probabilistic distributions because the combination of different features is performed via parameterization."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Following #TARGET_REF we used the MNCut spectral clustering (#REF ) which has a wide applicability and a clear probabilistic interpretation (von #REF; #REF) . However, we extended the method to determine the optimal number of clusters automatically using the technique proposed by (#REF) . Clustering groups a given set of verbs V = {v n } N n=1 into a disjoint partition of K classes. SPEC takes a similarity matrix as input. All our features can be viewed as probabilistic distributions because the combination of different features is performed via parameterization.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"Following #TARGET_REF we used the MNCut spectral clustering (#REF ) which has a wide applicability and a clear probabilistic interpretation (von #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We employ the same measures for evaluation as previously employed e.g. byÓ Séaghdha and #REF and #TARGET_REF .",
                "The first measure is modified purity (mPUR) -a global measure which evaluates the mean precision of clusters.",
                "Each cluster is associated with its prevalent class.",
                "The number of verbs in a cluster K that take this class is denoted by n prevalent (K).",
                "Verbs that do not take it are considered as errors."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We employ the same measures for evaluation as previously employed e.g. byÓ Séaghdha and #REF and #TARGET_REF . The first measure is modified purity (mPUR) -a global measure which evaluates the mean precision of clusters. Each cluster is associated with its prevalent class. The number of verbs in a cluster K that take this class is denoted by n prevalent (K). Verbs that do not take it are considered as errors.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"We employ the same measures for evaluation as previously employed e.g. by\\u00d3 S\\u00e9aghdha and #REF and #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "This large subcategorization lexicon provides SCF frequency information for 3,297 French verbs.",
                "It was acquired fully automatically from Le Monde newspaper corpus (200M words from years 1991-2000) using ASSCI -a recent subcategorization acquisition system for French (#REF) .",
                "Systems similar to ASSCI have been used in recent verb classification works e.g. (Schulte im #REF; #REF; #TARGET_REF ).",
                "Like these other systems, ASSCI takes raw corpus data as input.",
                "The data is first tagged and lemmatized using the Tree-Tagger and then parsed using Syntex (#REF) ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "This large subcategorization lexicon provides SCF frequency information for 3,297 French verbs. It was acquired fully automatically from Le Monde newspaper corpus (200M words from years 1991-2000) using ASSCI -a recent subcategorization acquisition system for French (#REF) . Systems similar to ASSCI have been used in recent verb classification works e.g. (Schulte im #REF; #REF; #TARGET_REF ). Like these other systems, ASSCI takes raw corpus data as input. The data is first tagged and lemmatized using the Tree-Tagger and then parsed using Syntex (#REF) .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Systems similar to ASSCI have been used in recent verb classification works e.g. (Schulte im #REF; #REF; #TARGET_REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 2 shows F-measure results for all the features.",
                "The 4th column of the table shows, for comparison, the results of #TARGET_REF obtained for English when they used the same features as us, clustered them using SPEC, and evaluated them against the English version of our gold standard, also using F-measure 2 .",
                "As expected, SPEC (the 2nd column) outperforms K-Means (the 3rd column).",
                "Looking at the basic SCF features F1-F3, we can see that they perform significantly better than the BL method.",
                "F3 performs the best among the three features both in French (50.6 F) and in English (63.3 F)."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Table 2 shows F-measure results for all the features. The 4th column of the table shows, for comparison, the results of #TARGET_REF obtained for English when they used the same features as us, clustered them using SPEC, and evaluated them against the English version of our gold standard, also using F-measure 2 . As expected, SPEC (the 2nd column) outperforms K-Means (the 3rd column). Looking at the basic SCF features F1-F3, we can see that they perform significantly better than the BL method. F3 performs the best among the three features both in French (50.6 F) and in English (63.3 F).",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"The 4th column of the table shows, for comparison, the results of #TARGET_REF obtained for English when they used the same features as us, clustered them using SPEC, and evaluated them against the English version of our gold standard, also using F-measure 2 .\"]}"
    },
    {
        "gold": {
            "text": [
                "The COfeature F7 and the LP feature F13 are not nearly as good (53.4 and 51.0 F).",
                "Although the results at different thresholds are not comparable due to the different number of verbs and classes (see columns 2-3), the results for features at the same threshold are.",
                "Those results suggest that when 2000 or more occurrences per verb are used, most features perform like they performed for English in the experiment of #TARGET_REF which is not typical to many other classes.",
                "Interestingly, Levin classes 29.2, 36.1, 37.3, and 37.7 were among the best performing classes also in the supervised verb classification experiment of #REF because these classes have distinctive characteristics also in English.",
                "The benefit of sophisticated features which integrate also semantic (SP) information (F17) is particularly evident for classes with nondistinctive syntactic characteristics."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The COfeature F7 and the LP feature F13 are not nearly as good (53.4 and 51.0 F). Although the results at different thresholds are not comparable due to the different number of verbs and classes (see columns 2-3), the results for features at the same threshold are. Those results suggest that when 2000 or more occurrences per verb are used, most features perform like they performed for English in the experiment of #TARGET_REF which is not typical to many other classes. Interestingly, Levin classes 29.2, 36.1, 37.3, and 37.7 were among the best performing classes also in the supervised verb classification experiment of #REF because these classes have distinctive characteristics also in English. The benefit of sophisticated features which integrate also semantic (SP) information (F17) is particularly evident for classes with nondistinctive syntactic characteristics.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Those results suggest that when 2000 or more occurrences per verb are used, most features perform like they performed for English in the experiment of #TARGET_REF which is not typical to many other classes.\"]}"
    },
    {
        "gold": {
            "text": [
                "We adopt the best method of #REF where collocations (COs) are extracted from the window of words immediately preceding and following a lemmatized verb.",
                "Stop words are removed prior to extraction.",
                "We adopt a fully unsupervised approach to SP acquisition using the method of #TARGET_REF , with the difference that we determine the optimal number of SP clusters automatically following #REF .",
                "The method is introduced in the following section.",
                "The approach involves (i) taking the GRs (SUBJ, OBJ, IOBJ) associated with verbs, (ii) extracting all the argument heads in these GRs, and (iii) clustering the resulting N most frequent argument heads into M classes."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We adopt the best method of #REF where collocations (COs) are extracted from the window of words immediately preceding and following a lemmatized verb. Stop words are removed prior to extraction. We adopt a fully unsupervised approach to SP acquisition using the method of #TARGET_REF , with the difference that we determine the optimal number of SP clusters automatically following #REF . The method is introduced in the following section. The approach involves (i) taking the GRs (SUBJ, OBJ, IOBJ) associated with verbs, (ii) extracting all the argument heads in these GRs, and (iii) clustering the resulting N most frequent argument heads into M classes.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"We adopt a fully unsupervised approach to SP acquisition using the method of #TARGET_REF , with the difference that we determine the optimal number of SP clusters automatically following #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "When sufficient corpus data is available, there is a strong correlation between the types of features which perform the best in English and French.",
                "When the best features are used, many individual Levin classes have similar performance in the two languages.",
                "Due to differences in data sets direct comparison of performance figures for English and French is not possible.",
                "When considering the general level of performance, our best performance for French (65.4 F) is lower than the best performance for English in the experiment of #TARGET_REF .",
                "However, it does compare favourably to the performance of other stateof-the-art (even supervised) English systems (#REF; #REF; Ó Séaghdha and #REF; #REF) ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "When sufficient corpus data is available, there is a strong correlation between the types of features which perform the best in English and French. When the best features are used, many individual Levin classes have similar performance in the two languages. Due to differences in data sets direct comparison of performance figures for English and French is not possible. When considering the general level of performance, our best performance for French (65.4 F) is lower than the best performance for English in the experiment of #TARGET_REF . However, it does compare favourably to the performance of other stateof-the-art (even supervised) English systems (#REF; #REF; Ó Séaghdha and #REF; #REF) .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"When considering the general level of performance, our best performance for French (65.4 F) is lower than the best performance for English in the experiment of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "As seen in section 7.1, such differences in data can have significant impact on performance.",
                "However, parser and feature extraction performance can also play a big role in overall accuracy, and should therefore be investigated further #TARGET_REF ).",
                "The relatively low performance of basic LP features in French suggests that at least some of the current errors are due to parsing.",
                "Future research should investigate the source of error at different stages of processing.",
                "In addition, it would be interesting to investigate whether language-specific tuning (e.g. using language specific features such as auxiliary classes) can further improve performance on French."
            ],
            "label": [
                "FUT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "As seen in section 7.1, such differences in data can have significant impact on performance. However, parser and feature extraction performance can also play a big role in overall accuracy, and should therefore be investigated further #TARGET_REF ). The relatively low performance of basic LP features in French suggests that at least some of the current errors are due to parsing. Future research should investigate the source of error at different stages of processing. In addition, it would be interesting to investigate whether language-specific tuning (e.g. using language specific features such as auxiliary classes) can further improve performance on French.",
        "output": "{\"label\": [\"FUT\"], \"context\": [\"However, parser and feature extraction performance can also play a big role in overall accuracy, and should therefore be investigated further #TARGET_REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "The first is based on previous neural RC models.",
                "The earliest attempt in (#REF) concatenated all supporting documents and designed a recurrent layer to explicitly exploit the skip connections between entities given automatically gener-ated coreference annotations.",
                "Adding this layer to the neural RC models improved performance on multi-hop tasks.",
                "Recently, an attention based system #TARGET_REF utilizing both documentlevel and entity-level information achieved stateof-the-art results on WIKIHOP data set, proving that techniques like co-attention and self-attention widely employed in single-document RC tasks are also useful in multi-document RC tasks.",
                "The second type of research work is based on graph neural networks (GNN) for multi-hop reasoning."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The first is based on previous neural RC models. The earliest attempt in (#REF) concatenated all supporting documents and designed a recurrent layer to explicitly exploit the skip connections between entities given automatically gener-ated coreference annotations. Adding this layer to the neural RC models improved performance on multi-hop tasks. Recently, an attention based system #TARGET_REF utilizing both documentlevel and entity-level information achieved stateof-the-art results on WIKIHOP data set, proving that techniques like co-attention and self-attention widely employed in single-document RC tasks are also useful in multi-document RC tasks. The second type of research work is based on graph neural networks (GNN) for multi-hop reasoning.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Recently, an attention based system #TARGET_REF utilizing both documentlevel and entity-level information achieved stateof-the-art results on WIKIHOP data set, proving that techniques like co-attention and self-attention widely employed in single-document RC tasks are also useful in multi-document RC tasks.\"]}"
    },
    {
        "gold": {
            "text": [
                "Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (#REF; #REF) , and recently was applied to multiple-hop reading comprehension #TARGET_REF .",
                "Coattention enables the model to combine learned query contextual information attended by document and document contextual information attended by query, with inputs of one query and one document.",
                "We follow the implementation of coattention in (#REF) .",
                "We use the co-attention between a query and a supporting document for illustration.",
                "Same operations can be applied to other documents, or between the query and extracted entities."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (#REF; #REF) , and recently was applied to multiple-hop reading comprehension #TARGET_REF . Coattention enables the model to combine learned query contextual information attended by document and document contextual information attended by query, with inputs of one query and one document. We follow the implementation of coattention in (#REF) . We use the co-attention between a query and a supporting document for illustration. Same operations can be applied to other documents, or between the query and extracted entities.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (#REF; #REF) , and recently was applied to multiple-hop reading comprehension #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "• Instead of graphs with single type of nodes (#REF; De #REF) , the HDE graph contains different types of queryaware nodes representing different granularity levels of information.",
                "Specifically, instead of only entity nodes as in (#REF; De #REF) , we include nodes corresponding to candidates, documents and entities.",
                "In addition, following the success of Coarse-grain Fine-grain Coattention (CFC) network #TARGET_REF , we apply both co-attention and self-attention to learn queryaware node representations of candidates, documents and entities;",
                "• The HDE graph enables rich information interaction among different types of nodes thus facilitate accurate reasoning.",
                "Different types of nodes are connected with different types of edges to highlight the various structural information presented among query, document and candidates."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "• Instead of graphs with single type of nodes (#REF; De #REF) , the HDE graph contains different types of queryaware nodes representing different granularity levels of information. Specifically, instead of only entity nodes as in (#REF; De #REF) , we include nodes corresponding to candidates, documents and entities. In addition, following the success of Coarse-grain Fine-grain Coattention (CFC) network #TARGET_REF , we apply both co-attention and self-attention to learn queryaware node representations of candidates, documents and entities; • The HDE graph enables rich information interaction among different types of nodes thus facilitate accurate reasoning. Different types of nodes are connected with different types of edges to highlight the various structural information presented among query, document and candidates.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In addition, following the success of Coarse-grain Fine-grain Coattention (CFC) network #TARGET_REF , we apply both co-attention and self-attention to learn queryaware node representations of candidates, documents and entities;\"]}"
    },
    {
        "gold": {
            "text": [
                "The study presented in this paper is directly related to existing research on multi-hop reading comprehension across multiple documents (#REF; #REF; De #REF; #TARGET_REF; #REF) .",
                "The method presented in this paper is similar to previous studies using GNN for multi-hop reasoning (#REF; De #REF) .",
                "Our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information.",
                "The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model (#REF) because they show the effectiveness of attention mechanisms.",
                "Our model is very different from the other two studies (#REF; #REF) : these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The study presented in this paper is directly related to existing research on multi-hop reading comprehension across multiple documents (#REF; #REF; De #REF; #TARGET_REF; #REF) . The method presented in this paper is similar to previous studies using GNN for multi-hop reasoning (#REF; De #REF) . Our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information. The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model (#REF) because they show the effectiveness of attention mechanisms. Our model is very different from the other two studies (#REF; #REF) : these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The study presented in this paper is directly related to existing research on multi-hop reading comprehension across multiple documents (#REF; #REF; De #REF; #TARGET_REF; #REF) .\", \"The method presented in this paper is similar to previous studies using GNN for multi-hop reasoning (#REF; De #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information.",
                "The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model #TARGET_REF because they show the effectiveness of attention mechanisms.",
                "Our model is very different from the other two studies (#REF; #REF) : these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs.",
                "Besides these studies, our work is also related to the following research directions.",
                "Multi-hop RC: There exist several different data sets that require reasoning in multiple steps in literature, for example bAbI (#REF) , MultiRC (#REF) and OpenBookQA (#REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information. The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model #TARGET_REF because they show the effectiveness of attention mechanisms. Our model is very different from the other two studies (#REF; #REF) : these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs. Besides these studies, our work is also related to the following research directions. Multi-hop RC: There exist several different data sets that require reasoning in multiple steps in literature, for example bAbI (#REF) , MultiRC (#REF) and OpenBookQA (#REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model #TARGET_REF because they show the effectiveness of attention mechanisms.\"]}"
    },
    {
        "gold": {
            "text": [
                "Based on this observation, we propose to extract mentions of both query subject s and candidates C q from documents.",
                "We will show later that by including mentions of query subject the performance can be improved.",
                "We use simple exact match strategy (De #REF; #TARGET_REF to find the locations of mentions of query subject and candidates, i.e. we need the start and end positions of each mention.",
                "Each mention is treated as an entity.",
                "Then, representations of entities can be taken out from the i-th document encoding H i s ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Based on this observation, we propose to extract mentions of both query subject s and candidates C q from documents. We will show later that by including mentions of query subject the performance can be improved. We use simple exact match strategy (De #REF; #TARGET_REF to find the locations of mentions of query subject and candidates, i.e. we need the start and end positions of each mention. Each mention is treated as an entity. Then, representations of entities can be taken out from the i-th document encoding H i s .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use simple exact match strategy (De #REF; #TARGET_REF to find the locations of mentions of query subject and candidates, i.e. we need the start and end positions of each mention.\"]}"
    },
    {
        "gold": {
            "text": [
                "Then, representations of entities can be taken out from the i-th document encoding H i s .",
                "We denote an entity's representation as M ∈ R lm×h where l m is the length of the entity.",
                "Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (#REF; #REF) , and recently was applied to multiple-hop reading comprehension (#REF) .",
                "Coattention enables the model to combine learned query contextual information attended by document and document contextual information attended by query, with inputs of one query and one document.",
                "We follow the implementation of coattention in #TARGET_REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Then, representations of entities can be taken out from the i-th document encoding H i s . We denote an entity's representation as M ∈ R lm×h where l m is the length of the entity. Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (#REF; #REF) , and recently was applied to multiple-hop reading comprehension (#REF) . Coattention enables the model to combine learned query contextual information attended by document and document contextual information attended by query, with inputs of one query and one document. We follow the implementation of coattention in #TARGET_REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We follow the implementation of coattention in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We further encode the co-attended document context using a bidirectional RNN f with GRU:",
                "The final co-attention context is the columnwise concatenation of C s and D s :",
                "We expect S ca carries query-aware contextual information of supporting documents as shown by #TARGET_REF .",
                "The same co-attention module can also be applied to query and candidates, and query and entities (as shown in Figure 2 ) to get C ca and E ca . Note that we do not do coattention between query and entities corresponding to query subject because query subject is already a part of the query.",
                "To keep the dimensionality consistent, we apply a single-layer multi-layer perceptron (MLP) with tanh activation function to increase the dimension of the query subject entities to 2h."
            ],
            "label": [
                "USE",
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We further encode the co-attended document context using a bidirectional RNN f with GRU: The final co-attention context is the columnwise concatenation of C s and D s : We expect S ca carries query-aware contextual information of supporting documents as shown by #TARGET_REF . The same co-attention module can also be applied to query and candidates, and query and entities (as shown in Figure 2 ) to get C ca and E ca . Note that we do not do coattention between query and entities corresponding to query subject because query subject is already a part of the query. To keep the dimensionality consistent, we apply a single-layer multi-layer perceptron (MLP) with tanh activation function to increase the dimension of the query subject entities to 2h.",
        "output": "{\"label\": [\"USE\", \"DIFFER\"], \"context\": [\"We expect S ca carries query-aware contextual information of supporting documents as shown by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The final co-attention context is the columnwise concatenation of C s and D s :",
                "We expect S ca carries query-aware contextual information of supporting documents as shown by #REF .",
                "The same co-attention module can also be applied to query and candidates, and query and entities (as shown in Figure 2 ) to get C ca and E ca . Note that we do not do coattention between query and entities corresponding to query subject because query subject is already a part of the query.",
                "To keep the dimensionality consistent, we apply a single-layer multi-layer perceptron (MLP) with tanh activation function to increase the dimension of the query subject entities to 2h.",
                "Self-attentive pooling: while co-attention yields a query-aware contextual representation of documents, self-attentive pooling is designed to convert the sequential contextual representation to a fixed dimensional non-sequential feature vector by selecting important query-aware information #TARGET_REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The final co-attention context is the columnwise concatenation of C s and D s : We expect S ca carries query-aware contextual information of supporting documents as shown by #REF . The same co-attention module can also be applied to query and candidates, and query and entities (as shown in Figure 2 ) to get C ca and E ca . Note that we do not do coattention between query and entities corresponding to query subject because query subject is already a part of the query. To keep the dimensionality consistent, we apply a single-layer multi-layer perceptron (MLP) with tanh activation function to increase the dimension of the query subject entities to 2h. Self-attentive pooling: while co-attention yields a query-aware contextual representation of documents, self-attentive pooling is designed to convert the sequential contextual representation to a fixed dimensional non-sequential feature vector by selecting important query-aware information #TARGET_REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Self-attentive pooling: while co-attention yields a query-aware contextual representation of documents, self-attentive pooling is designed to convert the sequential contextual representation to a fixed dimensional non-sequential feature vector by selecting important query-aware information #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition, following the success of Coarse-grain Fine-grain Coattention (CFC) network (#REF) , we apply both co-attention and self-attention to learn queryaware node representations of candidates, documents and entities;",
                "• The HDE graph enables rich information interaction among different types of nodes thus facilitate accurate reasoning.",
                "Different types of nodes are connected with different types of edges to highlight the various structural information presented among query, document and candidates.",
                "Through ablation studies, we show the effectiveness of our proposed HDE graph for multi-hop multi-document RC task.",
                "Evaluated on the blind test set of WIKIHOP, our proposed end-to-end trained single neural model beats the current stateof-the-art results in #TARGET_REF 1 , without using pretrained contextual ELMo embedding (#REF) ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "In addition, following the success of Coarse-grain Fine-grain Coattention (CFC) network (#REF) , we apply both co-attention and self-attention to learn queryaware node representations of candidates, documents and entities; • The HDE graph enables rich information interaction among different types of nodes thus facilitate accurate reasoning. Different types of nodes are connected with different types of edges to highlight the various structural information presented among query, document and candidates. Through ablation studies, we show the effectiveness of our proposed HDE graph for multi-hop multi-document RC task. Evaluated on the blind test set of WIKIHOP, our proposed end-to-end trained single neural model beats the current stateof-the-art results in #TARGET_REF 1 , without using pretrained contextual ELMo embedding (#REF) .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Evaluated on the blind test set of WIKIHOP, our proposed end-to-end trained single neural model beats the current stateof-the-art results in #TARGET_REF 1 , without using pretrained contextual ELMo embedding (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "where M LP (·) is a two-layer MLP with tanh as activation function.",
                "Similarly, after self-attentive pooling, we can get c sa and e sa for each candidate and entity.",
                "Our context encoding module is different from the one used in #TARGET_REF in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model.",
                "2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while #REF first do self-attention on entity word sequences to get a sequence of entity vectors in each documents.",
                "Then, they apply coattention with query."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "where M LP (·) is a two-layer MLP with tanh as activation function. Similarly, after self-attentive pooling, we can get c sa and e sa for each candidate and entity. Our context encoding module is different from the one used in #TARGET_REF in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model. 2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while #REF first do self-attention on entity word sequences to get a sequence of entity vectors in each documents. Then, they apply coattention with query.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Our context encoding module is different from the one used in #TARGET_REF in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model.\"]}"
    },
    {
        "gold": {
            "text": [
                "where M LP (·) is a two-layer MLP with tanh as activation function.",
                "Similarly, after self-attentive pooling, we can get c sa and e sa for each candidate and entity.",
                "Our context encoding module is different from the one used in #REF in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model.",
                "2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while #TARGET_REF first do self-attention on entity word sequences to get a sequence of entity vectors in each documents.",
                "Then, they apply coattention with query."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "where M LP (·) is a two-layer MLP with tanh as activation function. Similarly, after self-attentive pooling, we can get c sa and e sa for each candidate and entity. Our context encoding module is different from the one used in #REF in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model. 2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while #TARGET_REF first do self-attention on entity word sequences to get a sequence of entity vectors in each documents. Then, they apply coattention with query.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while #TARGET_REF first do self-attention on entity word sequences to get a sequence of entity vectors in each documents.\"]}"
    },
    {
        "gold": {
            "text": [
                "In Table 1 , we show the results of the our proposed HDE graph based model on both development and test set and compare it with previously published results.",
                "We show that our proposed HDE graph based model improves the state-of-the-art accuracy on development set from 67.1% (#REF) to 68.1%, on the blind test set from 70.6% #TARGET_REF to 70.9%.",
                "Compared to two previous studies using GNN for multi-hop reading comprehension (#REF; De #REF) , our model surpasses them by a large margin even though we do not use better pre-trained contextual embedding ELMo (#REF) ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0
            ]
        },
        "input": "In Table 1 , we show the results of the our proposed HDE graph based model on both development and test set and compare it with previously published results. We show that our proposed HDE graph based model improves the state-of-the-art accuracy on development set from 67.1% (#REF) to 68.1%, on the blind test set from 70.6% #TARGET_REF to 70.9%. Compared to two previous studies using GNN for multi-hop reading comprehension (#REF; De #REF) , our model surpasses them by a large margin even though we do not use better pre-trained contextual embedding ELMo (#REF) .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We show that our proposed HDE graph based model improves the state-of-the-art accuracy on development set from 67.1% (#REF) to 68.1%, on the blind test set from 70.6% #TARGET_REF to 70.9%.\"]}"
    },
    {
        "gold": {
            "text": [
                "These methods make limited use of the social context in which the authors are tweeting -our research question is \"Can we identify the language of a tweet using the social graph of the tweeter?\".",
                "Label propagation approaches [8] are powerful techniques for semi-supervised learning where the domain can naturally be described using an undirected graph.",
                "Each node contains a probability distribution over labels, which may be empty for unlabelled nodes, and these labels are propagated over the graph in an iterative fashion.",
                "Modified Adsorption (mad) [6] , is an extension that allows more control of the random walk through the graph.",
                "Applications of lp and mad are varied, including video recommendation [1] and sentiment analysis over Twitter #TARGET_REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "These methods make limited use of the social context in which the authors are tweeting -our research question is \"Can we identify the language of a tweet using the social graph of the tweeter?\". Label propagation approaches [8] are powerful techniques for semi-supervised learning where the domain can naturally be described using an undirected graph. Each node contains a probability distribution over labels, which may be empty for unlabelled nodes, and these labels are propagated over the graph in an iterative fashion. Modified Adsorption (mad) [6] , is an extension that allows more control of the random walk through the graph. Applications of lp and mad are varied, including video recommendation [1] and sentiment analysis over Twitter #TARGET_REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Applications of lp and mad are varied, including video recommendation [1] and sentiment analysis over Twitter #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Earlier works on conversation summarization have mainly focused on extractive techniques.",
                "However, as pointed out in (#REF) and #TARGET_REF , abstractive summaries are preferred to extractive ones by human judges.",
                "The possible reason for this is that extractive techniques are not well suited for the conversation summarization, since there are style differences between spoken conversations and humanauthored summaries.",
                "Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (#REF; #REF) .",
                "The authors cluster conversation sentences/utterances into communities to identify most relevant ones and aggregate them using word-graph models."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Earlier works on conversation summarization have mainly focused on extractive techniques. However, as pointed out in (#REF) and #TARGET_REF , abstractive summaries are preferred to extractive ones by human judges. The possible reason for this is that extractive techniques are not well suited for the conversation summarization, since there are style differences between spoken conversations and humanauthored summaries. Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (#REF; #REF) . The authors cluster conversation sentences/utterances into communities to identify most relevant ones and aggregate them using word-graph models.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"However, as pointed out in (#REF) and #TARGET_REF , abstractive summaries are preferred to extractive ones by human judges.\"]}"
    },
    {
        "gold": {
            "text": [
                "Earlier works on conversation summarization have mainly focused on extractive techniques.",
                "However, as pointed out in (#REF) and (#REF) , abstractive summaries are preferred to extractive ones by human judges.",
                "The possible reason for this is that extractive techniques are not well suited for the conversation summarization, since there are style differences between spoken conversations and humanauthored summaries.",
                "Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (#REF; #TARGET_REF .",
                "The authors cluster conversation sentences/utterances into communities to identify most relevant ones and aggregate them using word-graph models."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Earlier works on conversation summarization have mainly focused on extractive techniques. However, as pointed out in (#REF) and (#REF) , abstractive summaries are preferred to extractive ones by human judges. The possible reason for this is that extractive techniques are not well suited for the conversation summarization, since there are style differences between spoken conversations and humanauthored summaries. Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (#REF; #TARGET_REF . The authors cluster conversation sentences/utterances into communities to identify most relevant ones and aggregate them using word-graph models.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (#REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Unfortunately, such manual links are rarely available.",
                "In this paper we evaluate a set of heuristics for automatic linking of summary and conversations sentences, i.e. 'community' creation.",
                "The heuristics rely on the similarity between the two, and we experiment with the cosine similarity computation on different levels of representation -raw text, text after replacing the verbs with their WordNet SynSet IDs, and the similarity computed using distributed word embeddings.",
                "The heuristics are evaluated within the template-based abstractive summarization system of #TARGET_REF .",
                "We extend this system to Italian using required NLP tools."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Unfortunately, such manual links are rarely available. In this paper we evaluate a set of heuristics for automatic linking of summary and conversations sentences, i.e. 'community' creation. The heuristics rely on the similarity between the two, and we experiment with the cosine similarity computation on different levels of representation -raw text, text after replacing the verbs with their WordNet SynSet IDs, and the similarity computed using distributed word embeddings. The heuristics are evaluated within the template-based abstractive summarization system of #TARGET_REF . We extend this system to Italian using required NLP tools.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The heuristics are evaluated within the template-based abstractive summarization system of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Template Generation follows the approach of #TARGET_REF and, starting from human-authored summaries, produces abstract templates applying slot labeling, summary clustering and template fusion steps.",
                "The information required for the template generation are part-of-speech (POS) tags, noun and verb phrase chunks, and root verbs from dependency parsing.",
                "For English, we use Illinois Chunker (#REF) to identify noun phrases and extract part-of-speech tags; and the the tool of (De #REF) for generating dependency parses.",
                "For Italian, on the other hand, we use TextPro 2.0 (#REF) to perform all the Natural Language Processing tasks.",
                "In the slot labeling step, noun phrases from human-authored summaries are replaced by WordNet (#REF ) SynSet IDs of the head nouns (right most for English)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Template Generation follows the approach of #TARGET_REF and, starting from human-authored summaries, produces abstract templates applying slot labeling, summary clustering and template fusion steps. The information required for the template generation are part-of-speech (POS) tags, noun and verb phrase chunks, and root verbs from dependency parsing. For English, we use Illinois Chunker (#REF) to identify noun phrases and extract part-of-speech tags; and the the tool of (De #REF) for generating dependency parses. For Italian, on the other hand, we use TextPro 2.0 (#REF) to perform all the Natural Language Processing tasks. In the slot labeling step, noun phrases from human-authored summaries are replaced by WordNet (#REF ) SynSet IDs of the head nouns (right most for English).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Template Generation follows the approach of #TARGET_REF and, starting from human-authored summaries, produces abstract templates applying slot labeling, summary clustering and template fusion steps.\"]}"
    },
    {
        "gold": {
            "text": [
                "The two corpora used for the evaluation of the heuristics are AMI and LUNA.",
                "The AMI meeting corpus (#REF ) is a collection of 139 meeting records where groups of people are engaged in a 'roleplay' as a team and each speaker assumes a certain role in a team (e.g. project manager (PM)).",
                "Following #TARGET_REF , we removed 20 dialogs used by the authors for development, and use the remaining dialogs for the threefold cross-validation.",
                "The LUNA Human-Human corpus (#REF ) consists of 572 call-center dialogs where a client and an agent are engaged in a problem solving task over the phone.",
                "The 200 Italian LUNA dialogs have been annotated with summaries by 5 native speakers (5 summaries per dialog)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The two corpora used for the evaluation of the heuristics are AMI and LUNA. The AMI meeting corpus (#REF ) is a collection of 139 meeting records where groups of people are engaged in a 'roleplay' as a team and each speaker assumes a certain role in a team (e.g. project manager (PM)). Following #TARGET_REF , we removed 20 dialogs used by the authors for development, and use the remaining dialogs for the threefold cross-validation. The LUNA Human-Human corpus (#REF ) consists of 572 call-center dialogs where a client and an agent are engaged in a problem solving task over the phone. The 200 Italian LUNA dialogs have been annotated with summaries by 5 native speakers (5 summaries per dialog).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following #TARGET_REF , we removed 20 dialogs used by the authors for development, and use the remaining dialogs for the threefold cross-validation.\"]}"
    },
    {
        "gold": {
            "text": [
                "The metric considers bigram-level precision, recall and F-measure between a set of reference and hypothesis summaries.",
                "For AMI corpus, following #TARGET_REF , we report ROUGE-2 F-measures on 3-fold cross-validation.",
                "For LUNA Corpus, on the other hand, we have used the modified version of ROUGE 1.5.5 toolkit from the CCCS Shared Task , which was adapted to deal with a conversation-dependent length limit of 7%.",
                "Unlike the AMI Corpus, the official reported results for the CCCS Shared Task were recall; thus, for LUNA Corpus the reported values are ROUGE-2 recall.",
                "For statistical significance testing, we use a paired bootstrap resampling method proposed in (#REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The metric considers bigram-level precision, recall and F-measure between a set of reference and hypothesis summaries. For AMI corpus, following #TARGET_REF , we report ROUGE-2 F-measures on 3-fold cross-validation. For LUNA Corpus, on the other hand, we have used the modified version of ROUGE 1.5.5 toolkit from the CCCS Shared Task , which was adapted to deal with a conversation-dependent length limit of 7%. Unlike the AMI Corpus, the official reported results for the CCCS Shared Task were recall; thus, for LUNA Corpus the reported values are ROUGE-2 recall. For statistical significance testing, we use a paired bootstrap resampling method proposed in (#REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For AMI corpus, following #TARGET_REF , we report ROUGE-2 F-measures on 3-fold cross-validation.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section we report on the results of the abstractive summarization system using the community creation heuristics described in Section 2.",
                "Following the Call-Center Conversation Summarization Shared Task at #REF , for LUNA Corpus (#REF) we compare performances to three extractive baselines: (1) the longest turn in the conversation up to the length limit (7% of a conversation) (Baseline-L), (2) the longest turn in the first 25% of the conversation up to the length limit (Baseline-LB) (#REF) , and (3) Maximal Marginal Relevance (MMR) (#REF ) with λ = 0.7.",
                "For AMI corpus, on the other hand, we compare performances to the abstractive systems reported in #TARGET_REF .",
                "The performances of the heuristics on AMI corpus are given in Table 1 .",
                "In the table we also report the performances of the previously published summarization systems that make use of the manual communities - (#REF) and (#REF) ; and our run of the system of (#REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In this section we report on the results of the abstractive summarization system using the community creation heuristics described in Section 2. Following the Call-Center Conversation Summarization Shared Task at #REF , for LUNA Corpus (#REF) we compare performances to three extractive baselines: (1) the longest turn in the conversation up to the length limit (7% of a conversation) (Baseline-L), (2) the longest turn in the first 25% of the conversation up to the length limit (Baseline-LB) (#REF) , and (3) Maximal Marginal Relevance (MMR) (#REF ) with λ = 0.7. For AMI corpus, on the other hand, we compare performances to the abstractive systems reported in #TARGET_REF . The performances of the heuristics on AMI corpus are given in Table 1 . In the table we also report the performances of the previously published summarization systems that make use of the manual communities - (#REF) and (#REF) ; and our run of the system of (#REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For AMI corpus, on the other hand, we compare performances to the abstractive systems reported in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section we report on the results of the abstractive summarization system using the community creation heuristics described in Section 2.",
                "Following the Call-Center Conversation Summarization Shared Task at #REF , for LUNA Corpus (#REF) we compare performances to three extractive baselines: (1) the longest turn in the conversation up to the length limit (7% of a conversation) (Baseline-L), (2) the longest turn in the first 25% of the conversation up to the length limit (Baseline-LB) (#REF) , and (3) Maximal Marginal Relevance (MMR) (#REF ) with λ = 0.7.",
                "For AMI corpus, on the other hand, we compare performances to the abstractive systems reported in (#REF) .",
                "The performances of the heuristics on AMI corpus are given in Table 1 .",
                "In the table we also report the performances of the previously published summarization systems that make use of the manual communities - #TARGET_REF and (#REF) ; and our run of the system of #TARGET_REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "In this section we report on the results of the abstractive summarization system using the community creation heuristics described in Section 2. Following the Call-Center Conversation Summarization Shared Task at #REF , for LUNA Corpus (#REF) we compare performances to three extractive baselines: (1) the longest turn in the conversation up to the length limit (7% of a conversation) (Baseline-L), (2) the longest turn in the first 25% of the conversation up to the length limit (Baseline-LB) (#REF) , and (3) Maximal Marginal Relevance (MMR) (#REF ) with λ = 0.7. For AMI corpus, on the other hand, we compare performances to the abstractive systems reported in (#REF) . The performances of the heuristics on AMI corpus are given in Table 1 . In the table we also report the performances of the previously published summarization systems that make use of the manual communities - #TARGET_REF and (#REF) ; and our run of the system of #TARGET_REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In the table we also report the performances of the previously published summarization systems that make use of the manual communities - #TARGET_REF and (#REF) ; and our run of the system of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The clustering of the abstract templates generated in the previous step is performed using the WordNet hierarchy of the root verb of a sentence.",
                "The similarity between verbs is computed with respect to the shortest path that connects the senses in the hypernym taxonomy of WordNet.",
                "The template graphs, created using this similarity, are then clustered using the Normalized Cuts method (#REF) .",
                "The clustered templates are further generalized using a word graph algorithm extended to templates in #TARGET_REF .",
                "The paths in the word graph are ranked using language models trained on the abstract templates and the top 10 are selected as a template for the cluster."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The clustering of the abstract templates generated in the previous step is performed using the WordNet hierarchy of the root verb of a sentence. The similarity between verbs is computed with respect to the shortest path that connects the senses in the hypernym taxonomy of WordNet. The template graphs, created using this similarity, are then clustered using the Normalized Cuts method (#REF) . The clustered templates are further generalized using a word graph algorithm extended to templates in #TARGET_REF . The paths in the word graph are ranked using language models trained on the abstract templates and the top 10 are selected as a template for the cluster.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"The clustered templates are further generalized using a word graph algorithm extended to templates in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Since the system produces many sentences that might repeat the same information, the final set of automatic sentences is selected from these filled templates with respect to the ranking using the token and part-of-speech tag 3-gram language models.",
                "In this paper, different from #TARGET_REF , the sentence ranking is based solely on the n-gram language models trained on the tokens and part-ofspeech tags from the human-authored summaries."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "Since the system produces many sentences that might repeat the same information, the final set of automatic sentences is selected from these filled templates with respect to the ranking using the token and part-of-speech tag 3-gram language models. In this paper, different from #TARGET_REF , the sentence ranking is based solely on the n-gram language models trained on the tokens and part-ofspeech tags from the human-authored summaries.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"In this paper, different from #TARGET_REF , the sentence ranking is based solely on the n-gram language models trained on the tokens and part-ofspeech tags from the human-authored summaries.\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF introduces a thesaurus-based approach to statistical sense disambiguation which works on monolingual corpora without the need for sense-tagged training data.",
                "By collecting statistical data of word occurrences in the context of different thesaurus categories from a relatively large corpus (10 million words), the system can identify salient words for each category.",
                "Using these salient words, the system is able to disambiguate polysemous words with respect to thesaurus categories.",
                "Statistical approaches like these generally suffer from the problem of data sparseness.",
                "To estimate the salience of a word with reasonable accuracy, the system needs the word to have a significant number of occurrences in the corpus."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "#TARGET_REF introduces a thesaurus-based approach to statistical sense disambiguation which works on monolingual corpora without the need for sense-tagged training data. By collecting statistical data of word occurrences in the context of different thesaurus categories from a relatively large corpus (10 million words), the system can identify salient words for each category. Using these salient words, the system is able to disambiguate polysemous words with respect to thesaurus categories. Statistical approaches like these generally suffer from the problem of data sparseness. To estimate the salience of a word with reasonable accuracy, the system needs the word to have a significant number of occurrences in the corpus.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF introduces a thesaurus-based approach to statistical sense disambiguation which works on monolingual corpora without the need for sense-tagged training data.\"]}"
    },
    {
        "gold": {
            "text": [
                "Since the training data is not sense tagged, the data collected will contain noise due to spurious senses of polysemous words.",
                "Like the thesaurusbased approach of #TARGET_REF , our approach relies on the dilution of this noise by their distribution through all the 1792 defining concepts.",
                "Different words in the corpus have different numbers of senses and different senses have definitions of varying lengths.",
                "The principle adopted in collecting co-occurrence data is that every pair of content words which co-occur in a sentence should have equal contribution to the conceptual cooccurrence data regardless of the number of definitions (senses) of the words and the lengths of the definitions.",
                "In addition, the contribution of a word should be evenly distributed between all the senses of a word and the contribution of a sense should be evenly distributed between all the concepts in a sense."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Since the training data is not sense tagged, the data collected will contain noise due to spurious senses of polysemous words. Like the thesaurusbased approach of #TARGET_REF , our approach relies on the dilution of this noise by their distribution through all the 1792 defining concepts. Different words in the corpus have different numbers of senses and different senses have definitions of varying lengths. The principle adopted in collecting co-occurrence data is that every pair of content words which co-occur in a sentence should have equal contribution to the conceptual cooccurrence data regardless of the number of definitions (senses) of the words and the lengths of the definitions. In addition, the contribution of a word should be evenly distributed between all the senses of a word and the contribution of a sense should be evenly distributed between all the concepts in a sense.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Like the thesaurusbased approach of #TARGET_REF , our approach relies on the dilution of this noise by their distribution through all the 1792 defining concepts.\"]}"
    },
    {
        "gold": {
            "text": [
                "6. In our experiment, the words are disambiguated between all the senses listed except the ones marked with 7.",
                "The rare senses listed in LDOCE are not listed here.",
                "For some of the words, more than one sense listed in LDOCE corresponds to a sense as used in #TARGET_REF .",
                "In these cases, the senses used by Yarowsky are adopted for easier comparison.",
                "8. All results are based on 100% recall."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "6. In our experiment, the words are disambiguated between all the senses listed except the ones marked with 7. The rare senses listed in LDOCE are not listed here. For some of the words, more than one sense listed in LDOCE corresponds to a sense as used in #TARGET_REF . In these cases, the senses used by Yarowsky are adopted for easier comparison. 8. All results are based on 100% recall.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"For some of the words, more than one sense listed in LDOCE corresponds to a sense as used in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Our system is tested on the twelve words discussed in #TARGET_REF and previous publications on sense disambiguation.",
                "Results are shown in Table 1 .",
                "Our system achieves an average accuracy of 77% on a mean 3-way sense distinction over the twelve words.",
                "Numerically, the result is not as good as the 92% as reported in #REF .",
                "However, direct comparison between the numerical results can be misleading since the experiments are carried out on two very different corpora both in size and genre."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Our system is tested on the twelve words discussed in #TARGET_REF and previous publications on sense disambiguation. Results are shown in Table 1 . Our system achieves an average accuracy of 77% on a mean 3-way sense distinction over the twelve words. Numerically, the result is not as good as the 92% as reported in #REF . However, direct comparison between the numerical results can be misleading since the experiments are carried out on two very different corpora both in size and genre.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Our system is tested on the twelve words discussed in #TARGET_REF and previous publications on sense disambiguation.\"]}"
    },
    {
        "gold": {
            "text": [
                "1. N marks the column with the number of tcst samples for each sense.",
                "DBCC (Defmition-Bascd Conceptual Cooccurrence) and Human mark the columns with the results of our system and the human subject in disambiguating the occurrences of the 12 words in the Brown corpus, respectively.",
                "Thes.",
                "(thesaurus) marks the column with the results of #TARGET_REF tested on the Grolier's Encyclopedia.",
                "2. The \"correct\" sense of each test sample is chosen by hand disambiguation carried out by the author using the sentence as the context."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "1. N marks the column with the number of tcst samples for each sense. DBCC (Defmition-Bascd Conceptual Cooccurrence) and Human mark the columns with the results of our system and the human subject in disambiguating the occurrences of the 12 words in the Brown corpus, respectively. Thes. (thesaurus) marks the column with the results of #TARGET_REF tested on the Grolier's Encyclopedia. 2. The \"correct\" sense of each test sample is chosen by hand disambiguation carried out by the author using the sentence as the context.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"(thesaurus) marks the column with the results of #TARGET_REF tested on the Grolier's Encyclopedia.\"]}"
    },
    {
        "gold": {
            "text": [
                "A small proportion of test samples cannot be disambiguated within the given context and are excluded from the experiment.",
                "3. The senses marked with * are used in #TARGET_REF but no corresponding sense is found in LDOCE.",
                "4. The sense marked with ** is defined in LDOCE but not used in #REF .",
                "6. In our experiment, the words are disambiguated between all the senses listed except the ones marked with 7.",
                "The rare senses listed in LDOCE are not listed here."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "A small proportion of test samples cannot be disambiguated within the given context and are excluded from the experiment. 3. The senses marked with * are used in #TARGET_REF but no corresponding sense is found in LDOCE. 4. The sense marked with ** is defined in LDOCE but not used in #REF . 6. In our experiment, the words are disambiguated between all the senses listed except the ones marked with 7. The rare senses listed in LDOCE are not listed here.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"3. The senses marked with * are used in #TARGET_REF but no corresponding sense is found in LDOCE.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our system is tested on the twelve words discussed in #REF and previous publications on sense disambiguation.",
                "Results are shown in Table 1 .",
                "Our system achieves an average accuracy of 77% on a mean 3-way sense distinction over the twelve words.",
                "Numerically, the result is not as good as the 92% as reported in #TARGET_REF .",
                "However, direct comparison between the numerical results can be misleading since the experiments are carried out on two very different corpora both in size and genre."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Our system is tested on the twelve words discussed in #REF and previous publications on sense disambiguation. Results are shown in Table 1 . Our system achieves an average accuracy of 77% on a mean 3-way sense distinction over the twelve words. Numerically, the result is not as good as the 92% as reported in #TARGET_REF . However, direct comparison between the numerical results can be misleading since the experiments are carried out on two very different corpora both in size and genre.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Numerically, the result is not as good as the 92% as reported in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "s The subject has not read through the whole corpus.",
                "approaches in a large proportion of cases.",
                "9 The average sentence length in the Brown corpus is 19.41° words which is 5 times smaller than the 100 word window used in #REF and #TARGET_REF .",
                "Our approach works well even with a small \"window\" because it is based on the identification of salient concepts rather than salient words.",
                "In salient word based approaches, due to the problem of data sparseness, many less frequently occurring words which are intuitively salient to a particular word sense will not be identified in practice unless an extremely large corpus is used."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "s The subject has not read through the whole corpus. approaches in a large proportion of cases. 9 The average sentence length in the Brown corpus is 19.41° words which is 5 times smaller than the 100 word window used in #REF and #TARGET_REF . Our approach works well even with a small \"window\" because it is based on the identification of salient concepts rather than salient words. In salient word based approaches, due to the problem of data sparseness, many less frequently occurring words which are intuitively salient to a particular word sense will not be identified in practice unless an extremely large corpus is used.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"9 The average sentence length in the Brown corpus is 19.41\\u00b0 words which is 5 times smaller than the 100 word window used in #REF and #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "A small proportion of test samples cannot be disambiguated within the given context and are excluded from the experiment.",
                "3. The senses marked with * are used in #REF but no corresponding sense is found in LDOCE.",
                "4. The sense marked with ** is defined in LDOCE but not used in #TARGET_REF .",
                "6. In our experiment, the words are disambiguated between all the senses listed except the ones marked with 7.",
                "The rare senses listed in LDOCE are not listed here."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "A small proportion of test samples cannot be disambiguated within the given context and are excluded from the experiment. 3. The senses marked with * are used in #REF but no corresponding sense is found in LDOCE. 4. The sense marked with ** is defined in LDOCE but not used in #TARGET_REF . 6. In our experiment, the words are disambiguated between all the senses listed except the ones marked with 7. The rare senses listed in LDOCE are not listed here.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"4. The sense marked with ** is defined in LDOCE but not used in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "However, deriving these sets of similar words requires a substantial amount of statistical data and thus these approaches require relatively large corpora to start with.~ 2 Our definition-based approach to statistical sense disambiguation is similar in spirit to the similaritybased approaches, with respect to the \"specificity\" of modelling individual words.",
                "However, using definitions from existing dictionaries rather than derived sets of similar words allows our method to work on corpora of much smaller sizes.",
                "In our approach, each word is modelled by its own set of defining concepts.",
                "Although only 1792 defining concepts are used, the set of all possible combinations (a power set of the defining concepts) is so huge that it is very unlikely two word senses will have the same combination of defining concepts unless they are almost identical in meaning.",
                "On the other hand, the thesaurus-based method of #TARGET_REF may suffer from loss of information (since it is semi-class-based) as well as data sparseness (since H Classes used in #REF are based on the WordNet taxonomy while classes of #REF and #REF are derived from statistical data collected from corpora."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "However, deriving these sets of similar words requires a substantial amount of statistical data and thus these approaches require relatively large corpora to start with.~ 2 Our definition-based approach to statistical sense disambiguation is similar in spirit to the similaritybased approaches, with respect to the \"specificity\" of modelling individual words. However, using definitions from existing dictionaries rather than derived sets of similar words allows our method to work on corpora of much smaller sizes. In our approach, each word is modelled by its own set of defining concepts. Although only 1792 defining concepts are used, the set of all possible combinations (a power set of the defining concepts) is so huge that it is very unlikely two word senses will have the same combination of defining concepts unless they are almost identical in meaning. On the other hand, the thesaurus-based method of #TARGET_REF may suffer from loss of information (since it is semi-class-based) as well as data sparseness (since H Classes used in #REF are based on the WordNet taxonomy while classes of #REF and #REF are derived from statistical data collected from corpora.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"On the other hand, the thesaurus-based method of #TARGET_REF may suffer from loss of information (since it is semi-class-based) as well as data sparseness (since H Classes used in #REF are based on the WordNet taxonomy while classes of #REF and #REF are derived from statistical data collected from corpora.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, fully automatic lexically based approaches 3 #REF shows that the introduction of linguistic cues improves the performance of a statistical semantic knowledge acquisition system in the context of word grouping.",
                "such as that described in #TARGET_REF are very unlikely to be capable of acquiring this finer knowledge because the problem of data sparseness becomes even more serious with the introduction of syntactic constraints.",
                "Our approach has overcome the data sparseness problem by using the defining concepts of words.",
                "It is found to be effective in acquiring semantic coherence knowledge from a relatively small corpus.",
                "It is possible that a similar approach based on dictionary definitions will be successful in acquiring knowledge of local constraints from a reasonably sized corpus."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "However, fully automatic lexically based approaches 3 #REF shows that the introduction of linguistic cues improves the performance of a statistical semantic knowledge acquisition system in the context of word grouping. such as that described in #TARGET_REF are very unlikely to be capable of acquiring this finer knowledge because the problem of data sparseness becomes even more serious with the introduction of syntactic constraints. Our approach has overcome the data sparseness problem by using the defining concepts of words. It is found to be effective in acquiring semantic coherence knowledge from a relatively small corpus. It is possible that a similar approach based on dictionary definitions will be successful in acquiring knowledge of local constraints from a reasonably sized corpus.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"such as that described in #TARGET_REF are very unlikely to be capable of acquiring this finer knowledge because the problem of data sparseness becomes even more serious with the introduction of syntactic constraints.\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to detect the entities referred in the text, we need to partition the graph such that all nodes in each subgraph refer to the same entity.",
                "We have devised a graph partitioning method for coreference resolution, called BESTCUT, which is inspired from the well-known graph-partitioning algorithm Min-Cut (#REF) .",
                "BESTCUT has a different way of computing the cut weight than Min-Cut and a different way of stopping the cut 2 .",
                "Moreover, we have slightly modified the Min-Cut procedures.",
                "BESTCUT replaces the bottom-up search in a tree representation (as it was performed in #TARGET_REF ) with the top-down problem of obtaining the best partitioning of a graph."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                1,
                1
            ]
        },
        "input": "In order to detect the entities referred in the text, we need to partition the graph such that all nodes in each subgraph refer to the same entity. We have devised a graph partitioning method for coreference resolution, called BESTCUT, which is inspired from the well-known graph-partitioning algorithm Min-Cut (#REF) . BESTCUT has a different way of computing the cut weight than Min-Cut and a different way of stopping the cut 2 . Moreover, we have slightly modified the Min-Cut procedures. BESTCUT replaces the bottom-up search in a tree representation (as it was performed in #TARGET_REF ) with the top-down problem of obtaining the best partitioning of a graph.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"BESTCUT has a different way of computing the cut weight than Min-Cut and a different way of stopping the cut 2 .\", \"Moreover, we have slightly modified the Min-Cut procedures.\", \"BESTCUT replaces the bottom-up search in a tree representation (as it was performed in #TARGET_REF ) with the top-down problem of obtaining the best partitioning of a graph.\"]}"
    },
    {
        "gold": {
            "text": [
                "The coreference confidence values that become the weights in the starting graphs are provided by a maximum entropy model, trained on the training datasets of the corpora used in our experiments.",
                "For maximum entropy classification we used a maxent 4 tool.",
                "Based on the data seen, a maximum entropy model (#REF) offers an expression (1) for the probability that there exists coreference C between a mention m i and a mention m j .",
                "where g k (m i , m j , C) is a feature and λ k is its weight; Z(m i , m j ) is a normalizing factor.",
                "We created the training examples in the same way as #TARGET_REF , by pairing all mentions of the same type, obtaining their feature vectors and taking the outcome (coreferent/noncoreferent) from the key files."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The coreference confidence values that become the weights in the starting graphs are provided by a maximum entropy model, trained on the training datasets of the corpora used in our experiments. For maximum entropy classification we used a maxent 4 tool. Based on the data seen, a maximum entropy model (#REF) offers an expression (1) for the probability that there exists coreference C between a mention m i and a mention m j . where g k (m i , m j , C) is a feature and λ k is its weight; Z(m i , m j ) is a normalizing factor. We created the training examples in the same way as #TARGET_REF , by pairing all mentions of the same type, obtaining their feature vectors and taking the outcome (coreferent/noncoreferent) from the key files.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We created the training examples in the same way as #TARGET_REF , by pairing all mentions of the same type, obtaining their feature vectors and taking the outcome (coreferent/noncoreferent) from the key files.\"]}"
    },
    {
        "gold": {
            "text": [
                "The clusterization algorithms that we implemented to evaluate in comparison with our method are #TARGET_REF 's Belltree and Link-Best (best-first clusterization) from (#REF) .",
                "The features used were described in section 2.2.",
                "We experimented on the ACE Phase 2 (NIST, 2003) and MUC6 (MUC-6, 1995) corpora.",
                "Since we aimed to measure the performance of coreference, the metrics used for evaluation are the ECM-F (#REF) and the MUC P, R and F scores (#REF) .",
                "In our first experiment, we tested the three coreference clusterization algorithms on the development-test set of the ACE Phase 2 corpus, first on true mentions (i.e. the mentions annotated in the key files), then on detected mentions (i.e. the mentions output by our mention detection system presented in section 3) and finally without any prior knowledge of the mention types."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The clusterization algorithms that we implemented to evaluate in comparison with our method are #TARGET_REF 's Belltree and Link-Best (best-first clusterization) from (#REF) . The features used were described in section 2.2. We experimented on the ACE Phase 2 (NIST, 2003) and MUC6 (MUC-6, 1995) corpora. Since we aimed to measure the performance of coreference, the metrics used for evaluation are the ECM-F (#REF) and the MUC P, R and F scores (#REF) . In our first experiment, we tested the three coreference clusterization algorithms on the development-test set of the ACE Phase 2 corpus, first on true mentions (i.e. the mentions annotated in the key files), then on detected mentions (i.e. the mentions output by our mention detection system presented in section 3) and finally without any prior knowledge of the mention types.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The clusterization algorithms that we implemented to evaluate in comparison with our method are #TARGET_REF 's Belltree and Link-Best (best-first clusterization) from (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The clusterization algorithms that we implemented to evaluate in comparison with our method are (#REF) 's Belltree and Link-Best (best-first clusterization) from (#REF) .",
                "The features used were described in section 2.2.",
                "We experimented on the ACE Phase 2 (NIST, 2003) and MUC6 (MUC-6, 1995) corpora.",
                "Since we aimed to measure the performance of coreference, the metrics used for evaluation are the ECM-F #TARGET_REF and the MUC P, R and F scores (#REF) .",
                "In our first experiment, we tested the three coreference clusterization algorithms on the development-test set of the ACE Phase 2 corpus, first on true mentions (i.e. the mentions annotated in the key files), then on detected mentions (i.e. the mentions output by our mention detection system presented in section 3) and finally without any prior knowledge of the mention types."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The clusterization algorithms that we implemented to evaluate in comparison with our method are (#REF) 's Belltree and Link-Best (best-first clusterization) from (#REF) . The features used were described in section 2.2. We experimented on the ACE Phase 2 (NIST, 2003) and MUC6 (MUC-6, 1995) corpora. Since we aimed to measure the performance of coreference, the metrics used for evaluation are the ECM-F #TARGET_REF and the MUC P, R and F scores (#REF) . In our first experiment, we tested the three coreference clusterization algorithms on the development-test set of the ACE Phase 2 corpus, first on true mentions (i.e. the mentions annotated in the key files), then on detected mentions (i.e. the mentions output by our mention detection system presented in section 3) and finally without any prior knowledge of the mention types.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Since we aimed to measure the performance of coreference, the metrics used for evaluation are the ECM-F #TARGET_REF and the MUC P, R and F scores (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Our second experiment evaluates the impact that the different categories of our added features have on the performance of the BESTCUT system.",
                "The experiment was performed with a maxent classifier on the MUC6 corpus, which was priorly converted into ACE format, and employed mention information from the key annotations.",
                "Table 5 : Impact of feature categories on BEST-CUT on MUC6.",
                "Baseline system has the #TARGET_REF features.",
                "The system was tested on key mentions."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "Our second experiment evaluates the impact that the different categories of our added features have on the performance of the BESTCUT system. The experiment was performed with a maxent classifier on the MUC6 corpus, which was priorly converted into ACE format, and employed mention information from the key annotations. Table 5 : Impact of feature categories on BEST-CUT on MUC6. Baseline system has the #TARGET_REF features. The system was tested on key mentions.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Baseline system has the #TARGET_REF features.\", \"The system was tested on key mentions.\"]}"
    },
    {
        "gold": {
            "text": [
                "This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings.",
                "We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art #TARGET_REF; #REF; #REF; #REF) .",
                "The main contributions of the paper are:",
                "• Novel Self-Governing Neural Networks (SGNNs) for on-device deep learning for short text classification.",
                "• Compression technique that effectively captures low-dimensional semantic text representation and produces compact models that save on storage and computational cost."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art #TARGET_REF; #REF; #REF; #REF) . The main contributions of the paper are: • Novel Self-Governing Neural Networks (SGNNs) for on-device deep learning for short text classification. • Compression technique that effectively captures low-dimensional semantic text representation and produces compact models that save on storage and computational cost.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art #TARGET_REF; #REF; #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The main contributions of the paper are:",
                "• Novel Self-Governing Neural Networks (SGNNs) for on-device deep learning for short text classification.",
                "• Compression technique that effectively captures low-dimensional semantic text representation and produces compact models that save on storage and computational cost.",
                "• On the fly computation of projection vectors that eliminate the need for large pre-trained word embeddings or vocabulary pruning.",
                "• Exhaustive experimental evaluation on dialog act datasets, outperforming state-of-theart deep CNN #TARGET_REF and RNN variants (#REF; #REF )."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                1,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The main contributions of the paper are: • Novel Self-Governing Neural Networks (SGNNs) for on-device deep learning for short text classification. • Compression technique that effectively captures low-dimensional semantic text representation and produces compact models that save on storage and computational cost. • On the fly computation of projection vectors that eliminate the need for large pre-trained word embeddings or vocabulary pruning. • Exhaustive experimental evaluation on dialog act datasets, outperforming state-of-theart deep CNN #TARGET_REF and RNN variants (#REF; #REF ).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The main contributions of the paper are:\", \"\\u2022 Exhaustive experimental evaluation on dialog act datasets, outperforming state-of-theart deep CNN #TARGET_REF and RNN variants (#REF; #REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "For both datasets we used the following: 2-layer SGNN (P T =80,d=14 × FullyConnected 256 × FullyConnected 256 ), mini-batch size of 100, dropout rate of 0.25, learning rate was initialized to 0.025 with cosine annealing decay (#REF) .",
                "Unlike prior approaches #TARGET_REF; #REF ) that rely on pre-trained word embeddings, we learn the projection weights on the fly during training, i.e word embeddings (or vocabularies) do not need to be stored.",
                "Instead, features are computed on the fly and are dynamically compressed via the projection matrices into projection vectors.",
                "These values were chosen via a grid search on development sets, we do not perform any other dataset-specific tuning.",
                "Training is performed through stochastic gradient descent over shuffled mini-batches with Nesterov momentum optimizer (#REF) , run for 1M steps."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "For both datasets we used the following: 2-layer SGNN (P T =80,d=14 × FullyConnected 256 × FullyConnected 256 ), mini-batch size of 100, dropout rate of 0.25, learning rate was initialized to 0.025 with cosine annealing decay (#REF) . Unlike prior approaches #TARGET_REF; #REF ) that rely on pre-trained word embeddings, we learn the projection weights on the fly during training, i.e word embeddings (or vocabularies) do not need to be stored. Instead, features are computed on the fly and are dynamically compressed via the projection matrices into projection vectors. These values were chosen via a grid search on development sets, we do not perform any other dataset-specific tuning. Training is performed through stochastic gradient descent over shuffled mini-batches with Nesterov momentum optimizer (#REF) , run for 1M steps.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Unlike prior approaches #TARGET_REF; #REF ) that rely on pre-trained word embeddings, we learn the projection weights on the fly during training, i.e word embeddings (or vocabularies) do not need to be stored.\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare our model against a majority class baseline and Naive Bayes classifier #TARGET_REF .",
                "Our model significantly outperforms both baselines by 12 to 35% absolute."
            ],
            "label": [
                "DIFFER",
                "USE"
            ],
            "context": [
                1,
                1
            ]
        },
        "input": "We compare our model against a majority class baseline and Naive Bayes classifier #TARGET_REF . Our model significantly outperforms both baselines by 12 to 35% absolute.",
        "output": "{\"label\": [\"DIFFER\", \"USE\"], \"context\": [\"We compare our model against a majority class baseline and Naive Bayes classifier #TARGET_REF .\", \"Our model significantly outperforms both baselines by 12 to 35% absolute.\"]}"
    },
    {
        "gold": {
            "text": [
                "According to (#REF) , prior work by (#REF) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly.",
                "For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work.",
                "This is very impressive given that we work with very small memory footprint and we do not rely on pre-trained word embeddings.",
                "Our study also shows that the proposed method is very effective for such natural language tasks compared to more complex neural network architectures such as deep CNN #TARGET_REF and RNN variants (#REF; #REF) .",
                "We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing lowdimensional semantic text representations that are useful for text classification applications."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "According to (#REF) , prior work by (#REF) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly. For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work. This is very impressive given that we work with very small memory footprint and we do not rely on pre-trained word embeddings. Our study also shows that the proposed method is very effective for such natural language tasks compared to more complex neural network architectures such as deep CNN #TARGET_REF and RNN variants (#REF; #REF) . We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing lowdimensional semantic text representations that are useful for text classification applications.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Our study also shows that the proposed method is very effective for such natural language tasks compared to more complex neural network architectures such as deep CNN #TARGET_REF and RNN variants (#REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We proposed Self-Governing Neural Networks for on-device short text classification.",
                "Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods #TARGET_REF; #REF; #REF) .",
                "We introduced a compression technique that effectively captures low-dimensional semantic representation and produces compact models that significantly save on storage and computational cost.",
                "Our approach does not rely on pre-trained embeddings and efficiently computes the projection vectors on the fly.",
                "In the future, we are interested in extending this approach to more natural language tasks."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We proposed Self-Governing Neural Networks for on-device short text classification. Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods #TARGET_REF; #REF; #REF) . We introduced a compression technique that effectively captures low-dimensional semantic representation and produces compact models that significantly save on storage and computational cost. Our approach does not rely on pre-trained embeddings and efficiently computes the projection vectors on the fly. In the future, we are interested in extending this approach to more natural language tasks.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods #TARGET_REF; #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We conduct our experimental evaluation on two dialog act benchmark datasets.",
                "• SWDA: Switchboard Dialog Act Corpus (#REF; #REF) is a popular open domain dialogs corpus between two speakers with 42 dialogs acts.",
                "• MRDA: ICSI Meeting Recorder Dialog Act Corpus (#REF; #REF ) is a dialog corpus of multiparty meetings with 5 tags of dialog acts.",
                "Table 1 summarizes dataset statistics.",
                "We use the train, validation and test splits as defined in #TARGET_REF; #REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "We conduct our experimental evaluation on two dialog act benchmark datasets. • SWDA: Switchboard Dialog Act Corpus (#REF; #REF) is a popular open domain dialogs corpus between two speakers with 42 dialogs acts. • MRDA: ICSI Meeting Recorder Dialog Act Corpus (#REF; #REF ) is a dialog corpus of multiparty meetings with 5 tags of dialog acts. Table 1 summarizes dataset statistics. We use the train, validation and test splits as defined in #TARGET_REF; #REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use the train, validation and test splits as defined in #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We also compare our performance against prior work using HMMs (#REF) and recent deep learning methods like CNN #TARGET_REF , RNN (#REF) and RNN with gated attention (#REF) .",
                "To the best of our knowledge, (#REF; #REF; #REF) are the latest approaches in dialog act classification, which also reported on the same data splits.",
                "Therefore, we compare our research against these works.",
                "According to (#REF) , prior work by (#REF) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly.",
                "For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We also compare our performance against prior work using HMMs (#REF) and recent deep learning methods like CNN #TARGET_REF , RNN (#REF) and RNN with gated attention (#REF) . To the best of our knowledge, (#REF; #REF; #REF) are the latest approaches in dialog act classification, which also reported on the same data splits. Therefore, we compare our research against these works. According to (#REF) , prior work by (#REF) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly. For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We also compare our performance against prior work using HMMs (#REF) and recent deep learning methods like CNN #TARGET_REF , RNN (#REF) and RNN with gated attention (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We also compare our performance against prior work using HMMs (#REF) and recent deep learning methods like CNN (#REF) , RNN (#REF) and RNN with gated attention (#REF) .",
                "To the best of our knowledge, #TARGET_REF; #REF; #REF) are the latest approaches in dialog act classification, which also reported on the same data splits.",
                "Therefore, we compare our research against these works.",
                "According to (#REF) , prior work by (#REF) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly.",
                "For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We also compare our performance against prior work using HMMs (#REF) and recent deep learning methods like CNN (#REF) , RNN (#REF) and RNN with gated attention (#REF) . To the best of our knowledge, #TARGET_REF; #REF; #REF) are the latest approaches in dialog act classification, which also reported on the same data splits. Therefore, we compare our research against these works. According to (#REF) , prior work by (#REF) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly. For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 accuracy outperforming prior state-of-the-art work.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"To the best of our knowledge, #TARGET_REF; #REF; #REF) are the latest approaches in dialog act classification, which also reported on the same data splits.\"]}"
    },
    {
        "gold": {
            "text": [
                "How to determine the quality of an automatic transcription without reference transcripts and without confidence information?",
                "This is the key problem addressed by research on ASR quality estimation C. de #REF; #TARGET_REF , and the task for which TranscRater, the tool described in this paper, has been designed.",
                "The work on ASR quality estimation (ASR QE) has several motivations.",
                "First, the steady increase of applications involving automatic speech recognition (e.g. video/TV programs subtitling, voice search engines, voice question answering, spoken dialog systems, meeting and broadcast news transcriptions) calls for an accurate method to estimate ASR output quality at run-time.",
                "Often, indeed, the nature of such applications (consider for instance spoken dialog systems) requires quick response capabilities that are incompatible with traditional reference-based protocols."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "How to determine the quality of an automatic transcription without reference transcripts and without confidence information? This is the key problem addressed by research on ASR quality estimation C. de #REF; #TARGET_REF , and the task for which TranscRater, the tool described in this paper, has been designed. The work on ASR quality estimation (ASR QE) has several motivations. First, the steady increase of applications involving automatic speech recognition (e.g. video/TV programs subtitling, voice search engines, voice question answering, spoken dialog systems, meeting and broadcast news transcriptions) calls for an accurate method to estimate ASR output quality at run-time. Often, indeed, the nature of such applications (consider for instance spoken dialog systems) requires quick response capabilities that are incompatible with traditional reference-based protocols.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"This is the key problem addressed by research on ASR quality estimation C. de #REF; #TARGET_REF , and the task for which TranscRater, the tool described in this paper, has been designed.\"]}"
    },
    {
        "gold": {
            "text": [
                "According to the type of training labels, the problem can be approached either as a regression or as a classification task.",
                "As a consequence, also the evaluation metrics will change.",
                "Precision/recall/F1 (or other metrics, such as balanced accuracy, in case of very unbalanced distributions) will be used for classification while, similar to MT QE, the mean absolute error (MAE) or similar metrics will be used for regression.",
                "A variant of the basic ASR QE task is to consider it as a QE-based ranking problem #TARGET_REF , in which each utterance is captured by multiple microphones or transcribed by multiple ASR systems.",
                "In this case, the capability to rank transcriptions from the best to the worst can be evaluated in terms of normalized discounted cumulative gain (NDCG) or similar metrics."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "According to the type of training labels, the problem can be approached either as a regression or as a classification task. As a consequence, also the evaluation metrics will change. Precision/recall/F1 (or other metrics, such as balanced accuracy, in case of very unbalanced distributions) will be used for classification while, similar to MT QE, the mean absolute error (MAE) or similar metrics will be used for regression. A variant of the basic ASR QE task is to consider it as a QE-based ranking problem #TARGET_REF , in which each utterance is captured by multiple microphones or transcribed by multiple ASR systems. In this case, the capability to rank transcriptions from the best to the worst can be evaluated in terms of normalized discounted cumulative gain (NDCG) or similar metrics.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"A variant of the basic ASR QE task is to consider it as a QE-based ranking problem #TARGET_REF , in which each utterance is captured by multiple microphones or transcribed by multiple ASR systems.\"]}"
    },
    {
        "gold": {
            "text": [
                "For regression-based tasks (WER prediction), TranscRater includes an interface to the Scikitlearn package (#REF) , a Python machine learning library that contains a large set of classification and regression algorithms.",
                "Based on the empirical results reported in C. de #REF; #TARGET_REF , which indicate that Extremely Randomized Trees (XRT (#REF) ) is a very competitive algorithm in several WER prediction tasks, the current version of the tool exploits XRT.",
                "However, adapting the interface to apply other algorithms is an easy task and one of the future extension directions.",
                "The main hyper-parameters of the model, such as the number of tree bags, the number of trees per bag, the number of features per tree and the number of instances in the leaves, are tuned using grid search with k-fold cross-validation on the training set to minimize the mean absolute error (MAE) between the true WERs and the predicted ones.",
                "As mentioned before, TranscRater provides the possibility to evaluate multiple transcriptions (e.g. obtained from different microphones or ASR systems) and rank them based on their quality."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "For regression-based tasks (WER prediction), TranscRater includes an interface to the Scikitlearn package (#REF) , a Python machine learning library that contains a large set of classification and regression algorithms. Based on the empirical results reported in C. de #REF; #TARGET_REF , which indicate that Extremely Randomized Trees (XRT (#REF) ) is a very competitive algorithm in several WER prediction tasks, the current version of the tool exploits XRT. However, adapting the interface to apply other algorithms is an easy task and one of the future extension directions. The main hyper-parameters of the model, such as the number of tree bags, the number of trees per bag, the number of features per tree and the number of instances in the leaves, are tuned using grid search with k-fold cross-validation on the training set to minimize the mean absolute error (MAE) between the true WERs and the predicted ones. As mentioned before, TranscRater provides the possibility to evaluate multiple transcriptions (e.g. obtained from different microphones or ASR systems) and rank them based on their quality.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Based on the empirical results reported in C. de #REF; #TARGET_REF , which indicate that Extremely Randomized Trees (XRT (#REF) ) is a very competitive algorithm in several WER prediction tasks, the current version of the tool exploits XRT.\"]}"
    },
    {
        "gold": {
            "text": [
                "The features and algorithms contained in TranscRater have been successfully used in previous works C. de #REF; #TARGET_REF; #REFa) .",
                "To further investigate their effectiveness, in this section we provide new results, both in WER prediction (MAE) and transcription ranking (NDCG), together with some efficiency analysis (Time in seconds 9 ).",
                "To this aim, we use data from the 3 rd CHiME challenge, 10 which were collected for multiple distant microphone speech recognition in noisy environments (#REF) .",
                "CHiME-3 data consists of sentences of the Wall Street Journal corpus, uttered by four speakers in four noisy environments, and recorded by five microphones placed on the frame of a tablet PC (a sixth one, placed on the back, mainly records background noise).",
                "Training and test respectively contain 1,640 and 1,320 sentences."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The features and algorithms contained in TranscRater have been successfully used in previous works C. de #REF; #TARGET_REF; #REFa) . To further investigate their effectiveness, in this section we provide new results, both in WER prediction (MAE) and transcription ranking (NDCG), together with some efficiency analysis (Time in seconds 9 ). To this aim, we use data from the 3 rd CHiME challenge, 10 which were collected for multiple distant microphone speech recognition in noisy environments (#REF) . CHiME-3 data consists of sentences of the Wall Street Journal corpus, uttered by four speakers in four noisy environments, and recorded by five microphones placed on the frame of a tablet PC (a sixth one, placed on the back, mainly records background noise). Training and test respectively contain 1,640 and 1,320 sentences.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The features and algorithms contained in TranscRater have been successfully used in previous works C. de #REF; #TARGET_REF; #REFa) .\"]}"
    },
    {
        "gold": {
            "text": [
                "As mentioned before, TranscRater provides the possibility to evaluate multiple transcriptions (e.g. obtained from different microphones or ASR systems) and rank them based on their quality.",
                "This can be done either indirectly, by exploiting the predicted WER labels in a \"ranking by regression\" approach (RR) or directly, by exploiting machinelearned ranking methods (MLR).",
                "To train and test MLR models, TranscRater exploits RankLib 8 , a library of learning-to-rank algorithms.",
                "The current version of the tool includes an interface to the Random Forest algorithm (RF (#REF) ), the same used in #TARGET_REF .",
                "MLR predicts ranks through pairwise comparison between the transcriptions."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "As mentioned before, TranscRater provides the possibility to evaluate multiple transcriptions (e.g. obtained from different microphones or ASR systems) and rank them based on their quality. This can be done either indirectly, by exploiting the predicted WER labels in a \"ranking by regression\" approach (RR) or directly, by exploiting machinelearned ranking methods (MLR). To train and test MLR models, TranscRater exploits RankLib 8 , a library of learning-to-rank algorithms. The current version of the tool includes an interface to the Random Forest algorithm (RF (#REF) ), the same used in #TARGET_REF . MLR predicts ranks through pairwise comparison between the transcriptions.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"The current version of the tool includes an interface to the Random Forest algorithm (RF (#REF) ), the same used in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "One important reason seems to be that dependency parsing offers a good compromise between the conflicting demands of analysis depth, on the one hand, and robustness and efficiency, on the other.",
                "Thus, whereas a complete dependency structure provides a fully disambiguated analysis of a sentence, this analysis is typically less complex than in frameworks based on constituent analysis and can therefore often be computed deterministically with reasonable accuracy.",
                "Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (#REF) , English #TARGET_REF , Turkish (#REF) , and Swedish (#REF) .",
                "For English, the interest in dependency parsing has been weaker than for other languages.",
                "To some extent, this can probably be explained by the strong tradition of constituent analysis in Anglo-American linguistics, but this trend has been reinforced by the fact that the major treebank of American English, the Penn Treebank (#REF) , is annotated primarily with constituent analysis."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "One important reason seems to be that dependency parsing offers a good compromise between the conflicting demands of analysis depth, on the one hand, and robustness and efficiency, on the other. Thus, whereas a complete dependency structure provides a fully disambiguated analysis of a sentence, this analysis is typically less complex than in frameworks based on constituent analysis and can therefore often be computed deterministically with reasonable accuracy. Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (#REF) , English #TARGET_REF , Turkish (#REF) , and Swedish (#REF) . For English, the interest in dependency parsing has been weaker than for other languages. To some extent, this can probably be explained by the strong tradition of constituent analysis in Anglo-American linguistics, but this trend has been reinforced by the fact that the major treebank of American English, the Penn Treebank (#REF) , is annotated primarily with constituent analysis.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (#REF) , English #TARGET_REF , Turkish (#REF) , and Swedish (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Moreover, the deterministic dependency parser of #TARGET_REF , when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of #REF and #REF .",
                "The parser described in this paper is similar to that of #REF in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank.",
                "However, there are also important differences between the two approaches.",
                "First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (essentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithm proposed in #REF , which combines bottomup and top-down processing in a single pass in order to achieve incrementality.",
                "This also means that the time complexity of the algorithm used here is linear in the size of the input, while the algorithm of Yamada and Matsumoto is quadratic in the worst case."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Moreover, the deterministic dependency parser of #TARGET_REF , when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of #REF and #REF . The parser described in this paper is similar to that of #REF in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank. However, there are also important differences between the two approaches. First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (essentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithm proposed in #REF , which combines bottomup and top-down processing in a single pass in order to achieve incrementality. This also means that the time complexity of the algorithm used here is linear in the size of the input, while the algorithm of Yamada and Matsumoto is quadratic in the worst case.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Moreover, the deterministic dependency parser of #TARGET_REF , when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of #REF and #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Moreover, the deterministic dependency parser of #REF , when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of #REF and #REF .",
                "The parser described in this paper is similar to that of #TARGET_REF in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank.",
                "However, there are also important differences between the two approaches.",
                "First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (essentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithm proposed in #REF , which combines bottomup and top-down processing in a single pass in order to achieve incrementality.",
                "This also means that the time complexity of the algorithm used here is linear in the size of the input, while the algorithm of Yamada and Matsumoto is quadratic in the worst case."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "Moreover, the deterministic dependency parser of #REF , when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of #REF and #REF . The parser described in this paper is similar to that of #TARGET_REF in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank. However, there are also important differences between the two approaches. First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (essentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithm proposed in #REF , which combines bottomup and top-down processing in a single pass in order to achieve incrementality. This also means that the time complexity of the algorithm used here is linear in the size of the input, while the algorithm of Yamada and Matsumoto is quadratic in the worst case.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"The parser described in this paper is similar to that of #TARGET_REF in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank.\", \"However, there are also important differences between the two approaches.\", \"First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (essentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithm proposed in #REF , which combines bottomup and top-down processing in a single pass in order to achieve incrementality.\"]}"
    },
    {
        "gold": {
            "text": [
                "Another difference is that Yamada and Matsumoto use support vector machines (#REF) , while we instead rely on memory-based learning (#REF) .",
                "Most importantly, however, the parser presented in this paper constructs labeled dependency graphs, i.e. dependency graphs where arcs are labeled with dependency types.",
                "As far as we know, this makes it different from all previous systems for dependency parsing applied to the Penn Treebank (#REF; #TARGET_REF , although there are systems that extract labeled grammatical relations based on shallow parsing, e.g. #REF .",
                "The fact that we are working with labeled dependency graphs is also one of the motivations for choosing memory-based learning over support vector machines, since we require a multi-class classifier.",
                "Even though it is possible to use SVM for multi-class classification, this can get cumbersome when the number of classes is large."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Another difference is that Yamada and Matsumoto use support vector machines (#REF) , while we instead rely on memory-based learning (#REF) . Most importantly, however, the parser presented in this paper constructs labeled dependency graphs, i.e. dependency graphs where arcs are labeled with dependency types. As far as we know, this makes it different from all previous systems for dependency parsing applied to the Penn Treebank (#REF; #TARGET_REF , although there are systems that extract labeled grammatical relations based on shallow parsing, e.g. #REF . The fact that we are working with labeled dependency graphs is also one of the motivations for choosing memory-based learning over support vector machines, since we require a multi-class classifier. Even though it is possible to use SVM for multi-class classification, this can get cumbersome when the number of classes is large.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"As far as we know, this makes it different from all previous systems for dependency parsing applied to the Penn Treebank (#REF; #TARGET_REF , although there are systems that extract labeled grammatical relations based on shallow parsing, e.g. #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, we have also included the labeled attachment score restricted to the G set for the parser using the B set (BG), and we see then that the attachment score improves, especially for Model 2. (All differences are significant beyond the .01 level; McNemar's test.) Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with #REF (Model 3) , #REF , and #REF .",
                "5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of #TARGET_REF .",
                "We believe that there are mainly three reasons for this.",
                "First of all, the part-of-speech tagger used for preprocessing in our experiments has a lower accuracy than the one used by #REF (96.1% vs. 97.1%) .",
                "Although this is not a very interesting explanation, it undoubtedly accounts for part of the difference."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Therefore, we have also included the labeled attachment score restricted to the G set for the parser using the B set (BG), and we see then that the attachment score improves, especially for Model 2. (All differences are significant beyond the .01 level; McNemar's test.) Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with #REF (Model 3) , #REF , and #REF . 5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of #TARGET_REF . We believe that there are mainly three reasons for this. First of all, the part-of-speech tagger used for preprocessing in our experiments has a lower accuracy than the one used by #REF (96.1% vs. 97.1%) . Although this is not a very interesting explanation, it undoubtedly accounts for part of the difference.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We believe that there are mainly three reasons for this.",
                "First of all, the part-of-speech tagger used for preprocessing in our experiments has a lower accuracy than the one used by #TARGET_REF (96.1% vs. 97.1%) .",
                "Although this is not a very interesting explanation, it undoubtedly accounts for part of the difference.",
                "Secondly, since 5 The information in the first three rows is taken directly from #REF .",
                "our parser makes crucial use of dependency type information in predicting the next action of the parser, it is very likely that it suffers from the lack of real dependency labels in the converted treebank."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We believe that there are mainly three reasons for this. First of all, the part-of-speech tagger used for preprocessing in our experiments has a lower accuracy than the one used by #TARGET_REF (96.1% vs. 97.1%) . Although this is not a very interesting explanation, it undoubtedly accounts for part of the difference. Secondly, since 5 The information in the first three rows is taken directly from #REF . our parser makes crucial use of dependency type information in predicting the next action of the parser, it is very likely that it suffers from the lack of real dependency labels in the converted treebank.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"First of all, the part-of-speech tagger used for preprocessing in our experiments has a lower accuracy than the one used by #TARGET_REF (96.1% vs. 97.1%) .\"]}"
    },
    {
        "gold": {
            "text": [
                "In another study, #REF report an F-measure of 98.9% for the assignment of Penn Treebank grammatical role labels (our G set) to phrases that were correctly parsed by the parser described in #REF .",
                "If null labels (corresponding to our DEP labels) are excluded, the F-score drops to 95.7%.",
                "The corresponding F-measures for our best parser (Model 2, BG) are 99.0% and 94.7%.",
                "For the larger B set, our best parser achieves an F-measure of 96.9% (DEP labels included), which can be compared with 97.0% for a similar (but larger) set of labels in #REF .",
                "6 Although none of the previous results on labeling accuracy is strictly comparable to ours, it nevertheless seems fair to conclude that the #TARGET_REF labeling accuracy of the present parser is close to the state of the art, even if its capacity to derive correct structures is not."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "In another study, #REF report an F-measure of 98.9% for the assignment of Penn Treebank grammatical role labels (our G set) to phrases that were correctly parsed by the parser described in #REF . If null labels (corresponding to our DEP labels) are excluded, the F-score drops to 95.7%. The corresponding F-measures for our best parser (Model 2, BG) are 99.0% and 94.7%. For the larger B set, our best parser achieves an F-measure of 96.9% (DEP labels included), which can be compared with 97.0% for a similar (but larger) set of labels in #REF . 6 Although none of the previous results on labeling accuracy is strictly comparable to ours, it nevertheless seems fair to conclude that the #TARGET_REF labeling accuracy of the present parser is close to the state of the art, even if its capacity to derive correct structures is not.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"6 Although none of the previous results on labeling accuracy is strictly comparable to ours, it nevertheless seems fair to conclude that the #TARGET_REF labeling accuracy of the present parser is close to the state of the art, even if its capacity to derive correct structures is not.\"]}"
    },
    {
        "gold": {
            "text": [
                "This task, which we refer to as selective generation, is often formulated as two subproblems: content selection, which involves choosing a subset of relevant records to talk about from the exhaustive database, and surface realization, which is concerned with generating natural language descriptions for this subset.",
                "Learning to perform these tasks jointly is challenging due to the ambiguity in deciding which records are relevant, the complex dependencies between selected records, and the multiple ways in which these records can be described.",
                "Previous work has made significant progress on this task (#REF; #TARGET_REF; #REF) .",
                "However, most approaches solve the two content selection and surface realization subtasks separately, use manual domain-dependent resources (e.g., semantic parsers) and features, or employ template-based generation.",
                "This limits domain adaptability and reduces coherence."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "This task, which we refer to as selective generation, is often formulated as two subproblems: content selection, which involves choosing a subset of relevant records to talk about from the exhaustive database, and surface realization, which is concerned with generating natural language descriptions for this subset. Learning to perform these tasks jointly is challenging due to the ambiguity in deciding which records are relevant, the complex dependencies between selected records, and the multiple ways in which these records can be described. Previous work has made significant progress on this task (#REF; #TARGET_REF; #REF) . However, most approaches solve the two content selection and surface realization subtasks separately, use manual domain-dependent resources (e.g., semantic parsers) and features, or employ template-based generation. This limits domain adaptability and reduces coherence.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Learning to perform these tasks jointly is challenging due to the ambiguity in deciding which records are relevant, the complex dependencies between selected records, and the multiple ways in which these records can be described.\", \"Previous work has made significant progress on this task (#REF; #TARGET_REF; #REF) .\", \"However, most approaches solve the two content selection and surface realization subtasks separately, use manual domain-dependent resources (e.g., semantic parsers) and features, or employ template-based generation.\"]}"
    },
    {
        "gold": {
            "text": [
                "Other effective approaches include the use of tree conditional random fields (#REF) and template extraction within a log-linear framework #TARGET_REF .",
                "Recent work seeks to solve the full selective generation problem through a single framework.",
                "#REF and #REF learn alignments between comments and their corresponding event records using a translation model for parsing and generation.",
                "implement a two-stage framework that decides what to discuss using a combination of the methods of #REF and #REF , and then produces the text based on the generation system of #REF .",
                "#REF propose a unified conceptto-text model that treats joint content selection and surface realization as a sequence of local decisions represented by a log-linear model."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Other effective approaches include the use of tree conditional random fields (#REF) and template extraction within a log-linear framework #TARGET_REF . Recent work seeks to solve the full selective generation problem through a single framework. #REF and #REF learn alignments between comments and their corresponding event records using a translation model for parsing and generation. implement a two-stage framework that decides what to discuss using a combination of the methods of #REF and #REF , and then produces the text based on the generation system of #REF . #REF propose a unified conceptto-text model that treats joint content selection and surface realization as a sequence of local decisions represented by a log-linear model.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Other effective approaches include the use of tree conditional random fields (#REF) and template extraction within a log-linear framework #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF and #REF learn alignments between comments and their corresponding event records using a translation model for parsing and generation.",
                "implement a two-stage framework that decides what to discuss using a combination of the methods of #REF and #REF , and then produces the text based on the generation system of #REF .",
                "#TARGET_REF propose a unified conceptto-text model that treats joint content selection and surface realization as a sequence of local decisions represented by a log-linear model.",
                "Similar to other work, they train their model using external alignments from #REF .",
                "Generation then follows as inference over this model, where they first choose an event record, then the record's fields (i.e., attributes), and finally a set of templates that they then fill in with words for the selected fields."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "#REF and #REF learn alignments between comments and their corresponding event records using a translation model for parsing and generation. implement a two-stage framework that decides what to discuss using a combination of the methods of #REF and #REF , and then produces the text based on the generation system of #REF . #TARGET_REF propose a unified conceptto-text model that treats joint content selection and surface realization as a sequence of local decisions represented by a log-linear model. Similar to other work, they train their model using external alignments from #REF . Generation then follows as inference over this model, where they first choose an event record, then the record's fields (i.e., attributes), and finally a set of templates that they then fill in with words for the selected fields.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF propose a unified conceptto-text model that treats joint content selection and surface realization as a sequence of local decisions represented by a log-linear model.\"]}"
    },
    {
        "gold": {
            "text": [
                "Aligner Ablation First, we evaluate the contribution of our proposed coarse-to-fine aligner by comparing our model with the basic encoder-alignerdecoder model introduced by #REF .",
                "Table 3 reports the results demonstrating that our aligner yields superior F-1 and BLEU scores relative to a standard aligner.",
                "Encoder Ablation Next, we consider the effectiveness of the encoder.",
                "Table 4 compares the results with and without the encoder on the development set, and demonstrates that there is a significant gain from encoding the event records using the LSTM-RNN.",
                "We attribute this improvement to the LSTM-RNN's ability to capture the relationships that exist among the records, which is known to be essential to selective generation (#REF; #TARGET_REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Aligner Ablation First, we evaluate the contribution of our proposed coarse-to-fine aligner by comparing our model with the basic encoder-alignerdecoder model introduced by #REF . Table 3 reports the results demonstrating that our aligner yields superior F-1 and BLEU scores relative to a standard aligner. Encoder Ablation Next, we consider the effectiveness of the encoder. Table 4 compares the results with and without the encoder on the development set, and demonstrates that there is a significant gain from encoding the event records using the LSTM-RNN. We attribute this improvement to the LSTM-RNN's ability to capture the relationships that exist among the records, which is known to be essential to selective generation (#REF; #TARGET_REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"We attribute this improvement to the LSTM-RNN's ability to capture the relationships that exist among the records, which is known to be essential to selective generation (#REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We take an alternative, neural encoder-aligner-decoder approach to free-form selective generation that jointly performs content selection and surface realization, without using any specialized features, resources, or generation templates.",
                "This enables our approach to generalize to new domains.",
                "Further, our memorybased model captures the long-range contextual dependencies among records and descriptions, which are integral to this task #TARGET_REF .",
                "We formulate our model as an encoder-alignerdecoder framework that uses recurrent neural networks with long short-term memory units (LSTMRNNs) (#REF) together with a coarse-to-fine aligner to select and \"translate\" the rich world state into a natural language description.",
                "Our model first encodes the full set of over-determined event records using a bidirectional LSTM-RNN."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We take an alternative, neural encoder-aligner-decoder approach to free-form selective generation that jointly performs content selection and surface realization, without using any specialized features, resources, or generation templates. This enables our approach to generalize to new domains. Further, our memorybased model captures the long-range contextual dependencies among records and descriptions, which are integral to this task #TARGET_REF . We formulate our model as an encoder-alignerdecoder framework that uses recurrent neural networks with long short-term memory units (LSTMRNNs) (#REF) together with a coarse-to-fine aligner to select and \"translate\" the rich world state into a natural language description. Our model first encodes the full set of over-determined event records using a bidirectional LSTM-RNN.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Further, our memorybased model captures the long-range contextual dependencies among records and descriptions, which are integral to this task #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Note that when γ is equal to N , the pre-selector is forced to select all the records (p j = 1.0 for all j), and the coarse-to-fine alignment reverts to the standard alignment introduced by #REF .",
                "Together with the negative loglikelihood of the ground-truth description x * 1:T , our loss function becomes",
                "Having trained the model, we generate the natural language description by finding the maximum a posteriori words under the learned model (Eqn. 1).",
                "For inference, we perform greedy search starting with the first word x 1 .",
                "Beam search offers a way to perform approximate joint inference -however, we empirically found that beam search does not perform any better than greedy search on the datasets that we consider, an observation that is shared with previous work #TARGET_REF ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Note that when γ is equal to N , the pre-selector is forced to select all the records (p j = 1.0 for all j), and the coarse-to-fine alignment reverts to the standard alignment introduced by #REF . Together with the negative loglikelihood of the ground-truth description x * 1:T , our loss function becomes Having trained the model, we generate the natural language description by finding the maximum a posteriori words under the learned model (Eqn. 1). For inference, we perform greedy search starting with the first word x 1 . Beam search offers a way to perform approximate joint inference -however, we empirically found that beam search does not perform any better than greedy search on the datasets that we consider, an observation that is shared with previous work #TARGET_REF .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Beam search offers a way to perform approximate joint inference -however, we empirically found that beam search does not perform any better than greedy search on the datasets that we consider, an observation that is shared with previous work #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Datasets We analyze our model on the benchmark WEATHERGOV dataset, and use the data-starved ROBOCUP dataset to demonstrate the model's generalizability.",
                "Following #TARGET_REF , we use WEATHERGOV training, development, and test splits of size 25000, 1000, and 3528, respectively.",
                "For ROBOCUP, we follow the evaluation methodology of previous work (#REF) , performing three-fold cross-validation whereby we train on three games (approximately 1000 scenarios) and test on the fourth.",
                "Within each split, we hold out 10% of the training data as the development set to tune the early-stopping criterion and γ.",
                "We then report the standard average performance (weighted by the number of scenarios) over these four splits."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Datasets We analyze our model on the benchmark WEATHERGOV dataset, and use the data-starved ROBOCUP dataset to demonstrate the model's generalizability. Following #TARGET_REF , we use WEATHERGOV training, development, and test splits of size 25000, 1000, and 3528, respectively. For ROBOCUP, we follow the evaluation methodology of previous work (#REF) , performing three-fold cross-validation whereby we train on three games (approximately 1000 scenarios) and test on the fourth. Within each split, we hold out 10% of the training data as the development set to tune the early-stopping criterion and γ. We then report the standard average performance (weighted by the number of scenarios) over these four splits.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following #TARGET_REF , we use WEATHERGOV training, development, and test splits of size 25000, 1000, and 3528, respectively.\"]}"
    },
    {
        "gold": {
            "text": [
                "We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of #TARGET_REF), respectively (Sec. 5).",
                "Table 1 compares our test results against previous methods that include KL12 (#REF) , KL13 (#REF) , and ALK10 (#REF) .",
                "Our method achieves the best results reported to-date on all three metrics, with relative improvements of 11.94% (F-1), 58.88% (sBLEU), and 36.68% (cBLEU) over the previous state-of-the-art."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of #TARGET_REF), respectively (Sec. 5). Table 1 compares our test results against previous methods that include KL12 (#REF) , KL13 (#REF) , and ALK10 (#REF) . Our method achieves the best results reported to-date on all three metrics, with relative improvements of 11.94% (F-1), 58.88% (sBLEU), and 36.68% (cBLEU) over the previous state-of-the-art.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of #TARGET_REF), respectively (Sec. 5).\"]}"
    },
    {
        "gold": {
            "text": [
                "We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of #REF), respectively (Sec. 5).",
                "Table 1 compares our test results against previous methods that include KL12 (#REF) , KL13 (#REF) , and ALK10 #TARGET_REF .",
                "Our method achieves the best results reported to-date on all three metrics, with relative improvements of 11.94% (F-1), 58.88% (sBLEU), and 36.68% (cBLEU) over the previous state-of-the-art."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1
            ]
        },
        "input": "We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of #REF), respectively (Sec. 5). Table 1 compares our test results against previous methods that include KL12 (#REF) , KL13 (#REF) , and ALK10 #TARGET_REF . Our method achieves the best results reported to-date on all three metrics, with relative improvements of 11.94% (F-1), 58.88% (sBLEU), and 36.68% (cBLEU) over the previous state-of-the-art.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Table 1 compares our test results against previous methods that include KL12 (#REF) , KL13 (#REF) , and ALK10 #TARGET_REF .\", \"Our method achieves the best results reported to-date on all three metrics, with relative improvements of 11.94% (F-1), 58.88% (sBLEU), and 36.68% (cBLEU) over the previous state-of-the-art.\"]}"
    },
    {
        "gold": {
            "text": [
                "We considered beam search as an alternative to greedy search in our primary setup (Eqn. 1), but this performs worse, similar to what previous work found on this dataset #TARGET_REF .",
                "As an alternative, we consider a beam filter based on a knearest neighborhood.",
                "See Supplementary Material for details.",
                "Table 9 shows that this k-NN beam filter improves results over the primary greedy results.",
                "Aligner Ablation First, we evaluate the contribution of our proposed coarse-to-fine aligner by comparing our model with the basic encoder-alignerdecoder model introduced by #REF ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We considered beam search as an alternative to greedy search in our primary setup (Eqn. 1), but this performs worse, similar to what previous work found on this dataset #TARGET_REF . As an alternative, we consider a beam filter based on a knearest neighborhood. See Supplementary Material for details. Table 9 shows that this k-NN beam filter improves results over the primary greedy results. Aligner Ablation First, we evaluate the contribution of our proposed coarse-to-fine aligner by comparing our model with the basic encoder-alignerdecoder model introduced by #REF .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We considered beam search as an alternative to greedy search in our primary setup (Eqn. 1), but this performs worse, similar to what previous work found on this dataset #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Examples of supervised learning methods for WSD appear in [2] [3] [4] , [7] [8] .",
                "The learning algorithms applied including: decision tree, decisionlist [15] , neural networks [7] , naïve Bayesian learning ( [5] , #TARGET_REF ) and maximum entropy [10] .",
                "Among these leaning methods, the most important issue is what features will be used to construct the classifier.",
                "It is common in WSD to use contextual information that can be found in the neighborhood of the ambiguous word in training data ( [6] , [16] [17] [18] ).",
                "It is generally true that when words are used in the same sense, they have similar context and co-occurrence information [13] ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Examples of supervised learning methods for WSD appear in [2] [3] [4] , [7] [8] . The learning algorithms applied including: decision tree, decisionlist [15] , neural networks [7] , naïve Bayesian learning ( [5] , #TARGET_REF ) and maximum entropy [10] . Among these leaning methods, the most important issue is what features will be used to construct the classifier. It is common in WSD to use contextual information that can be found in the neighborhood of the ambiguous word in training data ( [6] , [16] [17] [18] ). It is generally true that when words are used in the same sense, they have similar context and co-occurrence information [13] .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The learning algorithms applied including: decision tree, decisionlist [15] , neural networks [7] , na\\u00efve Bayesian learning ( [5] , #TARGET_REF ) and maximum entropy [10] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Niu #TARGET_REF proved in his experiments that Naïve Bayes classifier achieved best disambiguation accuracy with small topical context window size (< 10 words).",
                "We follow their method and set the contextual window size as 10 in our system.",
                "Each of the Chinese words except the stop words inside the window range will be considered as one topical feature.",
                "Their frequencies are calculated over the entire corpus with respect to each sense of an ambiguous word w. The sense definitions are obtained from HowNet."
            ],
            "label": [
                "USE",
                "BACK"
            ],
            "context": [
                1,
                1,
                0,
                0
            ]
        },
        "input": "Niu #TARGET_REF proved in his experiments that Naïve Bayes classifier achieved best disambiguation accuracy with small topical context window size (< 10 words). We follow their method and set the contextual window size as 10 in our system. Each of the Chinese words except the stop words inside the window range will be considered as one topical feature. Their frequencies are calculated over the entire corpus with respect to each sense of an ambiguous word w. The sense definitions are obtained from HowNet.",
        "output": "{\"label\": [\"USE\", \"BACK\"], \"context\": [\"Niu #TARGET_REF proved in his experiments that Na\\u00efve Bayes classifier achieved best disambiguation accuracy with small topical context window size (< 10 words).\", \"We follow their method and set the contextual window size as 10 in our system.\"]}"
    },
    {
        "gold": {
            "text": [
                "The sources of the collocations will be explained in Section 4.1.",
                "In both Niu #TARGET_REF and Dang's [10] work, topical features as well as the so called collocational features were used.",
                "However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features.",
                "However, bi-gram co-occurrences only indicate statistical significance which may not actually satisfy the conceptual definition of collocations.",
                "Thus instead of using co-occurrences of bigrams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                1
            ]
        },
        "input": "The sources of the collocations will be explained in Section 4.1. In both Niu #TARGET_REF and Dang's [10] work, topical features as well as the so called collocational features were used. However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features. However, bi-gram co-occurrences only indicate statistical significance which may not actually satisfy the conceptual definition of collocations. Thus instead of using co-occurrences of bigrams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"In both Niu #TARGET_REF and Dang's [10] work, topical features as well as the so called collocational features were used.\", \"Thus instead of using co-occurrences of bigrams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, researchers proposed several domain adaptation techniques by using the unlabeled test data to obtain better performance [5, 14, 16, 25, 37] .",
                "Interestingly, some recent works [13, 18] indicate that string kernels can yield robust results in the cross-domain setting without any domain adaptation.",
                "In fact, methods based on string kernels have demonstrated impressive results in various text classification tasks ranging from native language identification [22] [23] [24] 36] and authorship identification [34] to dialect identification [4, 18, 21] , sentiment analysis [13, 35] and automatic essay scoring [7] .",
                "As long as a labeled training set is available, string kernels can reach state-of-the-art results in various languages including English [7, #TARGET_REF 23] , Arabic [4, 17, 18, 24] , #REF and #REF .",
                "Different from all these recent approaches, we use unlabeled data from the test set in a transductive setting in order to significantly increase the performance of string kernels."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "However, researchers proposed several domain adaptation techniques by using the unlabeled test data to obtain better performance [5, 14, 16, 25, 37] . Interestingly, some recent works [13, 18] indicate that string kernels can yield robust results in the cross-domain setting without any domain adaptation. In fact, methods based on string kernels have demonstrated impressive results in various text classification tasks ranging from native language identification [22] [23] [24] 36] and authorship identification [34] to dialect identification [4, 18, 21] , sentiment analysis [13, 35] and automatic essay scoring [7] . As long as a labeled training set is available, string kernels can reach state-of-the-art results in various languages including English [7, #TARGET_REF 23] , Arabic [4, 17, 18, 24] , #REF and #REF . Different from all these recent approaches, we use unlabeled data from the test set in a transductive setting in order to significantly increase the performance of string kernels.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"As long as a labeled training set is available, string kernels can reach state-of-the-art results in various languages including English [7, #TARGET_REF 23] , Arabic [4, 17, 18, 24] , #REF and #REF .\", \"Different from all these recent approaches, we use unlabeled data from the test set in a transductive setting in order to significantly increase the performance of string kernels.\"]}"
    },
    {
        "gold": {
            "text": [
                "Giménez-Pérez et al. [13] have used string kernels for single-source and multi-source polarity classification.",
                "Remarkably, they obtain state-of-the-art performance without using knowledge from the target domain, which indicates that string kernels provide robust results in the cross-domain setting without any domain adaptation.",
                "#REF obtained the best performance in the Arabic Dialect Identification Shared Task of the 2017 VarDial Evaluation #REF , with an improvement of 4.6% over the second-best method.",
                "It is important to note that the training and the test speech samples prepared for the shared task were recorded in different setups [41] , or in other words, the training and the test sets are drawn from different distributions.",
                "Different from all these recent approaches #TARGET_REF 18, 23] , we use unlabeled data from the target domain to significantly increase the performance of string kernels in cross-domain text classification, particularly in English polarity classification."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Giménez-Pérez et al. [13] have used string kernels for single-source and multi-source polarity classification. Remarkably, they obtain state-of-the-art performance without using knowledge from the target domain, which indicates that string kernels provide robust results in the cross-domain setting without any domain adaptation. #REF obtained the best performance in the Arabic Dialect Identification Shared Task of the 2017 VarDial Evaluation #REF , with an improvement of 4.6% over the second-best method. It is important to note that the training and the test speech samples prepared for the shared task were recorded in different setups [41] , or in other words, the training and the test sets are drawn from different distributions. Different from all these recent approaches #TARGET_REF 18, 23] , we use unlabeled data from the target domain to significantly increase the performance of string kernels in cross-domain text classification, particularly in English polarity classification.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Different from all these recent approaches #TARGET_REF 18, 23] , we use unlabeled data from the target domain to significantly increase the performance of string kernels in cross-domain text classification, particularly in English polarity classification.\"]}"
    },
    {
        "gold": {
            "text": [
                "Baselines.",
                "We compare our approach with several methods [3, 12, 13, 15, 32, 40] in two cross-domain settings.",
                "Using string kernels, Giménez-Pérez et al. #TARGET_REF reported better performance than SST [3] and KE-#REF in the multi-source domain setting.",
                "In addition, we compare our approach with SFA [32] , CORAL [40] and TR-#REF in the single-source setting.",
                "Method DEK→B BEK→D BDK→E BDE→K SST [3] 76.3 78.3 83.9 85.2 KE-#REF 77.9 80.4 78.9 82.5 K 0/1 [13] 82 Table 1 ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Baselines. We compare our approach with several methods [3, 12, 13, 15, 32, 40] in two cross-domain settings. Using string kernels, Giménez-Pérez et al. #TARGET_REF reported better performance than SST [3] and KE-#REF in the multi-source domain setting. In addition, we compare our approach with SFA [32] , CORAL [40] and TR-#REF in the single-source setting. Method DEK→B BEK→D BDK→E BDE→K SST [3] 76.3 78.3 83.9 85.2 KE-#REF 77.9 80.4 78.9 82.5 K 0/1 [13] 82 Table 1 .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Using string kernels, Gim\\u00e9nez-P\\u00e9rez et al. #TARGET_REF reported better performance than SST [3] and KE-#REF in the multi-source domain setting.\"]}"
    },
    {
        "gold": {
            "text": [
                "Furthermore, we use the same kernels, namely the presence bits string kernel (K 0/1 ) and the intersection string kernel (K ∩ ), and the same range of character n-grams (5) (6) (7) (8) . To compute the string kernels, we used the open-source code provided by Ionescu et al. [20, 23] .",
                "For the transductive kernel classifier, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training.",
                "We choose Kernel Ridge #REF as classifier and set its regularization parameter to 10 −5 in all our experiments.",
                "Although Giménez-Pérez et al. #TARGET_REF used a different classifier, namely Kernel Discriminant Analysis, we observed that Kernel Ridge Regression produces similar results (±0.1%) when we employ the same string kernels.",
                "As Giménez-Pérez et al. [13] , we evaluate our approach in two cross-domain settings."
            ],
            "label": [
                "DIFFER",
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Furthermore, we use the same kernels, namely the presence bits string kernel (K 0/1 ) and the intersection string kernel (K ∩ ), and the same range of character n-grams (5) (6) (7) (8) . To compute the string kernels, we used the open-source code provided by Ionescu et al. [20, 23] . For the transductive kernel classifier, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training. We choose Kernel Ridge #REF as classifier and set its regularization parameter to 10 −5 in all our experiments. Although Giménez-Pérez et al. #TARGET_REF used a different classifier, namely Kernel Discriminant Analysis, we observed that Kernel Ridge Regression produces similar results (±0.1%) when we employ the same string kernels. As Giménez-Pérez et al. [13] , we evaluate our approach in two cross-domain settings.",
        "output": "{\"label\": [\"DIFFER\", \"SIM\"], \"context\": [\"Although Gim\\u00e9nez-P\\u00e9rez et al. #TARGET_REF used a different classifier, namely Kernel Discriminant Analysis, we observed that Kernel Ridge Regression produces similar results (\\u00b10.1%) when we employ the same string kernels.\"]}"
    },
    {
        "gold": {
            "text": [
                "We considered all possible combinations of source and target domains in this experiment, and we improve the results in each and every case.",
                "Without exception, the accuracy rates reached by the transductive string kernels are significantly better than the best baseline string kernel #TARGET_REF , according to the McNemar's test performed at a confidence level of 0.01.",
                "The highest improvements (above 2.7%) are obtained when the source domain contains Books reviews and the target domain contains Kitchen reviews.",
                "As in the multi-source setting, we obtain much better results when the transductive classifier is employed for the learning task.",
                "In all cases, the accuracy rates of the transductive classifier are more than 2% better than the best baseline string kernel."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We considered all possible combinations of source and target domains in this experiment, and we improve the results in each and every case. Without exception, the accuracy rates reached by the transductive string kernels are significantly better than the best baseline string kernel #TARGET_REF , according to the McNemar's test performed at a confidence level of 0.01. The highest improvements (above 2.7%) are obtained when the source domain contains Books reviews and the target domain contains Kitchen reviews. As in the multi-source setting, we obtain much better results when the transductive classifier is employed for the learning task. In all cases, the accuracy rates of the transductive classifier are more than 2% better than the best baseline string kernel.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Without exception, the accuracy rates reached by the transductive string kernels are significantly better than the best baseline string kernel #TARGET_REF , according to the McNemar's test performed at a confidence level of 0.01.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we presented two domain adaptation approaches that can be used together to improve the results of string kernels in cross-domain settings.",
                "We provided empirical evidence indicating that our framework can be successfully applied in cross-domain text classification, particularly in cross-domain English polarity classification.",
                "Indeed, the polarity classification experiments demonstrate that our framework achieves better accuracy rates than other state-ofthe-art methods [3, 12, #TARGET_REF 15, 32, 40] .",
                "By using the same parameters across all the experiments, we showed that our transductive transfer learning framework can bring significant improvements without having to fine-tune the parameters for each individual setting.",
                "Although the framework described in this paper can be generally applied to any kernel method, we focused our work only on string kernel approaches used in text classification."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In this paper, we presented two domain adaptation approaches that can be used together to improve the results of string kernels in cross-domain settings. We provided empirical evidence indicating that our framework can be successfully applied in cross-domain text classification, particularly in cross-domain English polarity classification. Indeed, the polarity classification experiments demonstrate that our framework achieves better accuracy rates than other state-ofthe-art methods [3, 12, #TARGET_REF 15, 32, 40] . By using the same parameters across all the experiments, we showed that our transductive transfer learning framework can bring significant improvements without having to fine-tune the parameters for each individual setting. Although the framework described in this paper can be generally applied to any kernel method, we focused our work only on string kernel approaches used in text classification.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Indeed, the polarity classification experiments demonstrate that our framework achieves better accuracy rates than other state-ofthe-art methods [3, 12, #TARGET_REF 15, 32, 40] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Baselines.",
                "We compare our approach with several methods [3, 12, #TARGET_REF 15, 32, 40] in two cross-domain settings.",
                "Using string kernels, Giménez-Pérez et al. [13] reported better performance than SST [3] and KE-#REF in the multi-source domain setting.",
                "In addition, we compare our approach with SFA [32] , CORAL [40] and TR-#REF in the single-source setting.",
                "Method DEK→B BEK→D BDK→E BDE→K SST [3] 76.3 78.3 83.9 85.2 KE-#REF 77.9 80.4 78.9 82.5 K 0/1 [13] 82 Table 1 ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Baselines. We compare our approach with several methods [3, 12, #TARGET_REF 15, 32, 40] in two cross-domain settings. Using string kernels, Giménez-Pérez et al. [13] reported better performance than SST [3] and KE-#REF in the multi-source domain setting. In addition, we compare our approach with SFA [32] , CORAL [40] and TR-#REF in the single-source setting. Method DEK→B BEK→D BDK→E BDE→K SST [3] 76.3 78.3 83.9 85.2 KE-#REF 77.9 80.4 78.9 82.5 K 0/1 [13] 82 Table 1 .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We compare our approach with several methods [3, 12, #TARGET_REF 15, 32, 40] in two cross-domain settings.\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition, we compare our approach with SFA [32] , CORAL [40] and TR-#REF in the single-source setting.",
                "Method DEK→B BEK→D BDK→E BDE→K SST [3] 76.3 78.3 83.9 85.2 KE-#REF 77.9 80.4 78.9 82.5 K 0/1 [13] 82 Table 1 .",
                "Multi-source cross-domain polarity classification accuracy rates (in %) of our transductive approaches versus a state-of-the-art baseline based on string kernels #TARGET_REF , as well as SST [3] and KE-#REF .",
                "The best accuracy rates are highlighted in bold.",
                "The marker * indicates that the performance is significantly better than the best baseline string kernel according to a paired McNemar's test performed at a significance level of 0.01."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In addition, we compare our approach with SFA [32] , CORAL [40] and TR-#REF in the single-source setting. Method DEK→B BEK→D BDK→E BDE→K SST [3] 76.3 78.3 83.9 85.2 KE-#REF 77.9 80.4 78.9 82.5 K 0/1 [13] 82 Table 1 . Multi-source cross-domain polarity classification accuracy rates (in %) of our transductive approaches versus a state-of-the-art baseline based on string kernels #TARGET_REF , as well as SST [3] and KE-#REF . The best accuracy rates are highlighted in bold. The marker * indicates that the performance is significantly better than the best baseline string kernel according to a paired McNemar's test performed at a significance level of 0.01.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Multi-source cross-domain polarity classification accuracy rates (in %) of our transductive approaches versus a state-of-the-art baseline based on string kernels #TARGET_REF , as well as SST [3] and KE-#REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We follow the same evaluation methodology of Giménez-Pérez et al. #TARGET_REF , to ensure a fair comparison.",
                "Furthermore, we use the same kernels, namely the presence bits string kernel (K 0/1 ) and the intersection string kernel (K ∩ ), and the same range of character n-grams (5) (6) (7) (8) . To compute the string kernels, we used the open-source code provided by Ionescu et al. [20, 23] .",
                "For the transductive kernel classifier, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training.",
                "We choose Kernel Ridge #REF as classifier and set its regularization parameter to 10 −5 in all our experiments.",
                "Although Giménez-Pérez et al. [13] used a different classifier, namely Kernel Discriminant Analysis, we observed that Kernel Ridge Regression produces similar results (±0.1%) when we employ the same string kernels."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We follow the same evaluation methodology of Giménez-Pérez et al. #TARGET_REF , to ensure a fair comparison. Furthermore, we use the same kernels, namely the presence bits string kernel (K 0/1 ) and the intersection string kernel (K ∩ ), and the same range of character n-grams (5) (6) (7) (8) . To compute the string kernels, we used the open-source code provided by Ionescu et al. [20, 23] . For the transductive kernel classifier, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training. We choose Kernel Ridge #REF as classifier and set its regularization parameter to 10 −5 in all our experiments. Although Giménez-Pérez et al. [13] used a different classifier, namely Kernel Discriminant Analysis, we observed that Kernel Ridge Regression produces similar results (±0.1%) when we employ the same string kernels.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We follow the same evaluation methodology of Gim\\u00e9nez-P\\u00e9rez et al. #TARGET_REF , to ensure a fair comparison.\"]}"
    },
    {
        "gold": {
            "text": [
                "For the transductive kernel classifier, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training.",
                "We choose Kernel Ridge #REF as classifier and set its regularization parameter to 10 −5 in all our experiments.",
                "Although Giménez-Pérez et al. [13] used a different classifier, namely Kernel Discriminant Analysis, we observed that Kernel Ridge Regression produces similar results (±0.1%) when we employ the same string kernels.",
                "As Giménez-Pérez et al. #TARGET_REF , we evaluate our approach in two cross-domain settings.",
                "In the multi-source setting, we train the models on all domains, except the one used for testing."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "For the transductive kernel classifier, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training. We choose Kernel Ridge #REF as classifier and set its regularization parameter to 10 −5 in all our experiments. Although Giménez-Pérez et al. [13] used a different classifier, namely Kernel Discriminant Analysis, we observed that Kernel Ridge Regression produces similar results (±0.1%) when we employ the same string kernels. As Giménez-Pérez et al. #TARGET_REF , we evaluate our approach in two cross-domain settings. In the multi-source setting, we train the models on all domains, except the one used for testing.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"As Gim\\u00e9nez-P\\u00e9rez et al. #TARGET_REF , we evaluate our approach in two cross-domain settings.\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, on the Books domain the accuracy of the transductive classifier based on the presence bits kernel (84.1%) is 2.1% above the best baseline (82.0%) represented by the intersection string kernel.",
                "Remark- Table 2 .",
                "Single-source cross-domain polarity classification accuracy rates (in %) of our transductive approaches versus a state-of-the-art baseline based on string kernels #TARGET_REF , as well as SFA [32] , CORAL [40] and TR-#REF .",
                "The best accuracy rates are highlighted in bold.",
                "The marker * indicates that the performance is significantly better than the best baseline string kernel according to a paired McNemar's test performed at a significance level of 0.01."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "For example, on the Books domain the accuracy of the transductive classifier based on the presence bits kernel (84.1%) is 2.1% above the best baseline (82.0%) represented by the intersection string kernel. Remark- Table 2 . Single-source cross-domain polarity classification accuracy rates (in %) of our transductive approaches versus a state-of-the-art baseline based on string kernels #TARGET_REF , as well as SFA [32] , CORAL [40] and TR-#REF . The best accuracy rates are highlighted in bold. The marker * indicates that the performance is significantly better than the best baseline string kernel according to a paired McNemar's test performed at a significance level of 0.01.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Single-source cross-domain polarity classification accuracy rates (in %) of our transductive approaches versus a state-of-the-art baseline based on string kernels #TARGET_REF , as well as SFA [32] , CORAL [40] and TR-#REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.",
                "This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #REF) , memory-augmented neural network (#REF; #REF) , stochastic neural network #TARGET_REF , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) .",
                "We present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language.",
                "The variational inference and sampling method are formulated to tackle the optimization for complicated models (#REF) .",
                "The word and sentence embeddings, clustering and co-clustering are merged with linguistic and semantic constraints."
            ],
            "label": [
                "USE",
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The \"distribution function\" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference. This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #REF) , memory-augmented neural network (#REF; #REF) , stochastic neural network #TARGET_REF , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) . We present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language. The variational inference and sampling method are formulated to tackle the optimization for complicated models (#REF) . The word and sentence embeddings, clustering and co-clustering are merged with linguistic and semantic constraints.",
        "output": "{\"label\": [\"USE\", \"BACK\"], \"context\": [\"This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process , Chinese restaurant process (#REF) , hierarchical Pitman-Yor process (#REF) , Indian buffet process (#REF) , recurrent neural network (#REF; Van Den #REF) , long short-term memory (#REF; , sequence-to-sequence model (#REF), variational auto-encoder (#REF) , generative adversarial network (#REF) , attention mechanism (#REF; #REF) , memory-augmented neural network (#REF; #REF) , stochastic neural network #TARGET_REF , predictive state neural network (#REF) , policy gradient (#REF) and reinforcement learning (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "These distributions were produced from the gold keys (or synsets) of the test documents by querying BabelNet for the polysemy of each word.",
                "Each distribution was normalised with one sense per discourse assumed, therefore duplicate synsets were ignored.",
                "Lastly the difference in F-Score between the conventional Run1 and the iterative Run2 and Run3 is listed beside each distribution.",
                "Firstly WSD tasks before 2013 generally relied on only a lexicon, such as WordNet (#REF) or an alternative equivalent, whereas #REF Task 12 WSD and this task #TARGET_REF included Entity Linking (EL) using the encyclopaedia Wikipedia via BabelNet (#REF) .",
                "Secondly, as shown by #REF with a simple linear regression, the iterative approach increases WSD performance for documents that have a higher degree of document monosemy -the percentage of unique monosemous lemmas in a document."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "These distributions were produced from the gold keys (or synsets) of the test documents by querying BabelNet for the polysemy of each word. Each distribution was normalised with one sense per discourse assumed, therefore duplicate synsets were ignored. Lastly the difference in F-Score between the conventional Run1 and the iterative Run2 and Run3 is listed beside each distribution. Firstly WSD tasks before 2013 generally relied on only a lexicon, such as WordNet (#REF) or an alternative equivalent, whereas #REF Task 12 WSD and this task #TARGET_REF included Entity Linking (EL) using the encyclopaedia Wikipedia via BabelNet (#REF) . Secondly, as shown by #REF with a simple linear regression, the iterative approach increases WSD performance for documents that have a higher degree of document monosemy -the percentage of unique monosemous lemmas in a document.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Firstly WSD tasks before 2013 generally relied on only a lexicon, such as WordNet (#REF) or an alternative equivalent, whereas #REF Task 12 WSD and this task #TARGET_REF included Entity Linking (EL) using the encyclopaedia Wikipedia via BabelNet (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "While the iterative approach achieved reasonably competitive results in English, this success did not translate as well to Spanish and Italian.",
                "The Italian Biomedical domain had the highest document monosemy, observable in Figure 1 (g ), yet this did not help the iterative Run2 and Run3.",
                "Yet it is worth noting the results of the task paper #TARGET_REF report that SUDOKU Run2 and Run3 achieved very low F-Scores for named entity disambiguation (<28.6) in Spanish and Italian.",
                "Given that more than half of the named entities were monosemous in Figure 1 (d) and (g), the WSD system either did not capture them in text or filtered them out during subgraph construction (see BabelNet API).",
                "This underscores the importance of named entities being included in disambiguation tasks."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "While the iterative approach achieved reasonably competitive results in English, this success did not translate as well to Spanish and Italian. The Italian Biomedical domain had the highest document monosemy, observable in Figure 1 (g ), yet this did not help the iterative Run2 and Run3. Yet it is worth noting the results of the task paper #TARGET_REF report that SUDOKU Run2 and Run3 achieved very low F-Scores for named entity disambiguation (<28.6) in Spanish and Italian. Given that more than half of the named entities were monosemous in Figure 1 (d) and (g), the WSD system either did not capture them in text or filtered them out during subgraph construction (see BabelNet API). This underscores the importance of named entities being included in disambiguation tasks.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Yet it is worth noting the results of the task paper #TARGET_REF report that SUDOKU Run2 and Run3 achieved very low F-Scores for named entity disambiguation (<28.6) in Spanish and Italian.\"]}"
    },
    {
        "gold": {
            "text": [
                "In hindsight, the iterative approach could be restricted to the parts of speech it is known to improve, while remaining with the conventional approach on others.",
                "To the right in Table 3 the author's SUDOKU runs are compared against the team with the most competitive results -LIMSI.",
                "The author could not improve on their superior results achieved in English, however for Spanish and Italian the BabelNet First Sense (BFS) baseline was much lower since it often resorted to lexicographic sorting in the absence of WordNet synsets -see (#REF) .",
                "The author's baseline-independent submissions were unaffected by this, which on reviewing results in #TARGET_REF appears to have helped SUDOKU do best for these languages.",
                "Table 3 : F1 scores for each domain/language for SUDOKU and LIMSI."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "In hindsight, the iterative approach could be restricted to the parts of speech it is known to improve, while remaining with the conventional approach on others. To the right in Table 3 the author's SUDOKU runs are compared against the team with the most competitive results -LIMSI. The author could not improve on their superior results achieved in English, however for Spanish and Italian the BabelNet First Sense (BFS) baseline was much lower since it often resorted to lexicographic sorting in the absence of WordNet synsets -see (#REF) . The author's baseline-independent submissions were unaffected by this, which on reviewing results in #TARGET_REF appears to have helped SUDOKU do best for these languages. Table 3 : F1 scores for each domain/language for SUDOKU and LIMSI.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The author's baseline-independent submissions were unaffected by this, which on reviewing results in #TARGET_REF appears to have helped SUDOKU do best for these languages.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our primary and secondary contributions therefore, are to carry out the first study that reports results across all three different dataset classes, and to release a open source code framework implementing three complementary groups of TDSA methods.",
                "In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (#REF; #REF; #REF; #TARGET_REF but other papers have not released code (#REF; #REF) .",
                "In some cases, the code was initially made available, then removed, and is now back online (#REFa) .",
                "Unfortunately, in some cases even when code has been published, different results have been obtained relative to the original paper.",
                "This can be seen when #REF used the code and embeddings in Tang et al. (2016b) they observe different results."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Our primary and secondary contributions therefore, are to carry out the first study that reports results across all three different dataset classes, and to release a open source code framework implementing three complementary groups of TDSA methods. In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (#REF; #REF; #REF; #TARGET_REF but other papers have not released code (#REF; #REF) . In some cases, the code was initially made available, then removed, and is now back online (#REFa) . Unfortunately, in some cases even when code has been published, different results have been obtained relative to the original paper. This can be seen when #REF used the code and embeddings in Tang et al. (2016b) they observe different results.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In terms of reproducibility via code release, recent TDSA papers have generally been very good with regards to publishing code alongside their papers (#REF; #REF; #REF; #TARGET_REF but other papers have not released code (#REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "In IR, specific reproducible research tracks have been created 3 and we are pleased to see the same happening at COLING 2018 4 .",
                "Turning now to the focus of our investigations, Target Dependent sentiment analysis (TDSA) research (#REF) arose as an extension to the coarse grained analysis of document level sentiment analysis (#REF; #REF) .",
                "Since its inception, papers have applied different methods such as feature based (#REF) , Recursive Neural Networks (RecNN) (#REF) , Recurrent Neural Networks (RNN) (#REFa) , attention applied to RNN (#REF; #REF; #REF) , Neural Pooling (NP) #TARGET_REF , RNN combined with NP (#REF) , and attention based neural networks (#REFb) .",
                "Others have tackled TDSA as a joint task with target extraction, thus treating it as a sequence labelling problem.",
                "#REF carried out this task using Conditional Random Fields (CRF), and this work was then extended using a neural CRF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In IR, specific reproducible research tracks have been created 3 and we are pleased to see the same happening at COLING 2018 4 . Turning now to the focus of our investigations, Target Dependent sentiment analysis (TDSA) research (#REF) arose as an extension to the coarse grained analysis of document level sentiment analysis (#REF; #REF) . Since its inception, papers have applied different methods such as feature based (#REF) , Recursive Neural Networks (RecNN) (#REF) , Recurrent Neural Networks (RNN) (#REFa) , attention applied to RNN (#REF; #REF; #REF) , Neural Pooling (NP) #TARGET_REF , RNN combined with NP (#REF) , and attention based neural networks (#REFb) . Others have tackled TDSA as a joint task with target extraction, thus treating it as a sequence labelling problem. #REF carried out this task using Conditional Random Fields (CRF), and this work was then extended using a neural CRF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Since its inception, papers have applied different methods such as feature based (#REF) , Recursive Neural Networks (RecNN) (#REF) , Recurrent Neural Networks (RNN) (#REFa) , attention applied to RNN (#REF; #REF; #REF) , Neural Pooling (NP) #TARGET_REF , RNN combined with NP (#REF) , and attention based neural networks (#REFb) .\"]}"
    },
    {
        "gold": {
            "text": [
                "They inputted the features into a linear SVM, and showed the importance of using the left and right context for the first time.",
                "They found in their study that using a combination of Word2Vec embeddings and sentiment embeddings performed best alongside using sentiment lexicons to filter the embedding space.",
                "Other studies have adopted more linguistic approaches.",
                "#TARGET_REF extended the work of by using the dependency linked words from the target.",
                "#REF used the dependency tree to create a Recursive Neural Network (RecNN) inspired by #REF but compared to #REF they also utilised the dependency tags to create an Adaptive RecNN (ARecNN)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "They inputted the features into a linear SVM, and showed the importance of using the left and right context for the first time. They found in their study that using a combination of Word2Vec embeddings and sentiment embeddings performed best alongside using sentiment lexicons to filter the embedding space. Other studies have adopted more linguistic approaches. #TARGET_REF extended the work of by using the dependency linked words from the target. #REF used the dependency tree to create a Recursive Neural Network (RecNN) inspired by #REF but compared to #REF they also utilised the dependency tags to create an Adaptive RecNN (ARecNN).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF extended the work of by using the dependency linked words from the target.\"]}"
    },
    {
        "gold": {
            "text": [
                "All of their experiments are performed on #REF Twitter data set.",
                "For each of the experiments below we used the following configurations unless otherwise stated: we performed 5 fold stratified cross validation, features are scaled using Max Min scaling before inputting into the SVM, and used the respective C-Values for the SVM stated in the paper for each of the models.",
                "One major difficulty with the description of the method in the paper and re-implementation is handling the same target multiple appearances issue as originally raised by #TARGET_REF .",
                "As the method requires context with regards to the target word, if there is more than one appearance of the target word then the method does not specify which to use.",
                "We therefore took the approach of #REF and found all of the features for each appearance and performed median pooling over features."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "All of their experiments are performed on #REF Twitter data set. For each of the experiments below we used the following configurations unless otherwise stated: we performed 5 fold stratified cross validation, features are scaled using Max Min scaling before inputting into the SVM, and used the respective C-Values for the SVM stated in the paper for each of the models. One major difficulty with the description of the method in the paper and re-implementation is handling the same target multiple appearances issue as originally raised by #TARGET_REF . As the method requires context with regards to the target word, if there is more than one appearance of the target word then the method does not specify which to use. We therefore took the approach of #REF and found all of the features for each appearance and performed median pooling over features.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"One major difficulty with the description of the method in the paper and re-implementation is handling the same target multiple appearances issue as originally raised by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF extended the NP work of and instead of using the full tweet/sentence/text contexts they used the full dependency graph of the target word.",
                "Thus, they created three different methods: 1. TDParse-uses only the full dependency graph context, 2.",
                "TDParse the feature of TDParse-and the left and right contexts, and 3.",
                "TDParse+ the features of TDParse and LS and RS contexts.",
                "The experiments are performed on the #REF and #REF Twitter datasets where we train and test on the previously specified train and test splits."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "#TARGET_REF extended the NP work of and instead of using the full tweet/sentence/text contexts they used the full dependency graph of the target word. Thus, they created three different methods: 1. TDParse-uses only the full dependency graph context, 2. TDParse the feature of TDParse-and the left and right contexts, and 3. TDParse+ the features of TDParse and LS and RS contexts. The experiments are performed on the #REF and #REF Twitter datasets where we train and test on the previously specified train and test splits.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF extended the NP work of and instead of using the full tweet/sentence/text contexts they used the full dependency graph of the target word.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our observations within this one sub-field motivates the need to investigate further and understand how such problems can be avoided in the future.",
                "In some cases, when code has been released, it is difficult to use which could explain why the results were not reproduced.",
                "Of course, we would not expect researchers to produce industrial strength code, or provide continuing free ongoing support for multiple years after publication, but the situation is clearly problematic for the development of the new field in general.",
                "In this paper, we therefore reproduce three papers chosen as they employ widely differing methods: Neural Pooling (NP) , NP with dependency parsing #TARGET_REF , and RNN (#REFa) , as well as having been applied largely to different datasets.",
                "At the end of the paper, we reflect on bringing together elements of repeatability and generalisability which we find are crucial to NLP and data science based disciplines more widely to enable others to make use of the science created."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Our observations within this one sub-field motivates the need to investigate further and understand how such problems can be avoided in the future. In some cases, when code has been released, it is difficult to use which could explain why the results were not reproduced. Of course, we would not expect researchers to produce industrial strength code, or provide continuing free ongoing support for multiple years after publication, but the situation is clearly problematic for the development of the new field in general. In this paper, we therefore reproduce three papers chosen as they employ widely differing methods: Neural Pooling (NP) , NP with dependency parsing #TARGET_REF , and RNN (#REFa) , as well as having been applied largely to different datasets. At the end of the paper, we reflect on bringing together elements of repeatability and generalisability which we find are crucial to NLP and data science based disciplines more widely to enable others to make use of the science created.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Of course, we would not expect researchers to produce industrial strength code, or provide continuing free ongoing support for multiple years after publication, but the situation is clearly problematic for the development of the new field in general.\", \"In this paper, we therefore reproduce three papers chosen as they employ widely differing methods: Neural Pooling (NP) , NP with dependency parsing #TARGET_REF , and RNN (#REFa) , as well as having been applied largely to different datasets.\"]}"
    },
    {
        "gold": {
            "text": [
                "For each of the experiments below we used the following configurations unless otherwise stated: we performed 5 fold stratified cross validation, features are scaled using Max Min scaling before inputting into the SVM, and used the respective C-Values for the SVM stated in the paper for each of the models.",
                "One major difficulty with the description of the method in the paper and re-implementation is handling the same target multiple appearances issue as originally raised by #REF .",
                "As the method requires context with regards to the target word, if there is more than one appearance of the target word then the method does not specify which to use.",
                "We therefore took the approach of #TARGET_REF and found all of the features for each appearance and performed median pooling over features.",
                "This change could explain the subtle differences between the results we report and those of the original paper."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "For each of the experiments below we used the following configurations unless otherwise stated: we performed 5 fold stratified cross validation, features are scaled using Max Min scaling before inputting into the SVM, and used the respective C-Values for the SVM stated in the paper for each of the models. One major difficulty with the description of the method in the paper and re-implementation is handling the same target multiple appearances issue as originally raised by #REF . As the method requires context with regards to the target word, if there is more than one appearance of the target word then the method does not specify which to use. We therefore took the approach of #TARGET_REF and found all of the features for each appearance and performed median pooling over features. This change could explain the subtle differences between the results we report and those of the original paper.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We therefore took the approach of #TARGET_REF and found all of the features for each appearance and performed median pooling over features.\"]}"
    },
    {
        "gold": {
            "text": [
                "TDParse the feature of TDParse-and the left and right contexts, and 3.",
                "TDParse+ the features of TDParse and LS and RS contexts.",
                "The experiments are performed on the #REF and #TARGET_REF Twitter datasets where we train and test on the previously specified train and test splits.",
                "We also scale our features using Max Min scaling before inputting into the SVM.",
                "We used all three sentiment lexicons as in the original paper, and we found the C-Value by performing 5 fold stratified cross validation on the training datasets."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "TDParse the feature of TDParse-and the left and right contexts, and 3. TDParse+ the features of TDParse and LS and RS contexts. The experiments are performed on the #REF and #TARGET_REF Twitter datasets where we train and test on the previously specified train and test splits. We also scale our features using Max Min scaling before inputting into the SVM. We used all three sentiment lexicons as in the original paper, and we found the C-Value by performing 5 fold stratified cross validation on the training datasets.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The experiments are performed on the #REF and #TARGET_REF Twitter datasets where we train and test on the previously specified train and test splits.\"]}"
    },
    {
        "gold": {
            "text": [
                "It is, however, affected by domain variation - #TARGET_REF report that its f-score drops by approximately 8 percentage points when applied to the BNC domain.",
                "Our training size is 500,000 sentences.",
                "We conduct two experiments: the first, in which 500,000 sentences are extracted randomly from the BNC (minus the test set sentences), and the second in which only shorter sentences, of length ≤ 20 words, are chosen as training material.",
                "The rationale behind the second experiment is that shorter sentences are less likely to contain parser errors.",
                "We use the BLEU evaluation metric for our experiments."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "It is, however, affected by domain variation - #TARGET_REF report that its f-score drops by approximately 8 percentage points when applied to the BNC domain. Our training size is 500,000 sentences. We conduct two experiments: the first, in which 500,000 sentences are extracted randomly from the BNC (minus the test set sentences), and the second in which only shorter sentences, of length ≤ 20 words, are chosen as training material. The rationale behind the second experiment is that shorter sentences are less likely to contain parser errors. We use the BLEU evaluation metric for our experiments.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"It is, however, affected by domain variation - #TARGET_REF report that its f-score drops by approximately 8 percentage points when applied to the BNC domain.\"]}"
    },
    {
        "gold": {
            "text": [
                "For the baseline system, the generator is trained on f-structure-annotated trees derived from gold trees.",
                "The new system is trained on f-structureannotated parser output trees, and the performance of Charniak and Johnson's parser degrades when applied to BNC data #TARGET_REF .",
                "The second reason has been suggested by #REF : WSJ data is easier to learn than the more varied data in the Brown Corpus or BNC.",
                "Perhaps even if gold standard BNC parse trees were available for training, the system would not behave as well as it does for WSJ material.",
                "It is interesting to note that training on 500,000 shorter sentences does not appear to help."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "For the baseline system, the generator is trained on f-structure-annotated trees derived from gold trees. The new system is trained on f-structureannotated parser output trees, and the performance of Charniak and Johnson's parser degrades when applied to BNC data #TARGET_REF . The second reason has been suggested by #REF : WSJ data is easier to learn than the more varied data in the Brown Corpus or BNC. Perhaps even if gold standard BNC parse trees were available for training, the system would not behave as well as it does for WSJ material. It is interesting to note that training on 500,000 shorter sentences does not appear to help.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The new system is trained on f-structureannotated parser output trees, and the performance of Charniak and Johnson's parser degrades when applied to BNC data #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Performance drops from a BLEU score of 0.66 on WSJ test data to 0.54 on the BNC test set.",
                "Encouragingly, we have also shown that domain-specific training material produced by a parser can be used to claw back a significant portion of this performance degradation.",
                "Our method is general and could be applied to other WSJ-trained generators (e.g. (Nakanishi et , 2007) ).",
                "We intend to continue this research by training our generator on parse trees produced by a BNC-self-trained version of the Charniak and Johnson reranking parser #TARGET_REF .",
                "We also hope to extend the evaluation beyond the BLEU metric by carrying out a human judgement evaluation."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Performance drops from a BLEU score of 0.66 on WSJ test data to 0.54 on the BNC test set. Encouragingly, we have also shown that domain-specific training material produced by a parser can be used to claw back a significant portion of this performance degradation. Our method is general and could be applied to other WSJ-trained generators (e.g. (Nakanishi et , 2007) ). We intend to continue this research by training our generator on parse trees produced by a BNC-self-trained version of the Charniak and Johnson reranking parser #TARGET_REF . We also hope to extend the evaluation beyond the BLEU metric by carrying out a human judgement evaluation.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"We intend to continue this research by training our generator on parse trees produced by a BNC-self-trained version of the Charniak and Johnson reranking parser #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Traditionally, supervised approaches have attained the best results in the area, but they are expensive to build because of the need of large amounts of manually annotated examples.",
                "Alternatively, knowledge based approaches rely on lexical resources such as WordNet, which are nowadays widely available in many languages (#REF) 1 .",
                "In particular, graph-based approaches represent the knowledge base as a graph, and apply several well-known graph analysis algorithms to perform WSD.",
                "UKB is a collection of programs which was first released for performing graph-based Word Sense Disambiguation using a preexisting knowledge base such as WordNet, and attained state-of-the-art results among knowledge-based systems when evaluated on standard benchmarks #TARGET_REF .",
                "In addition, UKB has been extended to perform disambiguation of medical entities (#REF) , named-entities (#REF; , word similarity and to create knowledge-based word embeddings (#REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Traditionally, supervised approaches have attained the best results in the area, but they are expensive to build because of the need of large amounts of manually annotated examples. Alternatively, knowledge based approaches rely on lexical resources such as WordNet, which are nowadays widely available in many languages (#REF) 1 . In particular, graph-based approaches represent the knowledge base as a graph, and apply several well-known graph analysis algorithms to perform WSD. UKB is a collection of programs which was first released for performing graph-based Word Sense Disambiguation using a preexisting knowledge base such as WordNet, and attained state-of-the-art results among knowledge-based systems when evaluated on standard benchmarks #TARGET_REF . In addition, UKB has been extended to perform disambiguation of medical entities (#REF) , named-entities (#REF; , word similarity and to create knowledge-based word embeddings (#REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"UKB is a collection of programs which was first released for performing graph-based Word Sense Disambiguation using a preexisting knowledge base such as WordNet, and attained state-of-the-art results among knowledge-based systems when evaluated on standard benchmarks #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "All programs are open source 2 ,3 and are accompanied by the resources and instructions necessary to reproduce the results.",
                "The software is quite popular, with 60 stars and 26 forks in github, as well as more than eight thousand direct downloads from the website since 2011.",
                "The software is coded in C++ and released under the GPL v3.0 license.",
                "When UKB was released, the papers specified the optimal parameters for WSD #TARGET_REF , as well as other key issues like the underlying knowledge-base version, specific set of relations to be used, and method to pre-process the input text.",
                "At the time, we assumed that future researchers would use the optimal parameters and settings specified in the papers, and that they would contact the authors if in doubt."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "All programs are open source 2 ,3 and are accompanied by the resources and instructions necessary to reproduce the results. The software is quite popular, with 60 stars and 26 forks in github, as well as more than eight thousand direct downloads from the website since 2011. The software is coded in C++ and released under the GPL v3.0 license. When UKB was released, the papers specified the optimal parameters for WSD #TARGET_REF , as well as other key issues like the underlying knowledge-base version, specific set of relations to be used, and method to pre-process the input text. At the time, we assumed that future researchers would use the optimal parameters and settings specified in the papers, and that they would contact the authors if in doubt.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"When UKB was released, the papers specified the optimal parameters for WSD #TARGET_REF , as well as other key issues like the underlying knowledge-base version, specific set of relations to be used, and method to pre-process the input text.\"]}"
    },
    {
        "gold": {
            "text": [
                "The frequencies are often derived from manually sense annotated corpora, such as Semcor (#REF) .",
                "We use the sense frequency accompanying Wordnet, which, according to the documentation, \"represents the decimal number of times the sense is tagged in various semantic concordance texts\".",
                "The frequencies are smoothed adding one to all counts (dict weight smooth).",
                "The sense frequency is used when initializing context words, and is also used to produce the final sense weights as a linear combination of sense frequencies and graph-based sense probabilities.",
                "The use of sense frequencies with UKB was introduced in #TARGET_REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The frequencies are often derived from manually sense annotated corpora, such as Semcor (#REF) . We use the sense frequency accompanying Wordnet, which, according to the documentation, \"represents the decimal number of times the sense is tagged in various semantic concordance texts\". The frequencies are smoothed adding one to all counts (dict weight smooth). The sense frequency is used when initializing context words, and is also used to produce the final sense weights as a linear combination of sense frequencies and graph-based sense probabilities. The use of sense frequencies with UKB was introduced in #TARGET_REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The use of sense frequencies with UKB was introduced in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "When using UKB for WSD, the main parameters and settings can be classified in five main categories.",
                "For each of those we mention the best options and the associated UKB parameter when relevant (in italics), as taken from #TARGET_REF ):",
                "• Pre-processing of input text.",
                "When running UKB for WSD, one needs to define which 2 http://ixa2.si.ehu.eus/ukb 3 https://github.com/asoroa/ukb window of words is to be used as context to initialize the random walks.",
                "One option is to take just the sentence, but given that in some cases the sentences are very short, better results are obtained when considering previous and following sentences."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "When using UKB for WSD, the main parameters and settings can be classified in five main categories. For each of those we mention the best options and the associated UKB parameter when relevant (in italics), as taken from #TARGET_REF ): • Pre-processing of input text. When running UKB for WSD, one needs to define which 2 http://ixa2.si.ehu.eus/ukb 3 https://github.com/asoroa/ukb window of words is to be used as context to initialize the random walks. One option is to take just the sentence, but given that in some cases the sentences are very short, better results are obtained when considering previous and following sentences.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"When using UKB for WSD, the main parameters and settings can be classified in five main categories.\", \"For each of those we mention the best options and the associated UKB parameter when relevant (in italics), as taken from #TARGET_REF ):\"]}"
    },
    {
        "gold": {
            "text": [
                "As the results show, that paper reports a suboptimal use of UKB.",
                "In more recent work, #REF take up that result and report it in their paper as well.",
                "The difference is of nearly 10 absolute F1 points overall.",
                "5 This decrease could be caused by the fact that Raganato et al. (2017a) did not use sense frequencies.",
                "In addition to UKB, the table also reports the 5 Note that the UKB results for S2, S3 and S07 (62.6, 63.0 and 48.6 respectively) are different from those in #TARGET_REF , which is to be expected, as the new datasets have been converted to WordNet 3.0 (we confirmed experimentally that this is the sole difference between the two experiments)."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "As the results show, that paper reports a suboptimal use of UKB. In more recent work, #REF take up that result and report it in their paper as well. The difference is of nearly 10 absolute F1 points overall. 5 This decrease could be caused by the fact that Raganato et al. (2017a) did not use sense frequencies. In addition to UKB, the table also reports the 5 Note that the UKB results for S2, S3 and S07 (62.6, 63.0 and 48.6 respectively) are different from those in #TARGET_REF , which is to be expected, as the new datasets have been converted to WordNet 3.0 (we confirmed experimentally that this is the sole difference between the two experiments).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"In addition to UKB, the table also reports the 5 Note that the UKB results for S2, S3 and S07 (62.6, 63.0 and 48.6 respectively) are different from those in #TARGET_REF , which is to be expected, as the new datasets have been converted to WordNet 3.0 (we confirmed experimentally that this is the sole difference between the two experiments).\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition to the results of UKB using the setting in #TARGET_REF as specified in Section 3, we checked whether some reasonable settings would obtain better results.",
                "Table 3 shows the results when applying the three algorithms described in Section 3, both with and without sense frequencies, as well as using a single sentence for context or extended context.",
                "The table shows that the key factor is the use of sense frequencies, and systems that do not use them (those with a nf subscript) suffer a loss between 7 and 8 percentage points in F1.",
                "This would explain part of the decrease in performance reported in (#REFa) , as they explicitly mention that they did not activate the use of sense frequencies in UKB.",
                "The table also shows that extending the context is mildly effective."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In addition to the results of UKB using the setting in #TARGET_REF as specified in Section 3, we checked whether some reasonable settings would obtain better results. Table 3 shows the results when applying the three algorithms described in Section 3, both with and without sense frequencies, as well as using a single sentence for context or extended context. The table shows that the key factor is the use of sense frequencies, and systems that do not use them (those with a nf subscript) suffer a loss between 7 and 8 percentage points in F1. This would explain part of the decrease in performance reported in (#REFa) , as they explicitly mention that they did not activate the use of sense frequencies in UKB. The table also shows that extending the context is mildly effective.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"In addition to the results of UKB using the setting in #TARGET_REF as specified in Section 3, we checked whether some reasonable settings would obtain better results.\"]}"
    },
    {
        "gold": {
            "text": [
                "The most accurate approaches to FSD involve a runtime of O(n 2 ) and cannot scale to unbounded high volume streams such as Twitter.",
                "We present a novel approach to FSD that operates in O(1) per tweet.",
                "Our method is able to process the load of the average Twitter Firehose 3 stream on a single core of modest hardware while retaining effectiveness on par with one of the most accurate FSD systems.",
                "During the TDT program, FSD was applied to news wire documents and solely focused on effectiveness, neglecting efficiency and scalability.",
                "The traditional approach to FSD #TARGET_REF computes the distance of each incoming document 1 e.g. a natural disaster or a scandal 2 TDT by NIST -1998 NIST - -2004 ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The most accurate approaches to FSD involve a runtime of O(n 2 ) and cannot scale to unbounded high volume streams such as Twitter. We present a novel approach to FSD that operates in O(1) per tweet. Our method is able to process the load of the average Twitter Firehose 3 stream on a single core of modest hardware while retaining effectiveness on par with one of the most accurate FSD systems. During the TDT program, FSD was applied to news wire documents and solely focused on effectiveness, neglecting efficiency and scalability. The traditional approach to FSD #TARGET_REF computes the distance of each incoming document 1 e.g. a natural disaster or a scandal 2 TDT by NIST -1998 NIST - -2004 .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The traditional approach to FSD #TARGET_REF computes the distance of each incoming document 1 e.g. a natural disaster or a scandal 2 TDT by NIST -1998 NIST - -2004 .\"]}"
    },
    {
        "gold": {
            "text": [
                "Operation in constant space was ensured by keeping the number of tweets per bucket constant.",
                "Because LSH alone performed ineffectively, #TARGET_REF additionally compared each incoming tweet with the k most recent tweets.",
                "#REF analysed scoring functions for novelty detection while focusing on their effectiveness.",
                "They presented a language-model (LM) based novelty measure using the KL divergence between the LM of a document and a single LM built on all previously scored documents, which they referred to as an aggregate measure language model.",
                "The idea of maintaining a single representation covering all previously seen documents, instead of performing pairwise comparisons with every document is closely related to the approach presented in this paper."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Operation in constant space was ensured by keeping the number of tweets per bucket constant. Because LSH alone performed ineffectively, #TARGET_REF additionally compared each incoming tweet with the k most recent tweets. #REF analysed scoring functions for novelty detection while focusing on their effectiveness. They presented a language-model (LM) based novelty measure using the KL divergence between the LM of a document and a single LM built on all previously scored documents, which they referred to as an aggregate measure language model. The idea of maintaining a single representation covering all previously seen documents, instead of performing pairwise comparisons with every document is closely related to the approach presented in this paper.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Because LSH alone performed ineffectively, #TARGET_REF additionally compared each incoming tweet with the k most recent tweets.\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare our system (k-term) against 3 baselines.",
                "UMass is a state-of-the-art FSD system, developed by #REF .",
                "It is known for its high effectiveness in the TDT2 and TDT3 competitions (#REF) and widely used as a benchmark for FSD systems #TARGET_REF; #REF; #REF; ) .",
                "UMass makes use of an inverted index and k-nearest-neighbour clustering, which optimize the system for speed by ensuring a minimal number of comparisons.",
                "To maximise efficiency, we set-up UMass to operate in-memory by turning off its default memory mapping to disk."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We compare our system (k-term) against 3 baselines. UMass is a state-of-the-art FSD system, developed by #REF . It is known for its high effectiveness in the TDT2 and TDT3 competitions (#REF) and widely used as a benchmark for FSD systems #TARGET_REF; #REF; #REF; ) . UMass makes use of an inverted index and k-nearest-neighbour clustering, which optimize the system for speed by ensuring a minimal number of comparisons. To maximise efficiency, we set-up UMass to operate in-memory by turning off its default memory mapping to disk.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"It is known for its high effectiveness in the TDT2 and TDT3 competitions (#REF) and widely used as a benchmark for FSD systems #TARGET_REF; #REF; #REF; ) .\"]}"
    },
    {
        "gold": {
            "text": [
                "It is known for its high effectiveness in the TDT2 and TDT3 competitions (#REF) and widely used as a benchmark for FSD systems (#REF; #REF; #REF; ) .",
                "UMass makes use of an inverted index and k-nearest-neighbour clustering, which optimize the system for speed by ensuring a minimal number of comparisons.",
                "To maximise efficiency, we set-up UMass to operate in-memory by turning off its default memory mapping to disk.",
                "This ensures fair comparisons, as all algorithms operate in memory.",
                "LSH-FSD is a highly-scalable system by #TARGET_REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "It is known for its high effectiveness in the TDT2 and TDT3 competitions (#REF) and widely used as a benchmark for FSD systems (#REF; #REF; #REF; ) . UMass makes use of an inverted index and k-nearest-neighbour clustering, which optimize the system for speed by ensuring a minimal number of comparisons. To maximise efficiency, we set-up UMass to operate in-memory by turning off its default memory mapping to disk. This ensures fair comparisons, as all algorithms operate in memory. LSH-FSD is a highly-scalable system by #TARGET_REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"LSH-FSD is a highly-scalable system by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "To maximise efficiency, we set-up UMass to operate in-memory by turning off its default memory mapping to disk.",
                "This ensures fair comparisons, as all algorithms operate in memory.",
                "LSH-FSD is a highly-scalable system by #REF .",
                "It is based on Locality Sensitive Hashing (LSH) and claims to operate in constant time and space while performing on a comparable level of accuracy as UMass.",
                "We configure their system using the default parameters #TARGET_REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "To maximise efficiency, we set-up UMass to operate in-memory by turning off its default memory mapping to disk. This ensures fair comparisons, as all algorithms operate in memory. LSH-FSD is a highly-scalable system by #REF . It is based on Locality Sensitive Hashing (LSH) and claims to operate in constant time and space while performing on a comparable level of accuracy as UMass. We configure their system using the default parameters #TARGET_REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We configure their system using the default parameters #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The pair-wise comparison of UMass causes it's throughput to decrease drastically with every new document.",
                "In Figure 2 we compare the memory requirements of k-term and LSH-FSD at different points in the stream.",
                "Although #TARGET_REF designed their system (LSH-FSD) to operate in constant space, we found that the memory requirement gradually increases with the number of documents processed, as seen in Figure 3 .",
                "We hypothesise that this increase results from new terms added to the vocabulary.",
                "Our system has a strictly constant memory footprint."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The pair-wise comparison of UMass causes it's throughput to decrease drastically with every new document. In Figure 2 we compare the memory requirements of k-term and LSH-FSD at different points in the stream. Although #TARGET_REF designed their system (LSH-FSD) to operate in constant space, we found that the memory requirement gradually increases with the number of documents processed, as seen in Figure 3 . We hypothesise that this increase results from new terms added to the vocabulary. Our system has a strictly constant memory footprint.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Although #TARGET_REF designed their system (LSH-FSD) to operate in constant space, we found that the memory requirement gradually increases with the number of documents processed, as seen in Figure 3 .\"]}"
    },
    {
        "gold": {
            "text": [
                "In contrast, collecting large amount of unlabeled indomain data and labeled out-of-domain data can be fast and economical.",
                "Hence, an important question arises for this scenario: how can we do unsupervised adaptation for acoustic models by utilizing labeled out-of-domain data and unlabeled in-domain data, in order to achieve good performance on in-domain data?",
                "Research on unsupervised adaptation for acoustic models can be roughly divided into three categories: (1) constrained model adaptation [5, 6, 7] , (2) domain-invariant feature extraction [8, 9, 10] , and (3) labeled in-domain data augmentation by synthesis [11, 12, #TARGET_REF .",
                "Among these approaches, data augmentation-based adaptation is favorable, because it does not require extra hyperparameter tuning for acoustic model training, and can utilize full model capacity by training a model with as much and as diverse a dataset as possible.",
                "Another benefit of this approach is that data in their original domain are more intuitive to humans."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In contrast, collecting large amount of unlabeled indomain data and labeled out-of-domain data can be fast and economical. Hence, an important question arises for this scenario: how can we do unsupervised adaptation for acoustic models by utilizing labeled out-of-domain data and unlabeled in-domain data, in order to achieve good performance on in-domain data? Research on unsupervised adaptation for acoustic models can be roughly divided into three categories: (1) constrained model adaptation [5, 6, 7] , (2) domain-invariant feature extraction [8, 9, 10] , and (3) labeled in-domain data augmentation by synthesis [11, 12, #TARGET_REF . Among these approaches, data augmentation-based adaptation is favorable, because it does not require extra hyperparameter tuning for acoustic model training, and can utilize full model capacity by training a model with as much and as diverse a dataset as possible. Another benefit of this approach is that data in their original domain are more intuitive to humans.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Research on unsupervised adaptation for acoustic models can be roughly divided into three categories: (1) constrained model adaptation [5, 6, 7] , (2) domain-invariant feature extraction [8, 9, 10] , and (3) labeled in-domain data augmentation by synthesis [11, 12, #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Research on unsupervised adaptation for acoustic models can be roughly divided into three categories: (1) constrained model adaptation [5, 6, 7] , (2) domain-invariant feature extraction [8, 9, 10] , and (3) labeled in-domain data augmentation by synthesis [11, 12, 13] .",
                "Among these approaches, data augmentation-based adaptation is favorable, because it does not require extra hyperparameter tuning for acoustic model training, and can utilize full model capacity by training a model with as much and as diverse a dataset as possible.",
                "Another benefit of this approach is that data in their original domain are more intuitive to humans.",
                "In other words, it is easier for us to inspect and manipulate the data.",
                "Furthermore, with the recent progress on domain translation #TARGET_REF 14, 15] , conditional synthesis of indomain data without parallel data has become achievable, which makes data augmentation-based adaptation a more promising direction to investigate."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Research on unsupervised adaptation for acoustic models can be roughly divided into three categories: (1) constrained model adaptation [5, 6, 7] , (2) domain-invariant feature extraction [8, 9, 10] , and (3) labeled in-domain data augmentation by synthesis [11, 12, 13] . Among these approaches, data augmentation-based adaptation is favorable, because it does not require extra hyperparameter tuning for acoustic model training, and can utilize full model capacity by training a model with as much and as diverse a dataset as possible. Another benefit of this approach is that data in their original domain are more intuitive to humans. In other words, it is easier for us to inspect and manipulate the data. Furthermore, with the recent progress on domain translation #TARGET_REF 14, 15] , conditional synthesis of indomain data without parallel data has become achievable, which makes data augmentation-based adaptation a more promising direction to investigate.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Furthermore, with the recent progress on domain translation #TARGET_REF 14, 15] , conditional synthesis of indomain data without parallel data has become achievable, which makes data augmentation-based adaptation a more promising direction to investigate.\"]}"
    },
    {
        "gold": {
            "text": [
                "A key observation made in #TARGET_REF is that nuisance factors, such as speaker identity and room acoustics, are generally constant over segments within an utterance, while linguistic content changes from segment to segment.",
                "In other words, latent nuisance vectors zn are relatively consistent within an utterance, while the distribution of z conditioned on an utterance can be assumed to have the same distribution as the prior.",
                "Therefore, suppose the prior is a diagonal Gaussian with zero mean.",
                "Given an utterance",
                "of N segments, we have:"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "A key observation made in #TARGET_REF is that nuisance factors, such as speaker identity and room acoustics, are generally constant over segments within an utterance, while linguistic content changes from segment to segment. In other words, latent nuisance vectors zn are relatively consistent within an utterance, while the distribution of z conditioned on an utterance can be assumed to have the same distribution as the prior. Therefore, suppose the prior is a diagonal Gaussian with zero mean. Given an utterance of N segments, we have:",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"A key observation made in #TARGET_REF is that nuisance factors, such as speaker identity and room acoustics, are generally constant over segments within an utterance, while linguistic content changes from segment to segment.\"]}"
    },
    {
        "gold": {
            "text": [
                "With a trained FHVAE, we are able to infer disentangled latent representations that capture linguistic factors z1 and nuisance factors z2.",
                "To transform nuisance factors of an utterance X without changing the corresponding transcript, one only needs to perturb Z2.",
                "Furthermore, since each z2 within an utterance is generated conditioned on a Gaussian whose mean is µ2, we can regard µ2 as the representation of nuisance factors of an utterance.",
                "We now derive two data augmentation methods similar to those proposed in #TARGET_REF , named nuisance factor replacement and nuisance factor perturbation."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1
            ]
        },
        "input": "With a trained FHVAE, we are able to infer disentangled latent representations that capture linguistic factors z1 and nuisance factors z2. To transform nuisance factors of an utterance X without changing the corresponding transcript, one only needs to perturb Z2. Furthermore, since each z2 within an utterance is generated conditioned on a Gaussian whose mean is µ2, we can regard µ2 as the representation of nuisance factors of an utterance. We now derive two data augmentation methods similar to those proposed in #TARGET_REF , named nuisance factor replacement and nuisance factor perturbation.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We now derive two data augmentation methods similar to those proposed in #TARGET_REF , named nuisance factor replacement and nuisance factor perturbation.\"]}"
    },
    {
        "gold": {
            "text": [
                "We propose to draw a random perturbation vector p and computê z2,out = z2,out + p for each segment in an utterance, in order to synthesize an utterance with perturbed nuisance factors.",
                "Naively, we may want to sample p from a centered isotropic Gaussian.",
                "However, in practice, VAE-type of models suffer from an over-pruning issue [21] in that some latent variables become inactive, which we do not want to perturb.",
                "Instead, we only want to perturb the linear subspace which models the variation of nuisance factors between utterances.",
                "Therefore, we adopt a similar soft perturbation scheme as in #TARGET_REF ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "We propose to draw a random perturbation vector p and computê z2,out = z2,out + p for each segment in an utterance, in order to synthesize an utterance with perturbed nuisance factors. Naively, we may want to sample p from a centered isotropic Gaussian. However, in practice, VAE-type of models suffer from an over-pruning issue [21] in that some latent variables become inactive, which we do not want to perturb. Instead, we only want to perturb the linear subspace which models the variation of nuisance factors between utterances. Therefore, we adopt a similar soft perturbation scheme as in #TARGET_REF .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Therefore, we adopt a similar soft perturbation scheme as in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Both augmentation methods outperform their VAE counterparts and the domain invariant feature baseline using the same FHVAE model.",
                "We attribute the improvement to the better quality of the transformed IHM data, which covers the nuisance factors of the SDM data, without altering the original linguistic content.",
                "To verify the superiority of the proposed method of drawing random perturbation vectors, we compare two alternative sampling methods: rev-p and uni-p, similar to #TARGET_REF , with the same expected squared Euclidean norm as the proposed method.",
                "Table 2 confirm that the proposed sampling method is more effective under the same perturbation scale γ = 1.0 compared to the alternative methods as expected.",
                "Due to imperfect reconstruction using FHVAE models, some linguistic information may be lost in this process."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Both augmentation methods outperform their VAE counterparts and the domain invariant feature baseline using the same FHVAE model. We attribute the improvement to the better quality of the transformed IHM data, which covers the nuisance factors of the SDM data, without altering the original linguistic content. To verify the superiority of the proposed method of drawing random perturbation vectors, we compare two alternative sampling methods: rev-p and uni-p, similar to #TARGET_REF , with the same expected squared Euclidean norm as the proposed method. Table 2 confirm that the proposed sampling method is more effective under the same perturbation scale γ = 1.0 compared to the alternative methods as expected. Due to imperfect reconstruction using FHVAE models, some linguistic information may be lost in this process.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"To verify the superiority of the proposed method of drawing random perturbation vectors, we compare two alternative sampling methods: rev-p and uni-p, similar to #TARGET_REF , with the same expected squared Euclidean norm as the proposed method.\"]}"
    },
    {
        "gold": {
            "text": [
                "VAE-DA #TARGET_REF results with nuisance factor replacement (repl) and latent nuisance perturbation (p) are shown in the last three rows.",
                "We then examine the effectiveness of our proposed method and show the results in the second, third, and fourth rows in Table 2 .",
                "We observe about 12% WER reduction on the in-domain development set for both nuisance factor perturbation (p) and nuisance factor replacement (repl), with little degradation on the out-of-domain development set.",
                "Both augmentation methods outperform their VAE counterparts and the domain invariant feature baseline using the same FHVAE model.",
                "We attribute the improvement to the better quality of the transformed IHM data, which covers the nuisance factors of the SDM data, without altering the original linguistic content."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "VAE-DA #TARGET_REF results with nuisance factor replacement (repl) and latent nuisance perturbation (p) are shown in the last three rows. We then examine the effectiveness of our proposed method and show the results in the second, third, and fourth rows in Table 2 . We observe about 12% WER reduction on the in-domain development set for both nuisance factor perturbation (p) and nuisance factor replacement (repl), with little degradation on the out-of-domain development set. Both augmentation methods outperform their VAE counterparts and the domain invariant feature baseline using the same FHVAE model. We attribute the improvement to the better quality of the transformed IHM data, which covers the nuisance factors of the SDM data, without altering the original linguistic content.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"VAE-DA #TARGET_REF results with nuisance factor replacement (repl) and latent nuisance perturbation (p) are shown in the last three rows.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF have proposed a task of generating a definition for a phrase given its local context.",
                "However, they follow the strict assumption that the target phrase is newly emerged and there is only a single local context available for the phrase, which makes the task of generating an accurate and coherent definition difficult (perhaps as difficult as a human comprehending the phrase itself).",
                "On the other hand, #TARGET_REF attempted to generate a definition of a word from an embedding induced from massive text (which can be seen as global context).",
                "This is followed by #REF that refers to a local context to disambiguate polysemous words by choosing relevant dimensions of their word embeddings.",
                "Al-though these research efforts revealed that both local and global contexts are useful in generating definitions, none of these studies exploited both contexts directly to describe unknown phrases."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "#REF have proposed a task of generating a definition for a phrase given its local context. However, they follow the strict assumption that the target phrase is newly emerged and there is only a single local context available for the phrase, which makes the task of generating an accurate and coherent definition difficult (perhaps as difficult as a human comprehending the phrase itself). On the other hand, #TARGET_REF attempted to generate a definition of a word from an embedding induced from massive text (which can be seen as global context). This is followed by #REF that refers to a local context to disambiguate polysemous words by choosing relevant dimensions of their word embeddings. Al-though these research efforts revealed that both local and global contexts are useful in generating definitions, none of these studies exploited both contexts directly to describe unknown phrases.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"On the other hand, #TARGET_REF attempted to generate a definition of a word from an embedding induced from massive text (which can be seen as global context).\"]}"
    },
    {
        "gold": {
            "text": [
                "Although a few studies (#REF; #REF; #REF) consider subsentential (context-sensitive) paraphrases, they do not intend to obtain a definition-like description as a paraphrase of a word.",
                "Recently, #TARGET_REF introduced a task of generating a definition sentence of a word from its pre-trained embedding.",
                "Since their task does not take local contexts of words as inputs, their method cannot generate an appropriate definition for a polysemous word for a specific context.",
                "To cope with this problem, #REF proposed a definition generation method that works with polysemous words in dictionaries.",
                "They presented a model that utilizes local context to filter out the unrelated meanings from a pre-trained word embedding in a specific context."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Although a few studies (#REF; #REF; #REF) consider subsentential (context-sensitive) paraphrases, they do not intend to obtain a definition-like description as a paraphrase of a word. Recently, #TARGET_REF introduced a task of generating a definition sentence of a word from its pre-trained embedding. Since their task does not take local contexts of words as inputs, their method cannot generate an appropriate definition for a polysemous word for a specific context. To cope with this problem, #REF proposed a definition generation method that works with polysemous words in dictionaries. They presented a model that utilizes local context to filter out the unrelated meanings from a pre-trained word embedding in a specific context.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Recently, #TARGET_REF introduced a task of generating a definition sentence of a word from its pre-trained embedding.\"]}"
    },
    {
        "gold": {
            "text": [
                "The model therefore combines both pieces of information to generate a natural language description.",
                "Considering various applications where we need definitions of expressions, we evaluated our method with four datasets including WordNet #TARGET_REF for general words, the Oxford dictionary (#REF) for polysemous words, Urban Dictionary (#REF) for rare idioms or slang, and a newlycreated Wikipedia dataset for entities.",
                "Our contributions are as follows:",
                "• We propose a general task of defining unknown phrases given their contexts.",
                "This task is a generalization of three related tasks (#REF; #REF; #REF) and involves various situations where we need definitions of unknown phrases ( § 2)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The model therefore combines both pieces of information to generate a natural language description. Considering various applications where we need definitions of expressions, we evaluated our method with four datasets including WordNet #TARGET_REF for general words, the Oxford dictionary (#REF) for polysemous words, Urban Dictionary (#REF) for rare idioms or slang, and a newlycreated Wikipedia dataset for entities. Our contributions are as follows: • We propose a general task of defining unknown phrases given their contexts. This task is a generalization of three related tasks (#REF; #REF; #REF) and involves various situations where we need definitions of unknown phrases ( § 2).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Considering various applications where we need definitions of expressions, we evaluated our method with four datasets including WordNet #TARGET_REF for general words, the Oxford dictionary (#REF) for polysemous words, Urban Dictionary (#REF) for rare idioms or slang, and a newlycreated Wikipedia dataset for entities.\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following #TARGET_REF .",
                "Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following #REF, we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0
            ]
        },
        "input": "In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following #TARGET_REF . Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following #REF, we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following (#REF) .",
                "Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following #TARGET_REF, we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following (#REF) . Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \" ,\" such as \"sonic boom.\" Following #TARGET_REF, we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character \\\" ,\\\" such as \\\"sonic boom.\\\" Following #TARGET_REF, we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg .\"]}"
    },
    {
        "gold": {
            "text": [
                "8 For all of these datasets, a given word or phrase has an inventory of senses with corresponding definitions and usage examples.",
                "These definitions are regarded as groundtruth descriptions.",
                "Datasets To evaluate our model on the word description task on WordNet, we followed #TARGET_REF and extracted data from WordNet using the dict-definition 9 toolkit.",
                "Each entry in the data consists of three elements: (1) a word, (2) its definition, and (3) a usage example of the Table 2 : Domains, expressions to be described, and the coverage of pre-trained embeddings of the expressions to be described.",
                "word."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "8 For all of these datasets, a given word or phrase has an inventory of senses with corresponding definitions and usage examples. These definitions are regarded as groundtruth descriptions. Datasets To evaluate our model on the word description task on WordNet, we followed #TARGET_REF and extracted data from WordNet using the dict-definition 9 toolkit. Each entry in the data consists of three elements: (1) a word, (2) its definition, and (3) a usage example of the Table 2 : Domains, expressions to be described, and the coverage of pre-trained embeddings of the expressions to be described. word.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Datasets To evaluate our model on the word description task on WordNet, we followed #TARGET_REF and extracted data from WordNet using the dict-definition 9 toolkit.\"]}"
    },
    {
        "gold": {
            "text": [
                "The only difference between our dataset and theirs is that we extract the tuples only if the words have their usage examples in WordNet.",
                "Since not all entries in WordNet have usage examples, our dataset is a small subset of #TARGET_REF .",
                "In addition to WordNet, we use the Oxford Dictionary following #REF , the Urban Dictionary following #REF and our Wikipedia dataset described in the previous section.",
                "Table 1 and Table 2 show the properties and statistics of the four datasets, respectively.",
                "To simulate a situation in a real application where we might not have access to global context for the target phrases, we did not train domainspecific word embeddings on each dataset."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The only difference between our dataset and theirs is that we extract the tuples only if the words have their usage examples in WordNet. Since not all entries in WordNet have usage examples, our dataset is a small subset of #TARGET_REF . In addition to WordNet, we use the Oxford Dictionary following #REF , the Urban Dictionary following #REF and our Wikipedia dataset described in the previous section. Table 1 and Table 2 show the properties and statistics of the four datasets, respectively. To simulate a situation in a real application where we might not have access to global context for the target phrases, we did not train domainspecific word embeddings on each dataset.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Since not all entries in WordNet have usage examples, our dataset is a small subset of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Global #TARGET_REF , (2) Local (#REF) with CNN, (3) I-Attention (#REF) , and our proposed model, (4) LOGCaD. The Global model is our reimplementation of the best model (S + G + CH) in #TARGET_REF .",
                "It can access the global context of a phrase to be described, but has no ability to read the local context.",
                "The Local model is the reimplementation of the best model (dual encoder) in #REF .",
                "In order to make a fair comparison of the effectiveness of local and global contexts, we slightly modify the original implementation by #REF; as the character-level encoder in the Local model, we adopt CNNs that are exactly the same as the other two models instead of the original LSTMs.",
                "The I-Attention is our reimplementation of the best model (S + I-Attention) in #REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Global #TARGET_REF , (2) Local (#REF) with CNN, (3) I-Attention (#REF) , and our proposed model, (4) LOGCaD. The Global model is our reimplementation of the best model (S + G + CH) in #TARGET_REF . It can access the global context of a phrase to be described, but has no ability to read the local context. The Local model is the reimplementation of the best model (dual encoder) in #REF . In order to make a fair comparison of the effectiveness of local and global contexts, we slightly modify the original implementation by #REF; as the character-level encoder in the Local model, we adopt CNNs that are exactly the same as the other two models instead of the original LSTMs. The I-Attention is our reimplementation of the best model (S + I-Attention) in #REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Global #TARGET_REF , (2) Local (#REF) with CNN, (3) I-Attention (#REF) , and our proposed model, (4) LOGCaD. The Global model is our reimplementation of the best model (S + G + CH) in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "To verify this idea, we propose to incorporate both local and global contexts to describe an unknown phrase.",
                "Figure 1 shows an illustration of our LOG-CaD model.",
                "Similarly to the standard encoder-decoder model with attention (#REF; #REF) , it has a context encoder and a description decoder.",
                "The challenge here is that the decoder needs to be conditioned not only on the local context, but also on its global context.",
                "To incorporate the different types of contexts, we propose to use a gate function similar to #TARGET_REF to dynamically control how the global and local contexts influence the description."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "To verify this idea, we propose to incorporate both local and global contexts to describe an unknown phrase. Figure 1 shows an illustration of our LOG-CaD model. Similarly to the standard encoder-decoder model with attention (#REF; #REF) , it has a context encoder and a description decoder. The challenge here is that the decoder needs to be conditioned not only on the local context, but also on its global context. To incorporate the different types of contexts, we propose to use a gate function similar to #TARGET_REF to dynamically control how the global and local contexts influence the description.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"To incorporate the different types of contexts, we propose to use a gate function similar to #TARGET_REF to dynamically control how the global and local contexts influence the description.\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to capture the interaction between the local and global contexts, we adopt a GATE(·) function (Eq. (7)) which is similar to #TARGET_REF .",
                "The GATE(·) function updates the LSTM output s t to s t depending on the global context x trg , local context d t , and character-level information c trg as",
                "where σ(·), and ; denote the sigmoid function, element-wise multiplication, and vector concatenation, respectively.",
                "W * and b * are weight matrices and bias terms, respectively.",
                "Here, the update gate z t controls how much the original hidden state s t is to be changed, and the reset gate r t controls how much the information from f t contributes to word generation at each time step."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In order to capture the interaction between the local and global contexts, we adopt a GATE(·) function (Eq. (7)) which is similar to #TARGET_REF . The GATE(·) function updates the LSTM output s t to s t depending on the global context x trg , local context d t , and character-level information c trg as where σ(·), and ; denote the sigmoid function, element-wise multiplication, and vector concatenation, respectively. W * and b * are weight matrices and bias terms, respectively. Here, the update gate z t controls how much the original hidden state s t is to be changed, and the reset gate r t controls how much the information from f t contributes to word generation at each time step.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"In order to capture the interaction between the local and global contexts, we adopt a GATE(\\u00b7) function (Eq. (7)) which is similar to #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "A new paradigm called the symbolic neural learning is introduced to extend how data analysis is performed from language processing to semantic learning and memory networking.",
                "Secondly, we address a number of Bayesian models ranging from latent variable model to VB inference (#REF; #REF; #REFb) , MCMC sampling (#REF) and BNP learning (#REF; #REFa; #REF) for hierarchical, thematic and sparse topics from natural language.",
                "In the third part, a series of deep models including deep unfolding (#REF) , Bayesian RNN (#REF; #REF) , sequence-to-sequence learning (#REF; #REF) , CNN ( #TARGET_REF; #REF; , GAN (#REF) and VAE are introduced.",
                "The coffee break is arranged within this part.",
                "Next, the fourth part focuses on a variety of advanced studies which illustrate how deep Bayesian learning is developed to infer the sophisticated recurrent models for natural language understanding."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "A new paradigm called the symbolic neural learning is introduced to extend how data analysis is performed from language processing to semantic learning and memory networking. Secondly, we address a number of Bayesian models ranging from latent variable model to VB inference (#REF; #REF; #REFb) , MCMC sampling (#REF) and BNP learning (#REF; #REFa; #REF) for hierarchical, thematic and sparse topics from natural language. In the third part, a series of deep models including deep unfolding (#REF) , Bayesian RNN (#REF; #REF) , sequence-to-sequence learning (#REF; #REF) , CNN ( #TARGET_REF; #REF; , GAN (#REF) and VAE are introduced. The coffee break is arranged within this part. Next, the fourth part focuses on a variety of advanced studies which illustrate how deep Bayesian learning is developed to infer the sophisticated recurrent models for natural language understanding.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In the third part, a series of deep models including deep unfolding (#REF) , Bayesian RNN (#REF; #REF) , sequence-to-sequence learning (#REF; #REF) , CNN ( #TARGET_REF; #REF; , GAN (#REF) and VAE are introduced.\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer, syntactic representations of units of bilingual language models (BiLMs).",
                "Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm.",
                "The approach is evaluated in a series of ArabicEnglish and Chinese-English translation experiments.",
                "The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline, as well as over the lexicalized BiLM by #TARGET_REF .",
                "Further improvements of up to 0.45 BLEU for ArabicEnglish and up to 0.59 BLEU for ChineseEnglish are obtained by combining our dependency BiLM with a lexicalized BiLM."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                1,
                0
            ]
        },
        "input": "This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer, syntactic representations of units of bilingual language models (BiLMs). Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm. The approach is evaluated in a series of ArabicEnglish and Chinese-English translation experiments. The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline, as well as over the lexicalized BiLM by #TARGET_REF . Further improvements of up to 0.45 BLEU for ArabicEnglish and up to 0.59 BLEU for ChineseEnglish are obtained by combining our dependency BiLM with a lexicalized BiLM.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer, syntactic representations of units of bilingual language models (BiLMs).\", \"The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline, as well as over the lexicalized BiLM by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Lexical distortion modeling (#REF) conditions reordering probabilities on the phrase pairs translated at the current and previous steps.",
                "Unlike linear distortion, it characterizes reordering not in terms of distance but type: monotone, swap, or discontinuous.",
                "In this paper, we base our approach to reordering on bilingual language models (#REF; #TARGET_REF .",
                "Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process.",
                "1 Originally, #REF used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems (#REF) ."
            ],
            "label": [
                "BACK",
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Lexical distortion modeling (#REF) conditions reordering probabilities on the phrase pairs translated at the current and previous steps. Unlike linear distortion, it characterizes reordering not in terms of distance but type: monotone, swap, or discontinuous. In this paper, we base our approach to reordering on bilingual language models (#REF; #TARGET_REF . Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process. 1 Originally, #REF used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems (#REF) .",
        "output": "{\"label\": [\"BACK\", \"SIM\"], \"context\": [\"In this paper, we base our approach to reordering on bilingual language models (#REF; #TARGET_REF .\", \"Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process.\"]}"
    },
    {
        "gold": {
            "text": [
                "Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process.",
                "1 Originally, #REF used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems #TARGET_REF .",
                "We adopt and generalize the approach of #REF to investigate several variations of bilingual language models.",
                "Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties.",
                "What kind of contextual information should be incorporated in a reordering model?"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process. 1 Originally, #REF used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems #TARGET_REF . We adopt and generalize the approach of #REF to investigate several variations of bilingual language models. Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties. What kind of contextual information should be incorporated in a reordering model?",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"1 Originally, #REF used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties.",
                "What kind of contextual information should be incorporated in a reordering model?",
                "Lexical information has been used by #REF but is known to suffer from data sparsity (#REF) .",
                "Also previous contributions to bilingual language modeling (#REF; #TARGET_REF have mostly used lexical information, although Crego and Yvon (2010a) and Crego and Yvon (2010b) label bilingual to-kens with a rich set of POS tags.",
                "But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (#REF; #REF) ."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties. What kind of contextual information should be incorporated in a reordering model? Lexical information has been used by #REF but is known to suffer from data sparsity (#REF) . Also previous contributions to bilingual language modeling (#REF; #TARGET_REF have mostly used lexical information, although Crego and Yvon (2010a) and Crego and Yvon (2010b) label bilingual to-kens with a rich set of POS tags. But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (#REF; #REF) .",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Also previous contributions to bilingual language modeling (#REF; #TARGET_REF have mostly used lexical information, although Crego and Yvon (2010a) and Crego and Yvon (2010b) label bilingual to-kens with a rich set of POS tags.\", \"But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (#REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "During decoding we have full access to the source sentence, which allows us to obtain a better syntactic analysis (than for a partial sentence) and to precompute the units that the model operates with.",
                "We investigate the following research questions: How well can we capture reordering regularities of a language pair by incorporating source syntactic parameters into the units of a bilingual language model? What kind of source syntactic parameters are necessary and sufficient?",
                "Our contributions can be summarized as follows: We argue that the contextual information used in the original bilingual models #TARGET_REF is insufficient and introduce a simple model that exploits source-side syntax to improve reordering (Sections 2 and 3).",
                "We perform a thorough comparison between different variants of our general model and compare them to the original approach.",
                "We carry out translation experiments on multiple test sets, two language pairs (ArabicEnglish and Chinese-English), and with respect to two metrics (BLEU and TER)."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "During decoding we have full access to the source sentence, which allows us to obtain a better syntactic analysis (than for a partial sentence) and to precompute the units that the model operates with. We investigate the following research questions: How well can we capture reordering regularities of a language pair by incorporating source syntactic parameters into the units of a bilingual language model? What kind of source syntactic parameters are necessary and sufficient? Our contributions can be summarized as follows: We argue that the contextual information used in the original bilingual models #TARGET_REF is insufficient and introduce a simple model that exploits source-side syntax to improve reordering (Sections 2 and 3). We perform a thorough comparison between different variants of our general model and compare them to the original approach. We carry out translation experiments on multiple test sets, two language pairs (ArabicEnglish and Chinese-English), and with respect to two metrics (BLEU and TER).",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Our contributions can be summarized as follows: We argue that the contextual information used in the original bilingual models #TARGET_REF is insufficient and introduce a simple model that exploits source-side syntax to improve reordering (Sections 2 and 3).\"]}"
    },
    {
        "gold": {
            "text": [
                "In other words, the i-th translation event consists of the i-th target word and all source words aligned to it.",
                "#TARGET_REF refer to the defined translation events t i as bilingual tokens and we adopt this terminology.",
                "There are alternative definitions of bilingual language models.",
                "Our choice of the above definition is supported by the fact that it produces an unambiguous segmentation of a parallel sentence into tokens.",
                "Ambiguous segmentation is undesirable because it increases the token vocabulary, and thus the model sparsity."
            ],
            "label": [
                "BACK",
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                1,
                0
            ]
        },
        "input": "In other words, the i-th translation event consists of the i-th target word and all source words aligned to it. #TARGET_REF refer to the defined translation events t i as bilingual tokens and we adopt this terminology. There are alternative definitions of bilingual language models. Our choice of the above definition is supported by the fact that it produces an unambiguous segmentation of a parallel sentence into tokens. Ambiguous segmentation is undesirable because it increases the token vocabulary, and thus the model sparsity.",
        "output": "{\"label\": [\"BACK\", \"SIM\"], \"context\": [\"#TARGET_REF refer to the defined translation events t i as bilingual tokens and we adopt this terminology.\", \"Our choice of the above definition is supported by the fact that it produces an unambiguous segmentation of a parallel sentence into tokens.\"]}"
    },
    {
        "gold": {
            "text": [
                "By using unnecessarily fine-grained categories we risk running into sparsity issues.",
                "#TARGET_REF also described an alternative variant of the original BiLM, where words are substituted by their POS tags (Figure 2 .a, shaded part).",
                "Also, however, POS information by itself may be insufficiently expressive to separate cor- , it still is a likely sequence.",
                "Indeed, the log-probabilities of the two sequences with respect to a 4-gram BiLM model 5 result in a higher probability of −10.25 for the incorrect reordering than for the correct one (−10.39).",
                "Since fully lexicalized bilingual tokens suffer from data sparsity and POS-based bilingual tokens are insufficiently expressive, the question is which level of syntactic information strikes the right balance between expressiveness and generality."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                1,
                1,
                1,
                1
            ]
        },
        "input": "By using unnecessarily fine-grained categories we risk running into sparsity issues. #TARGET_REF also described an alternative variant of the original BiLM, where words are substituted by their POS tags (Figure 2 .a, shaded part). Also, however, POS information by itself may be insufficiently expressive to separate cor- , it still is a likely sequence. Indeed, the log-probabilities of the two sequences with respect to a 4-gram BiLM model 5 result in a higher probability of −10.25 for the incorrect reordering than for the correct one (−10.39). Since fully lexicalized bilingual tokens suffer from data sparsity and POS-based bilingual tokens are insufficiently expressive, the question is which level of syntactic information strikes the right balance between expressiveness and generality.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"#TARGET_REF also described an alternative variant of the original BiLM, where words are substituted by their POS tags (Figure 2 .a, shaded part).\", \"Also, however, POS information by itself may be insufficiently expressive to separate cor- , it still is a likely sequence.\", \"Indeed, the log-probabilities of the two sequences with respect to a 4-gram BiLM model 5 result in a higher probability of \\u221210.25 for the incorrect reordering than for the correct one (\\u221210.39).\", \"Since fully lexicalized bilingual tokens suffer from data sparsity and POS-based bilingual tokens are insufficiently expressive, the question is which level of syntactic information strikes the right balance between expressiveness and generality.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section, we introduce our model which combines the BiLM from #TARGET_REF with source dependency information.",
                "We further give details on how the proposed models are trained and integrated into a phrase-based decoder."
            ],
            "label": [
                "BACK",
                "SIM"
            ],
            "context": [
                1,
                1
            ]
        },
        "input": "In this section, we introduce our model which combines the BiLM from #TARGET_REF with source dependency information. We further give details on how the proposed models are trained and integrated into a phrase-based decoder.",
        "output": "{\"label\": [\"BACK\", \"SIM\"], \"context\": [\"In this section, we introduce our model which combines the BiLM from #TARGET_REF with source dependency information.\", \"We further give details on how the proposed models are trained and integrated into a phrase-based decoder.\"]}"
    },
    {
        "gold": {
            "text": [
                "We are interested in how a translation system with an integrated dependency-based BiLM feaand several gale corpora.",
                "ture performs as compared to the standard PB-SMT baseline and, more importantly, to the original BiLM model.",
                "We consider two variants of BiLM discussed by #TARGET_REF : the standard one, Lex•Lex, and the simplest syntactic one, Pos•Pos.",
                "Results for the experiments can be found in Table 2 .",
                "In the discussion below we mostly focus on the experimental results for the large, combined test set MT08+MT09."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "We are interested in how a translation system with an integrated dependency-based BiLM feaand several gale corpora. ture performs as compared to the standard PB-SMT baseline and, more importantly, to the original BiLM model. We consider two variants of BiLM discussed by #TARGET_REF : the standard one, Lex•Lex, and the simplest syntactic one, Pos•Pos. Results for the experiments can be found in Table 2 . In the discussion below we mostly focus on the experimental results for the large, combined test set MT08+MT09.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"We consider two variants of BiLM discussed by #TARGET_REF : the standard one, Lex\\u2022Lex, and the simplest syntactic one, Pos\\u2022Pos.\", \"Results for the experiments can be found in Table 2 .\"]}"
    },
    {
        "gold": {
            "text": [
                "We argued that the very limited contextual information used in the original bilingual models #TARGET_REF can capture reorderings only to a limited degree and proposed a method to incorporate information from a source dependency tree in bilingual units.",
                "In a series of translation experiments we performed a thorough comparison between various syntacticallyenriched BiLMs and competing models.",
                "The results demonstrated that adding syntactic information from a source dependency tree to the representations of bilingual tokens in an n-gram model can yield statistically significant improvements over the competing systems.",
                "A number of additional evaluations provided an indication for better modeling of reordering phenomena.",
                "The proposed dependency-based BiLMs resulted in an increase in 4-gram precision and provided further significant improvements over all considered metrics in experiments with an increased distortion limit."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We argued that the very limited contextual information used in the original bilingual models #TARGET_REF can capture reorderings only to a limited degree and proposed a method to incorporate information from a source dependency tree in bilingual units. In a series of translation experiments we performed a thorough comparison between various syntacticallyenriched BiLMs and competing models. The results demonstrated that adding syntactic information from a source dependency tree to the representations of bilingual tokens in an n-gram model can yield statistically significant improvements over the competing systems. A number of additional evaluations provided an indication for better modeling of reordering phenomena. The proposed dependency-based BiLMs resulted in an increase in 4-gram precision and provided further significant improvements over all considered metrics in experiments with an increased distortion limit.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"We argued that the very limited contextual information used in the original bilingual models #TARGET_REF can capture reorderings only to a limited degree and proposed a method to incorporate information from a source dependency tree in bilingual units.\"]}"
    },
    {
        "gold": {
            "text": [
                "Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process.",
                "1 Originally, #REF used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems (#REF) .",
                "We adopt and generalize the approach of #TARGET_REF to investigate several variations of bilingual language models.",
                "Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties.",
                "What kind of contextual information should be incorporated in a reordering model?"
            ],
            "label": [
                "USE",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Instead of directly characterizing reordering, they model sequences of elementary translation events as a Markov process. 1 Originally, #REF used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems (#REF) . We adopt and generalize the approach of #TARGET_REF to investigate several variations of bilingual language models. Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties. What kind of contextual information should be incorporated in a reordering model?",
        "output": "{\"label\": [\"USE\", \"EXT\"], \"context\": [\"We adopt and generalize the approach of #TARGET_REF to investigate several variations of bilingual language models.\", \"Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF introduce an alternative method for unambiguous bilingual segmentation where tokens are defined as minimal phrases, called minimal translation units (MTUs).",
                "Figure 1 compares the BiLM and MTU tokenization for a specific example.",
                "Since #TARGET_REF have shown their model to work successfully as an additional feature in combination with commonly used standard phrase-based features, we use their approach as the main point of reference and base our approach on their segmentation method.",
                "In the rest of the text we refer to #REF as the original BiLM.",
                "4 At the same time, we do not see any specific obstacles for combining our work with MTUs."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "#REF introduce an alternative method for unambiguous bilingual segmentation where tokens are defined as minimal phrases, called minimal translation units (MTUs). Figure 1 compares the BiLM and MTU tokenization for a specific example. Since #TARGET_REF have shown their model to work successfully as an additional feature in combination with commonly used standard phrase-based features, we use their approach as the main point of reference and base our approach on their segmentation method. In the rest of the text we refer to #REF as the original BiLM. 4 At the same time, we do not see any specific obstacles for combining our work with MTUs.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Since #TARGET_REF have shown their model to work successfully as an additional feature in combination with commonly used standard phrase-based features, we use their approach as the main point of reference and base our approach on their segmentation method.\"]}"
    },
    {
        "gold": {
            "text": [
                "Verbal irony is commonly defined as a figure of speech where the speaker intends to communicate the opposite of what is literally said (#REF) .",
                "Situational irony, instead refers to a contradictory or unexpected outcome of events (#REF) .",
                "In Twitter we can find many examples both of verbal irony and of posts where users describe aspects of an ironic situation.",
                "Most of the proposed approaches to the automatic detection of irony in social media (#REF; #REF; Ptáček et al., 2014 )take advantage of lexical factors such as n-grams, punctuation marks, among others.",
                "Information related to affect has been also exploited (#REF; #TARGET_REF; Hernández Farías et al., 2015) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Verbal irony is commonly defined as a figure of speech where the speaker intends to communicate the opposite of what is literally said (#REF) . Situational irony, instead refers to a contradictory or unexpected outcome of events (#REF) . In Twitter we can find many examples both of verbal irony and of posts where users describe aspects of an ironic situation. Most of the proposed approaches to the automatic detection of irony in social media (#REF; #REF; Ptáček et al., 2014 )take advantage of lexical factors such as n-grams, punctuation marks, among others. Information related to affect has been also exploited (#REF; #TARGET_REF; Hernández Farías et al., 2015) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Information related to affect has been also exploited (#REF; #TARGET_REF; Hern\\u00e1ndez Far\\u00edas et al., 2015) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Irony is a very subjective language device that involves the expression of affective contents such as emotions, attitudes, or evaluations towards a particular target.",
                "Attempting to take advantage of the emotionally-laden characteristics of ironic expressions, we relied on emotIDM, an irony detection model that, taking advantage of several affective resources available for English (#REF) , exploits various facets of affective information from sentiment to finer-grained emotions for characterizing the presence of irony in Twitter .",
                "In ) the robustness of emotIDM was assessed over different Twitter state-of-the-art corpora for irony detection (#REF; #TARGET_REF; #REF; Ptáček et al., 2014; #REF) .",
                "The obtained results outperform those in the previous works confirming the significance of affective features for irony detection.",
                "An additional aspect to be mentioned about emotIDM is that it was designed to identify ironic content in a general sense, i.e. considering irony as a broad term covering different types of irony in tweets."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Irony is a very subjective language device that involves the expression of affective contents such as emotions, attitudes, or evaluations towards a particular target. Attempting to take advantage of the emotionally-laden characteristics of ironic expressions, we relied on emotIDM, an irony detection model that, taking advantage of several affective resources available for English (#REF) , exploits various facets of affective information from sentiment to finer-grained emotions for characterizing the presence of irony in Twitter . In ) the robustness of emotIDM was assessed over different Twitter state-of-the-art corpora for irony detection (#REF; #TARGET_REF; #REF; Ptáček et al., 2014; #REF) . The obtained results outperform those in the previous works confirming the significance of affective features for irony detection. An additional aspect to be mentioned about emotIDM is that it was designed to identify ironic content in a general sense, i.e. considering irony as a broad term covering different types of irony in tweets.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In ) the robustness of emotIDM was assessed over different Twitter state-of-the-art corpora for irony detection (#REF; #TARGET_REF; #REF; Pt\\u00e1\\u010dek et al., 2014; #REF) .\", \"The obtained results outperform those in the previous works confirming the significance of affective features for irony detection.\"]}"
    },
    {
        "gold": {
            "text": [
                "From these tweets, 265 were belonging to the ironic class, while 592 were labeled as non-ironic.",
                "Notice that, in (Hernández-#REF) , the authors found a similar behavior regarding URL information in the dataset provided by the organizers of SentiPOLC-2014 (#REF) .",
                "Furthermore, #TARGET_REF exploited a feature for alerting the existence of an URL in a tweet; such feature was ranked among the most discriminative ones according to an information gain analysis.",
                "Since, information regarding to the presence of URL in a tweet has proven to be useful for detecting irony in Twitter, we decided to enrich emotIDM by adding a binary feature for reflecting the presence of URL in a tweet.",
                "Below, we describe our participation in the task."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "From these tweets, 265 were belonging to the ironic class, while 592 were labeled as non-ironic. Notice that, in (Hernández-#REF) , the authors found a similar behavior regarding URL information in the dataset provided by the organizers of SentiPOLC-2014 (#REF) . Furthermore, #TARGET_REF exploited a feature for alerting the existence of an URL in a tweet; such feature was ranked among the most discriminative ones according to an information gain analysis. Since, information regarding to the presence of URL in a tweet has proven to be useful for detecting irony in Twitter, we decided to enrich emotIDM by adding a binary feature for reflecting the presence of URL in a tweet. Below, we describe our participation in the task.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Furthermore, #TARGET_REF exploited a feature for alerting the existence of an URL in a tweet; such feature was ranked among the most discriminative ones according to an information gain analysis.\"]}"
    },
    {
        "gold": {
            "text": [
                "Distinguishing between different kinds of ironic devices is still a controversial issue.",
                "In computational linguistics, only few research works have attempted to address such a difficult task (#REF; #TARGET_REF; #REF; Van #REF) .",
                "We are interested in assessing the performance of emotIDM when it deals with different types of irony, in order to test if a wide variety of affective features can help in discriminating also in the finer-grained classification task here proposed.",
                "This could give some insights on the role of affective content among ironic devices having different communication purposes.",
                "emotIDM+URL was trained with the dataset provided for Task B (constrained setting) to test the effectiveness of affective features in such finergrained task."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Distinguishing between different kinds of ironic devices is still a controversial issue. In computational linguistics, only few research works have attempted to address such a difficult task (#REF; #TARGET_REF; #REF; Van #REF) . We are interested in assessing the performance of emotIDM when it deals with different types of irony, in order to test if a wide variety of affective features can help in discriminating also in the finer-grained classification task here proposed. This could give some insights on the role of affective content among ironic devices having different communication purposes. emotIDM+URL was trained with the dataset provided for Task B (constrained setting) to test the effectiveness of affective features in such finergrained task.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Distinguishing between different kinds of ironic devices is still a controversial issue.\", \"In computational linguistics, only few research works have attempted to address such a difficult task (#REF; #TARGET_REF; #REF; Van #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We took advantage of a set of corpora previously used in the state of the art in irony detection.",
                "We exploited data from a set of corpora collected exploiting different approaches: self-tagging or manual annotation or crowd-sourcing 3 .",
                "We exploited the corpora developed by (#REF) , #TARGET_REF , (#REF) , (Ptáček et al., 2014) , (#REF) , (#REF) , (#REF) , and (#REF) .",
                "Besides, we also take advantage of an in-house collection of tweets containing the hashtags #irony and #sarcasm 4 .",
                "Table 1 shows the obtained results during the developing phase for Task A. We experimented with different sets of features and classifiers considering a five fold-cross validation setting."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We took advantage of a set of corpora previously used in the state of the art in irony detection. We exploited data from a set of corpora collected exploiting different approaches: self-tagging or manual annotation or crowd-sourcing 3 . We exploited the corpora developed by (#REF) , #TARGET_REF , (#REF) , (Ptáček et al., 2014) , (#REF) , (#REF) , (#REF) , and (#REF) . Besides, we also take advantage of an in-house collection of tweets containing the hashtags #irony and #sarcasm 4 . Table 1 shows the obtained results during the developing phase for Task A. We experimented with different sets of features and classifiers considering a five fold-cross validation setting.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We exploited the corpora developed by (#REF) , #TARGET_REF , (#REF) , (Pt\\u00e1\\u010dek et al., 2014) , (#REF) , (#REF) , (#REF) , and (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Although translation can be an option, human translation is very costly and for many language pairs, any available domain-specific parallel corpora are too small to train high-quality machine translation systems.",
                "Cross-lingual systems rely on training data from one language to train a model that can be applied to other languages (de #REF) , alleviating the training bottleneck issues for low-resource languages.",
                "This is facilitated by recent advances in learning joint multilingual representations (#REF; #REF; #REF) .",
                "In our work, we propose a self-learning framework to incorporate the predictions of the multilingual BERT model #TARGET_REF on non-English data into an English training procedure.",
                "The initial multilingual BERT model was simultaneously pretrained on 104 languages, and has shown to perform well for cross-lingual transfer of natural language tasks (#REF) ."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Although translation can be an option, human translation is very costly and for many language pairs, any available domain-specific parallel corpora are too small to train high-quality machine translation systems. Cross-lingual systems rely on training data from one language to train a model that can be applied to other languages (de #REF) , alleviating the training bottleneck issues for low-resource languages. This is facilitated by recent advances in learning joint multilingual representations (#REF; #REF; #REF) . In our work, we propose a self-learning framework to incorporate the predictions of the multilingual BERT model #TARGET_REF on non-English data into an English training procedure. The initial multilingual BERT model was simultaneously pretrained on 104 languages, and has shown to perform well for cross-lingual transfer of natural language tasks (#REF) .",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"In our work, we propose a self-learning framework to incorporate the predictions of the multilingual BERT model #TARGET_REF on non-English data into an English training procedure.\"]}"
    },
    {
        "gold": {
            "text": [
                "We tune the hyper-parameters for our neural network architecture based on each non-English validation set.",
                "For the encoder, we invoke the multilingual BERT model #TARGET_REF , which supports 104 languages 1 .",
                "It relies on a shared 110k WordPiece vocabulary across all languages and yields sentence representations in a common multilingual space.",
                "Most model hyperparameters are the same as in pretraining, with the exception of the batch size, max.",
                "sequence length, and number of training epochs."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We tune the hyper-parameters for our neural network architecture based on each non-English validation set. For the encoder, we invoke the multilingual BERT model #TARGET_REF , which supports 104 languages 1 . It relies on a shared 110k WordPiece vocabulary across all languages and yields sentence representations in a common multilingual space. Most model hyperparameters are the same as in pretraining, with the exception of the batch size, max. sequence length, and number of training epochs.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For the encoder, we invoke the multilingual BERT model #TARGET_REF , which supports 104 languages 1 .\"]}"
    },
    {
        "gold": {
            "text": [
                "Deep learning techniques and distributed word representations appeared on recent studies like [17] where the role of RNNs (Recurrent Neural Networks), and CNNs (Convolutional Neural Networks) is explored.",
                "The author reports that CNNs perform best.",
                "An important work that has relevance here is #TARGET_REF where authors present an even larger movie review dataset of 50,000 movie reviews from IMBD.",
                "This dataset has been used in various works such as [5] , [16] etc.",
                "For our experiments we used a chunk of 10K (MR10K) as well as the full set (MR50K)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Deep learning techniques and distributed word representations appeared on recent studies like [17] where the role of RNNs (Recurrent Neural Networks), and CNNs (Convolutional Neural Networks) is explored. The author reports that CNNs perform best. An important work that has relevance here is #TARGET_REF where authors present an even larger movie review dataset of 50,000 movie reviews from IMBD. This dataset has been used in various works such as [5] , [16] etc. For our experiments we used a chunk of 10K (MR10K) as well as the full set (MR50K).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"An important work that has relevance here is #TARGET_REF where authors present an even larger movie review dataset of 50,000 movie reviews from IMBD.\"]}"
    },
    {
        "gold": {
            "text": [
                "Again wikigiga models are positioned in the middle of the list and the worst performing models are MoodyCorpus and Text8Corpus.",
                "Our scores on this task are somehow lower than those reported from various studies that explore advanced deep learning constructs on same dataset.",
                "In #TARGET_REF for example, authors who created movie review dataset try on it their probabilistic model that is able to capture semantic similarities between words.",
                "They report a maximal accuracy of 0.88.",
                "A study that uses a very similar method is [16] where authors combine random forest with word vector average values."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Again wikigiga models are positioned in the middle of the list and the worst performing models are MoodyCorpus and Text8Corpus. Our scores on this task are somehow lower than those reported from various studies that explore advanced deep learning constructs on same dataset. In #TARGET_REF for example, authors who created movie review dataset try on it their probabilistic model that is able to capture semantic similarities between words. They report a maximal accuracy of 0.88. A study that uses a very similar method is [16] where authors combine random forest with word vector average values.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"Our scores on this task are somehow lower than those reported from various studies that explore advanced deep learning constructs on same dataset.\", \"In #TARGET_REF for example, authors who created movie review dataset try on it their probabilistic model that is able to capture semantic similarities between words.\"]}"
    },
    {
        "gold": {
            "text": [
                "Learning System KRISP (Kernel-based Robust Interpretation for Semantic Parsing) #TARGET_REF ) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data.",
                "The productions of the formal MRL grammar are treated like semantic concepts.",
                "For each of these productions, a Support-Vector Machine (SVM) (#REF) classifier is trained using string similarity as the kernel (#REF) .",
                "Each classifier can then estimate the probability of any NL substring representing the semantic concept for its production.",
                "During semantic parsing, the classifiers are called to estimate probabilities on different substrings of the sentence to compositionally build the most probable meaning representation (MR) of the sentence."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Learning System KRISP (Kernel-based Robust Interpretation for Semantic Parsing) #TARGET_REF ) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data. The productions of the formal MRL grammar are treated like semantic concepts. For each of these productions, a Support-Vector Machine (SVM) (#REF) classifier is trained using string similarity as the kernel (#REF) . Each classifier can then estimate the probability of any NL substring representing the semantic concept for its production. During semantic parsing, the classifiers are called to estimate probabilities on different substrings of the sentence to compositionally build the most probable meaning representation (MR) of the sentence.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Learning System KRISP (Kernel-based Robust Interpretation for Semantic Parsing) #TARGET_REF ) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data.\"]}"
    },
    {
        "gold": {
            "text": [
                "The set of negative examples includes all of the other training sentences.",
                "Using these positive and negative examples, an SVM classifier is trained for each production using a string kernel.",
                "In subsequent iterations, the parser learned from the previous iteration is applied to the training examples and more refined positive and negative examples, which are more specific substrings within the sentences, are collected for training.",
                "Iterations are continued until the classifiers converge, analogous to iterations in EM (#REF) .",
                "Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data #TARGET_REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The set of negative examples includes all of the other training sentences. Using these positive and negative examples, an SVM classifier is trained for each production using a string kernel. In subsequent iterations, the parser learned from the previous iteration is applied to the training examples and more refined positive and negative examples, which are more specific substrings within the sentences, are collected for training. Iterations are continued until the classifiers converge, analogous to iterations in EM (#REF) . Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data #TARGET_REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In contrast, unannotated NL sentences are usually easily available.",
                "Semi-supervised learning methods utilize cheaply available unannotated data during training along with annotated data and often perform better than purely supervised learning methods trained on the same amount of annotated data (#REF) .",
                "In this paper we present, to our knowledge, the first semi-supervised learning system for semantic parsing.",
                "We modify KRISP, a supervised learning system for semantic parsing presented in #TARGET_REF , to make a semi-supervised system we call SEMISUP-KRISP.",
                "Experiments on a realworld dataset show the improvements SEMISUP-KRISP obtains over KRISP by utilizing unannotated sentences."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "In contrast, unannotated NL sentences are usually easily available. Semi-supervised learning methods utilize cheaply available unannotated data during training along with annotated data and often perform better than purely supervised learning methods trained on the same amount of annotated data (#REF) . In this paper we present, to our knowledge, the first semi-supervised learning system for semantic parsing. We modify KRISP, a supervised learning system for semantic parsing presented in #TARGET_REF , to make a semi-supervised system we call SEMISUP-KRISP. Experiments on a realworld dataset show the improvements SEMISUP-KRISP obtains over KRISP by utilizing unannotated sentences.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"We modify KRISP, a supervised learning system for semantic parsing presented in #TARGET_REF , to make a semi-supervised system we call SEMISUP-KRISP.\"]}"
    },
    {
        "gold": {
            "text": [
                "The SINNET system is the result of several years of research #TARGET_REF; #REF) .",
                "In , we introduced the notion of social events.",
                "A social event is a happening between two people, at least one of whom is cognizant of the other and of the event taking place.",
                "At a broad level, there are two types of social events: interaction (INR) and observation (OBS).",
                "INR is a bi-directional event in which both parties are mutually aware of each other."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The SINNET system is the result of several years of research #TARGET_REF; #REF) . In , we introduced the notion of social events. A social event is a happening between two people, at least one of whom is cognizant of the other and of the event taking place. At a broad level, there are two types of social events: interaction (INR) and observation (OBS). INR is a bi-directional event in which both parties are mutually aware of each other.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The SINNET system is the result of several years of research #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "In , we presented a preliminary system that uses tree kernels and Support Vector Machines (SVMs) to extract social events from news articles.",
                "In #TARGET_REF , we presented a case study on a manually extracted network from Alice in Wonderland, showing that analyzing networks based on these social events gives us insight into the roles of characters in the story.",
                "Also, static network analysis has limitations which become apparent from our analysis.",
                "We propose the use of dynamic network analysis to overcome these limitations.",
                "In Agarwal et al. the social event extraction task and show that our system trained on a news corpus using tree kernels and support vector machines beats the baseline systems by a statistically significant margin."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In , we presented a preliminary system that uses tree kernels and Support Vector Machines (SVMs) to extract social events from news articles. In #TARGET_REF , we presented a case study on a manually extracted network from Alice in Wonderland, showing that analyzing networks based on these social events gives us insight into the roles of characters in the story. Also, static network analysis has limitations which become apparent from our analysis. We propose the use of dynamic network analysis to overcome these limitations. In Agarwal et al. the social event extraction task and show that our system trained on a news corpus using tree kernels and support vector machines beats the baseline systems by a statistically significant margin.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In #TARGET_REF , we presented a case study on a manually extracted network from Alice in Wonderland, showing that analyzing networks based on these social events gives us insight into the roles of characters in the story.\"]}"
    },
    {
        "gold": {
            "text": [
                "In the second figure there are two entity mentions: Alice and Mouse.",
                "There is a bidirectional interaction link between the Alice and Mouse triggered by the word asked.",
                "Figure 2 shows the network extracted from an abridged version of Alice in Wonderland #TARGET_REF .",
                "Figure 3 shows the output of running the Hubs and Authority algorithm (#REF ) on the network.",
                "In information retrieval, an authority is a webpage that many hubs point to and a hub is a webpage that points to many authorities."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In the second figure there are two entity mentions: Alice and Mouse. There is a bidirectional interaction link between the Alice and Mouse triggered by the word asked. Figure 2 shows the network extracted from an abridged version of Alice in Wonderland #TARGET_REF . Figure 3 shows the output of running the Hubs and Authority algorithm (#REF ) on the network. In information retrieval, an authority is a webpage that many hubs point to and a hub is a webpage that points to many authorities.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Figure 2 shows the network extracted from an abridged version of Alice in Wonderland #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In our network, webpages are synonymous to characters.",
                "Figure 3a shows the hubs in decreasing order of hub weights.",
                "Figure 3b shows the authorities in decreasing order of authority weights.",
                "We see that the main character of the story, Alice, is the main authority but not the main hub.",
                "This network may be used for other In #TARGET_REF , we argued that a static network does not bring out the true nature of a network."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "In our network, webpages are synonymous to characters. Figure 3a shows the hubs in decreasing order of hub weights. Figure 3b shows the authorities in decreasing order of authority weights. We see that the main character of the story, Alice, is the main authority but not the main hub. This network may be used for other In #TARGET_REF , we argued that a static network does not bring out the true nature of a network.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"This network may be used for other In #TARGET_REF , we argued that a static network does not bring out the true nature of a network.\"]}"
    },
    {
        "gold": {
            "text": [
                "A breakthrough has come in the form of research by McClosky et al. (2006a; #TARGET_REF ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (#REF) .",
                "Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (#REF) .",
                "McClosky et al. (2006a; 2006b ) proceed as follows: sentences * Now affiliated to Lalic, Université Paris 4 La Sorbonne.",
                "from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker.",
                "The highest ranked parse trees are added to the training set of the parser and the parser is retrained."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "A breakthrough has come in the form of research by McClosky et al. (2006a; #TARGET_REF ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (#REF) . Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (#REF) . McClosky et al. (2006a; 2006b ) proceed as follows: sentences * Now affiliated to Lalic, Université Paris 4 La Sorbonne. from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker. The highest ranked parse trees are added to the training set of the parser and the parser is retrained.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"A breakthrough has come in the form of research by McClosky et al. (2006a; #TARGET_REF ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "A breakthrough has come in the form of research by McClosky et al. (2006a; 2006b ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (#REF) .",
                "Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (#REF) .",
                "McClosky et al. (2006a; #TARGET_REF ) proceed as follows: sentences * Now affiliated to Lalic, Université Paris 4 La Sorbonne.",
                "from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker.",
                "The highest ranked parse trees are added to the training set of the parser and the parser is retrained."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "A breakthrough has come in the form of research by McClosky et al. (2006a; 2006b ) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (#REF) . Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (#REF) . McClosky et al. (2006a; #TARGET_REF ) proceed as follows: sentences * Now affiliated to Lalic, Université Paris 4 La Sorbonne. from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker. The highest ranked parse trees are added to the training set of the parser and the parser is retrained.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"McClosky et al. (2006a; #TARGET_REF ) proceed as follows: sentences * Now affiliated to Lalic, Universit\\u00e9 Paris 4 La Sorbonne.\", \"from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker.\"]}"
    },
    {
        "gold": {
            "text": [
                "This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Kučera, 1979 ) (an absolute fscore improvement of 2.6%).",
                "In the experiments of McClosky et al. (2006a; #TARGET_REF , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material.",
                "#REF find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences).",
                "They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees.",
                "However, McCloskey et al. (2006b) report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Kučera, 1979 ) (an absolute fscore improvement of 2.6%). In the experiments of McClosky et al. (2006a; #TARGET_REF , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. #REF find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees. However, McCloskey et al. (2006b) report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In the experiments of McClosky et al. (2006a; #TARGET_REF , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material.\"]}"
    },
    {
        "gold": {
            "text": [
                "In the experiments of McClosky et al. (2006a; 2006b) , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material.",
                "#REF find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences).",
                "They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees.",
                "However, McCloskey et al. ( #TARGET_REF report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training.",
                "In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a; 2006b) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "In the experiments of McClosky et al. (2006a; 2006b) , the parse trees used for self-training come from the same domain (American newspaper text) as the parser's original seed training material. #REF find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees. However, McCloskey et al. ( #TARGET_REF report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training. In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a; 2006b) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"However, McCloskey et al. ( #TARGET_REF report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training.\"]}"
    },
    {
        "gold": {
            "text": [
                "In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a; #TARGET_REF .",
                "We parse the BNC (#REF) in its entirety using the reranking parser of #REF .",
                "1,000 BNC sentences are manually annotated for constituent structure, resulting in the first gold standard set for this corpus.",
                "The gold standard set is split into a development set of 500 parse trees and a test set of 500 parse trees and used in a series of self-training experiments: Charniak and Johnson's parser is retrained on combinations of WSJ treebank data and its own parses of BNC sentences.",
                "These combinations are tested on the BNC development set and Section 00 of the WSJ."
            ],
            "label": [
                "USE",
                "DIFFER"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a; #TARGET_REF . We parse the BNC (#REF) in its entirety using the reranking parser of #REF . 1,000 BNC sentences are manually annotated for constituent structure, resulting in the first gold standard set for this corpus. The gold standard set is split into a development set of 500 parse trees and a test set of 500 parse trees and used in a series of self-training experiments: Charniak and Johnson's parser is retrained on combinations of WSJ treebank data and its own parses of BNC sentences. These combinations are tested on the BNC development set and Section 00 of the WSJ.",
        "output": "{\"label\": [\"USE\", \"DIFFER\"], \"context\": [\"In contrast, we report successful in-domain 1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Charniak and Johnson's reranking parser (#REF version) is evaluated against the BNC gold standard development set.",
                "Labelled precision (LP), recall (LR) and f-score measures 2 for this parser are shown in the first row of Table 1 .",
                "The f-score of 83.7% is lower than the f-score of 85.2% reported by #TARGET_REF for the same parser on Brown corpus data.",
                "This difference is reasonable since there is greater domain variation between the WSJ and the BNC than between the WSJ and the Brown corpus, and all BNC gold standard sentences contain verbs not attested in WSJ Sections 2-21.",
                "We retrain the first-stage generative statistical parser of Charniak and Johnson using combinations of BNC trees (parsed using the reranking parser) and WSJ treebank trees."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Charniak and Johnson's reranking parser (#REF version) is evaluated against the BNC gold standard development set. Labelled precision (LP), recall (LR) and f-score measures 2 for this parser are shown in the first row of Table 1 . The f-score of 83.7% is lower than the f-score of 85.2% reported by #TARGET_REF for the same parser on Brown corpus data. This difference is reasonable since there is greater domain variation between the WSJ and the BNC than between the WSJ and the Brown corpus, and all BNC gold standard sentences contain verbs not attested in WSJ Sections 2-21. We retrain the first-stage generative statistical parser of Charniak and Johnson using combinations of BNC trees (parsed using the reranking parser) and WSJ treebank trees.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The f-score of 83.7% is lower than the f-score of 85.2% reported by #TARGET_REF for the same parser on Brown corpus data.\"]}"
    },
    {
        "gold": {
            "text": [
                "These sentiments can be used to provided targeted advertising, automatically generate reviews, and make various predictions, such as political outcomes.",
                "In this paper we develop a sentiment detection algorithm for social media that classifies the polarity of sentence phrases as positive, negative, or neutral and test its performance in Twitter through the participation in the expression level task (subtask A) of the #REF Task 2: Sentiment Analysis in Twitter (#REF) which the authors helped organize.",
                "To do so, we build on previous work on sentiment detection algorithms for the more formal news genre, notably the work of #TARGET_REF , but adapt it for the language of social media, in particular Twitter.",
                "We show that exploiting lexical-stylistic features and dictionaries geared toward social media are useful in detecting sentiment.",
                "In this rest of this paper, we discuss related work, including the state of the art sentiment system (#REF) our method is based on, the lexicons we used, our method, and experiments and results."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "These sentiments can be used to provided targeted advertising, automatically generate reviews, and make various predictions, such as political outcomes. In this paper we develop a sentiment detection algorithm for social media that classifies the polarity of sentence phrases as positive, negative, or neutral and test its performance in Twitter through the participation in the expression level task (subtask A) of the #REF Task 2: Sentiment Analysis in Twitter (#REF) which the authors helped organize. To do so, we build on previous work on sentiment detection algorithms for the more formal news genre, notably the work of #TARGET_REF , but adapt it for the language of social media, in particular Twitter. We show that exploiting lexical-stylistic features and dictionaries geared toward social media are useful in detecting sentiment. In this rest of this paper, we discuss related work, including the state of the art sentiment system (#REF) our method is based on, the lexicons we used, our method, and experiments and results.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"To do so, we build on previous work on sentiment detection algorithms for the more formal news genre, notably the work of #TARGET_REF , but adapt it for the language of social media, in particular Twitter.\"]}"
    },
    {
        "gold": {
            "text": [
                "These sentiments can be used to provided targeted advertising, automatically generate reviews, and make various predictions, such as political outcomes.",
                "In this paper we develop a sentiment detection algorithm for social media that classifies the polarity of sentence phrases as positive, negative, or neutral and test its performance in Twitter through the participation in the expression level task (subtask A) of the #REF Task 2: Sentiment Analysis in Twitter (#REF) which the authors helped organize.",
                "To do so, we build on previous work on sentiment detection algorithms for the more formal news genre, notably the work of Agarwal et al (2009) , but adapt it for the language of social media, in particular Twitter.",
                "We show that exploiting lexical-stylistic features and dictionaries geared toward social media are useful in detecting sentiment.",
                "In this rest of this paper, we discuss related work, including the state of the art sentiment system #TARGET_REF our method is based on, the lexicons we used, our method, and experiments and results."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "These sentiments can be used to provided targeted advertising, automatically generate reviews, and make various predictions, such as political outcomes. In this paper we develop a sentiment detection algorithm for social media that classifies the polarity of sentence phrases as positive, negative, or neutral and test its performance in Twitter through the participation in the expression level task (subtask A) of the #REF Task 2: Sentiment Analysis in Twitter (#REF) which the authors helped organize. To do so, we build on previous work on sentiment detection algorithms for the more formal news genre, notably the work of Agarwal et al (2009) , but adapt it for the language of social media, in particular Twitter. We show that exploiting lexical-stylistic features and dictionaries geared toward social media are useful in detecting sentiment. In this rest of this paper, we discuss related work, including the state of the art sentiment system #TARGET_REF our method is based on, the lexicons we used, our method, and experiments and results.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"In this rest of this paper, we discuss related work, including the state of the art sentiment system #TARGET_REF our method is based on, the lexicons we used, our method, and experiments and results.\"]}"
    },
    {
        "gold": {
            "text": [
                "Several lexicons are used in our system.",
                "We use the DAL and expand it with WordNet, as it was used in the original work #TARGET_REF , and expand it further to use Wiktionary and an emoticon lexicon.",
                "We consider proper nouns that are not in the DAL to be objective.",
                "We also shorten words that are lengthened to see if we can find the shortened version in the lexicons (e.g. sweeeet → sweet).",
                "The coverage of the lexicons for each corpus is shown in Table 1 ."
            ],
            "label": [
                "EXT",
                "SIM",
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Several lexicons are used in our system. We use the DAL and expand it with WordNet, as it was used in the original work #TARGET_REF , and expand it further to use Wiktionary and an emoticon lexicon. We consider proper nouns that are not in the DAL to be objective. We also shorten words that are lengthened to see if we can find the shortened version in the lexicons (e.g. sweeeet → sweet). The coverage of the lexicons for each corpus is shown in Table 1 .",
        "output": "{\"label\": [\"EXT\", \"SIM\", \"USE\"], \"context\": [\"We use the DAL and expand it with WordNet, as it was used in the original work #TARGET_REF , and expand it further to use Wiktionary and an emoticon lexicon.\"]}"
    },
    {
        "gold": {
            "text": [
                "The Dictionary of Affect and Language (DAL) (#REF ) is an English language dictionary of 8742 words built to measure the emotional meaning of texts.",
                "In addition to using newswire, it was also built from individual sources such as interviews on abuse, students' retelling of a story, and adolescent's descriptions of emotions.",
                "It therefore covers a broad set of words.",
                "Each word is given three scores (pleasantness -also called evaluation (ee), activeness (aa), and imagery (ii)) on a scale of 1 (low) to 3 (high).",
                "We compute the polarity of a chunk in the same manner as the original work #TARGET_REF , using the sum of the AE Space Score's (| √ ee 2 + aa 2 |) of each word within the chunk."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The Dictionary of Affect and Language (DAL) (#REF ) is an English language dictionary of 8742 words built to measure the emotional meaning of texts. In addition to using newswire, it was also built from individual sources such as interviews on abuse, students' retelling of a story, and adolescent's descriptions of emotions. It therefore covers a broad set of words. Each word is given three scores (pleasantness -also called evaluation (ee), activeness (aa), and imagery (ii)) on a scale of 1 (low) to 3 (high). We compute the polarity of a chunk in the same manner as the original work #TARGET_REF , using the sum of the AE Space Score's (| √ ee 2 + aa 2 |) of each word within the chunk.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"We compute the polarity of a chunk in the same manner as the original work #TARGET_REF , using the sum of the AE Space Score's (| \\u221a ee 2 + aa 2 |) of each word within the chunk.\"]}"
    },
    {
        "gold": {
            "text": [
                "We include POS tags and the top 500 n-gram features (#REF) .",
                "We experimented with different amounts of n-grams and found that more than 500 n-grams reduced performance.",
                "The DAL and other dictionaries are used along with a negation state machine (#REF) to determine the polarity for each word in the sentence.",
                "We include all the features described in the original system #TARGET_REF )."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1
            ]
        },
        "input": "We include POS tags and the top 500 n-gram features (#REF) . We experimented with different amounts of n-grams and found that more than 500 n-grams reduced performance. The DAL and other dictionaries are used along with a negation state machine (#REF) to determine the polarity for each word in the sentence. We include all the features described in the original system #TARGET_REF ).",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"We include all the features described in the original system #TARGET_REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "We include POS tags and the top 500 n-gram features #TARGET_REF .",
                "We experimented with different amounts of n-grams and found that more than 500 n-grams reduced performance.",
                "The DAL and other dictionaries are used along with a negation state machine (#REF) to determine the polarity for each word in the sentence.",
                "We include all the features described in the original system (#REF )."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "We include POS tags and the top 500 n-gram features #TARGET_REF . We experimented with different amounts of n-grams and found that more than 500 n-grams reduced performance. The DAL and other dictionaries are used along with a negation state machine (#REF) to determine the polarity for each word in the sentence. We include all the features described in the original system (#REF ).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We include POS tags and the top 500 n-gram features #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We include POS tags and the top 500 n-gram features (#REF) .",
                "We experimented with different amounts of n-grams and found that more than 500 n-grams reduced performance.",
                "The DAL and other dictionaries are used along with a negation state machine #TARGET_REF to determine the polarity for each word in the sentence.",
                "We include all the features described in the original system (#REF )."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0
            ]
        },
        "input": "We include POS tags and the top 500 n-gram features (#REF) . We experimented with different amounts of n-grams and found that more than 500 n-grams reduced performance. The DAL and other dictionaries are used along with a negation state machine #TARGET_REF to determine the polarity for each word in the sentence. We include all the features described in the original system (#REF ).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The DAL and other dictionaries are used along with a negation state machine #TARGET_REF to determine the polarity for each word in the sentence.\"]}"
    },
    {
        "gold": {
            "text": [
                "Although these issues would be somewhat alleviated by the creation of larger datasets, collecting human judgments for all possible word pairs is impractical.",
                "Therefore, the ability to predict association strengths between arbitrary word pairs represents the best solution to these coverage issues (#REF) .",
                "Although the prediction of Evocation ratings has attracted some attention (#REF; #TARGET_REF) , to the best of our knowledge this is the first work to focus on the prediction of USF or EAT strengths.",
                "As described in Sec-tion 2, USF and EAT have several advantages over Evocation, such as the ability to work with ambiguous words instead of WordNet synsets.",
                "Following #REF's work on Evocation prediction, we frame word association prediction as a supervised regression task and introduce several new and modified features, including the first use of Gaussian embeddings (#REF) to better capture the asymmetric nature of word associations."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Although these issues would be somewhat alleviated by the creation of larger datasets, collecting human judgments for all possible word pairs is impractical. Therefore, the ability to predict association strengths between arbitrary word pairs represents the best solution to these coverage issues (#REF) . Although the prediction of Evocation ratings has attracted some attention (#REF; #TARGET_REF) , to the best of our knowledge this is the first work to focus on the prediction of USF or EAT strengths. As described in Sec-tion 2, USF and EAT have several advantages over Evocation, such as the ability to work with ambiguous words instead of WordNet synsets. Following #REF's work on Evocation prediction, we frame word association prediction as a supervised regression task and introduce several new and modified features, including the first use of Gaussian embeddings (#REF) to better capture the asymmetric nature of word associations.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Although the prediction of Evocation ratings has attracted some attention (#REF; #TARGET_REF) , to the best of our knowledge this is the first work to focus on the prediction of USF or EAT strengths.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, this approach is somewhat limited in that it frames Evocation prediction as a classification task, considering only five Evocation levels.",
                "The main drawback of Evocation prediction as a classification task is that it is too coarse-grained to deal with very weak associations, such as those in remote triads (De #REFa) , or very slight variations in association strength, such as those useful for computational humour (#REF) .",
                "To this end, #TARGET_REF framed Evocation prediction as a supervised regression task.",
                "They employed a combination of WordNet structure-based features, word embedding-based features, and lexical features and found that vector offsets, i.e. the mathematical difference between vectors, were a strong indicator of Evocation ratings.",
                "While Evocation's use of unambiguous synsets is useful for many applications, it is not without its own drawbacks."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "However, this approach is somewhat limited in that it frames Evocation prediction as a classification task, considering only five Evocation levels. The main drawback of Evocation prediction as a classification task is that it is too coarse-grained to deal with very weak associations, such as those in remote triads (De #REFa) , or very slight variations in association strength, such as those useful for computational humour (#REF) . To this end, #TARGET_REF framed Evocation prediction as a supervised regression task. They employed a combination of WordNet structure-based features, word embedding-based features, and lexical features and found that vector offsets, i.e. the mathematical difference between vectors, were a strong indicator of Evocation ratings. While Evocation's use of unambiguous synsets is useful for many applications, it is not without its own drawbacks.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"To this end, #TARGET_REF framed Evocation prediction as a supervised regression task.\"]}"
    },
    {
        "gold": {
            "text": [
                "To this end, #REF framed Evocation prediction as a supervised regression task.",
                "#TARGET_REF employed a combination of WordNet structure-based features, word embedding-based features, and lexical features and found that vector offsets, i.e. the mathematical difference between vectors, were a strong indicator of Evocation ratings.",
                "While Evocation's use of unambiguous synsets is useful for many applications, it is not without its own drawbacks.",
                "First, it requires texts to be word sense disambiguated; a non-trivial task.",
                "Second, since humans do not conceptualize words as a discrete set of independent word senses, Evocation is unable to capture natural associations owing to homography, homophony, or polysemy (#REF)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "To this end, #REF framed Evocation prediction as a supervised regression task. #TARGET_REF employed a combination of WordNet structure-based features, word embedding-based features, and lexical features and found that vector offsets, i.e. the mathematical difference between vectors, were a strong indicator of Evocation ratings. While Evocation's use of unambiguous synsets is useful for many applications, it is not without its own drawbacks. First, it requires texts to be word sense disambiguated; a non-trivial task. Second, since humans do not conceptualize words as a discrete set of independent word senses, Evocation is unable to capture natural associations owing to homography, homophony, or polysemy (#REF).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF employed a combination of WordNet structure-based features, word embedding-based features, and lexical features and found that vector offsets, i.e. the mathematical difference between vectors, were a strong indicator of Evocation ratings.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our word association prediction system extends the method in #REF with several modifications to make it better suited to the USF and EAT datasets.",
                "First, we modify #REF's lexVector.",
                "#TARGET_REF represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet).",
                "Similarly, they represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file).",
                "This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Our word association prediction system extends the method in #REF with several modifications to make it better suited to the USF and EAT datasets. First, we modify #REF's lexVector. #TARGET_REF represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet). Similarly, they represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file). This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet).\"]}"
    },
    {
        "gold": {
            "text": [
                "First, we modify #REF's lexVector.",
                "#REF represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet).",
                "Similarly, #TARGET_REF represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file).",
                "This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response.",
                "Since words in USF and EAT can be associated with multiple synsets and we want to be able to capture associations related to polysemy, instead using a one-hot encoding we employ count vectors specifying the number of synsets from each POS/lexical category each word belongs to."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "First, we modify #REF's lexVector. #REF represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet). Similarly, #TARGET_REF represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file). This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response. Since words in USF and EAT can be associated with multiple synsets and we want to be able to capture associations related to polysemy, instead using a one-hot encoding we employ count vectors specifying the number of synsets from each POS/lexical category each word belongs to.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Similarly, #TARGET_REF represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file).\"]}"
    },
    {
        "gold": {
            "text": [
                "Third, we extend the notion of dirRel, introduced in #REF to leverage the semantic network structure of WordNet.",
                "Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. #TARGET_REF, s and t are nodes representing a single synset.",
                "We instead consider a set of nodes S and a set of nodes T representing the set of synsets associated with the cue and response words, respectively, as shown in Equation 1.",
                "This may increase the probability that |nb(S, k)∩nb(T, k)| > 0, a shortcoming of the original dirRel due to WordNet's \"relatively sparse connective structure\" (#REF) .",
                "Fourth, in addition to the Word2Vec (w2v) cosine similarity between cue/response pairs calculated using Google's pre-trained 300 dimension Word2Vec embeddings 2 ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Third, we extend the notion of dirRel, introduced in #REF to leverage the semantic network structure of WordNet. Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. #TARGET_REF, s and t are nodes representing a single synset. We instead consider a set of nodes S and a set of nodes T representing the set of synsets associated with the cue and response words, respectively, as shown in Equation 1. This may increase the probability that |nb(S, k)∩nb(T, k)| > 0, a shortcoming of the original dirRel due to WordNet's \"relatively sparse connective structure\" (#REF) . Fourth, in addition to the Word2Vec (w2v) cosine similarity between cue/response pairs calculated using Google's pre-trained 300 dimension Word2Vec embeddings 2 .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. #TARGET_REF, s and t are nodes representing a single synset.\"]}"
    },
    {
        "gold": {
            "text": [
                "Although these issues would be somewhat alleviated by the creation of larger datasets, collecting human judgments for all possible word pairs is impractical.",
                "Therefore, the ability to predict association strengths between arbitrary word pairs represents the best solution to these coverage issues (#REF) .",
                "Although the prediction of Evocation ratings has attracted some attention (#REF; #REF) , to the best of our knowledge this is the first work to focus on the prediction of USF or EAT strengths.",
                "As described in Sec-tion 2, USF and EAT have several advantages over Evocation, such as the ability to work with ambiguous words instead of WordNet synsets.",
                "Following #TARGET_REF on Evocation prediction, we frame word association prediction as a supervised regression task and introduce several new and modified features, including the first use of Gaussian embeddings (#REF) to better capture the asymmetric nature of word associations."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Although these issues would be somewhat alleviated by the creation of larger datasets, collecting human judgments for all possible word pairs is impractical. Therefore, the ability to predict association strengths between arbitrary word pairs represents the best solution to these coverage issues (#REF) . Although the prediction of Evocation ratings has attracted some attention (#REF; #REF) , to the best of our knowledge this is the first work to focus on the prediction of USF or EAT strengths. As described in Sec-tion 2, USF and EAT have several advantages over Evocation, such as the ability to work with ambiguous words instead of WordNet synsets. Following #TARGET_REF on Evocation prediction, we frame word association prediction as a supervised regression task and introduce several new and modified features, including the first use of Gaussian embeddings (#REF) to better capture the asymmetric nature of word associations.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"Following #TARGET_REF on Evocation prediction, we frame word association prediction as a supervised regression task and introduce several new and modified features, including the first use of Gaussian embeddings (#REF) to better capture the asymmetric nature of word associations.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our word association prediction system extends the method in #TARGET_REF with several modifications to make it better suited to the USF and EAT datasets.",
                "First, we modify #REF's lexVector.",
                "#REF represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet).",
                "Similarly, they represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file).",
                "This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Our word association prediction system extends the method in #TARGET_REF with several modifications to make it better suited to the USF and EAT datasets. First, we modify #REF's lexVector. #REF represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet). Similarly, they represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file). This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"Our word association prediction system extends the method in #TARGET_REF with several modifications to make it better suited to the USF and EAT datasets.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our word association prediction system extends the method in #REF with several modifications to make it better suited to the USF and EAT datasets.",
                "First, we modify #TARGET_REF lexVector.",
                "#REF represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet).",
                "Similarly, they represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file).",
                "This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Our word association prediction system extends the method in #REF with several modifications to make it better suited to the USF and EAT datasets. First, we modify #TARGET_REF lexVector. #REF represent each word's part-ofspeech (POS) using a one-hot encoded five dimensional vector (one of each POS in WordNet). Similarly, they represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file). This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"First, we modify #TARGET_REF lexVector.\"]}"
    },
    {
        "gold": {
            "text": [
                "Second, instead of computing Wu-Palmer similarity (WUP, #REF ) between a single synset pair, we compute it for all cue synset/response synset pairs and record the maximum and average values.",
                "Following #REF and #REF, we also explored the use of path and Leacock-Chodorow (#REF) similarities but found they did not add any advantage over WUP alone.",
                "We take a similar approach for adapting load and betweenness centralities (#REF) as well as AutoExtend (AutoEx, Rothe and Schütze, 2015) similarity.",
                "Third, we extend the notion of dirRel, introduced in #TARGET_REF to leverage the semantic network structure of WordNet.",
                "Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. In the original formula, s and t are nodes representing a single synset."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Second, instead of computing Wu-Palmer similarity (WUP, #REF ) between a single synset pair, we compute it for all cue synset/response synset pairs and record the maximum and average values. Following #REF and #REF, we also explored the use of path and Leacock-Chodorow (#REF) similarities but found they did not add any advantage over WUP alone. We take a similar approach for adapting load and betweenness centralities (#REF) as well as AutoExtend (AutoEx, Rothe and Schütze, 2015) similarity. Third, we extend the notion of dirRel, introduced in #TARGET_REF to leverage the semantic network structure of WordNet. Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. In the original formula, s and t are nodes representing a single synset.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"Third, we extend the notion of dirRel, introduced in #TARGET_REF to leverage the semantic network structure of WordNet.\"]}"
    },
    {
        "gold": {
            "text": [
                "Similarly, they represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file).",
                "This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response.",
                "Since words in USF and EAT can be associated with multiple synsets and we want to be able to capture associations related to polysemy, #TARGET_REF we employ count vectors specifying the number of synsets from each POS/lexical category each word belongs to.",
                "Second, instead of computing Wu-Palmer similarity (WUP, #REF ) between a single synset pair, we compute it for all cue synset/response synset pairs and record the maximum and average values.",
                "Following #REF and #REF, we also explored the use of path and Leacock-Chodorow (#REF) similarities but found they did not add any advantage over WUP alone."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Similarly, they represent each word's lexical category using a one-hot encoded 45 dimensional vector (one for each WordNet lexicographer file). This results in a 100 dimensional vector representing the POS and lexical categories of both the cue and the response. Since words in USF and EAT can be associated with multiple synsets and we want to be able to capture associations related to polysemy, #TARGET_REF we employ count vectors specifying the number of synsets from each POS/lexical category each word belongs to. Second, instead of computing Wu-Palmer similarity (WUP, #REF ) between a single synset pair, we compute it for all cue synset/response synset pairs and record the maximum and average values. Following #REF and #REF, we also explored the use of path and Leacock-Chodorow (#REF) similarities but found they did not add any advantage over WUP alone.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Since words in USF and EAT can be associated with multiple synsets and we want to be able to capture associations related to polysemy, #TARGET_REF we employ count vectors specifying the number of synsets from each POS/lexical category each word belongs to.\"]}"
    },
    {
        "gold": {
            "text": [
                "Since words in USF and EAT can be associated with multiple synsets and we want to be able to capture associations related to polysemy, instead using a one-hot encoding we employ count vectors specifying the number of synsets from each POS/lexical category each word belongs to.",
                "Second, #TARGET_REF (WUP, #REF ) between a single synset pair, we compute it for all cue synset/response synset pairs and record the maximum and average values.",
                "Following #REF and #REF, we also explored the use of path and Leacock-Chodorow (#REF) similarities but found they did not add any advantage over WUP alone.",
                "We take a similar approach for adapting load and betweenness centralities (#REF) as well as AutoExtend (AutoEx, Rothe and Schütze, 2015) similarity.",
                "Third, we extend the notion of dirRel, introduced in #REF to leverage the semantic network structure of WordNet."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Since words in USF and EAT can be associated with multiple synsets and we want to be able to capture associations related to polysemy, instead using a one-hot encoding we employ count vectors specifying the number of synsets from each POS/lexical category each word belongs to. Second, #TARGET_REF (WUP, #REF ) between a single synset pair, we compute it for all cue synset/response synset pairs and record the maximum and average values. Following #REF and #REF, we also explored the use of path and Leacock-Chodorow (#REF) similarities but found they did not add any advantage over WUP alone. We take a similar approach for adapting load and betweenness centralities (#REF) as well as AutoExtend (AutoEx, Rothe and Schütze, 2015) similarity. Third, we extend the notion of dirRel, introduced in #REF to leverage the semantic network structure of WordNet.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Second, #TARGET_REF (WUP, #REF ) between a single synset pair, we compute it for all cue synset/response synset pairs and record the maximum and average values.\"]}"
    },
    {
        "gold": {
            "text": [
                "Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. In the original formula, s and t are nodes representing a single synset.",
                "We #TARGET_REF a set of nodes S and a set of nodes T representing the set of synsets associated with the cue and response words, respectively, as shown in Equation 1.",
                "This may increase the probability that |nb(S, k)∩nb(T, k)| > 0, a shortcoming of the original dirRel due to WordNet's \"relatively sparse connective structure\" (#REF) .",
                "Fourth, in addition to the Word2Vec (w2v) cosine similarity between cue/response pairs calculated using Google's pre-trained 300 dimension Word2Vec embeddings 2 .",
                "We also examine the effectiveness of Stanford's pre-trained 300 dimension GloVe embeddings 3 ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. In the original formula, s and t are nodes representing a single synset. We #TARGET_REF a set of nodes S and a set of nodes T representing the set of synsets associated with the cue and response words, respectively, as shown in Equation 1. This may increase the probability that |nb(S, k)∩nb(T, k)| > 0, a shortcoming of the original dirRel due to WordNet's \"relatively sparse connective structure\" (#REF) . Fourth, in addition to the Word2Vec (w2v) cosine similarity between cue/response pairs calculated using Google's pre-trained 300 dimension Word2Vec embeddings 2 . We also examine the effectiveness of Stanford's pre-trained 300 dimension GloVe embeddings 3 .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We #TARGET_REF a set of nodes S and a set of nodes T representing the set of synsets associated with the cue and response words, respectively, as shown in Equation 1.\"]}"
    },
    {
        "gold": {
            "text": [
                "Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. In the original formula, s and t are nodes representing a single synset.",
                "We instead consider a set of nodes S and a set of nodes T representing the set of synsets associated with the cue and response words, respectively, as shown in Equation 1.",
                "This may increase the probability that |nb(S, k)∩nb(T, k)| > 0, a #TARGET_REF due to WordNet's \"relatively sparse connective structure\" ( #TARGET_REF) .",
                "Fourth, in addition to the Word2Vec (w2v) cosine similarity between cue/response pairs calculated using Google's pre-trained 300 dimension Word2Vec embeddings 2 .",
                "We also examine the effectiveness of Stanford's pre-trained 300 dimension GloVe embeddings 3 ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Given a graph where nodes represent synsets and arcs represent WordNet relations such as hypernym/hyponym and holonym/meronym, dirRel(s,t,k) is the proportion of k-step neighbours of s that are also k-step neighbours of t. In the original formula, s and t are nodes representing a single synset. We instead consider a set of nodes S and a set of nodes T representing the set of synsets associated with the cue and response words, respectively, as shown in Equation 1. This may increase the probability that |nb(S, k)∩nb(T, k)| > 0, a #TARGET_REF due to WordNet's \"relatively sparse connective structure\" ( #TARGET_REF) . Fourth, in addition to the Word2Vec (w2v) cosine similarity between cue/response pairs calculated using Google's pre-trained 300 dimension Word2Vec embeddings 2 . We also examine the effectiveness of Stanford's pre-trained 300 dimension GloVe embeddings 3 .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"This may increase the probability that |nb(S, k)\\u2229nb(T, k)| > 0, a #TARGET_REF due to WordNet's \\\"relatively sparse connective structure\\\" ( #TARGET_REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Gaussian embeddings (#REF) represent words not as a fixed point in vector space but as \"potential functions\", continuous densities in latent space; therefore, they are more suitable for capturing asymmetric relationships.",
                "More specifically, for each cue/response pair, we calculate both the KL-divergence and cosine similarities of their Gaussian embeddings.",
                "The embeddings have a dimensionality of 300 and are trained on English Wikipedia using the Word2Gauss 4 (w2g) and the hyperparameters reported by the developer 5 Sixth, since cue and response words are not associated with a single synset, the AutoEx embeddings employed in #TARGET_REF to compute vector offsets are not well suited for our task.",
                "Instead, we experiment with offsets calculated using w2v, GloVe, and w2g embeddings.",
                "Finally, our 300 topic LDA model (#REF) was trained using Gensim 6 on full English Wikipedia instead of the subset of English Wikipedia used in #REF ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Gaussian embeddings (#REF) represent words not as a fixed point in vector space but as \"potential functions\", continuous densities in latent space; therefore, they are more suitable for capturing asymmetric relationships. More specifically, for each cue/response pair, we calculate both the KL-divergence and cosine similarities of their Gaussian embeddings. The embeddings have a dimensionality of 300 and are trained on English Wikipedia using the Word2Gauss 4 (w2g) and the hyperparameters reported by the developer 5 Sixth, since cue and response words are not associated with a single synset, the AutoEx embeddings employed in #TARGET_REF to compute vector offsets are not well suited for our task. Instead, we experiment with offsets calculated using w2v, GloVe, and w2g embeddings. Finally, our 300 topic LDA model (#REF) was trained using Gensim 6 on full English Wikipedia instead of the subset of English Wikipedia used in #REF .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The embeddings have a dimensionality of 300 and are trained on English Wikipedia using the Word2Gauss 4 (w2g) and the hyperparameters reported by the developer 5 Sixth, since cue and response words are not associated with a single synset, the AutoEx embeddings employed in #TARGET_REF to compute vector offsets are not well suited for our task.\"]}"
    },
    {
        "gold": {
            "text": [
                "The embeddings have a dimensionality of 300 and are trained on English Wikipedia using the Word2Gauss 4 (w2g) and the hyperparameters reported by the developer 5 Sixth, since cue and response words are not associated with a single synset, the AutoEx embeddings employed in #REF to compute vector offsets are not well suited for our task.",
                "#TARGET_REF experiment with offsets calculated using w2v, GloVe, and w2g embeddings.",
                "Finally, our 300 topic LDA model (#REF) was trained using Gensim 6 on full English Wikipedia instead of the subset of English Wikipedia used in #REF .",
                "Using the above features, we trained a multilayer perceptron for each of our three datasets; Evocation, USF, and EAT.",
                "In the case of Evocation, we discarded any synset information and Table 1 : Individual feature performance after 50 epochs simply use each synset's headword (e.g. given the sysnet entity.n.01, we only considered the word entity)."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The embeddings have a dimensionality of 300 and are trained on English Wikipedia using the Word2Gauss 4 (w2g) and the hyperparameters reported by the developer 5 Sixth, since cue and response words are not associated with a single synset, the AutoEx embeddings employed in #REF to compute vector offsets are not well suited for our task. #TARGET_REF experiment with offsets calculated using w2v, GloVe, and w2g embeddings. Finally, our 300 topic LDA model (#REF) was trained using Gensim 6 on full English Wikipedia instead of the subset of English Wikipedia used in #REF . Using the above features, we trained a multilayer perceptron for each of our three datasets; Evocation, USF, and EAT. In the case of Evocation, we discarded any synset information and Table 1 : Individual feature performance after 50 epochs simply use each synset's headword (e.g. given the sysnet entity.n.01, we only considered the word entity).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"#TARGET_REF experiment with offsets calculated using w2v, GloVe, and w2g embeddings.\"]}"
    },
    {
        "gold": {
            "text": [
                "The embeddings have a dimensionality of 300 and are trained on English Wikipedia using the Word2Gauss 4 (w2g) and the hyperparameters reported by the developer 5 Sixth, since cue and response words are not associated with a single synset, the AutoEx embeddings employed in #REF to compute vector offsets are not well suited for our task.",
                "Instead, we experiment with offsets calculated using w2v, GloVe, and w2g embeddings.",
                "Finally, our 300 topic LDA model (#REF) was trained using Gensim 6 on full English Wikipedia instead of the subset of English Wikipedia used in #TARGET_REF) .",
                "Using the above features, we trained a multilayer perceptron for each of our three datasets; Evocation, USF, and EAT.",
                "In the case of Evocation, we discarded any synset information and Table 1 : Individual feature performance after 50 epochs simply use each synset's headword (e.g. given the sysnet entity.n.01, we only considered the word entity)."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The embeddings have a dimensionality of 300 and are trained on English Wikipedia using the Word2Gauss 4 (w2g) and the hyperparameters reported by the developer 5 Sixth, since cue and response words are not associated with a single synset, the AutoEx embeddings employed in #REF to compute vector offsets are not well suited for our task. Instead, we experiment with offsets calculated using w2v, GloVe, and w2g embeddings. Finally, our 300 topic LDA model (#REF) was trained using Gensim 6 on full English Wikipedia instead of the subset of English Wikipedia used in #TARGET_REF) . Using the above features, we trained a multilayer perceptron for each of our three datasets; Evocation, USF, and EAT. In the case of Evocation, we discarded any synset information and Table 1 : Individual feature performance after 50 epochs simply use each synset's headword (e.g. given the sysnet entity.n.01, we only considered the word entity).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Finally, our 300 topic LDA model (#REF) was trained using Gensim 6 on full English Wikipedia instead of the subset of English Wikipedia used in #TARGET_REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Our slightly lower Pearson's R may be due to differences in way we split our training and test data as well as due to randomness in the training process itself.",
                "On Evocation, our system does not perform as well as #TARGET_REF .",
                "This is expected as we explicitly ignore any synset information and instead attempt to predict association strengths between word-sense ambiguous words.",
                "Despite this, our performance is not appreciably lower, indicating the fitness of our system.",
                "The fact that we perform better on Evocation than either USF or EAT is quite interesting considering our system was designed with USF and EAT in mind."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Our slightly lower Pearson's R may be due to differences in way we split our training and test data as well as due to randomness in the training process itself. On Evocation, our system does not perform as well as #TARGET_REF . This is expected as we explicitly ignore any synset information and instead attempt to predict association strengths between word-sense ambiguous words. Despite this, our performance is not appreciably lower, indicating the fitness of our system. The fact that we perform better on Evocation than either USF or EAT is quite interesting considering our system was designed with USF and EAT in mind.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"On Evocation, our system does not perform as well as #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper we explored the effectiveness of various features for predicting Evocation, USF, and EAT association strengths, finding GloVe and Word2Vec cosine similarities as well as vector offsets to be the most useful features.",
                "We also examined the effectiveness of Gaussian embeddings for capturing the asymmetric nature of word embeddings but found it to be less effective than traditional word embeddings.",
                "Although we report a lower performance than that in #TARGET_REF , potentially indicating that predicting association strengths in word-sense ambiguous contexts is a harder task, we believe our results are a promising start.",
                "Training Gaussian embeddings on a larger corpus may lead to improved effectiveness.",
                "Future works should also consider incorporating word-sense frequencies or developing word-sense agnostic features, with a particular focus on asymmetric features."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In this paper we explored the effectiveness of various features for predicting Evocation, USF, and EAT association strengths, finding GloVe and Word2Vec cosine similarities as well as vector offsets to be the most useful features. We also examined the effectiveness of Gaussian embeddings for capturing the asymmetric nature of word embeddings but found it to be less effective than traditional word embeddings. Although we report a lower performance than that in #TARGET_REF , potentially indicating that predicting association strengths in word-sense ambiguous contexts is a harder task, we believe our results are a promising start. Training Gaussian embeddings on a larger corpus may lead to improved effectiveness. Future works should also consider incorporating word-sense frequencies or developing word-sense agnostic features, with a particular focus on asymmetric features.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Although we report a lower performance than that in #TARGET_REF , potentially indicating that predicting association strengths in word-sense ambiguous contexts is a harder task, we believe our results are a promising start.\"]}"
    },
    {
        "gold": {
            "text": [
                "Using the above features, we trained a multilayer perceptron for each of our three datasets; Evocation, USF, and EAT.",
                "In the case of Evocation, we discarded any synset information and Table 1 : Individual feature performance after 50 epochs simply use each synset's headword (e.g. given the sysnet entity.n.01, we only considered the word entity).",
                "Following the setup used in #TARGET_REF , all neural networks are trained using the Chainer 7 Python library with rectified linear units, dropout, and two hidden layers, each with 50% of the number of units in the input layer.",
                "All were trained on 80% of their respective dataset, with 20% held out for testing.",
                "Mean squared error was used as a loss function and optimization was performed using Adam algorithm (#REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Using the above features, we trained a multilayer perceptron for each of our three datasets; Evocation, USF, and EAT. In the case of Evocation, we discarded any synset information and Table 1 : Individual feature performance after 50 epochs simply use each synset's headword (e.g. given the sysnet entity.n.01, we only considered the word entity). Following the setup used in #TARGET_REF , all neural networks are trained using the Chainer 7 Python library with rectified linear units, dropout, and two hidden layers, each with 50% of the number of units in the input layer. All were trained on 80% of their respective dataset, with 20% held out for testing. Mean squared error was used as a loss function and optimization was performed using Adam algorithm (#REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following the setup used in #TARGET_REF , all neural networks are trained using the Chainer 7 Python library with rectified linear units, dropout, and two hidden layers, each with 50% of the number of units in the input layer.\"]}"
    },
    {
        "gold": {
            "text": [
                "Following the setup used in #REF , all neural networks are trained using the Chainer 7 Python library with rectified linear units, dropout, and two hidden layers, each with 50% of the number of units in the input layer.",
                "All were trained on 80% of their respective dataset, with 20% held out for testing.",
                "Mean squared error was used as a loss function and optimization was performed using Adam algorithm (#REF) .",
                "To act as a baseline, we also reimplemented the system described in #TARGET_REF and trained #TARGET_REF on the same 80/20 split of Evocation as our system.",
                "In addition to the reported results, we also performed feature selection experiments using 20% of the training sets as validation."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Following the setup used in #REF , all neural networks are trained using the Chainer 7 Python library with rectified linear units, dropout, and two hidden layers, each with 50% of the number of units in the input layer. All were trained on 80% of their respective dataset, with 20% held out for testing. Mean squared error was used as a loss function and optimization was performed using Adam algorithm (#REF) . To act as a baseline, we also reimplemented the system described in #TARGET_REF and trained #TARGET_REF on the same 80/20 split of Evocation as our system. In addition to the reported results, we also performed feature selection experiments using 20% of the training sets as validation.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"To act as a baseline, we also reimplemented the system described in #TARGET_REF and trained #TARGET_REF on the same 80/20 split of Evocation as our system.\"]}"
    },
    {
        "gold": {
            "text": [
                "This is not the case, with GloVe offsets being the best performing single feature on EAT and the removal of w2v offsets causing the greatest drop in performance in the EAT ablation tests.",
                "The results of our #TARGET_REF implementation are roughly comparable to those reported in the #TARGET_REF (r = 0.374, ρ = 0.401 compared to r = 0.439, ρ = 0.400).",
                "Our slightly lower Pearson's R may be due to differences in way we split our training and test data as well as due to randomness in the training process itself.",
                "On Evocation, our system does not perform as well as #REF .",
                "This is expected as we explicitly ignore any synset information and instead attempt to predict association strengths between word-sense ambiguous words."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "This is not the case, with GloVe offsets being the best performing single feature on EAT and the removal of w2v offsets causing the greatest drop in performance in the EAT ablation tests. The results of our #TARGET_REF implementation are roughly comparable to those reported in the #TARGET_REF (r = 0.374, ρ = 0.401 compared to r = 0.439, ρ = 0.400). Our slightly lower Pearson's R may be due to differences in way we split our training and test data as well as due to randomness in the training process itself. On Evocation, our system does not perform as well as #REF . This is expected as we explicitly ignore any synset information and instead attempt to predict association strengths between word-sense ambiguous words.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"The results of our #TARGET_REF implementation are roughly comparable to those reported in the #TARGET_REF (r = 0.374, \\u03c1 = 0.401 compared to r = 0.439, \\u03c1 = 0.400).\"]}"
    },
    {
        "gold": {
            "text": [
                "To cater to these target reader populations, language teachers, linguists and other editors are often called upon to manually adapt a text.",
                "To automate this time-consuming task, there has been much effort in developing systems for lexical simplification (#REF; #REF) and syntactic simplification (#REF; #REF) .",
                "The performance of the state-of-the-art systems has improved significantly #TARGET_REF; #REF) .",
                "Nonetheless, one cannot expect any single system, trained on a particular dataset, to simplify arbitrary texts in a way that would suit all readers -for example, the kinds of English words and structures suitable for a native speaker in Grade 6 are unlikely to be suitable for a non-native speaker in Grade 4.",
                "Hence, human effort is generally needed for modifying the system output."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "To cater to these target reader populations, language teachers, linguists and other editors are often called upon to manually adapt a text. To automate this time-consuming task, there has been much effort in developing systems for lexical simplification (#REF; #REF) and syntactic simplification (#REF; #REF) . The performance of the state-of-the-art systems has improved significantly #TARGET_REF; #REF) . Nonetheless, one cannot expect any single system, trained on a particular dataset, to simplify arbitrary texts in a way that would suit all readers -for example, the kinds of English words and structures suitable for a native speaker in Grade 6 are unlikely to be suitable for a non-native speaker in Grade 4. Hence, human effort is generally needed for modifying the system output.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The performance of the state-of-the-art systems has improved significantly #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "By default, the editor uses a list of approximately 4,000 words that all students in Hong Kong are expected to know upon graduation from primary school (EDB, 2012).",
                "However, the user can also upload his or her own vocabulary list.",
                "Given an input sentence, we first identify the target words, namely those words that do not appear in the vocabulary list.",
                "Following #TARGET_REF , our system simplifies neither proper nouns, as identified by the Natural Language Toolkit (#REF) , nor words in our stoplist, which are already simple.",
                "In terms of the three-step framework described above, we use the word2vec model 1 to retrieve candidates for substitution in the first step."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "By default, the editor uses a list of approximately 4,000 words that all students in Hong Kong are expected to know upon graduation from primary school (EDB, 2012). However, the user can also upload his or her own vocabulary list. Given an input sentence, we first identify the target words, namely those words that do not appear in the vocabulary list. Following #TARGET_REF , our system simplifies neither proper nouns, as identified by the Natural Language Toolkit (#REF) , nor words in our stoplist, which are already simple. In terms of the three-step framework described above, we use the word2vec model 1 to retrieve candidates for substitution in the first step.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following #TARGET_REF , our system simplifies neither proper nouns, as identified by the Natural Language Toolkit (#REF) , nor words in our stoplist, which are already simple.\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluated the performance of our algorithm on the Mechanical Turk Lexical Simplification Data Set #TARGET_REF .",
                "This dataset contains 500 manually annotated sentences; the target word in each sentence was annotated by 50 independent annotators.",
                "To simulate a teacher adapting an English text for Hong Kong pupils, we used the vocabulary list from the Hong Kong Education Bureau (EDB, 2012) .",
                "To enable automatic evaluation, we considered only the 249 sentences in the dataset whose target word is not in our vocabulary list, but whose human annotations contain at least one word in the list.",
                "Precision is at 31% for the top candidate; it is at 57% for the top ten candidates."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We evaluated the performance of our algorithm on the Mechanical Turk Lexical Simplification Data Set #TARGET_REF . This dataset contains 500 manually annotated sentences; the target word in each sentence was annotated by 50 independent annotators. To simulate a teacher adapting an English text for Hong Kong pupils, we used the vocabulary list from the Hong Kong Education Bureau (EDB, 2012) . To enable automatic evaluation, we considered only the 249 sentences in the dataset whose target word is not in our vocabulary list, but whose human annotations contain at least one word in the list. Precision is at 31% for the top candidate; it is at 57% for the top ten candidates.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We evaluated the performance of our algorithm on the Mechanical Turk Lexical Simplification Data Set #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluated the quality of syntactic simplification on the first 300 sentences in the Mechanical Turk Lexical Simplification Data Set #TARGET_REF .",
                "For each sentence, we asked a professor of linguistics to mark the types of syntactic simplification ("
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0
            ]
        },
        "input": "We evaluated the quality of syntactic simplification on the first 300 sentences in the Mechanical Turk Lexical Simplification Data Set #TARGET_REF . For each sentence, we asked a professor of linguistics to mark the types of syntactic simplification (",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We evaluated the quality of syntactic simplification on the first 300 sentences in the Mechanical Turk Lexical Simplification Data Set #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Explanations can help formulate clear and accurate mental models of autonomous systems and robots.",
                "Mental models, in cognitive theory, provide one view on how humans reason either functionally (understanding what the robot does) or structurally (understanding how it works).",
                "Mental models are important as they strongly impact how and whether robots and systems are used.",
                "In previous work, explanations have been categorised as either explaining 1) machine learning as in [11] who showed that they can increase trust; 2) explaining plans [2, 13] ; 3) verbalising robot [12] or agent rationalisation #TARGET_REF .",
                "However, humans do not present a constant verbalisation of their actions but they do need to be able to provide information on-demand about what they are doing and why during a live mission."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Explanations can help formulate clear and accurate mental models of autonomous systems and robots. Mental models, in cognitive theory, provide one view on how humans reason either functionally (understanding what the robot does) or structurally (understanding how it works). Mental models are important as they strongly impact how and whether robots and systems are used. In previous work, explanations have been categorised as either explaining 1) machine learning as in [11] who showed that they can increase trust; 2) explaining plans [2, 13] ; 3) verbalising robot [12] or agent rationalisation #TARGET_REF . However, humans do not present a constant verbalisation of their actions but they do need to be able to provide information on-demand about what they are doing and why during a live mission.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In previous work, explanations have been categorised as either explaining 1) machine learning as in [11] who showed that they can increase trust; 2) explaining plans [2, 13] ; 3) verbalising robot [12] or agent rationalisation #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "This has the advantage of being agnostic to the method of autonomy and could be used to describe rule-based autonomous behaviours but also complex deep models.",
                "Similar human-provided rationalisation has been used to generate explanations of deep neural models for game play #TARGET_REF .",
                "An interpretable model of autonomy was then derived from the expert, as partially shown in Figure 2 .",
                "If a why request is made, the decision tree is checked against the current mission status and history and the possible reasons are determined, along with a probability.",
                "As we can see from example outputs in Figure 3A , there may be multiple reasons with varying levels of certainty depending on the information available at a given point in the mission."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "This has the advantage of being agnostic to the method of autonomy and could be used to describe rule-based autonomous behaviours but also complex deep models. Similar human-provided rationalisation has been used to generate explanations of deep neural models for game play #TARGET_REF . An interpretable model of autonomy was then derived from the expert, as partially shown in Figure 2 . If a why request is made, the decision tree is checked against the current mission status and history and the possible reasons are determined, along with a probability. As we can see from example outputs in Figure 3A , there may be multiple reasons with varying levels of certainty depending on the information available at a given point in the mission.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Similar human-provided rationalisation has been used to generate explanations of deep neural models for game play #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "They investigated the influence of various hyperparameters and configurations.",
                "Our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects: 1) Their experiments are based on a BiLSTM with handcrafted word features, while our experiments are based on end-to-end neural models without human knowledge.",
                "2) Their system gives relatively low performances on standard benchmarks 2 , while ours can give comparable or better results with state-of-the-art models, rendering our observations more informative for practitioners.",
                "3) Our findings are more consistent with #TARGET_REF on configurations such as usefulness of character information (#REF; #REF) , optimizer ( #TARGET_REF; #REF; #REF) and tag scheme (#REF; #REF) .",
                "In contrast, many results of Reimers and Gurevych (2017b) contradict existing reports."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "They investigated the influence of various hyperparameters and configurations. Our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects: 1) Their experiments are based on a BiLSTM with handcrafted word features, while our experiments are based on end-to-end neural models without human knowledge. 2) Their system gives relatively low performances on standard benchmarks 2 , while ours can give comparable or better results with state-of-the-art models, rendering our observations more informative for practitioners. 3) Our findings are more consistent with #TARGET_REF on configurations such as usefulness of character information (#REF; #REF) , optimizer ( #TARGET_REF; #REF; #REF) and tag scheme (#REF; #REF) . In contrast, many results of Reimers and Gurevych (2017b) contradict existing reports.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"3) Our findings are more consistent with #TARGET_REF on configurations such as usefulness of character information (#REF; #REF) , optimizer ( #TARGET_REF; #REF; #REF) and tag scheme (#REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Optimizer.",
                "We compare different optimizers including SGD, Adagrad (#REF ), Adadelta (#REF , RMSProp (#REF) and Adam (#REF) .",
                "The results are shown in Figure 5 5 .",
                "In contrast to Reimers and Gurevych (2017b) , who reported that SGD is the worst optimizer, our results show that SGD outperforms all other optimizers significantly (p < 0.01), with a slower convergence process during training.",
                "Our observation is consistent with most literature ( #TARGET_REF; #REF; #REF) ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Optimizer. We compare different optimizers including SGD, Adagrad (#REF ), Adadelta (#REF , RMSProp (#REF) and Adam (#REF) . The results are shown in Figure 5 5 . In contrast to Reimers and Gurevych (2017b) , who reported that SGD is the worst optimizer, our results show that SGD outperforms all other optimizers significantly (p < 0.01), with a slower convergence process during training. Our observation is consistent with most literature ( #TARGET_REF; #REF; #REF) .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Our observation is consistent with most literature ( #TARGET_REF; #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Similarly, \"WLSTM\" and \"WCNN\" represent word LSTM and word CNN structure, respectively.",
                "As shown in Table 4 , most NER work focuses on WLSTM+CRF structures with different character sequence representations.",
                "We re-implement the structure of several reports ( #TARGET_REF; #REF; #REF) , which take the CCNN+WLSTM+CRF architecture.",
                "Our reproduced models give slightly better performances.",
                "The results of #REF can be reproduced by our CLSTM+WLSTM+CRF."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Similarly, \"WLSTM\" and \"WCNN\" represent word LSTM and word CNN structure, respectively. As shown in Table 4 , most NER work focuses on WLSTM+CRF structures with different character sequence representations. We re-implement the structure of several reports ( #TARGET_REF; #REF; #REF) , which take the CCNN+WLSTM+CRF architecture. Our reproduced models give slightly better performances. The results of #REF can be reproduced by our CLSTM+WLSTM+CRF.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We re-implement the structure of several reports ( #TARGET_REF; #REF; #REF) , which take the CCNN+WLSTM+CRF architecture.\"]}"
    },
    {
        "gold": {
            "text": [
                "Pronouns are frequently omitted in pro-drop languages, such as Chinese, generally leading to significant challenges with respect to the production of complete translations.",
                "Recently, #TARGET_REF proposed a novel reconstruction-based approach to alleviating dropped pronoun (DP) translation problems for neural machine translation models.",
                "In this work, we improve the original model from two perspectives.",
                "First, we employ a shared reconstructor to better exploit encoder and decoder representations.",
                "Second, we jointly learn to translate and predict DPs in an end-to-end manner, to avoid the errors propagated from an external DP prediction model."
            ],
            "label": [
                "BACK",
                "USE",
                "EXT"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "Pronouns are frequently omitted in pro-drop languages, such as Chinese, generally leading to significant challenges with respect to the production of complete translations. Recently, #TARGET_REF proposed a novel reconstruction-based approach to alleviating dropped pronoun (DP) translation problems for neural machine translation models. In this work, we improve the original model from two perspectives. First, we employ a shared reconstructor to better exploit encoder and decoder representations. Second, we jointly learn to translate and predict DPs in an end-to-end manner, to avoid the errors propagated from an external DP prediction model.",
        "output": "{\"label\": [\"BACK\", \"USE\", \"EXT\"], \"context\": [\"Recently, #TARGET_REF proposed a novel reconstruction-based approach to alleviating dropped pronoun (DP) translation problems for neural machine translation models.\", \"In this work, we improve the original model from two perspectives.\", \"First, we employ a shared reconstructor to better exploit encoder and decoder representations.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, in pro-drop languages such as Chinese and Japanese, pronouns are frequently omitted when their referents can be pragmatically inferred from the context.",
                "When translating sentences from a pro-drop language into a non-pro-drop language (e.g. Chinese-to-English), translation models generally fail to translate invisible dropped pronouns (DPs).",
                "This phenomenon leads to various translation problems in terms of completeness, syntax and even semantics of translations.",
                "A number of approaches have been investigated for DP translation (Le #REF; #REF; #REF #TARGET_REF .",
                "#REF is a pioneering work to model DP translation for neural machine trans- * Zhaopeng Tu is the corresponding author of the paper."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "However, in pro-drop languages such as Chinese and Japanese, pronouns are frequently omitted when their referents can be pragmatically inferred from the context. When translating sentences from a pro-drop language into a non-pro-drop language (e.g. Chinese-to-English), translation models generally fail to translate invisible dropped pronouns (DPs). This phenomenon leads to various translation problems in terms of completeness, syntax and even semantics of translations. A number of approaches have been investigated for DP translation (Le #REF; #REF; #REF #TARGET_REF . #REF is a pioneering work to model DP translation for neural machine trans- * Zhaopeng Tu is the corresponding author of the paper.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"A number of approaches have been investigated for DP translation (Le #REF; #REF; #REF #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "When translating sentences from a pro-drop language into a non-pro-drop language (e.g. Chinese-to-English), translation models generally fail to translate invisible dropped pronouns (DPs).",
                "This phenomenon leads to various translation problems in terms of completeness, syntax and even semantics of translations.",
                "A number of approaches have been investigated for DP translation (Le #REF; #REF; #REF #REF .",
                "#TARGET_REF is a pioneering work to model DP translation for neural machine trans- * Zhaopeng Tu is the corresponding author of the paper.",
                "This work was conducted when Longyue Wang was studying and Qun Liu was working at the ADAPT Centre in the School of Computing at Dublin City University."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "When translating sentences from a pro-drop language into a non-pro-drop language (e.g. Chinese-to-English), translation models generally fail to translate invisible dropped pronouns (DPs). This phenomenon leads to various translation problems in terms of completeness, syntax and even semantics of translations. A number of approaches have been investigated for DP translation (Le #REF; #REF; #REF #REF . #TARGET_REF is a pioneering work to model DP translation for neural machine trans- * Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Longyue Wang was studying and Qun Liu was working at the ADAPT Centre in the School of Computing at Dublin City University.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF is a pioneering work to model DP translation for neural machine trans- * Zhaopeng Tu is the corresponding author of the paper.\"]}"
    },
    {
        "gold": {
            "text": [
                "As shown in Figure 1 , #TARGET_REF introduced two independent reconstructors with their own parameters, which reconstruct the DPannotated source sentence from the encoder and decoder hidden states, respectively.",
                "The central Table 1 : Evaluation of external models on predicting the positions of DPs (\"DP Position\") and the exact words of DP (\"DP Words\").",
                "idea underpinning their approach is to guide the corresponding hidden states to embed the recalled source-side DP information and subsequently to help the NMT model generate the missing pronouns with these enhanced hidden representations.",
                "The DPs can be automatically annotated for training and test data using two different strategies (#REF) .",
                "In the training phase, where the target sentence is available, we annotate DPs for the source sentence using alignment information."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "As shown in Figure 1 , #TARGET_REF introduced two independent reconstructors with their own parameters, which reconstruct the DPannotated source sentence from the encoder and decoder hidden states, respectively. The central Table 1 : Evaluation of external models on predicting the positions of DPs (\"DP Position\") and the exact words of DP (\"DP Words\"). idea underpinning their approach is to guide the corresponding hidden states to embed the recalled source-side DP information and subsequently to help the NMT model generate the missing pronouns with these enhanced hidden representations. The DPs can be automatically annotated for training and test data using two different strategies (#REF) . In the training phase, where the target sentence is available, we annotate DPs for the source sentence using alignment information.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"As shown in Figure 1 , #TARGET_REF introduced two independent reconstructors with their own parameters, which reconstruct the DPannotated source sentence from the encoder and decoder hidden states, respectively.\"]}"
    },
    {
        "gold": {
            "text": [
                "In the training phase, where the target sentence is available, we annotate DPs for the source sentence using alignment information.",
                "These annotated source sentences can be used to build a neural-based DP predictor, which can be used to annotate test sentences since the target sentence is not available during the testing phase.",
                "As shown in Table 1 , Wang et al. (2016 #TARGET_REF explored to predict the exact DP words 1 , the accuracy of which is only 66% in F1-score.",
                "By analyzing the translation outputs, we found that 16.2% of errors are newly introduced and caused by errors from the DP predictor.",
                "Fortunately, the accuracy of predicting DP positions (DPPs) is much higher, which provides the chance to alleviate the error propagation problem."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "In the training phase, where the target sentence is available, we annotate DPs for the source sentence using alignment information. These annotated source sentences can be used to build a neural-based DP predictor, which can be used to annotate test sentences since the target sentence is not available during the testing phase. As shown in Table 1 , Wang et al. (2016 #TARGET_REF explored to predict the exact DP words 1 , the accuracy of which is only 66% in F1-score. By analyzing the translation outputs, we found that 16.2% of errors are newly introduced and caused by errors from the DP predictor. Fortunately, the accuracy of predicting DP positions (DPPs) is much higher, which provides the chance to alleviate the error propagation problem.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"These annotated source sentences can be used to build a neural-based DP predictor, which can be used to annotate test sentences since the target sentence is not available during the testing phase.\", \"As shown in Table 1 , Wang et al. (2016 #TARGET_REF explored to predict the exact DP words 1 , the accuracy of which is only 66% in F1-score.\"]}"
    },
    {
        "gold": {
            "text": [
                "To compare our work with the results reported by previous work #TARGET_REF , we conducted experiments on their released Chinese⇒English TV Subtitle corpus.",
                "2 The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sentence pairs, respectively.",
                "We used case-insensitive 4-gram NIST BLEU metrics (#REF) for evaluation, and sign-test (#REF) to test for statistical significance.",
                "We implemented our models on the code repository released by #REF .",
                "3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "To compare our work with the results reported by previous work #TARGET_REF , we conducted experiments on their released Chinese⇒English TV Subtitle corpus. 2 The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sentence pairs, respectively. We used case-insensitive 4-gram NIST BLEU metrics (#REF) for evaluation, and sign-test (#REF) to test for statistical significance. We implemented our models on the code repository released by #REF . 3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"To compare our work with the results reported by previous work #TARGET_REF , we conducted experiments on their released Chinese\\u21d2English TV Subtitle corpus.\"]}"
    },
    {
        "gold": {
            "text": [
                "To compare our work with the results reported by previous work (#REF) , we conducted experiments on their released Chinese⇒English TV Subtitle corpus.",
                "2 The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sentence pairs, respectively.",
                "We used case-insensitive 4-gram NIST BLEU metrics (#REF) for evaluation, and sign-test (#REF) to test for statistical significance.",
                "We implemented our models on the code repository released by #TARGET_REF .",
                "3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "To compare our work with the results reported by previous work (#REF) , we conducted experiments on their released Chinese⇒English TV Subtitle corpus. 2 The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sentence pairs, respectively. We used case-insensitive 4-gram NIST BLEU metrics (#REF) for evaluation, and sign-test (#REF) to test for statistical significance. We implemented our models on the code repository released by #TARGET_REF . 3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We implemented our models on the code repository released by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Baselines (Rows 1-4): The three baselines (Rows 1, 2, and 4) differ regarding the training data used.",
                "\"Separate-Recs⇒(+DPs)\" (Row 3) is the best model reported in #TARGET_REF , which we employed as another strong baseline.",
                "The baseline trained on the DPP-annotated data (\"Baseline (+DPPs)\", Row 4) outperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating DPs.",
                "It suggests the necessity of jointly learning to translate and predict DPs.",
                "Our Models (Rows 5-8): Using our shared reconstructor (Row 5) not only outperforms the corresponding baseline (Row 4), but also surpasses its separate reconstructor counterpart (Row 3)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Baselines (Rows 1-4): The three baselines (Rows 1, 2, and 4) differ regarding the training data used. \"Separate-Recs⇒(+DPs)\" (Row 3) is the best model reported in #TARGET_REF , which we employed as another strong baseline. The baseline trained on the DPP-annotated data (\"Baseline (+DPPs)\", Row 4) outperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating DPs. It suggests the necessity of jointly learning to translate and predict DPs. Our Models (Rows 5-8): Using our shared reconstructor (Row 5) not only outperforms the corresponding baseline (Row 4), but also surpasses its separate reconstructor counterpart (Row 3).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"\\\"Separate-Recs\\u21d2(+DPs)\\\" (Row 3) is the best model reported in #TARGET_REF , which we employed as another strong baseline.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this work, we propose to improve the original model from two perspectives.",
                "First, we use a shared reconstructor to read hidden states from both encoder and decoder.",
                "Second, we integrate a DP predictor into NMT to jointly learn to translate and predict DPs.",
                "Incorporating these as two auxiliary loss terms can guide both the encoder and decoder states to learn critical information relevant to DPs.",
                "Experimental results on a largescale Chinese-English subtitle corpus show that the two modifications can accumulatively improve translation performance, and the best result is +1.5 BLEU points better than that reported by #TARGET_REF ."
            ],
            "label": [
                "EXT",
                "DIFFER"
            ],
            "context": [
                1,
                1,
                1,
                1,
                1
            ]
        },
        "input": "In this work, we propose to improve the original model from two perspectives. First, we use a shared reconstructor to read hidden states from both encoder and decoder. Second, we integrate a DP predictor into NMT to jointly learn to translate and predict DPs. Incorporating these as two auxiliary loss terms can guide both the encoder and decoder states to learn critical information relevant to DPs. Experimental results on a largescale Chinese-English subtitle corpus show that the two modifications can accumulatively improve translation performance, and the best result is +1.5 BLEU points better than that reported by #TARGET_REF .",
        "output": "{\"label\": [\"EXT\", \"DIFFER\"], \"context\": [\"In this work, we propose to improve the original model from two perspectives.\", \"First, we use a shared reconstructor to read hidden states from both encoder and decoder.\", \"Second, we integrate a DP predictor into NMT to jointly learn to translate and predict DPs.\", \"Incorporating these as two auxiliary loss terms can guide both the encoder and decoder states to learn critical information relevant to DPs.\", \"Experimental results on a largescale Chinese-English subtitle corpus show that the two modifications can accumulatively improve translation performance, and the best result is +1.5 BLEU points better than that reported by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Following Table 2 : Evaluation of translation performance for Chinese-English. \"Baseline\" is trained and evaluated on the original data, while \"Baseline (+DPs)\" and \"Baseline (+DPPs)\" are trained on the data annotated with DPs and DPPs, respectively.",
                "Training and decoding (beam size is 10) speeds are measured in words/second.",
                "\" †\" and \" ‡\" indicate statistically significant difference (p < 0.01) from \"Baseline (+DDPs)\" and \"Separate-Recs⇒(+DPs)\", respectively.",
                "as a reranking technique to select the best translation candidate from the generated n-best list at testing time.",
                "Different from #TARGET_REF, we reconstruct DPP-annotated source sentence, which is predicted by an external model."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Following Table 2 : Evaluation of translation performance for Chinese-English. \"Baseline\" is trained and evaluated on the original data, while \"Baseline (+DPs)\" and \"Baseline (+DPPs)\" are trained on the data annotated with DPs and DPPs, respectively. Training and decoding (beam size is 10) speeds are measured in words/second. \" †\" and \" ‡\" indicate statistically significant difference (p < 0.01) from \"Baseline (+DDPs)\" and \"Separate-Recs⇒(+DPs)\", respectively. as a reranking technique to select the best translation candidate from the generated n-best list at testing time. Different from #TARGET_REF, we reconstruct DPP-annotated source sentence, which is predicted by an external model.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Different from #TARGET_REF, we reconstruct DPP-annotated source sentence, which is predicted by an external model.\"]}"
    },
    {
        "gold": {
            "text": [
                "We implemented our models on the code repository released by #REF .",
                "3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results.",
                "It should be emphasized that we did not use the pre-train strategy as done in #TARGET_REF , since we found training from scratch achieved a better performance in the shared reconstructor setting.",
                "2 https://github.com/longyuewangdcu/ tvsub 3 https://github.com/tuzhaopeng/nmt Table 2 shows the translation results.",
                "It is clear that the proposed models significantly outperform the baselines in all cases, although there are considerable differences among different variations."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We implemented our models on the code repository released by #REF . 3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results. It should be emphasized that we did not use the pre-train strategy as done in #TARGET_REF , since we found training from scratch achieved a better performance in the shared reconstructor setting. 2 https://github.com/longyuewangdcu/ tvsub 3 https://github.com/tuzhaopeng/nmt Table 2 shows the translation results. It is clear that the proposed models significantly outperform the baselines in all cases, although there are considerable differences among different variations.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"It should be emphasized that we did not use the pre-train strategy as done in #TARGET_REF , since we found training from scratch achieved a better performance in the shared reconstructor setting.\"]}"
    },
    {
        "gold": {
            "text": [
                "These results verify that shared reconstructor and jointly predicting DPs can accumulatively improve translation performance.",
                "Among the variations of shared reconstructors (Rows 6-8), we found that an interaction attention from encoder to decoder (Row 7) achieves the best performance, which is +3.45 BLEU points better than our baseline (Row 4) and +1.45 BLEU points better than the best result reported by #TARGET_REF (Row 3) .",
                "We attribute the superior performance of \"Shared-Rec enc→dec \" to the fact that the attention context over encoder representations embeds useful DP information, which can help to better attend to the representations of the corresponding pronouns in the decoder side.",
                "Similar to #REF , the proposed approach improves BLEU scores at the cost of decreased training and decoding speed, which is due to the large number of newly introduced parameters resulting from the incorporation of reconstructors into the NMT model.",
                "Table 3 : Evaluation of DP prediction accuracy. \"External\" model is separately trained on DP-annotated data with external neural methods (#REF) , while \"Joint\" model is jointly trained with the NMT model (Section 3.2)."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "These results verify that shared reconstructor and jointly predicting DPs can accumulatively improve translation performance. Among the variations of shared reconstructors (Rows 6-8), we found that an interaction attention from encoder to decoder (Row 7) achieves the best performance, which is +3.45 BLEU points better than our baseline (Row 4) and +1.45 BLEU points better than the best result reported by #TARGET_REF (Row 3) . We attribute the superior performance of \"Shared-Rec enc→dec \" to the fact that the attention context over encoder representations embeds useful DP information, which can help to better attend to the representations of the corresponding pronouns in the decoder side. Similar to #REF , the proposed approach improves BLEU scores at the cost of decreased training and decoding speed, which is due to the large number of newly introduced parameters resulting from the incorporation of reconstructors into the NMT model. Table 3 : Evaluation of DP prediction accuracy. \"External\" model is separately trained on DP-annotated data with external neural methods (#REF) , while \"Joint\" model is jointly trained with the NMT model (Section 3.2).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Among the variations of shared reconstructors (Rows 6-8), we found that an interaction attention from encoder to decoder (Row 7) achieves the best performance, which is +3.45 BLEU points better than our baseline (Row 4) and +1.45 BLEU points better than the best result reported by #TARGET_REF (Row 3) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We attribute this to the useful contextual information embedded in the reconstructor representations, which are used to generate the exact DP words.",
                "Table 4 : Translation results when reconstruction is used in training only while not used in testing.",
                "Table 4 lists translation results when the reconstruction model is used in training only.",
                "We can see that the proposed model outperforms both the strong baseline and the best model reported in #TARGET_REF .",
                "This is encouraging since no extra resources and computation are introduced to online decoding, which makes the approach highly practical, for example for translation in industry applications."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We attribute this to the useful contextual information embedded in the reconstructor representations, which are used to generate the exact DP words. Table 4 : Translation results when reconstruction is used in training only while not used in testing. Table 4 lists translation results when the reconstruction model is used in training only. We can see that the proposed model outperforms both the strong baseline and the best model reported in #TARGET_REF . This is encouraging since no extra resources and computation are introduced to online decoding, which makes the approach highly practical, for example for translation in industry applications.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We can see that the proposed model outperforms both the strong baseline and the best model reported in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "These results verify that shared reconstructor and jointly predicting DPs can accumulatively improve translation performance.",
                "Among the variations of shared reconstructors (Rows 6-8), we found that an interaction attention from encoder to decoder (Row 7) achieves the best performance, which is +3.45 BLEU points better than our baseline (Row 4) and +1.45 BLEU points better than the best result reported by #REF (Row 3) .",
                "We attribute the superior performance of \"Shared-Rec enc→dec \" to the fact that the attention context over encoder representations embeds useful DP information, which can help to better attend to the representations of the corresponding pronouns in the decoder side.",
                "Similar to #TARGET_REF , the proposed approach improves BLEU scores at the cost of decreased training and decoding speed, which is due to the large number of newly introduced parameters resulting from the incorporation of reconstructors into the NMT model.",
                "Table 3 : Evaluation of DP prediction accuracy. \"External\" model is separately trained on DP-annotated data with external neural methods (#REF) , while \"Joint\" model is jointly trained with the NMT model (Section 3.2)."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "These results verify that shared reconstructor and jointly predicting DPs can accumulatively improve translation performance. Among the variations of shared reconstructors (Rows 6-8), we found that an interaction attention from encoder to decoder (Row 7) achieves the best performance, which is +3.45 BLEU points better than our baseline (Row 4) and +1.45 BLEU points better than the best result reported by #REF (Row 3) . We attribute the superior performance of \"Shared-Rec enc→dec \" to the fact that the attention context over encoder representations embeds useful DP information, which can help to better attend to the representations of the corresponding pronouns in the decoder side. Similar to #TARGET_REF , the proposed approach improves BLEU scores at the cost of decreased training and decoding speed, which is due to the large number of newly introduced parameters resulting from the incorporation of reconstructors into the NMT model. Table 3 : Evaluation of DP prediction accuracy. \"External\" model is separately trained on DP-annotated data with external neural methods (#REF) , while \"Joint\" model is jointly trained with the NMT model (Section 3.2).",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Similar to #TARGET_REF , the proposed approach improves BLEU scores at the cost of decreased training and decoding speed, which is due to the large number of newly introduced parameters resulting from the incorporation of reconstructors into the NMT model.\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 1 .",
                "Compared to previous work (e.g. #TARGET_REF ) on the TextVQA task, our model, accompanied by rich features for image text, handles all modalities with a multimodal transformer over a joint embedding space instead of pairwise fusion mechanisms between modalities.",
                "Furthermore, answers are predicted through iterative decoding with pointers instead of one-step classification over a fixed vocabulary or copying single text token from the image.",
                "The TextVQA task distinctively requires models to see, read and reason over three modalities: the input question, the visual contents in the image such as visual objects, and the text in the image.",
                "Several approaches [44, 8, 37, 7] have been proposed for the TextVQA task, based on OCR results of the image."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Figure 1 . Compared to previous work (e.g. #TARGET_REF ) on the TextVQA task, our model, accompanied by rich features for image text, handles all modalities with a multimodal transformer over a joint embedding space instead of pairwise fusion mechanisms between modalities. Furthermore, answers are predicted through iterative decoding with pointers instead of one-step classification over a fixed vocabulary or copying single text token from the image. The TextVQA task distinctively requires models to see, read and reason over three modalities: the input question, the visual contents in the image such as visual objects, and the text in the image. Several approaches [44, 8, 37, 7] have been proposed for the TextVQA task, based on OCR results of the image.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Compared to previous work (e.g. #TARGET_REF ) on the TextVQA task, our model, accompanied by rich features for image text, handles all modalities with a multimodal transformer over a joint embedding space instead of pairwise fusion mechanisms between modalities.\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally, we introduce a rich representation for text tokens in the images based on multiple cues, including its word embedding, appearance, location, and character-level information.",
                "Our contributions in this paper are as follows: 1) We show that multiple (more than two) input modalities can be naturally fused and jointly modeled through our multimodal transformer architecture.",
                "2) Unlike previous work on TextVQA, our model reasons about the answer beyond a single classification step and predicts it through our pointeraugmented multi-step decoder.",
                "3) We adopt a rich feature representation for text tokens in images and show that it is better than features based only on word embedding in previous work.",
                "4) Our model significantly outperforms previous work on three challenging datasets for the TextVQA task: TextVQA #TARGET_REF (+25% relative), ST-VQA [8] (+65% relative), and OCR-VQA [37] (+32% relative)."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Finally, we introduce a rich representation for text tokens in the images based on multiple cues, including its word embedding, appearance, location, and character-level information. Our contributions in this paper are as follows: 1) We show that multiple (more than two) input modalities can be naturally fused and jointly modeled through our multimodal transformer architecture. 2) Unlike previous work on TextVQA, our model reasons about the answer beyond a single classification step and predicts it through our pointeraugmented multi-step decoder. 3) We adopt a rich feature representation for text tokens in images and show that it is better than features based only on word embedding in previous work. 4) Our model significantly outperforms previous work on three challenging datasets for the TextVQA task: TextVQA #TARGET_REF (+25% relative), ST-VQA [8] (+65% relative), and OCR-VQA [37] (+32% relative).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"4) Our model significantly outperforms previous work on three challenging datasets for the TextVQA task: TextVQA #TARGET_REF (+25% relative), ST-VQA [8] (+65% relative), and OCR-VQA [37] (+32% relative).\"]}"
    },
    {
        "gold": {
            "text": [
                "We fine-tune the last layer of the Faster R-CNN detector during training.",
                "Embedding of OCR tokens with rich representations.",
                "Intuitively, to represent text in images, one needs to encode not only its characters, but also its appearance (e.g. color, font, and background) and spatial location in the image (e.g. words appearing on the top of a book cover are more likely to be book titles).",
                "We follow this intuition in our model and use a rich OCR representation consisting of four types of features, which is shown in our experiments to be significantly better than word embedding (such as FastText) alone in prior work #TARGET_REF .",
                "After obtaining a set of N OCR tokens in an image through external OCR systems, from the n-th token (where n = 1, · · · , N ) we extract 1) a 300-dimensional #REF vector x ft n , which is a word embedding with sub-word information, 2) an appearance feature x fr n from the same Faster R-CNN detector in the object detection above, extracted via RoI-Pooling on the OCR token's bounding box, 3) a 604-dimensional Pyramidal Histogram of Characters (PHOC) [2] vector x p n , capturing what characters are present in the token -this is more robust to OCR errors and can be seen as a coarse character model, and 4) a 4-dimensional location feature x b n based on the OCR token's relative bounding box coordinates [x min /W im , y min /H im , x max /W im , y max /H im ]."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We fine-tune the last layer of the Faster R-CNN detector during training. Embedding of OCR tokens with rich representations. Intuitively, to represent text in images, one needs to encode not only its characters, but also its appearance (e.g. color, font, and background) and spatial location in the image (e.g. words appearing on the top of a book cover are more likely to be book titles). We follow this intuition in our model and use a rich OCR representation consisting of four types of features, which is shown in our experiments to be significantly better than word embedding (such as FastText) alone in prior work #TARGET_REF . After obtaining a set of N OCR tokens in an image through external OCR systems, from the n-th token (where n = 1, · · · , N ) we extract 1) a 300-dimensional #REF vector x ft n , which is a word embedding with sub-word information, 2) an appearance feature x fr n from the same Faster R-CNN detector in the object detection above, extracted via RoI-Pooling on the OCR token's bounding box, 3) a 604-dimensional Pyramidal Histogram of Characters (PHOC) [2] vector x p n , capturing what characters are present in the token -this is more robust to OCR errors and can be seen as a coarse character model, and 4) a 4-dimensional location feature x b n based on the OCR token's relative bounding box coordinates [x min /W im , y min /H im , x max /W im , y max /H im ].",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We follow this intuition in our model and use a rich OCR representation consisting of four types of features, which is shown in our experiments to be significantly better than word embedding (such as FastText) alone in prior work #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally, we extract text tokens on each image using the Rosetta OCR system [10] .",
                "Unlike the prior work LoRRA #TARGET_REF that uses a multilingual Rosetta version, in our model we use an English-only version of Rosetta that we find has higher recall.",
                "We refer to these two versions as Rosettaml and Rosetta-en, respectively.",
                "As mentioned in Sec. 3.1, from each OCR token we extract #REF feature, appearance feature from Faster R-CNN (FRCN), PHOC [2] feature, and bounding box (bbox) feature.",
                "In our multimodal transformer, we use L = 4 layers of multimodal transformer with 12 attention heads."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Finally, we extract text tokens on each image using the Rosetta OCR system [10] . Unlike the prior work LoRRA #TARGET_REF that uses a multilingual Rosetta version, in our model we use an English-only version of Rosetta that we find has higher recall. We refer to these two versions as Rosettaml and Rosetta-en, respectively. As mentioned in Sec. 3.1, from each OCR token we extract #REF feature, appearance feature from Faster R-CNN (FRCN), PHOC [2] feature, and bounding box (bbox) feature. In our multimodal transformer, we use L = 4 layers of multimodal transformer with 12 attention heads.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Unlike the prior work LoRRA #TARGET_REF that uses a multilingual Rosetta version, in our model we use an English-only version of Rosetta that we find has higher recall.\"]}"
    },
    {
        "gold": {
            "text": [
                "4 We compare are from fixed answer vocabulary).",
                "Compared to the previous work LoRRA #TARGET_REF which selects one answer from training set or copies only a single OCR token, our model can copy multiple OCR tokens and combine them with its fixed vocabulary through iterative decoding.",
                "our method to DCD [32] (the challenge winner, based on ensemble) and MSFT VTI [46] (the top entry after the challenge), both relying on one-step prediction.",
                "We show that our single model (line 10) significantly outperforms these challenge winning entries on the TextVQA test set by a large margin.",
                "We also experiment with using the ST-VQA dataset [8] as additional training data (a practice used by some of the previous challenge participants), which gives another 1% improvement and 40.46% final test accuracya new state-of-the-art on the TextVQA dataset."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "4 We compare are from fixed answer vocabulary). Compared to the previous work LoRRA #TARGET_REF which selects one answer from training set or copies only a single OCR token, our model can copy multiple OCR tokens and combine them with its fixed vocabulary through iterative decoding. our method to DCD [32] (the challenge winner, based on ensemble) and MSFT VTI [46] (the top entry after the challenge), both relying on one-step prediction. We show that our single model (line 10) significantly outperforms these challenge winning entries on the TextVQA test set by a large margin. We also experiment with using the ST-VQA dataset [8] as additional training data (a practice used by some of the previous challenge participants), which gives another 1% improvement and 40.46% final test accuracya new state-of-the-art on the TextVQA dataset.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Compared to the previous work LoRRA #TARGET_REF which selects one answer from training set or copies only a single OCR token, our model can copy multiple OCR tokens and combine them with its fixed vocabulary through iterative decoding.\"]}"
    },
    {
        "gold": {
            "text": [
                "During the iterative answer decoding process, at each step our M4C model can decode an answer word either from the model's fixed vocabulary, or from the OCR tokens extracted from the image.",
                "We find in our experiments that it is necessary to have both the fixed vocabulary space and the OCR tokens.",
                "Table 5 shows our ablation study where we remove the fixed answer vocabulary or the dynamic pointer network for OCR copying from our M4C.",
                "Both these two ablated versions have a large accuracy drop compared to our full model.",
                "However, we note that even without fixed answer vocabulary, our restricted model (M4C w/o fixed vocabulary in Table 5 ) still outperforms the previous work LoRRA #TARGET_REF , suggesting that it is particularly important to learn to copy multiple OCR tokens to form an answer (a key feature in our model but not in LoRRA)."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "During the iterative answer decoding process, at each step our M4C model can decode an answer word either from the model's fixed vocabulary, or from the OCR tokens extracted from the image. We find in our experiments that it is necessary to have both the fixed vocabulary space and the OCR tokens. Table 5 shows our ablation study where we remove the fixed answer vocabulary or the dynamic pointer network for OCR copying from our M4C. Both these two ablated versions have a large accuracy drop compared to our full model. However, we note that even without fixed answer vocabulary, our restricted model (M4C w/o fixed vocabulary in Table 5 ) still outperforms the previous work LoRRA #TARGET_REF , suggesting that it is particularly important to learn to copy multiple OCR tokens to form an answer (a key feature in our model but not in LoRRA).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"However, we note that even without fixed answer vocabulary, our restricted model (M4C w/o fixed vocabulary in Table 5 ) still outperforms the previous work LoRRA #TARGET_REF , suggesting that it is particularly important to learn to copy multiple OCR tokens to form an answer (a key feature in our model but not in LoRRA).\"]}"
    },
    {
        "gold": {
            "text": [
                "As it is intractable to have every possible text token in the answer vocabulary, copying text from the image would often be an easier option for answer prediction.",
                "Prior work has explored dynamically copying the inputs in different tasks such as text summarization [42] , knowledge retrieval [52] , and image captioning [35] based on Pointer #REF and its variants.",
                "For the TextVQA task, recent works #TARGET_REF 37] have proposed to copy OCR tokens by adding their indices to classifier outputs.",
                "However, apart from their limitation of copying only a single token (or block), one drawback of these approaches is that they require a pre-defined number of OCR tokens (since the classifier has a fixed output dimension) and their output is dependent on the ordering of the tokens.",
                "In this work, we overcome this drawback using a permutation-invariant pointer network together with our multimodal transformer."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                1
            ]
        },
        "input": "As it is intractable to have every possible text token in the answer vocabulary, copying text from the image would often be an easier option for answer prediction. Prior work has explored dynamically copying the inputs in different tasks such as text summarization [42] , knowledge retrieval [52] , and image captioning [35] based on Pointer #REF and its variants. For the TextVQA task, recent works #TARGET_REF 37] have proposed to copy OCR tokens by adding their indices to classifier outputs. However, apart from their limitation of copying only a single token (or block), one drawback of these approaches is that they require a pre-defined number of OCR tokens (since the classifier has a fixed output dimension) and their output is dependent on the ordering of the tokens. In this work, we overcome this drawback using a permutation-invariant pointer network together with our multimodal transformer.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"For the TextVQA task, recent works #TARGET_REF 37] have proposed to copy OCR tokens by adding their indices to classifier outputs.\", \"However, apart from their limitation of copying only a single token (or block), one drawback of these approaches is that they require a pre-defined number of OCR tokens (since the classifier has a fixed output dimension) and their output is dependent on the ordering of the tokens.\", \"In this work, we overcome this drawback using a permutation-invariant pointer network together with our multimodal transformer.\"]}"
    },
    {
        "gold": {
            "text": [
                "Embedding of detected objects.",
                "Given an image, we obtain a set of M visual objects through a pretrained detector (Faster R-CNN [41] in our case).",
                "Following prior work [3, 43, #TARGET_REF , we extract appearance feature x fr m using the detector's output from the m-th object (where m = 1, · · · , M ).",
                "To capture its location in the image, we introduce a 4-dimensional location fea-",
                "where W im and H im are image width and height respectively."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Embedding of detected objects. Given an image, we obtain a set of M visual objects through a pretrained detector (Faster R-CNN [41] in our case). Following prior work [3, 43, #TARGET_REF , we extract appearance feature x fr m using the detector's output from the m-th object (where m = 1, · · · , M ). To capture its location in the image, we introduce a 4-dimensional location fea- where W im and H im are image width and height respectively.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following prior work [3, 43, #TARGET_REF , we extract appearance feature x fr m using the detector's output from the m-th object (where m = 1, \\u00b7 \\u00b7 \\u00b7 , M ).\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate our model on three challenging datasets for the TextVQA task, including the TextVQA dataset #TARGET_REF , the ST-VQA dataset [8] , and the OCR-VQA dataset [37] .",
                "Our model outperforms previous work by a significant margin on all the three datasets."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0
            ]
        },
        "input": "We evaluate our model on three challenging datasets for the TextVQA task, including the TextVQA dataset #TARGET_REF , the ST-VQA dataset [8] , and the OCR-VQA dataset [37] . Our model outperforms previous work by a significant margin on all the three datasets.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We evaluate our model on three challenging datasets for the TextVQA task, including the TextVQA dataset #TARGET_REF , the ST-VQA dataset [8] , and the OCR-VQA dataset [37] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Similar to VQAv2 [17] , each question in the TextVQA dataset has 10 human annotated answers, and the final accuracy is measured via soft voting of the 10 answers.",
                "2 We use d = 768 as the dimensionality of the joint embedding space and extract question word features with BERT-BASE using the 768-dimensional outputs from its first three layers, which are fine-tuned during training.",
                "For visual objects, following #REF and LoRRA #TARGET_REF , we detect objects with a Faster R-CNN detector [41] pretrained on the Visual Genome dataset [26] , and keeps 100 top-scoring objects per image.",
                "Then, the fc6 feature vector is extracted from each detected object.",
                "We apply the Faster R-CNN fc7 weights on the extracted fc6 features to output 2048-dimensional fc7 appearance features and finetune fc7 weights during training."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Similar to VQAv2 [17] , each question in the TextVQA dataset has 10 human annotated answers, and the final accuracy is measured via soft voting of the 10 answers. 2 We use d = 768 as the dimensionality of the joint embedding space and extract question word features with BERT-BASE using the 768-dimensional outputs from its first three layers, which are fine-tuned during training. For visual objects, following #REF and LoRRA #TARGET_REF , we detect objects with a Faster R-CNN detector [41] pretrained on the Visual Genome dataset [26] , and keeps 100 top-scoring objects per image. Then, the fc6 feature vector is extracted from each detected object. We apply the Faster R-CNN fc7 weights on the extracted fc6 features to output 2048-dimensional fc7 appearance features and finetune fc7 weights during training.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For visual objects, following #REF and LoRRA #TARGET_REF , we detect objects with a Faster R-CNN detector [41] pretrained on the Visual Genome dataset [26] , and keeps 100 top-scoring objects per image.\"]}"
    },
    {
        "gold": {
            "text": [
                "The best snapshot is selected using the validation set accuracy.",
                "The entire training takes approximately 10 hours on 4 Nvidia Tesla V100 GPUs.",
                "As a notable prior work on this dataset, we show a stepby-step comparison with the LoRRA model #TARGET_REF .",
                "Ablations on pretrained question encoding and OCR systems.",
                "We first experiment with a restricted version of our model using the multimodal transformer architecture but without iterative decoding in answer prediction, i.e. M4C (w/o dec.) in Table 1 ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The best snapshot is selected using the validation set accuracy. The entire training takes approximately 10 hours on 4 Nvidia Tesla V100 GPUs. As a notable prior work on this dataset, we show a stepby-step comparison with the LoRRA model #TARGET_REF . Ablations on pretrained question encoding and OCR systems. We first experiment with a restricted version of our model using the multimodal transformer architecture but without iterative decoding in answer prediction, i.e. M4C (w/o dec.) in Table 1 .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"As a notable prior work on this dataset, we show a stepby-step comparison with the LoRRA model #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Here, we see that our model in line 8 still outperforms LoRRA (line 1) by as much as 9.5% (absolute) when using the same OCR system as LoRRA and even fewer pretrained components.",
                "We also analyze the performance of our model with respect to the maximum decoding steps, shown in Figure 3 , where decoding for multiple steps greatly improves the performance compared with a single step.",
                "Figure 4 shows qualitative examples (more examples in appendix) of our M4C model on the TextVQA dataset in comparison to LoRRA #TARGET_REF , where our model is capable of selecting multiple OCR tokens and combining them with its fixed vocabulary in predicted answers.",
                "Qualitative insights.",
                "When inspecting the errors, we find that a major source of errors is OCR failure (e.g. in the last example in Figure 4 , we find that the digits on the watch are not detected)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Here, we see that our model in line 8 still outperforms LoRRA (line 1) by as much as 9.5% (absolute) when using the same OCR system as LoRRA and even fewer pretrained components. We also analyze the performance of our model with respect to the maximum decoding steps, shown in Figure 3 , where decoding for multiple steps greatly improves the performance compared with a single step. Figure 4 shows qualitative examples (more examples in appendix) of our M4C model on the TextVQA dataset in comparison to LoRRA #TARGET_REF , where our model is capable of selecting multiple OCR tokens and combining them with its fixed vocabulary in predicted answers. Qualitative insights. When inspecting the errors, we find that a major source of errors is OCR failure (e.g. in the last example in Figure 4 , we find that the digits on the watch are not detected).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Figure 4 shows qualitative examples (more examples in appendix) of our M4C model on the TextVQA dataset in comparison to LoRRA #TARGET_REF , where our model is capable of selecting multiple OCR tokens and combining them with its fixed vocabulary in predicted answers.\"]}"
    },
    {
        "gold": {
            "text": [
                "Lexical simplification is the task to find and substitute a complex word or phrase in a sentence with its simpler synonymous expression.",
                "We define complex word as a word that has lexical and subjective difficulty in a sentence.",
                "It can help in reading comprehension for children and language learners (De #REF) .",
                "This task is a rather easier task which prepare a pair of complex and simple representations than a challenging task which changes the substitute pair in a given context #TARGET_REF; #REF) .",
                "Construction of a benchmark dataset is important to ensure the reliability and reproducibility of evaluation."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Lexical simplification is the task to find and substitute a complex word or phrase in a sentence with its simpler synonymous expression. We define complex word as a word that has lexical and subjective difficulty in a sentence. It can help in reading comprehension for children and language learners (De #REF) . This task is a rather easier task which prepare a pair of complex and simple representations than a challenging task which changes the substitute pair in a given context #TARGET_REF; #REF) . Construction of a benchmark dataset is important to ensure the reliability and reproducibility of evaluation.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Lexical simplification is the task to find and substitute a complex word or phrase in a sentence with its simpler synonymous expression.\", \"This task is a rather easier task which prepare a pair of complex and simple representations than a challenging task which changes the substitute pair in a given context #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The evaluation dataset for the English Lexical Simplification task #TARGET_REF Figure 1: A part of the dataset of #REF .",
                "notated on top of the evaluation dataset for English lexical substitution (#REF) .",
                "They asked university students to rerank substitutes according to simplification ranking.",
                "Sentences in their dataset do not always contain complex words, and it is not appropriate to evaluate simplification systems if a test sentence does not include any complex words.",
                "In addition, De #REF built an evaluation dataset for English lexical simplification based on that developed by #REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The evaluation dataset for the English Lexical Simplification task #TARGET_REF Figure 1: A part of the dataset of #REF . notated on top of the evaluation dataset for English lexical substitution (#REF) . They asked university students to rerank substitutes according to simplification ranking. Sentences in their dataset do not always contain complex words, and it is not appropriate to evaluate simplification systems if a test sentence does not include any complex words. In addition, De #REF built an evaluation dataset for English lexical simplification based on that developed by #REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The evaluation dataset for the English Lexical Simplification task #TARGET_REF Figure 1: A part of the dataset of #REF .\", \"notated on top of the evaluation dataset for English lexical substitution (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition, De #REF built an evaluation dataset for English lexical simplification based on that developed by #REF .",
                "They used Amazon's Mechanical Turk to rank substitutes and employed the reliability of annotators to remove outlier annotators and/or downweight unreliable annotators.",
                "The reliability was calculated on penalty based agreement (#REF) and Fleiss' Kappa.",
                "Unlike the dataset of #TARGET_REF , sentences in their dataset contain at least one complex word, but they might contain more than one complex word.",
                "Again, it is not adequate for the automatic evaluation of lexical simplification because the human ranking of the resulting simplification might be affected by the context containing complex words."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                1,
                0
            ]
        },
        "input": "In addition, De #REF built an evaluation dataset for English lexical simplification based on that developed by #REF . They used Amazon's Mechanical Turk to rank substitutes and employed the reliability of annotators to remove outlier annotators and/or downweight unreliable annotators. The reliability was calculated on penalty based agreement (#REF) and Fleiss' Kappa. Unlike the dataset of #TARGET_REF , sentences in their dataset contain at least one complex word, but they might contain more than one complex word. Again, it is not adequate for the automatic evaluation of lexical simplification because the human ranking of the resulting simplification might be affected by the context containing complex words.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In addition, De #REF built an evaluation dataset for English lexical simplification based on that developed by #REF .\", \"Unlike the dataset of #TARGET_REF , sentences in their dataset contain at least one complex word, but they might contain more than one complex word.\"]}"
    },
    {
        "gold": {
            "text": [
                "Unlike the dataset of #REF , sentences in their dataset contain at least one complex word, but they might contain more than one complex word.",
                "Again, it is not adequate for the automatic evaluation of lexical simplification because the human ranking of the resulting simplification might be affected by the context containing complex words.",
                "Furthermore, De Belder and Moens' (2012) dataset is too small to be used for achieving a reliable evaluation of lexical simplification systems.",
                "3 Problems in previous datasets for Japanese lexical simplification #REF followed #TARGET_REF to construct an evaluation dataset for Japanese lexical simplification.",
                "Namely, they split the data creation process into two steps: substitute extraction and simplification ranking."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Unlike the dataset of #REF , sentences in their dataset contain at least one complex word, but they might contain more than one complex word. Again, it is not adequate for the automatic evaluation of lexical simplification because the human ranking of the resulting simplification might be affected by the context containing complex words. Furthermore, De Belder and Moens' (2012) dataset is too small to be used for achieving a reliable evaluation of lexical simplification systems. 3 Problems in previous datasets for Japanese lexical simplification #REF followed #TARGET_REF to construct an evaluation dataset for Japanese lexical simplification. Namely, they split the data creation process into two steps: substitute extraction and simplification ranking.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"3 Problems in previous datasets for Japanese lexical simplification #REF followed #TARGET_REF to construct an evaluation dataset for Japanese lexical simplification.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our discussion in this paper is based on this example.",
                "Domain of the dataset is limited.",
                "Because #REF extracted sentences from a newswire corpus, their dataset has a poor variety of expression.",
                "English lexical simplification datasets #TARGET_REF; De #REF) do not have this problem because both of them use a balanced corpus of English (#REF) .",
                "Complex words might exist in context."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Our discussion in this paper is based on this example. Domain of the dataset is limited. Because #REF extracted sentences from a newswire corpus, their dataset has a poor variety of expression. English lexical simplification datasets #TARGET_REF; De #REF) do not have this problem because both of them use a balanced corpus of English (#REF) . Complex words might exist in context.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Because #REF extracted sentences from a newswire corpus, their dataset has a poor variety of expression.\", \"English lexical simplification datasets #TARGET_REF; De #REF) do not have this problem because both of them use a balanced corpus of English (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Ties are not permitted in simplification ranking.",
                "When each annotator assigns a simplification ranking to a substitution list, a tie cannot be assigned in previous datasets (#REF; #REF) .",
                "This deteriorates ranking consistency if some substitutes have a similar simplicity.",
                "De #REF allow ties in simplification ranking and report considerably higher agreement among annotators than #TARGET_REF .",
                "The method of ranking integration is naïve."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Ties are not permitted in simplification ranking. When each annotator assigns a simplification ranking to a substitution list, a tie cannot be assigned in previous datasets (#REF; #REF) . This deteriorates ranking consistency if some substitutes have a similar simplicity. De #REF allow ties in simplification ranking and report considerably higher agreement among annotators than #TARGET_REF . The method of ranking integration is naïve.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"De #REF allow ties in simplification ranking and report considerably higher agreement among annotators than #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "This deteriorates ranking consistency if some substitutes have a similar simplicity.",
                "De #REF allow ties in simplification ranking and report considerably higher agreement among annotators than #REF .",
                "The method of ranking integration is naïve.",
                "#REF and #TARGET_REF use an average score to integrate rankings, but it might be biased by outliers.",
                "De #REF report a slight increase in agreement by greedily removing annotators to maximize the agreement score."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "This deteriorates ranking consistency if some substitutes have a similar simplicity. De #REF allow ties in simplification ranking and report considerably higher agreement among annotators than #REF . The method of ranking integration is naïve. #REF and #TARGET_REF use an average score to integrate rankings, but it might be biased by outliers. De #REF report a slight increase in agreement by greedily removing annotators to maximize the agreement score.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"#REF and #TARGET_REF use an average score to integrate rankings, but it might be biased by outliers.\"]}"
    },
    {
        "gold": {
            "text": [
                "This score is higher than that from #REF by 0.190.",
                "This clearly shows the importance of allowing ties during the substitute ranking task.",
                "Table 2 shows the results of the ranking integration.",
                "Our method achieved better accuracy in ranking integration than previous methods #TARGET_REF; #REF) and is similar to the results from De #REF .",
                "This shows that the reliability score can be used for improving the quality."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "This score is higher than that from #REF by 0.190. This clearly shows the importance of allowing ties during the substitute ranking task. Table 2 shows the results of the ranking integration. Our method achieved better accuracy in ranking integration than previous methods #TARGET_REF; #REF) and is similar to the results from De #REF . This shows that the reliability score can be used for improving the quality.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Our method achieved better accuracy in ranking integration than previous methods #TARGET_REF; #REF) and is similar to the results from De #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, in this work, we extract sentences containing only one complex word.",
                "Ties are not permitted in simplification ranking.",
                "When each annotator assigns a simplification ranking to a substitution list, a tie cannot be assigned in previous datasets #TARGET_REF; #REF) .",
                "This deteriorates ranking consistency if some substitutes have a similar simplicity.",
                "De #REF allow ties in simplification ranking and report considerably higher agreement among annotators than #REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Therefore, in this work, we extract sentences containing only one complex word. Ties are not permitted in simplification ranking. When each annotator assigns a simplification ranking to a substitution list, a tie cannot be assigned in previous datasets #TARGET_REF; #REF) . This deteriorates ranking consistency if some substitutes have a similar simplicity. De #REF allow ties in simplification ranking and report considerably higher agreement among annotators than #REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Therefore, in this work, we extract sentences containing only one complex word.\", \"Ties are not permitted in simplification ranking.\", \"When each annotator assigns a simplification ranking to a substitution list, a tie cannot be assigned in previous datasets #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We applied the reliability score to exclude extraordinary annotators.",
                "Table 1 shows the characteristics of our dataset.",
                "It is about the same size as previous work #TARGET_REF; #REF) .",
                "Our dataset has two advantages: (1) improved correlation with human judgment by making a controlled and balanced dataset, and (2) enhanced consistency by allowing ties in ranking and removing outlier annotators.",
                "In the following subsections, we evaluate our dataset in detail."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "We applied the reliability score to exclude extraordinary annotators. Table 1 shows the characteristics of our dataset. It is about the same size as previous work #TARGET_REF; #REF) . Our dataset has two advantages: (1) improved correlation with human judgment by making a controlled and balanced dataset, and (2) enhanced consistency by allowing ties in ranking and removing outlier annotators. In the following subsections, we evaluate our dataset in detail.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Table 1 shows the characteristics of our dataset.\", \"It is about the same size as previous work #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Latent Dirichlet Allocation (LDA) is a topic modeling technique for textual data [5] that is widely applied in software engineering #TARGET_REF-4, 6, 10, 11, 14-16, 19, 24, 25] for different tasks such as requirements engineering [15] , software architecture [10] , source code analysis [9] , defect reports [16] , testing [14] and to bibliometric analysis of software engineering literature [11, 22] .",
                "A survey on topic modelling in software engineering has been conducted [24] and a book titled \"The art and science of analyzing software data\" [4] devoted a chapter for LDA analysis [6] .",
                "Many sources give methodological guidance on how to apply LDA topic modeling in software engineering [1, 3, 19] .",
                "Given all this, we think it is fair to say that LDA topic modelling is a relevant data analysis technique in empirical software engineering research.",
                "The quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [13] , perplexity of measure in the test data [13] , or Silhouette coefficient of resulting topics [19] ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Latent Dirichlet Allocation (LDA) is a topic modeling technique for textual data [5] that is widely applied in software engineering #TARGET_REF-4, 6, 10, 11, 14-16, 19, 24, 25] for different tasks such as requirements engineering [15] , software architecture [10] , source code analysis [9] , defect reports [16] , testing [14] and to bibliometric analysis of software engineering literature [11, 22] . A survey on topic modelling in software engineering has been conducted [24] and a book titled \"The art and science of analyzing software data\" [4] devoted a chapter for LDA analysis [6] . Many sources give methodological guidance on how to apply LDA topic modeling in software engineering [1, 3, 19] . Given all this, we think it is fair to say that LDA topic modelling is a relevant data analysis technique in empirical software engineering research. The quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [13] , perplexity of measure in the test data [13] , or Silhouette coefficient of resulting topics [19] .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Latent Dirichlet Allocation (LDA) is a topic modeling technique for textual data [5] that is widely applied in software engineering #TARGET_REF-4, 6, 10, 11, 14-16, 19, 24, 25] for different tasks such as requirements engineering [15] , software architecture [10] , source code analysis [9] , defect reports [16] , testing [14] and to bibliometric analysis of software engineering literature [11, 22] .\"]}"
    },
    {
        "gold": {
            "text": [
                "A survey on topic modelling in software engineering has been conducted [24] and a book titled \"The art and science of analyzing software data\" [4] devoted a chapter for LDA analysis [6] .",
                "Many sources give methodological guidance on how to apply LDA topic modeling in software engineering #TARGET_REF, 3, 19] .",
                "Given all this, we think it is fair to say that LDA topic modelling is a relevant data analysis technique in empirical software engineering research.",
                "The quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [13] , perplexity of measure in the test data [13] , or Silhouette coefficient of resulting topics [19] .",
                "Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs [1] ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "A survey on topic modelling in software engineering has been conducted [24] and a book titled \"The art and science of analyzing software data\" [4] devoted a chapter for LDA analysis [6] . Many sources give methodological guidance on how to apply LDA topic modeling in software engineering #TARGET_REF, 3, 19] . Given all this, we think it is fair to say that LDA topic modelling is a relevant data analysis technique in empirical software engineering research. The quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [13] , perplexity of measure in the test data [13] , or Silhouette coefficient of resulting topics [19] . Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs [1] .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Many sources give methodological guidance on how to apply LDA topic modeling in software engineering #TARGET_REF, 3, 19] .\"]}"
    },
    {
        "gold": {
            "text": [
                "A survey on topic modelling in software engineering has been conducted [24] and a book titled \"The art and science of analyzing software data\" [4] devoted a chapter for LDA analysis [6] .",
                "Many sources give methodological guidance on how to apply LDA topic modeling in software engineering [1, 3, 19] .",
                "Given all this, we think it is fair to say that LDA topic modelling is a relevant data analysis technique in empirical software engineering research.",
                "The quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [13] , perplexity of measure in the test data [13] , or Silhouette coefficient of resulting topics [19] .",
                "Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs #TARGET_REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "A survey on topic modelling in software engineering has been conducted [24] and a book titled \"The art and science of analyzing software data\" [4] devoted a chapter for LDA analysis [6] . Many sources give methodological guidance on how to apply LDA topic modeling in software engineering [1, 3, 19] . Given all this, we think it is fair to say that LDA topic modelling is a relevant data analysis technique in empirical software engineering research. The quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [13] , perplexity of measure in the test data [13] , or Silhouette coefficient of resulting topics [19] . Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs #TARGET_REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Given all this, we think it is fair to say that LDA topic modelling is a relevant data analysis technique in empirical software engineering research.",
                "The quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [13] , perplexity of measure in the test data [13] , or Silhouette coefficient of resulting topics [19] .",
                "Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs [1] .",
                "Recently, #TARGET_REF published a paper titled \"What is wrong with topic modeling? And how to fix it using search-based software engineering\", where #TARGET_REF claimed that the instability of topics is one major shortcoming of this technique.",
                "Indeed, studies could result in wrong conclusions if the results are based on instable topics."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Given all this, we think it is fair to say that LDA topic modelling is a relevant data analysis technique in empirical software engineering research. The quality of the resulting topic model can be evaluated with multiple metrics some inspired by mathematics such as the posterior probability of the topic model given the data [13] , perplexity of measure in the test data [13] , or Silhouette coefficient of resulting topics [19] . Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs [1] . Recently, #TARGET_REF published a paper titled \"What is wrong with topic modeling? And how to fix it using search-based software engineering\", where #TARGET_REF claimed that the instability of topics is one major shortcoming of this technique. Indeed, studies could result in wrong conclusions if the results are based on instable topics.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Recently, #TARGET_REF published a paper titled \\\"What is wrong with topic modeling? And how to fix it using search-based software engineering\\\", where #TARGET_REF claimed that the instability of topics is one major shortcoming of this technique.\"]}"
    },
    {
        "gold": {
            "text": [
                "Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs [1] .",
                "Recently, #REF published a paper titled \"What is wrong with topic modeling? And how to fix it using search-based software engineering\", where they claimed that the instability of topics is one major shortcoming of this technique.",
                "Indeed, studies could result in wrong conclusions if the results are based on instable topics.",
                "#TARGET_REF proposed using a differential evolution search algorithm to find the input parameters which maximize the topic model stability measured as the similarity of topics between multiple runs.",
                "This method reduces instability by finding optimal input parameter settings, but only uses the result of one LDA run which can still have some instable topics."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Other target metrics are based on empirical observations such as coherence, which measures topic model quality using word co-occurrences in publicly available texts [23] , or stability which investigates similarity of topics between different runs [1] . Recently, #REF published a paper titled \"What is wrong with topic modeling? And how to fix it using search-based software engineering\", where they claimed that the instability of topics is one major shortcoming of this technique. Indeed, studies could result in wrong conclusions if the results are based on instable topics. #TARGET_REF proposed using a differential evolution search algorithm to find the input parameters which maximize the topic model stability measured as the similarity of topics between multiple runs. This method reduces instability by finding optimal input parameter settings, but only uses the result of one LDA run which can still have some instable topics.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF proposed using a differential evolution search algorithm to find the input parameters which maximize the topic model stability measured as the similarity of topics between multiple runs.\"]}"
    },
    {
        "gold": {
            "text": [
                "Before topic generation, LDA requires that we set the input parameters such as the number of topics k, and hyper priors α and β.",
                "Past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [19] or differential evolution #TARGET_REF .",
                "As pointed out in Section 1, what is optimal can be measured with many metrics such as perplexity [13] , stability [1] , or coherence [23] .",
                "The stability of a topic model can be defined as the model's ability to replicate its solutions [8] .",
                "Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm [1] ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Before topic generation, LDA requires that we set the input parameters such as the number of topics k, and hyper priors α and β. Past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [19] or differential evolution #TARGET_REF . As pointed out in Section 1, what is optimal can be measured with many metrics such as perplexity [13] , stability [1] , or coherence [23] . The stability of a topic model can be defined as the model's ability to replicate its solutions [8] . Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm [1] .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [19] or differential evolution #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Before topic generation, LDA requires that we set the input parameters such as the number of topics k, and hyper priors α and β.",
                "Past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [19] or differential evolution [1] .",
                "As pointed out in Section 1, what is optimal can be measured with many metrics such as perplexity [13] , stability #TARGET_REF , or coherence [23] .",
                "The stability of a topic model can be defined as the model's ability to replicate its solutions [8] .",
                "Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm [1] ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Before topic generation, LDA requires that we set the input parameters such as the number of topics k, and hyper priors α and β. Past work in software engineering has used different techniques to find optimal input parameters such as genetic algorithms [19] or differential evolution [1] . As pointed out in Section 1, what is optimal can be measured with many metrics such as perplexity [13] , stability #TARGET_REF , or coherence [23] . The stability of a topic model can be defined as the model's ability to replicate its solutions [8] . Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm [1] .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"As pointed out in Section 1, what is optimal can be measured with many metrics such as perplexity [13] , stability #TARGET_REF , or coherence [23] .\"]}"
    },
    {
        "gold": {
            "text": [
                "The stability of a topic model can be defined as the model's ability to replicate its solutions [8] .",
                "Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm #TARGET_REF .",
                "Past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model [1, 8, 12] .",
                "We think using the results of a single LDA run, whether optimized for stability or not, is dangerous as perfect stability is impossible to reach.",
                "The next section shows a method that can be used to make more informed decisions."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The stability of a topic model can be defined as the model's ability to replicate its solutions [8] . Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm #TARGET_REF . Past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model [1, 8, 12] . We think using the results of a single LDA run, whether optimized for stability or not, is dangerous as perfect stability is impossible to reach. The next section shows a method that can be used to make more informed decisions.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The stability of a topic model can be defined as the model's ability to replicate its solutions [8] .",
                "Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm [1] .",
                "Past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model #TARGET_REF, 8, 12] .",
                "We think using the results of a single LDA run, whether optimized for stability or not, is dangerous as perfect stability is impossible to reach.",
                "The next section shows a method that can be used to make more informed decisions."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The stability of a topic model can be defined as the model's ability to replicate its solutions [8] . Instability (the lack of stability) is caused by the non-deterministic nature of Monte-Carlo simulation that is part of the LDA algorithm [1] . Past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model #TARGET_REF, 8, 12] . We think using the results of a single LDA run, whether optimized for stability or not, is dangerous as perfect stability is impossible to reach. The next section shows a method that can be used to make more informed decisions.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Past work has shown different stability measures and how to optimize the input parameters to provide a stable topic model #TARGET_REF, 8, 12] .\"]}"
    },
    {
        "gold": {
            "text": [
                "A problem occurs if two topics have the same words but in reverse order, the rank correlation between the topics would be -1 while one would still consider these two topics somewhat similar due to the same top words.",
                "Another anomaly is that for two topics with no intersecting top ten words, we would get a better Spearman correlation value than -1 (-0.86).",
                "Third, we can measure Jaccard similarity between the top words of any two topics.",
                "Extended Jaccard measures have been used in LDA stability task optimization #TARGET_REF 12] .",
                "When two topics have all the same top words, the Jaccard similarity would be 1."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "A problem occurs if two topics have the same words but in reverse order, the rank correlation between the topics would be -1 while one would still consider these two topics somewhat similar due to the same top words. Another anomaly is that for two topics with no intersecting top ten words, we would get a better Spearman correlation value than -1 (-0.86). Third, we can measure Jaccard similarity between the top words of any two topics. Extended Jaccard measures have been used in LDA stability task optimization #TARGET_REF 12] . When two topics have all the same top words, the Jaccard similarity would be 1.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Extended Jaccard measures have been used in LDA stability task optimization #TARGET_REF 12] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Past work in software engineering #TARGET_REF and machine learning [12] point out that LDA instability may lead to incorrect conclusions and proposes input parameter optimization to alleviate the problem.",
                "This paper suggests performing replicated runs, clustering the results and measuring the topic stability.",
                "These approach are not alternative but additive.",
                "Our approach can be combined with any LDA optimization technique that relies on input parameter optimization.",
                "Finally, our approach shows topic stability by providing a metric of topic stability and allowing further investigation of the clusters when desired."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Past work in software engineering #TARGET_REF and machine learning [12] point out that LDA instability may lead to incorrect conclusions and proposes input parameter optimization to alleviate the problem. This paper suggests performing replicated runs, clustering the results and measuring the topic stability. These approach are not alternative but additive. Our approach can be combined with any LDA optimization technique that relies on input parameter optimization. Finally, our approach shows topic stability by providing a metric of topic stability and allowing further investigation of the clusters when desired.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Past work in software engineering #TARGET_REF and machine learning [12] point out that LDA instability may lead to incorrect conclusions and proposes input parameter optimization to alleviate the problem.\"]}"
    },
    {
        "gold": {
            "text": [
                "We calculate automatic measures for each image-retrieved sentence pair against the five reference descriptions for the original image.",
                "The test data of #REF contains 101 images paired with three reference descriptions.",
                "The images were taken from the PAS-CAL VOC Action Recognition Task, the reference descriptions were collected from Mechanical Turk, and the judgements were also collected from Mechanical Turk.",
                "#TARGET_REF generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1-5 for each imagedescription pair, resulting in a total of 2,042 human judgement-description pairings.",
                "In this analysis, we use only the first sentence of the description, which describes the event depicted in the image."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "We calculate automatic measures for each image-retrieved sentence pair against the five reference descriptions for the original image. The test data of #REF contains 101 images paired with three reference descriptions. The images were taken from the PAS-CAL VOC Action Recognition Task, the reference descriptions were collected from Mechanical Turk, and the judgements were also collected from Mechanical Turk. #TARGET_REF generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1-5 for each imagedescription pair, resulting in a total of 2,042 human judgement-description pairings. In this analysis, we use only the first sentence of the description, which describes the event depicted in the image.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"#TARGET_REF generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1-5 for each imagedescription pair, resulting in a total of 2,042 human judgement-description pairings.\", \"In this analysis, we use only the first sentence of the description, which describes the event depicted in the image.\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally, Meteor is most strongly correlated measure against human judgements.",
                "A similar pattern is observed in the #TARGET_REF data set, though the correlations are lower across all measures.",
                "This could be caused by the smaller sample size or because the descriptions were generated by a computer, and not retrieved from a collection of human-written descriptions containing the goldstandard text, as in the Flickr8K data set.",
                "Figure 3 shows two images from the test collection of the Flickr8K data set with a low Meteor score and a maximum human judgement of semantic correctness.",
                "The main difference between the candidates and references are in deciding what to describe (content selection), and how to describe it (realisation)."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Finally, Meteor is most strongly correlated measure against human judgements. A similar pattern is observed in the #TARGET_REF data set, though the correlations are lower across all measures. This could be caused by the smaller sample size or because the descriptions were generated by a computer, and not retrieved from a collection of human-written descriptions containing the goldstandard text, as in the Flickr8K data set. Figure 3 shows two images from the test collection of the Flickr8K data set with a low Meteor score and a maximum human judgement of semantic correctness. The main difference between the candidates and references are in deciding what to describe (content selection), and how to describe it (realisation).",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"A similar pattern is observed in the #TARGET_REF data set, though the correlations are lower across all measures.\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare them with the original model released by #TARGET_REF.",
                "Both intrinsic and extrinsic (downstream) evaluations, including Named Entity Recognition (NER) and Sentiment Analysis (SA) were carried out.",
                "The downstream tasks reveal that the best model is task-specific, high analogy scores don't necessarily correlate positively with F1 scores and the same applies for more data.",
                "Increasing vector dimension size after a point leads to poor quality or performance.",
                "If ethical considerations to save time, energy and the environment are made, then reasonably smaller corpora may do just as well or even better in some cases."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We compare them with the original model released by #TARGET_REF. Both intrinsic and extrinsic (downstream) evaluations, including Named Entity Recognition (NER) and Sentiment Analysis (SA) were carried out. The downstream tasks reveal that the best model is task-specific, high analogy scores don't necessarily correlate positively with F1 scores and the same applies for more data. Increasing vector dimension size after a point leads to poor quality or performance. If ethical considerations to save time, energy and the environment are made, then reasonably smaller corpora may do just as well or even better in some cases.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We compare them with the original model released by #TARGET_REF.\"]}"
    },
    {
        "gold": {
            "text": [
                "Hyper-parameter details of the two networks for the downstream tasks are given in table 2.",
                "The metrics for extrinsic evaluation include F1, precision, recall and accuracy scores.",
                "In both tasks, the default pytorch embedding was tested before being replaced by pre-trained embeddings released by #TARGET_REF and ours.",
                "In each case, the dataset was shuffled before training and split in the ratio 70:15:15 for training, validation (dev) and test sets.",
                "Batch size of 64 was used."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Hyper-parameter details of the two networks for the downstream tasks are given in table 2. The metrics for extrinsic evaluation include F1, precision, recall and accuracy scores. In both tasks, the default pytorch embedding was tested before being replaced by pre-trained embeddings released by #TARGET_REF and ours. In each case, the dataset was shuffled before training and split in the ratio 70:15:15 for training, validation (dev) and test sets. Batch size of 64 was used.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In both tasks, the default pytorch embedding was tested before being replaced by pre-trained embeddings released by #TARGET_REF and ours.\"]}"
    },
    {
        "gold": {
            "text": [
                "Both intrinsic and extrinsic (downstream) evaluations, including Named Entity Recognition (NER) and Sentiment Analysis (SA) were carried out.",
                "The downstream tasks reveal that the best model is task-specific, high analogy scores don't necessarily correlate positively with F1 scores and the same applies for more data.",
                "Increasing vector dimension size after a point leads to poor quality or performance.",
                "If ethical considerations to save time, energy and the environment are made, then reasonably smaller corpora may do just as well or even better in some cases.",
                "Besides, using a small corpus, we obtain better human-assigned WordSim scores, corresponding Spearman correlation and better downstream (NER & SA) performance compared to #TARGET_REFs model, trained on 100 billion word corpus."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Both intrinsic and extrinsic (downstream) evaluations, including Named Entity Recognition (NER) and Sentiment Analysis (SA) were carried out. The downstream tasks reveal that the best model is task-specific, high analogy scores don't necessarily correlate positively with F1 scores and the same applies for more data. Increasing vector dimension size after a point leads to poor quality or performance. If ethical considerations to save time, energy and the environment are made, then reasonably smaller corpora may do just as well or even better in some cases. Besides, using a small corpus, we obtain better human-assigned WordSim scores, corresponding Spearman correlation and better downstream (NER & SA) performance compared to #TARGET_REFs model, trained on 100 billion word corpus.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Besides, using a small corpus, we obtain better human-assigned WordSim scores, corresponding Spearman correlation and better downstream (NER & SA) performance compared to #TARGET_REFs model, trained on 100 billion word corpus.\"]}"
    },
    {
        "gold": {
            "text": [
                "In terms of analogy score, for 10 epochs, w8s0h0 performs best while w8s1h0 performs best in terms of WordSim and corresponding Spearman correlation.",
                "Meanwhile, increasing the corpus size to BW, w4s1h0 performs best in terms of analogy score while w8s1h0 maintains its position as the best in terms of WordSim and Spearman correlation.",
                "Besides considering quality metrics, it can be observed from table 4 that comparative ratio of values between the models is not commensurate with the results in intrinsic or extrinsic values, especially when we consider the amount of time and energy spent, since more training time results in more energy consumption Information on the length of training time for the released #TARGET_REF model is not readily available.",
                "However, it's interesting to note that their presumed best model, which was released is also s1h0.",
                "Its analogy score, which we tested and report, is confirmed in their paper."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "In terms of analogy score, for 10 epochs, w8s0h0 performs best while w8s1h0 performs best in terms of WordSim and corresponding Spearman correlation. Meanwhile, increasing the corpus size to BW, w4s1h0 performs best in terms of analogy score while w8s1h0 maintains its position as the best in terms of WordSim and Spearman correlation. Besides considering quality metrics, it can be observed from table 4 that comparative ratio of values between the models is not commensurate with the results in intrinsic or extrinsic values, especially when we consider the amount of time and energy spent, since more training time results in more energy consumption Information on the length of training time for the released #TARGET_REF model is not readily available. However, it's interesting to note that their presumed best model, which was released is also s1h0. Its analogy score, which we tested and report, is confirmed in their paper.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Meanwhile, increasing the corpus size to BW, w4s1h0 performs best in terms of analogy score while w8s1h0 maintains its position as the best in terms of WordSim and Spearman correlation.\", \"Besides considering quality metrics, it can be observed from table 4 that comparative ratio of values between the models is not commensurate with the results in intrinsic or extrinsic values, especially when we consider the amount of time and energy spent, since more training time results in more energy consumption Information on the length of training time for the released #TARGET_REF model is not readily available.\"]}"
    },
    {
        "gold": {
            "text": [
                "This trend is true for all combinations for all tests.",
                "Polynomial interpolation may be used to determine the optimal dimension in both corpora.",
                "Our models are available for confirmation and source codes are available on github.",
                "2 With regards to NER, most pretrained embeddings outperformed the default pytorch embedding, with our BW w4s1h0 model (which is best in BW analogy score) performing best in F1 score and closely followed by #TARGET_REF model.",
                "On the other hand, with regards to SA, pytorch embedding outperformed the pretrained embeddings but was closely followed by our SW w8s0h0 model (which also had the best SW analogy score)."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "This trend is true for all combinations for all tests. Polynomial interpolation may be used to determine the optimal dimension in both corpora. Our models are available for confirmation and source codes are available on github. 2 With regards to NER, most pretrained embeddings outperformed the default pytorch embedding, with our BW w4s1h0 model (which is best in BW analogy score) performing best in F1 score and closely followed by #TARGET_REF model. On the other hand, with regards to SA, pytorch embedding outperformed the pretrained embeddings but was closely followed by our SW w8s0h0 model (which also had the best SW analogy score).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"2 With regards to NER, most pretrained embeddings outperformed the default pytorch embedding, with our BW w4s1h0 model (which is best in BW analogy score) performing best in F1 score and closely followed by #TARGET_REF model.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our models are available for confirmation and source codes are available on github.",
                "2 With regards to NER, most pretrained embeddings outperformed the default pytorch embedding, with our BW w4s1h0 model (which is best in BW analogy score) performing best in F1 score and closely followed by Mikolov et al. (2013a) model.",
                "On the other hand, with regards to SA, pytorch embedding outperformed the pretrained embeddings but was closely followed by our SW w8s0h0 model (which also had the best SW analogy score).",
                "#TARGET_REF performed second worst of all, despite originating from a very huge corpus.",
                "The combinations w8s0h0 & w4s0h0 of SW performed reasonably well in both extrinsic tasks, just as the default pytorch embedding did."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Our models are available for confirmation and source codes are available on github. 2 With regards to NER, most pretrained embeddings outperformed the default pytorch embedding, with our BW w4s1h0 model (which is best in BW analogy score) performing best in F1 score and closely followed by Mikolov et al. (2013a) model. On the other hand, with regards to SA, pytorch embedding outperformed the pretrained embeddings but was closely followed by our SW w8s0h0 model (which also had the best SW analogy score). #TARGET_REF performed second worst of all, despite originating from a very huge corpus. The combinations w8s0h0 & w4s0h0 of SW performed reasonably well in both extrinsic tasks, just as the default pytorch embedding did.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"#TARGET_REF performed second worst of all, despite originating from a very huge corpus.\"]}"
    },
    {
        "gold": {
            "text": [
                "There have been many implementations of the word2vec model in either of the two architectures it provides: continuous skipgram and continuous bag of words (CBoW) ( #TARGET_REF ).",
                "Similar distributed models of word or subword embeddings (or vector representations) find usage in sota, deep neural networks like Bidirectional Encoder Representations from Transformers (BERT) and its successors (#REF ; ; #REF ).",
                "These deep networks generate contextual representations of words after been trained for extended periods on large corpora, unsupervised, using the attention mechanisms (#REF ).",
                "It has been observed that various hyper-parameter combinations have been used in different research involving word2vec with the possibility of many of them being sub-optimal (#REF ; #REF ; #REF ).",
                "Therefore, the authors seek to address the research question: what is the optimal combination of word2vec hyperparameters for intrinsic and extrinsic NLP purposes?"
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "There have been many implementations of the word2vec model in either of the two architectures it provides: continuous skipgram and continuous bag of words (CBoW) ( #TARGET_REF ). Similar distributed models of word or subword embeddings (or vector representations) find usage in sota, deep neural networks like Bidirectional Encoder Representations from Transformers (BERT) and its successors (#REF ; ; #REF ). These deep networks generate contextual representations of words after been trained for extended periods on large corpora, unsupervised, using the attention mechanisms (#REF ). It has been observed that various hyper-parameter combinations have been used in different research involving word2vec with the possibility of many of them being sub-optimal (#REF ; #REF ; #REF ). Therefore, the authors seek to address the research question: what is the optimal combination of word2vec hyperparameters for intrinsic and extrinsic NLP purposes?",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"There have been many implementations of the word2vec model in either of the two architectures it provides: continuous skipgram and continuous bag of words (CBoW) ( #TARGET_REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "Breaking away from the non-distributed (high-dimensional, sparse) representations of words, typical of traditional bag-of-words or one-hot-encoding (#REF ), #TARGET_REF created word2vec.",
                "Word2Vec consists of two shallow neural network architectures: continuous skipgram and CBoW. It uses distributed (low-dimensional, dense) representations of words that group similar words.",
                "This new model traded the complexity of deep neural network architectures, by other researchers, for more efficient training over large corpora.",
                "Its architectures have two training algorithms: negative sampling and hierarchical softmax (Mikolov et al. (2013b) ).",
                "The released model was trained on Google news dataset of 100 billion words."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Breaking away from the non-distributed (high-dimensional, sparse) representations of words, typical of traditional bag-of-words or one-hot-encoding (#REF ), #TARGET_REF created word2vec. Word2Vec consists of two shallow neural network architectures: continuous skipgram and CBoW. It uses distributed (low-dimensional, dense) representations of words that group similar words. This new model traded the complexity of deep neural network architectures, by other researchers, for more efficient training over large corpora. Its architectures have two training algorithms: negative sampling and hierarchical softmax (Mikolov et al. (2013b) ). The released model was trained on Google news dataset of 100 billion words.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Breaking away from the non-distributed (high-dimensional, sparse) representations of words, typical of traditional bag-of-words or one-hot-encoding (#REF ), #TARGET_REF created word2vec.\"]}"
    },
    {
        "gold": {
            "text": [
                "This is similar to the traditional bag-of-words, which is oblivious of the order of words in its sequence.",
                "A loglinear classifier is used in both architectures ( #TARGET_REF ).",
                "In further work, they extended the model to be able to do phrase representations and subsample frequent words (Mikolov et al. (2013b) ).",
                "Being a Neural Network Language Model (NNLM), word2vec assigns probabilities to words in a sequence, like other NNLMs such as feedforward networks or recurrent neural networks (#REF ).",
                "Earlier models like latent dirichlet allocation (LDA) and latent semantic analysis (LSA) exist and effectively achieve low dimensional vectors by matrix factorization (#REF ; #REF )."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "This is similar to the traditional bag-of-words, which is oblivious of the order of words in its sequence. A loglinear classifier is used in both architectures ( #TARGET_REF ). In further work, they extended the model to be able to do phrase representations and subsample frequent words (Mikolov et al. (2013b) ). Being a Neural Network Language Model (NNLM), word2vec assigns probabilities to words in a sequence, like other NNLMs such as feedforward networks or recurrent neural networks (#REF ). Earlier models like latent dirichlet allocation (LDA) and latent semantic analysis (LSA) exist and effectively achieve low dimensional vectors by matrix factorization (#REF ; #REF ).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"A loglinear classifier is used in both architectures ( #TARGET_REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "Earlier models like latent dirichlet allocation (LDA) and latent semantic analysis (LSA) exist and effectively achieve low dimensional vectors by matrix factorization (#REF ; #REF ).",
                "It's been shown that word vectors are beneficial for NLP tasks (#REF ), such as sentiment analysis and named entity recognition.",
                "Besides, #TARGET_REF showed with vector space algebra that relationships among words can be evaluated, expressing the quality of vectors produced from the model.",
                "The famous, semantic example: vector(\"King\") -vector(\"Man\") + vector(\"Woman\") ≈ vector(\"Queen\") can be verified using cosine distance.",
                "Another type of semantic meaning is the relationship between a capital city and its corresponding country."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Earlier models like latent dirichlet allocation (LDA) and latent semantic analysis (LSA) exist and effectively achieve low dimensional vectors by matrix factorization (#REF ; #REF ). It's been shown that word vectors are beneficial for NLP tasks (#REF ), such as sentiment analysis and named entity recognition. Besides, #TARGET_REF showed with vector space algebra that relationships among words can be evaluated, expressing the quality of vectors produced from the model. The famous, semantic example: vector(\"King\") -vector(\"Man\") + vector(\"Woman\") ≈ vector(\"Queen\") can be verified using cosine distance. Another type of semantic meaning is the relationship between a capital city and its corresponding country.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Besides, #TARGET_REF showed with vector space algebra that relationships among words can be evaluated, expressing the quality of vectors produced from the model.\"]}"
    },
    {
        "gold": {
            "text": [
                "Syntactic relationship examples include plural verbs and past tense, among others.",
                "Combination of both syntactic and semantic analyses is possible and provided (totaling over 19,000 questions) as Google analogy test set by #TARGET_REF .",
                "WordSimilarity-353 test set is another analysis tool for word vectors (#REF ).",
                "Unlike Google analogy score, which is based on vector space algebra, WordSimilarity is based on human expert-assigned semantic similarity on two sets of English word pairs.",
                "Both tools rank from 0 (totally dissimilar) to 1 (very much similar or exact, in Google analogy case)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Syntactic relationship examples include plural verbs and past tense, among others. Combination of both syntactic and semantic analyses is possible and provided (totaling over 19,000 questions) as Google analogy test set by #TARGET_REF . WordSimilarity-353 test set is another analysis tool for word vectors (#REF ). Unlike Google analogy score, which is based on vector space algebra, WordSimilarity is based on human expert-assigned semantic similarity on two sets of English word pairs. Both tools rank from 0 (totally dissimilar) to 1 (very much similar or exact, in Google analogy case).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Combination of both syntactic and semantic analyses is possible and provided (totaling over 19,000 questions) as Google analogy test set by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Hyper-parameters are values which may be manually adjusted and include vector dimension size, type of algorithm and learning rate (#REF ).",
                "#TARGET_REF tried various hyper-parameters with both architectures of #TARGET_REF model, ranging from 50 to 1,000 dimensions, 30,000 to 3,000,000 vocabulary sizes, 1 to 3 epochs, among others.",
                "In our work, we extended research to 3,000 dimensions.",
                "Different observations were noted from the many trials.",
                "They observed diminishing returns after a certain point, despite additional dimensions or larger, unstructured training data."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Hyper-parameters are values which may be manually adjusted and include vector dimension size, type of algorithm and learning rate (#REF ). #TARGET_REF tried various hyper-parameters with both architectures of #TARGET_REF model, ranging from 50 to 1,000 dimensions, 30,000 to 3,000,000 vocabulary sizes, 1 to 3 epochs, among others. In our work, we extended research to 3,000 dimensions. Different observations were noted from the many trials. They observed diminishing returns after a certain point, despite additional dimensions or larger, unstructured training data.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF tried various hyper-parameters with both architectures of #TARGET_REF model, ranging from 50 to 1,000 dimensions, 30,000 to 3,000,000 vocabulary sizes, 1 to 3 epochs, among others.\"]}"
    },
    {
        "gold": {
            "text": [
                "The importance of ParFDA increases with the proliferation of training material available for building SMT systems.",
                "Table 1 presents the statistics of the available training and LM corpora for the constrained (C) systems in WMT15 #TARGET_REF as well as the statistics of the ParFDA selected training and LM data.",
                "ParFDA (Biçici, 2013; Biçici et al., 2014) runs separate FDA5 (Biçici and #REF) models on randomized subsets of the training data and combines the selections afterwards.",
                "FDA5 is available at http://github.com/bicici/FDA.",
                "We run ParFDA SMT experiments using Moses (#REF) in all language pairs in WMT15 (#REF) and obtain SMT performance close to the top constrained Moses systems."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The importance of ParFDA increases with the proliferation of training material available for building SMT systems. Table 1 presents the statistics of the available training and LM corpora for the constrained (C) systems in WMT15 #TARGET_REF as well as the statistics of the ParFDA selected training and LM data. ParFDA (Biçici, 2013; Biçici et al., 2014) runs separate FDA5 (Biçici and #REF) models on randomized subsets of the training data and combines the selections afterwards. FDA5 is available at http://github.com/bicici/FDA. We run ParFDA SMT experiments using Moses (#REF) in all language pairs in WMT15 (#REF) and obtain SMT performance close to the top constrained Moses systems.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Table 1 presents the statistics of the available training and LM corpora for the constrained (C) systems in WMT15 #TARGET_REF as well as the statistics of the ParFDA selected training and LM data.\"]}"
    },
    {
        "gold": {
            "text": [
                "ParFDA (Biçici, 2013; Biçici et al., 2014) runs separate FDA5 (Biçici and #REF) models on randomized subsets of the training data and combines the selections afterwards.",
                "FDA5 is available at http://github.com/bicici/FDA.",
                "We run ParFDA SMT experiments using Moses (#REF) in all language pairs in WMT15 #TARGET_REF and obtain SMT performance close to the top constrained Moses systems.",
                "ParFDA allows rapid prototyping of SMT systems for a given target domain or task.",
                "We use ParFDA for selecting parallel training data and LM data for building SMT systems."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "ParFDA (Biçici, 2013; Biçici et al., 2014) runs separate FDA5 (Biçici and #REF) models on randomized subsets of the training data and combines the selections afterwards. FDA5 is available at http://github.com/bicici/FDA. We run ParFDA SMT experiments using Moses (#REF) in all language pairs in WMT15 #TARGET_REF and obtain SMT performance close to the top constrained Moses systems. ParFDA allows rapid prototyping of SMT systems for a given target domain or task. We use ParFDA for selecting parallel training data and LM data for building SMT systems.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"We run ParFDA SMT experiments using Moses (#REF) in all language pairs in WMT15 #TARGET_REF and obtain SMT performance close to the top constrained Moses systems.\"]}"
    },
    {
        "gold": {
            "text": [
                "At the same time, a compact and more relevant LM corpus is also useful for modeling longer range dependencies with higher order ngram models.",
                "We use 3-grams for selecting training data and 2-grams for LM corpus selection.",
                "We run ParFDA SMT experiments for all language pairs in both directions in the WMT15 translation task #TARGET_REF , which include English-Czech (en-cs), English-German (en-de), English-Finnish (en-fi), English-French (en-fr), and English-Russian (en-ru).",
                "We truecase all of the corpora, set the maximum sentence length to 126, use 150-best lists during tuning, set the LM order to a value in [7, 10] for all language pairs, and train the LM using SRILM (#REF) with -unk option.",
                "For GIZA++ (#REF) , max-fertility is set to 10, with the number of iterations set to 7,3,5,5,7 for IBM models 1,2,3,4, and the HMM model, and 70 word 74 classes are learned over 3 iterations with the mkcls tool during training."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "At the same time, a compact and more relevant LM corpus is also useful for modeling longer range dependencies with higher order ngram models. We use 3-grams for selecting training data and 2-grams for LM corpus selection. We run ParFDA SMT experiments for all language pairs in both directions in the WMT15 translation task #TARGET_REF , which include English-Czech (en-cs), English-German (en-de), English-Finnish (en-fi), English-French (en-fr), and English-Russian (en-ru). We truecase all of the corpora, set the maximum sentence length to 126, use 150-best lists during tuning, set the LM order to a value in [7, 10] for all language pairs, and train the LM using SRILM (#REF) with -unk option. For GIZA++ (#REF) , max-fertility is set to 10, with the number of iterations set to 7,3,5,5,7 for IBM models 1,2,3,4, and the HMM model, and 70 word 74 classes are learned over 3 iterations with the mkcls tool during training.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We run ParFDA SMT experiments for all language pairs in both directions in the WMT15 translation task #TARGET_REF , which include English-Czech (en-cs), English-German (en-de), English-Finnish (en-fi), English-French (en-fr), and English-Russian (en-ru).\"]}"
    },
    {
        "gold": {
            "text": [
                "If some token does not appear in your language model (LM), then it becomes harder for the SMT engine to find its correct position in the translation.",
                "The importance of ParFDA increases with the proliferation of training material available for building SMT systems.",
                "Table 1 presents the statistics of the available training and LM corpora for the constrained (C) systems in WMT15 #TARGET_REF as well as the statistics of the ParFDA selected training and LM data.",
                "ParFDA (Biçici, 2013; Biçici et al., 2014) runs separate FDA5 (Biçici and #REF ) models on randomized subsets of the training data and combines the selections afterwards.",
                "FDA5 is available at http://github.com/bicici/FDA."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "If some token does not appear in your language model (LM), then it becomes harder for the SMT engine to find its correct position in the translation. The importance of ParFDA increases with the proliferation of training material available for building SMT systems. Table 1 presents the statistics of the available training and LM corpora for the constrained (C) systems in WMT15 #TARGET_REF as well as the statistics of the ParFDA selected training and LM data. ParFDA (Biçici, 2013; Biçici et al., 2014) runs separate FDA5 (Biçici and #REF ) models on randomized subsets of the training data and combines the selections afterwards. FDA5 is available at http://github.com/bicici/FDA.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Table 1 presents the statistics of the available training and LM corpora for the constrained (C) systems in WMT15 #TARGET_REF as well as the statistics of the ParFDA selected training and LM data.\"]}"
    },
    {
        "gold": {
            "text": [
                "ParFDA (Biçici, 2013; Biçici et al., 2014) runs separate FDA5 (Biçici and #REF ) models on randomized subsets of the training data and combines the selections afterwards.",
                "FDA5 is available at http://github.com/bicici/FDA.",
                "We run ParFDA SMT experiments using Moses (#REF) in all language pairs in WMT15 #TARGET_REF and obtain SMT performance close to the top constrained Moses systems.",
                "ParFDA allows rapid prototyping of SMT systems for a given target domain or task.",
                "We use ParFDA for selecting parallel training data and LM data for building SMT systems."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "ParFDA (Biçici, 2013; Biçici et al., 2014) runs separate FDA5 (Biçici and #REF ) models on randomized subsets of the training data and combines the selections afterwards. FDA5 is available at http://github.com/bicici/FDA. We run ParFDA SMT experiments using Moses (#REF) in all language pairs in WMT15 #TARGET_REF and obtain SMT performance close to the top constrained Moses systems. ParFDA allows rapid prototyping of SMT systems for a given target domain or task. We use ParFDA for selecting parallel training data and LM data for building SMT systems.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We run ParFDA SMT experiments using Moses (#REF) in all language pairs in WMT15 #TARGET_REF and obtain SMT performance close to the top constrained Moses systems.\"]}"
    },
    {
        "gold": {
            "text": [
                "We run ParFDA SMT experiments for all language pairs in both directions in the WMT15 translation task #TARGET_REF , which include English-Czech (en-cs), English-German (en-de), English-Finnish (en-fi), English-French (en-fr), and English-Russian (en-ru).",
                "We truecase all of the corpora, set the maximum sentence length to 126, use 150-best lists during tuning, set the LM order to a value in [7, 10] for all language pairs, and train the LM using SRILM (#REF) with -unk option.",
                "For GIZA++ (#REF) , max-fertility is set to 10, with the number of iterations set to 7,3,5,5,7 for IBM models 1,2,3,4, and the HMM model, and 70 word Table 1 : Data statistics for the available training and LM corpora in the constrained (C) setting compared with the ParFDA selected training and LM data.",
                "#words is in millions (M) and #sents in thousands (K).",
                "classes are learned over 3 iterations with the mkcls tool during training."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We run ParFDA SMT experiments for all language pairs in both directions in the WMT15 translation task #TARGET_REF , which include English-Czech (en-cs), English-German (en-de), English-Finnish (en-fi), English-French (en-fr), and English-Russian (en-ru). We truecase all of the corpora, set the maximum sentence length to 126, use 150-best lists during tuning, set the LM order to a value in [7, 10] for all language pairs, and train the LM using SRILM (#REF) with -unk option. For GIZA++ (#REF) , max-fertility is set to 10, with the number of iterations set to 7,3,5,5,7 for IBM models 1,2,3,4, and the HMM model, and 70 word Table 1 : Data statistics for the available training and LM corpora in the constrained (C) setting compared with the ParFDA selected training and LM data. #words is in millions (M) and #sents in thousands (K). classes are learned over 3 iterations with the mkcls tool during training.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We run ParFDA SMT experiments for all language pairs in both directions in the WMT15 translation task #TARGET_REF , which include English-Czech (en-cs), English-German (en-de), English-Finnish (en-fi), English-French (en-fr), and English-Russian (en-ru).\"]}"
    },
    {
        "gold": {
            "text": [
                "Natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity.",
                "Recently, #TARGET_REF presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU.",
                "In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU.",
                "The resulting system is capable of computing over 404 Viterbi parses per second-more than a 2x speedup-on the same hardware.",
                "Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning-nearly a 6x speedup."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity. Recently, #TARGET_REF presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU. In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU. The resulting system is capable of computing over 404 Viterbi parses per second-more than a 2x speedup-on the same hardware. Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning-nearly a 6x speedup.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Recently, #TARGET_REF presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU.\"]}"
    },
    {
        "gold": {
            "text": [
                "The challenge is that GPUs are not a good fit for the kinds of sparse computations that most current CPU-based NLP algorithms rely on.",
                "Recently, #TARGET_REF proposed a GPU implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that GPUs can provide.",
                "Their system uses a grammar based on the Berkeley parser (#REF) (which is particularly amenable to GPU processing), \"compiling\" the grammar into a sequence of GPU kernels that are applied densely to every item in the parse chart.",
                "Together these kernels implement the Viterbi inside algorithm.",
                "On a mid-range GPU, their system can compute Viterbi derivations at 164 sentences per second on sentences of length 40 or less (see timing details below)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The challenge is that GPUs are not a good fit for the kinds of sparse computations that most current CPU-based NLP algorithms rely on. Recently, #TARGET_REF proposed a GPU implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that GPUs can provide. Their system uses a grammar based on the Berkeley parser (#REF) (which is particularly amenable to GPU processing), \"compiling\" the grammar into a sequence of GPU kernels that are applied densely to every item in the parse chart. Together these kernels implement the Viterbi inside algorithm. On a mid-range GPU, their system can compute Viterbi derivations at 164 sentences per second on sentences of length 40 or less (see timing details below).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Recently, #TARGET_REF proposed a GPU implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that GPUs can provide.\"]}"
    },
    {
        "gold": {
            "text": [
                "Using this approach, we see a gain of more than 2x over the dense GPU implementation, resulting in overall speeds of up to 404 sentences per second.",
                "For comparison, the publicly available CPU implementation of #REF parses approximately 7 sentences per second per core on a modern CPU.",
                "A further drawback of the dense approach in #TARGET_REF is that it only computes Viterbi parses.",
                "As with other grammars with a parse/derivation distinction, the grammars of #REF only achieve their full accuracy using minimum-Bayes-risk parsing, with improvements of over 1.5 F1 over best-derivation Viterbi parsing on the Penn Treebank (#REF) .",
                "To that end, we extend our coarse-tofine GPU approach to computing marginals, along the way proposing a new way to exploit the coarse pass to avoid expensive log-domain computations in the fine pass."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Using this approach, we see a gain of more than 2x over the dense GPU implementation, resulting in overall speeds of up to 404 sentences per second. For comparison, the publicly available CPU implementation of #REF parses approximately 7 sentences per second per core on a modern CPU. A further drawback of the dense approach in #TARGET_REF is that it only computes Viterbi parses. As with other grammars with a parse/derivation distinction, the grammars of #REF only achieve their full accuracy using minimum-Bayes-risk parsing, with improvements of over 1.5 F1 over best-derivation Viterbi parsing on the Penn Treebank (#REF) . To that end, we extend our coarse-tofine GPU approach to computing marginals, along the way proposing a new way to exploit the coarse pass to avoid expensive log-domain computations in the fine pass.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"A further drawback of the dense approach in #TARGET_REF is that it only computes Viterbi parses.\"]}"
    },
    {
        "gold": {
            "text": [
                "This architecture environment puts very different constraints on parsing algorithms from a CPU environment.",
                "#TARGET_REF proposed an implementation of a PCFG parser that sacrifices standard sparse methods like coarse-to-fine pruning, focusing instead on maximizing the instruction and memory throughput of the parser.",
                "They assume that they are parsing many sentences at once, with throughput being more important than latency.",
                "In this section, we describe their dense algorithm, which we take as the baseline for our work; we present it in a way that sets up the changes to follow.",
                "At the top level, the CPU and GPU communicate via a work queue of parse items of the form (s, i, k, j), where s is an identifier of a sentence, i is the start of a span, k is the split point, and j Table 1 : Performance numbers for computing Viterbi inside charts on 20,000 sentences of length ≤40 from the Penn Treebank."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "This architecture environment puts very different constraints on parsing algorithms from a CPU environment. #TARGET_REF proposed an implementation of a PCFG parser that sacrifices standard sparse methods like coarse-to-fine pruning, focusing instead on maximizing the instruction and memory throughput of the parser. They assume that they are parsing many sentences at once, with throughput being more important than latency. In this section, we describe their dense algorithm, which we take as the baseline for our work; we present it in a way that sets up the changes to follow. At the top level, the CPU and GPU communicate via a work queue of parse items of the form (s, i, k, j), where s is an identifier of a sentence, i is the start of a span, k is the split point, and j Table 1 : Performance numbers for computing Viterbi inside charts on 20,000 sentences of length ≤40 from the Penn Treebank.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF proposed an implementation of a PCFG parser that sacrifices standard sparse methods like coarse-to-fine pruning, focusing instead on maximizing the instruction and memory throughput of the parser.\"]}"
    },
    {
        "gold": {
            "text": [
                "Speedups are measured in reference to this reimplementation.",
                "See Section 7 for discussion of the clustering algorithms and Section 6 for a description of the pruning methods.",
                "The #TARGET_REF system is benchmarked on a batch size of 1200 sentences, the others on 20,000.",
                "is the end point.",
                "The GPU takes large numbers of parse items and applies the entire grammar to them in parallel."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Speedups are measured in reference to this reimplementation. See Section 7 for discussion of the clustering algorithms and Section 6 for a description of the pruning methods. The #TARGET_REF system is benchmarked on a batch size of 1200 sentences, the others on 20,000. is the end point. The GPU takes large numbers of parse items and applies the entire grammar to them in parallel.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The #TARGET_REF system is benchmarked on a batch size of 1200 sentences, the others on 20,000.\"]}"
    },
    {
        "gold": {
            "text": [
                "One important feature of #TARGET_REF 's system is grammar compilation.",
                "Because registers are so much faster than thread-local memory, it is critical to keep as many variables in registers as possible.",
                "One way to accomplish this is to unroll loops at compilation time.",
                "Therefore, they inlined the iteration over the grammar directly into the GPU kernels (i.e. the code itself), which allows the compiler to more effectively use all of its registers.",
                "However, register space is limited on GPUs."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "One important feature of #TARGET_REF 's system is grammar compilation. Because registers are so much faster than thread-local memory, it is critical to keep as many variables in registers as possible. One way to accomplish this is to unroll loops at compilation time. Therefore, they inlined the iteration over the grammar directly into the GPU kernels (i.e. the code itself), which allows the compiler to more effectively use all of its registers. However, register space is limited on GPUs.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"One important feature of #TARGET_REF 's system is grammar compilation.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, register space is limited on GPUs.",
                "Because the Berkeley grammar is so large, the compiler is not able to efficiently schedule all of the operations in the grammar, resulting in register spills.",
                "#TARGET_REF found they had to partition the grammar into multiple different kernels.",
                "We discuss this partitioning in more detail in Section 7.",
                "However, in short, the entire grammar G is broken into multiple clusters G i where each rule belongs to exactly one cluster."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "However, register space is limited on GPUs. Because the Berkeley grammar is so large, the compiler is not able to efficiently schedule all of the operations in the grammar, resulting in register spills. #TARGET_REF found they had to partition the grammar into multiple different kernels. We discuss this partitioning in more detail in Section 7. However, in short, the entire grammar G is broken into multiple clusters G i where each rule belongs to exactly one cluster.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF found they had to partition the grammar into multiple different kernels.\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally, when pruning, it is best if symbols that have the same coarse projection are clustered together.",
                "That way, we are more likely to be able to skip a subcluster, since fewer distinct symbols need to be \"off\" for a parse item to be skipped in a given subcluster.",
                "#TARGET_REF clustered symbols of the grammar using a sophisticated spectral clustering algorithm to obtain a permutation of the symbols.",
                "Then the rules of the grammar were laid out in a (sparse) three-dimensional tensor, with one dimension representing the parent of the rule, one representing the left child, and one representing the right child.",
                "They then split the cube into 6x2x2 contiguous \"major cubes,\" giving a partition of the rules into 24 clusters."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Finally, when pruning, it is best if symbols that have the same coarse projection are clustered together. That way, we are more likely to be able to skip a subcluster, since fewer distinct symbols need to be \"off\" for a parse item to be skipped in a given subcluster. #TARGET_REF clustered symbols of the grammar using a sophisticated spectral clustering algorithm to obtain a permutation of the symbols. Then the rules of the grammar were laid out in a (sparse) three-dimensional tensor, with one dimension representing the parent of the rule, one representing the left child, and one representing the right child. They then split the cube into 6x2x2 contiguous \"major cubes,\" giving a partition of the rules into 24 clusters.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF clustered symbols of the grammar using a sophisticated spectral clustering algorithm to obtain a permutation of the symbols.\"]}"
    },
    {
        "gold": {
            "text": [
                "Apart from the model of #TARGET_REF , there have been a few attempts at using GPUs in NLP contexts before.",
                "#REF and #REF both had early attempts at porting parsing algorithms to the GPU.",
                "However, they did not demonstrate significantly increased speed over a CPU implementation.",
                "In machine translation, #REF adapted algorithms designed for GPUs in the computational biology literature to speed up on-demand phrase table extraction."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "Apart from the model of #TARGET_REF , there have been a few attempts at using GPUs in NLP contexts before. #REF and #REF both had early attempts at porting parsing algorithms to the GPU. However, they did not demonstrate significantly increased speed over a CPU implementation. In machine translation, #REF adapted algorithms designed for GPUs in the computational biology literature to speed up on-demand phrase table extraction.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Apart from the model of #TARGET_REF , there have been a few attempts at using GPUs in NLP contexts before.\"]}"
    },
    {
        "gold": {
            "text": [
                "Unless otherwise noted, all experiments are conducted on sentences of length ≤ 40 words, and we estimate times based on batches of 20K sentences.",
                "1 We should note that our experimental condition differs from that of #TARGET_REF : they evaluate on sentences of length ≤ 30.",
                "Furthermore, they 1 The implementation of #REF cannot handle batches so large, and so we tested it on batches of 1200 sentences.",
                "Our reimplementation is approximately the same speed for the same batch sizes.",
                "For batches of 20K sentences, we used sentences from the training set."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Unless otherwise noted, all experiments are conducted on sentences of length ≤ 40 words, and we estimate times based on batches of 20K sentences. 1 We should note that our experimental condition differs from that of #TARGET_REF : they evaluate on sentences of length ≤ 30. Furthermore, they 1 The implementation of #REF cannot handle batches so large, and so we tested it on batches of 1200 sentences. Our reimplementation is approximately the same speed for the same batch sizes. For batches of 20K sentences, we used sentences from the training set.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"1 We should note that our experimental condition differs from that of #TARGET_REF : they evaluate on sentences of length \\u2264 30.\"]}"
    },
    {
        "gold": {
            "text": [
                "1 We should note that our experimental condition differs from that of #REF : they evaluate on sentences of length ≤ 30.",
                "Furthermore, they 1 The implementation of #TARGET_REF cannot handle batches so large, and so we tested it on batches of 1200 sentences.",
                "Our reimplementation is approximately the same speed for the same batch sizes.",
                "For batches of 20K sentences, we used sentences from the training set.",
                "We verified that there was no significant difference in speed for sentences from the training set and from the test set."
            ],
            "label": [
                "DIFFER",
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "1 We should note that our experimental condition differs from that of #REF : they evaluate on sentences of length ≤ 30. Furthermore, they 1 The implementation of #TARGET_REF cannot handle batches so large, and so we tested it on batches of 1200 sentences. Our reimplementation is approximately the same speed for the same batch sizes. For batches of 20K sentences, we used sentences from the training set. We verified that there was no significant difference in speed for sentences from the training set and from the test set.",
        "output": "{\"label\": [\"DIFFER\", \"USE\"], \"context\": [\"Furthermore, they 1 The implementation of #TARGET_REF cannot handle batches so large, and so we tested it on batches of 1200 sentences.\"]}"
    },
    {
        "gold": {
            "text": [
                "GPUs work by executing thousands of threads at once, but impose the constraint that large blocks of threads must be executing the same (2013)'s system.",
                "The GPU and CPU communicate via a work queue, which ferries parse items from the CPU to the GPU.",
                "Our system uses a coarse-to-fine approach, where the coarse pass computes a pruning mask that is used by the CPU when deciding which items to queue during the fine pass.",
                "The original system of #TARGET_REF only used the fine pass, with no pruning.",
                "instructions in lockstep, differing only in their input data."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "GPUs work by executing thousands of threads at once, but impose the constraint that large blocks of threads must be executing the same (2013)'s system. The GPU and CPU communicate via a work queue, which ferries parse items from the CPU to the GPU. Our system uses a coarse-to-fine approach, where the coarse pass computes a pruning mask that is used by the CPU when deciding which items to queue during the fine pass. The original system of #TARGET_REF only used the fine pass, with no pruning. instructions in lockstep, differing only in their input data.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Our system uses a coarse-to-fine approach, where the coarse pass computes a pruning mask that is used by the CPU when deciding which items to queue during the fine pass.\", \"The original system of #TARGET_REF only used the fine pass, with no pruning.\"]}"
    },
    {
        "gold": {
            "text": [
                "Here, the rules of the grammar are clustered by their coarse parent symbol.",
                "We then have multiple work queues, with parse items only being enqueued if the span (i, j) allows that symbol in its pruning mask.",
                "All in all, #TARGET_REF 's system is able to compute Viterbi charts at 164 sentences per second, for sentences up to length 40.",
                "On larger batch sizes, our reimplementation of their approach is able to achieve 193 sentences per second on the same hardware.",
                "(See Table 1 .) 6 Pruning on a GPU Now we turn to the algorithmic and architectural changes in our approach."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Here, the rules of the grammar are clustered by their coarse parent symbol. We then have multiple work queues, with parse items only being enqueued if the span (i, j) allows that symbol in its pruning mask. All in all, #TARGET_REF 's system is able to compute Viterbi charts at 164 sentences per second, for sentences up to length 40. On larger batch sizes, our reimplementation of their approach is able to achieve 193 sentences per second on the same hardware. (See Table 1 .) 6 Pruning on a GPU Now we turn to the algorithmic and architectural changes in our approach.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"All in all, #TARGET_REF 's system is able to compute Viterbi charts at 164 sentences per second, for sentences up to length 40.\", \"On larger batch sizes, our reimplementation of their approach is able to achieve 193 sentences per second on the same hardware.\"]}"
    },
    {
        "gold": {
            "text": [
                "We found this approach to subclustering worked well in practice.",
                "Clustering using this method is labeled 'Parent' in Table 1 . Now, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to #TARGET_REF 's system, and nearly 50% over our reimplemented baseline.",
                "It turns out that this simple clustering algorithm produces relatively efficient kernels even in the unpruned case.",
                "The unpruned Viterbi computations in a fine grammar using the clustering method of #REF yields a speed of 193 sentences per second, whereas the same computation using coarse parent clustering has a speed of 159 sentences per second.",
                "(See Table 1 .) This is not as efficient as #REF 's highly tuned method, but it is still fairly fast, and much simpler to implement."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We found this approach to subclustering worked well in practice. Clustering using this method is labeled 'Parent' in Table 1 . Now, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to #TARGET_REF 's system, and nearly 50% over our reimplemented baseline. It turns out that this simple clustering algorithm produces relatively efficient kernels even in the unpruned case. The unpruned Viterbi computations in a fine grammar using the clustering method of #REF yields a speed of 193 sentences per second, whereas the same computation using coarse parent clustering has a speed of 159 sentences per second. (See Table 1 .) This is not as efficient as #REF 's highly tuned method, but it is still fairly fast, and much simpler to implement.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Clustering using this method is labeled 'Parent' in Table 1 . Now, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to #TARGET_REF 's system, and nearly 50% over our reimplemented baseline.\"]}"
    },
    {
        "gold": {
            "text": [
                "We found this approach to subclustering worked well in practice.",
                "Clustering using this method is labeled 'Parent' in Table 1 . Now, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to #REF 's system, and nearly 50% over our reimplemented baseline.",
                "It turns out that this simple clustering algorithm produces relatively efficient kernels even in the unpruned case.",
                "The unpruned Viterbi computations in a fine grammar using the clustering method of #TARGET_REF yields a speed of 193 sentences per second, whereas the same computation using coarse parent clustering has a speed of 159 sentences per second.",
                "(See Table 1 .) This is not as efficient as #REF 's highly tuned method, but it is still fairly fast, and much simpler to implement."
            ],
            "label": [
                "DIFFER",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We found this approach to subclustering worked well in practice. Clustering using this method is labeled 'Parent' in Table 1 . Now, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to #REF 's system, and nearly 50% over our reimplemented baseline. It turns out that this simple clustering algorithm produces relatively efficient kernels even in the unpruned case. The unpruned Viterbi computations in a fine grammar using the clustering method of #TARGET_REF yields a speed of 193 sentences per second, whereas the same computation using coarse parent clustering has a speed of 159 sentences per second. (See Table 1 .) This is not as efficient as #REF 's highly tuned method, but it is still fairly fast, and much simpler to implement.",
        "output": "{\"label\": [\"DIFFER\", \"USE\"], \"context\": [\"The unpruned Viterbi computations in a fine grammar using the clustering method of #TARGET_REF yields a speed of 193 sentences per second, whereas the same computation using coarse parent clustering has a speed of 159 sentences per second.\"]}"
    },
    {
        "gold": {
            "text": [
                "We found this approach to subclustering worked well in practice.",
                "Clustering using this method is labeled 'Parent' in Table 1 . Now, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to #REF 's system, and nearly 50% over our reimplemented baseline.",
                "It turns out that this simple clustering algorithm produces relatively efficient kernels even in the unpruned case.",
                "The unpruned Viterbi computations in a fine grammar using the clustering method of #REF yields a speed of 193 sentences per second, whereas the same computation using coarse parent clustering has a speed of 159 sentences per second.",
                "(See Table 1 .) This is not as efficient as #TARGET_REF 's highly tuned method, but it is still fairly fast, and much simpler to implement."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "We found this approach to subclustering worked well in practice. Clustering using this method is labeled 'Parent' in Table 1 . Now, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to #REF 's system, and nearly 50% over our reimplemented baseline. It turns out that this simple clustering algorithm produces relatively efficient kernels even in the unpruned case. The unpruned Viterbi computations in a fine grammar using the clustering method of #REF yields a speed of 193 sentences per second, whereas the same computation using coarse parent clustering has a speed of 159 sentences per second. (See Table 1 .) This is not as efficient as #TARGET_REF 's highly tuned method, but it is still fairly fast, and much simpler to implement.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"(See Table 1 .) This is not as efficient as #TARGET_REF 's highly tuned method, but it is still fairly fast, and much simpler to implement.\"]}"
    },
    {
        "gold": {
            "text": [
                "First, the span (i, j)'s pruning mask must have a non-empty intersection with the signature of the queue.",
                "Second, the pruning mask for the children (i, k) and (k, j) must be non-empty.",
                "Once on the GPU, parse items are processed using the same style of compiled kernel as in #TARGET_REF .",
                "Because the entire partition (though not necessarily the entire grammar) is applied to each item in the queue, we still do not need to worry about warp divergence.",
                "At the top level, our system first computes pruning masks with a coarse grammar."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "First, the span (i, j)'s pruning mask must have a non-empty intersection with the signature of the queue. Second, the pruning mask for the children (i, k) and (k, j) must be non-empty. Once on the GPU, parse items are processed using the same style of compiled kernel as in #TARGET_REF . Because the entire partition (though not necessarily the entire grammar) is applied to each item in the queue, we still do not need to worry about warp divergence. At the top level, our system first computes pruning masks with a coarse grammar.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"Once on the GPU, parse items are processed using the same style of compiled kernel as in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We leave the study of non-linear transformation and other additions for further work.",
                "In this paper, we propose a general framework to learn bilingual word embeddings.",
                "We start with a basic optimization objective (#REFb) and introduce several meaningful and intuitive constraints that are equivalent or closely related to previously proposed methods (#REF; #TARGET_REF .",
                "Our framework provides a more general view of bilingual word embedding mappings, showing the underlying connection between the existing methods, revealing some flaws in their theoretical justification and providing an alternative theoretical interpretation for them.",
                "Our experiments on an existing English-Italian word translation induction and an English word analogy task give strong empirical evidence in favor of our theoretical reasoning, while showing that one of our models clearly outperforms previous alternatives."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We leave the study of non-linear transformation and other additions for further work. In this paper, we propose a general framework to learn bilingual word embeddings. We start with a basic optimization objective (#REFb) and introduce several meaningful and intuitive constraints that are equivalent or closely related to previously proposed methods (#REF; #TARGET_REF . Our framework provides a more general view of bilingual word embedding mappings, showing the underlying connection between the existing methods, revealing some flaws in their theoretical justification and providing an alternative theoretical interpretation for them. Our experiments on an existing English-Italian word translation induction and an English word analogy task give strong empirical evidence in favor of our theoretical reasoning, while showing that one of our models clearly outperforms previous alternatives.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We start with a basic optimization objective (#REFb) and introduce several meaningful and intuitive constraints that are equivalent or closely related to previously proposed methods (#REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In all the cases, length normalization and mean centering were applied to all embeddings, even if missing from the dictionary.",
                "The results show that the orthogonality constraint is key to preserve monolingual performance, and it also improves bilingual performance by enforcing a relevant property (monolingual invariance) that the transformation to learn should intuitively have.",
                "The contribution of length normalization alone is marginal, but when followed by mean centering we obtain further improvements in bilingual performance without hurting monolingual performance.",
                "Table 2 shows the results for our best performing configuration in comparison to previous work.",
                "As discussed before, (#REFb) and #TARGET_REF were implemented as part of our framework, so they correspond to our uncostrained mapping with no preprocessing and orthogonal mapping with length normalization, respectively."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "In all the cases, length normalization and mean centering were applied to all embeddings, even if missing from the dictionary. The results show that the orthogonality constraint is key to preserve monolingual performance, and it also improves bilingual performance by enforcing a relevant property (monolingual invariance) that the transformation to learn should intuitively have. The contribution of length normalization alone is marginal, but when followed by mean centering we obtain further improvements in bilingual performance without hurting monolingual performance. Table 2 shows the results for our best performing configuration in comparison to previous work. As discussed before, (#REFb) and #TARGET_REF were implemented as part of our framework, so they correspond to our uncostrained mapping with no preprocessing and orthogonal mapping with length normalization, respectively.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"As discussed before, (#REFb) and #TARGET_REF were implemented as part of our framework, so they correspond to our uncostrained mapping with no preprocessing and orthogonal mapping with length normalization, respectively.\"]}"
    },
    {
        "gold": {
            "text": [
                "Since the original embeddings are the same in all the cases and it is only the transformation that is applied to them that changes, this affects all the methods in the exact same way, so the results are perfectly comparable among themselves.",
                "With these settings, we obtain a coverage of 64.98%.",
                "We implemented the proposed method in Python using NumPy, and make it available as an open source project 5 .",
                "The code for Mikolov et al. (2013b) and #TARGET_REF is not publicly available, so we implemented and tested them as part of the proposed framework, which only differs from the original systems in the optimization method (exact solution instead of gradient descent) and the length normalization approach in the case of #TARGET_REF (postprocessing instead of constrained training).",
                "As for the method by #REF , we used their original implementation in Python and MAT-LAB 6 , which we extended to cover cases where the dictionary contains more than one entry for the same word."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Since the original embeddings are the same in all the cases and it is only the transformation that is applied to them that changes, this affects all the methods in the exact same way, so the results are perfectly comparable among themselves. With these settings, we obtain a coverage of 64.98%. We implemented the proposed method in Python using NumPy, and make it available as an open source project 5 . The code for Mikolov et al. (2013b) and #TARGET_REF is not publicly available, so we implemented and tested them as part of the proposed framework, which only differs from the original systems in the optimization method (exact solution instead of gradient descent) and the length normalization approach in the case of #TARGET_REF (postprocessing instead of constrained training). As for the method by #REF , we used their original implementation in Python and MAT-LAB 6 , which we extended to cover cases where the dictionary contains more than one entry for the same word.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The code for Mikolov et al. (2013b) and #TARGET_REF is not publicly available, so we implemented and tested them as part of the proposed framework, which only differs from the original systems in the optimization method (exact solution instead of gradient descent) and the length normalization approach in the case of #TARGET_REF (postprocessing instead of constrained training).\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper develops a new framework to learn bilingual word embedding mappings, generalizing previous work and providing an efficient exact method to learn the optimal transformation.",
                "Our experiments show the effectiveness of the proposed model and give strong empirical evidence in favor of our reinterpretation of #TARGET_REF and #REF .",
                "It is the proposed method with the orthogonality constraint and a global preprocessing with length normalization and dimension-wise mean centering that achieves the best overall results both in monolingual and bilingual terms, surpassing those previous methods.",
                "In the future, we would like to study non-linear mappings (#REF) and the additional techniques in .",
                "ish Ministry of Economy and Competitiveness (TADEEP TIN2015-70214-P)."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "This paper develops a new framework to learn bilingual word embedding mappings, generalizing previous work and providing an efficient exact method to learn the optimal transformation. Our experiments show the effectiveness of the proposed model and give strong empirical evidence in favor of our reinterpretation of #TARGET_REF and #REF . It is the proposed method with the orthogonality constraint and a global preprocessing with length normalization and dimension-wise mean centering that achieves the best overall results both in monolingual and bilingual terms, surpassing those previous methods. In the future, we would like to study non-linear mappings (#REF) and the additional techniques in . ish Ministry of Economy and Competitiveness (TADEEP TIN2015-70214-P).",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"This paper develops a new framework to learn bilingual word embedding mappings, generalizing previous work and providing an efficient exact method to learn the optimal transformation.\", \"Our experiments show the effectiveness of the proposed model and give strong empirical evidence in favor of our reinterpretation of #TARGET_REF and #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In this work, we focus specifically on the evaluation of visual conversational agents and develop a human computation game to benchmark their performance as members of human-AI teams.",
                "Visual conversational agents (#REFa; #TARGET_REF; ) are AI agents Figure 1 : A human and an AI (a visual conversation agent called ALICE) play the proposed GuessWhich game.",
                "At the start of the game (top), ALICE is provided an image (shown above ALICE) which is unknown to the human.",
                "Both ALICE and the human are then provided a brief description of the image.",
                "The human then attempts to identify the secret image."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In this work, we focus specifically on the evaluation of visual conversational agents and develop a human computation game to benchmark their performance as members of human-AI teams. Visual conversational agents (#REFa; #TARGET_REF; ) are AI agents Figure 1 : A human and an AI (a visual conversation agent called ALICE) play the proposed GuessWhich game. At the start of the game (top), ALICE is provided an image (shown above ALICE) which is unknown to the human. Both ALICE and the human are then provided a brief description of the image. The human then attempts to identify the secret image.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Visual conversational agents (#REFa; #TARGET_REF; ) are AI agents Figure 1 : A human and an AI (a visual conversation agent called ALICE) play the proposed GuessWhich game.\"]}"
    },
    {
        "gold": {
            "text": [
                "Specifically, #TARGET_REF ) train two visual conversational agents -a questioning bot QBOT, and an answering bot ABOT -for an image-guessing task.",
                "Starting from a description of the scene, QBOT and ABOT converse over multiple rounds of questions (QBOT) and answers (ABOT) in order to improve QBOT's understanding of a secret image known only to ABOT.",
                "After a fixed number of rounds, QBOT must guess the secret image from a large pool and both QBOT and ABOT are evaluated based on this guess.",
                "(#REFb ) compare supervised baseline models with QBOT-ABOT teams trained through reinforcement learning based self-talk on this image-guessing task.",
                "They find that the AI-AI teams improve significantly at guessing the correct image after self-talk updates compared to the supervised pretraining."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Specifically, #TARGET_REF ) train two visual conversational agents -a questioning bot QBOT, and an answering bot ABOT -for an image-guessing task. Starting from a description of the scene, QBOT and ABOT converse over multiple rounds of questions (QBOT) and answers (ABOT) in order to improve QBOT's understanding of a secret image known only to ABOT. After a fixed number of rounds, QBOT must guess the secret image from a large pool and both QBOT and ABOT are evaluated based on this guess. (#REFb ) compare supervised baseline models with QBOT-ABOT teams trained through reinforcement learning based self-talk on this image-guessing task. They find that the AI-AI teams improve significantly at guessing the correct image after self-talk updates compared to the supervised pretraining.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Specifically, #TARGET_REF ) train two visual conversational agents -a questioning bot QBOT, and an answering bot ABOT -for an image-guessing task.\"]}"
    },
    {
        "gold": {
            "text": [
                "After a fixed number of rounds, QBOT must guess the secret image from a large pool and both QBOT and ABOT are evaluated based on this guess.",
                "#TARGET_REF ) compare supervised baseline models with QBOT-ABOT teams trained through reinforcement learning based self-talk on this image-guessing task.",
                "They find that the AI-AI teams improve significantly at guessing the correct image after self-talk updates compared to the supervised pretraining.",
                "While these results indicate that the self-talk fine-tuned agents are better visual conversational agents, crucially, it remains unclear if these agents are indeed better at this task when interacting with humans.",
                "GuessWhich."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "After a fixed number of rounds, QBOT must guess the secret image from a large pool and both QBOT and ABOT are evaluated based on this guess. #TARGET_REF ) compare supervised baseline models with QBOT-ABOT teams trained through reinforcement learning based self-talk on this image-guessing task. They find that the AI-AI teams improve significantly at guessing the correct image after self-talk updates compared to the supervised pretraining. While these results indicate that the self-talk fine-tuned agents are better visual conversational agents, crucially, it remains unclear if these agents are indeed better at this task when interacting with humans. GuessWhich.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF ) compare supervised baseline models with QBOT-ABOT teams trained through reinforcement learning based self-talk on this image-guessing task.\"]}"
    },
    {
        "gold": {
            "text": [
                "Visual Conversational Agents.",
                "Our AI agents are visual conversational models, which have recently emerged as a popular research area in visually-grounded language modeling (#REFa; #TARGET_REF; . (#REFa ) introduced the task of Visual Dialog and collected the VisDial dataset by pairing subjects on Amazon Mechanical Turk (AMT) to chat about an image (with assigned roles of questioner and answerer).",
                "(#REFb ) pre-trained questioner and answerer agents on this VisDial dataset via supervised learning and fine-tuned them via self-talk (reinforcement learning), observing that RL-fine-tuned QBOT-ABOT are better at image-guessing after interacting with each other.",
                "However, (#REF; Chamberlain, Poesio, and #REF) , movies (#REF) etc.",
                "While such games have traditionally focused on human-human collaboration, we extend these ideas to human-AI teams."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Visual Conversational Agents. Our AI agents are visual conversational models, which have recently emerged as a popular research area in visually-grounded language modeling (#REFa; #TARGET_REF; . (#REFa ) introduced the task of Visual Dialog and collected the VisDial dataset by pairing subjects on Amazon Mechanical Turk (AMT) to chat about an image (with assigned roles of questioner and answerer). (#REFb ) pre-trained questioner and answerer agents on this VisDial dataset via supervised learning and fine-tuned them via self-talk (reinforcement learning), observing that RL-fine-tuned QBOT-ABOT are better at image-guessing after interacting with each other. However, (#REF; Chamberlain, Poesio, and #REF) , movies (#REF) etc. While such games have traditionally focused on human-human collaboration, we extend these ideas to human-AI teams.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Our AI agents are visual conversational models, which have recently emerged as a popular research area in visually-grounded language modeling (#REFa; #TARGET_REF; . (#REFa ) introduced the task of Visual Dialog and collected the VisDial dataset by pairing subjects on Amazon Mechanical Turk (AMT) to chat about an image (with assigned roles of questioner and answerer).\"]}"
    },
    {
        "gold": {
            "text": [
                "Visual Conversational Agents.",
                "Our AI agents are visual conversational models, which have recently emerged as a popular research area in visually-grounded language modeling (#REFa; #REFb; . (#REFa ) introduced the task of Visual Dialog and collected the VisDial dataset by pairing subjects on Amazon Mechanical Turk (AMT) to chat about an image (with assigned roles of questioner and answerer).",
                "#TARGET_REF ) pre-trained questioner and answerer agents on this VisDial dataset via supervised learning and fine-tuned them via self-talk (reinforcement learning), observing that RL-fine-tuned QBOT-ABOT are better at image-guessing after interacting with each other.",
                "However, (#REF; Chamberlain, Poesio, and #REF) , movies (#REF) etc.",
                "While such games have traditionally focused on human-human collaboration, we extend these ideas to human-AI teams."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Visual Conversational Agents. Our AI agents are visual conversational models, which have recently emerged as a popular research area in visually-grounded language modeling (#REFa; #REFb; . (#REFa ) introduced the task of Visual Dialog and collected the VisDial dataset by pairing subjects on Amazon Mechanical Turk (AMT) to chat about an image (with assigned roles of questioner and answerer). #TARGET_REF ) pre-trained questioner and answerer agents on this VisDial dataset via supervised learning and fine-tuned them via self-talk (reinforcement learning), observing that RL-fine-tuned QBOT-ABOT are better at image-guessing after interacting with each other. However, (#REF; Chamberlain, Poesio, and #REF) , movies (#REF) etc. While such games have traditionally focused on human-human collaboration, we extend these ideas to human-AI teams.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF ) pre-trained questioner and answerer agents on this VisDial dataset via supervised learning and fine-tuned them via self-talk (reinforcement learning), observing that RL-fine-tuned QBOT-ABOT are better at image-guessing after interacting with each other.\"]}"
    },
    {
        "gold": {
            "text": [
                "Goal-driven (nonvisual) conversational models have typically been evaluated on task-completion rate or time-to-task-completion (#REF) , so shorter conversations are better.",
                "At the other end of the spectrum, free-form conversation models are often evaluated by metrics that rely on n-gram overlaps, such as BLEU, METEOR, ROUGE, but these have been shown to correlate poorly with human judgment (#REF) .",
                "Human evaluation of conversations is typically in the format where humans rate the quality of machine utterances given context, without actually taking part in the conversation, as in #TARGET_REF ) and (#REF) .",
                "To the best of our knowledge, we are the first to evaluate conversational models via team performance where humans are continuously interacting with agents to succeed at a downstream task.",
                "Turing Test."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Goal-driven (nonvisual) conversational models have typically been evaluated on task-completion rate or time-to-task-completion (#REF) , so shorter conversations are better. At the other end of the spectrum, free-form conversation models are often evaluated by metrics that rely on n-gram overlaps, such as BLEU, METEOR, ROUGE, but these have been shown to correlate poorly with human judgment (#REF) . Human evaluation of conversations is typically in the format where humans rate the quality of machine utterances given context, without actually taking part in the conversation, as in #TARGET_REF ) and (#REF) . To the best of our knowledge, we are the first to evaluate conversational models via team performance where humans are continuously interacting with agents to succeed at a downstream task. Turing Test.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Human evaluation of conversations is typically in the format where humans rate the quality of machine utterances given context, without actually taking part in the conversation, as in #TARGET_REF ) and (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Specifically, we are considering the question-answering agent ABOT from (#REFb) as ABOT is the agent more likely to be deployed with a human partner in real applications (e.g. to answer questions about visual content to aid a visually impaired user).",
                "For completeness, we will review this work in this section.",
                "#TARGET_REF ) formulate a self-supervised imageguessing task between a questioner bot (QBOT) and an answerer bot (ABOT) which plays out over multiple rounds of dialog.",
                "At the start of the task, QBOT and ABOT are shown a one sentence description (i.e. a caption) of an image (unknown to QBOT).",
                "The pair can then engage in question and answer based dialog for a fixed number of iterations after which QBOT must try to select the secret image from a pool."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Specifically, we are considering the question-answering agent ABOT from (#REFb) as ABOT is the agent more likely to be deployed with a human partner in real applications (e.g. to answer questions about visual content to aid a visually impaired user). For completeness, we will review this work in this section. #TARGET_REF ) formulate a self-supervised imageguessing task between a questioner bot (QBOT) and an answerer bot (ABOT) which plays out over multiple rounds of dialog. At the start of the task, QBOT and ABOT are shown a one sentence description (i.e. a caption) of an image (unknown to QBOT). The pair can then engage in question and answer based dialog for a fixed number of iterations after which QBOT must try to select the secret image from a pool.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF ) formulate a self-supervised imageguessing task between a questioner bot (QBOT) and an answerer bot (ABOT) which plays out over multiple rounds of dialog.\"]}"
    },
    {
        "gold": {
            "text": [
                "We compare the performance of the two agents ALICE SL and ALICE RL in the GuessWhich game.",
                "These bots are state-ofthe-art visual dialog agents with respect to emulating human responses and generating visually discriminative responses in AI-AI dialog.",
                "#TARGET_REF ) evaluate these agents against strong baselines and report AI-AI team results that are significantly better than chance on a pool of ∼10k images (rank ∼1000 for SL, rank ∼500 for RL).",
                "In addition to evaluating them in the context of human-AI teams we also report QBOT-ALICE team performances for reference.",
                "In Table 1 , we compare the performances of human-ALICE SL and human-ALICE RL teams according to Mean Rank (MR) and Mean Reciprocal Rank (MRR) of the secret image based on the guesses H makes at the end of dialog."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We compare the performance of the two agents ALICE SL and ALICE RL in the GuessWhich game. These bots are state-ofthe-art visual dialog agents with respect to emulating human responses and generating visually discriminative responses in AI-AI dialog. #TARGET_REF ) evaluate these agents against strong baselines and report AI-AI team results that are significantly better than chance on a pool of ∼10k images (rank ∼1000 for SL, rank ∼500 for RL). In addition to evaluating them in the context of human-AI teams we also report QBOT-ALICE team performances for reference. In Table 1 , we compare the performances of human-ALICE SL and human-ALICE RL teams according to Mean Rank (MR) and Mean Reciprocal Rank (MRR) of the secret image based on the guesses H makes at the end of dialog.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF ) evaluate these agents against strong baselines and report AI-AI team results that are significantly better than chance on a pool of \\u223c10k images (rank \\u223c1000 for SL, rank \\u223c500 for RL).\"]}"
    },
    {
        "gold": {
            "text": [
                "GuessWhich.",
                "In this work, we propose to evaluate if and how this progress in AI-AI evaluation translates to the performance of human-AI teams.",
                "Inspired by the popular GuessWhat or 20-Questions game, we design a human computation game -GuessWhich -which requires collaboration between human and visual conversational AI agents.",
                "Mirroring the setting of #TARGET_REF , GuessWhich is an image-guessing game that consists of 2 participants -questioner and answerer.",
                "At the start of the game, the answerer is provided an image that is unknown to the questioner and both questioner and answerer are given a brief description of the image content."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "GuessWhich. In this work, we propose to evaluate if and how this progress in AI-AI evaluation translates to the performance of human-AI teams. Inspired by the popular GuessWhat or 20-Questions game, we design a human computation game -GuessWhich -which requires collaboration between human and visual conversational AI agents. Mirroring the setting of #TARGET_REF , GuessWhich is an image-guessing game that consists of 2 participants -questioner and answerer. At the start of the game, the answerer is provided an image that is unknown to the questioner and both questioner and answerer are given a brief description of the image content.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"Inspired by the popular GuessWhat or 20-Questions game, we design a human computation game -GuessWhich -which requires collaboration between human and visual conversational AI agents.\", \"Mirroring the setting of #TARGET_REF , GuessWhich is an image-guessing game that consists of 2 participants -questioner and answerer.\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate human-AI team performance in GuessWhich, for the setting where the questioner is a human and the answerer is an AI (that we denote ALICE).",
                "Specifically, we evaluate two versions of ALICE for GuessWhich:",
                "1. ALICE SL which is trained in a supervised manner on the Visual Dialog dataset (#REFa ) to mimic the answers given by humans when engaged in a conversation with other humans about an image, and 2.",
                "ALICE RL which is pre-trained with supervised learning and fine-tuned via reinforcement learning for an imageguessing task as in #TARGET_REF .",
                "It is important to appreciate the difficulty and sensitivity of the GuessWhich game as an evaluation tool -agents have to understand human questions and respond with accurate, consistent, fluent and informative answers for the human-AI team to do well."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                1,
                1,
                1,
                1,
                0
            ]
        },
        "input": "We evaluate human-AI team performance in GuessWhich, for the setting where the questioner is a human and the answerer is an AI (that we denote ALICE). Specifically, we evaluate two versions of ALICE for GuessWhich: 1. ALICE SL which is trained in a supervised manner on the Visual Dialog dataset (#REFa ) to mimic the answers given by humans when engaged in a conversation with other humans about an image, and 2. ALICE RL which is pre-trained with supervised learning and fine-tuned via reinforcement learning for an imageguessing task as in #TARGET_REF . It is important to appreciate the difficulty and sensitivity of the GuessWhich game as an evaluation tool -agents have to understand human questions and respond with accurate, consistent, fluent and informative answers for the human-AI team to do well.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"We evaluate human-AI team performance in GuessWhich, for the setting where the questioner is a human and the answerer is an AI (that we denote ALICE).\", \"Specifically, we evaluate two versions of ALICE for GuessWhich:\", \"1. ALICE SL which is trained in a supervised manner on the Visual Dialog dataset (#REFa ) to mimic the answers given by humans when engaged in a conversation with other humans about an image, and 2.\", \"ALICE RL which is pre-trained with supervised learning and fine-tuned via reinforcement learning for an imageguessing task as in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "ALICE is assigned a secret image and answers questions asked about that image from a human for 9 rounds to help them identify the secret image (Sec. 4).",
                "• We evaluate human-AI team performance on this game for both supervised learning (SL) and reinforcement learning (RL) versions of ALICE.",
                "Our main experimental finding is that despite significant differences between SL and RL agents reported in previous work #TARGET_REF , we find no significant difference in performance between ALICE SL or ALICE RL when paired with human partners (Sec. 6.1).",
                "This suggests that while self-talk and RL are interesting directions to pursue for building better visual conversational agents, there appears to be a disconnect between AI-AI and human-AI evaluations -progress on former does not seem predictive of progress on latter.",
                "This is an important finding to guide future research."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "ALICE is assigned a secret image and answers questions asked about that image from a human for 9 rounds to help them identify the secret image (Sec. 4). • We evaluate human-AI team performance on this game for both supervised learning (SL) and reinforcement learning (RL) versions of ALICE. Our main experimental finding is that despite significant differences between SL and RL agents reported in previous work #TARGET_REF , we find no significant difference in performance between ALICE SL or ALICE RL when paired with human partners (Sec. 6.1). This suggests that while self-talk and RL are interesting directions to pursue for building better visual conversational agents, there appears to be a disconnect between AI-AI and human-AI evaluations -progress on former does not seem predictive of progress on latter. This is an important finding to guide future research.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Our main experimental finding is that despite significant differences between SL and RL agents reported in previous work #TARGET_REF , we find no significant difference in performance between ALICE SL or ALICE RL when paired with human partners (Sec. 6.1).\"]}"
    },
    {
        "gold": {
            "text": [
                "On both metrics, however, the differences are within the standard error margins (reported in the table) and not statisti- Table 1 : Performance of Human-ALICE teams with AL-ICE SL and ALICE RL measured by MR (lower is better) and MRR (higher is better).",
                "Error bars are 95% CIs from 1000 bootstrap samples.",
                "Unlike #TARGET_REF , we find no significant difference between ALICE SL and ALICE RL .",
                "cally significant.",
                "As we collected additional data, the error margins became smaller but the means also became closer."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "On both metrics, however, the differences are within the standard error margins (reported in the table) and not statisti- Table 1 : Performance of Human-ALICE teams with AL-ICE SL and ALICE RL measured by MR (lower is better) and MRR (higher is better). Error bars are 95% CIs from 1000 bootstrap samples. Unlike #TARGET_REF , we find no significant difference between ALICE SL and ALICE RL . cally significant. As we collected additional data, the error margins became smaller but the means also became closer.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Unlike #TARGET_REF , we find no significant difference between ALICE SL and ALICE RL .\"]}"
    },
    {
        "gold": {
            "text": [
                "Error bars are 95% CIs from 1000 bootstrap samples.",
                "Unlike (#REFb) , we find no significant difference between ALICE SL and ALICE RL .",
                "cally significant.",
                "As we collected additional data, the error margins became smaller but the means also became closer.",
                "This interesting finding stands in stark contrast to the results reported by #TARGET_REF , where ALICE RL was found to be significantly more accurate than ALICE SL when evaluated in an AI-AI team."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "Error bars are 95% CIs from 1000 bootstrap samples. Unlike (#REFb) , we find no significant difference between ALICE SL and ALICE RL . cally significant. As we collected additional data, the error margins became smaller but the means also became closer. This interesting finding stands in stark contrast to the results reported by #TARGET_REF , where ALICE RL was found to be significantly more accurate than ALICE SL when evaluated in an AI-AI team.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"As we collected additional data, the error margins became smaller but the means also became closer.\", \"This interesting finding stands in stark contrast to the results reported by #TARGET_REF , where ALICE RL was found to be significantly more accurate than ALICE SL when evaluated in an AI-AI team.\"]}"
    },
    {
        "gold": {
            "text": [
                "Question-answering characters are designed to sustain a conversation driven primarily by the user asking questions.",
                "#TARGET_REF developed algorithms for training such characters using linked questions and responses in the form of unstructured natural language text.",
                "Given a novel user question, the character finds an appropriate response from a list of available responses, and when a direct answer is not available, the character selects an \"off-topic\" response according to a set policy, ensuring that the conversation remains coherent even with a finite number of responses.",
                "The response selection algorithms are languageindependent, also allowing the questions and responses to be in separate languages.",
                "These algorithms have been incorporated into a tool which has been used to create characters for a variety of applications (e.g. #REF; #REF; #REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Question-answering characters are designed to sustain a conversation driven primarily by the user asking questions. #TARGET_REF developed algorithms for training such characters using linked questions and responses in the form of unstructured natural language text. Given a novel user question, the character finds an appropriate response from a list of available responses, and when a direct answer is not available, the character selects an \"off-topic\" response according to a set policy, ensuring that the conversation remains coherent even with a finite number of responses. The response selection algorithms are languageindependent, also allowing the questions and responses to be in separate languages. These algorithms have been incorporated into a tool which has been used to create characters for a variety of applications (e.g. #REF; #REF; #REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF developed algorithms for training such characters using linked questions and responses in the form of unstructured natural language text.\"]}"
    },
    {
        "gold": {
            "text": [
                "The response selection algorithms are languageindependent, also allowing the questions and responses to be in separate languages.",
                "These algorithms have been incorporated into a tool which has been used to create characters for a variety of applications (e.g. #TARGET_REF; #REF; #REF) .",
                "To date, most characters created using these principles understood and spoke only English; one fairly limited character spoke Pashto, a language of Afghanistan (#REF) .",
                "To test language portability we chose Tamil, a Dravidian language spoken primarily in southern India.",
                "Tamil has close to 70 million speakers worldwide (#REF) , is the official language of Tamil Nadu and Puducherry in India (#REF) , and an official language in Sri Lanka and Singapore."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The response selection algorithms are languageindependent, also allowing the questions and responses to be in separate languages. These algorithms have been incorporated into a tool which has been used to create characters for a variety of applications (e.g. #TARGET_REF; #REF; #REF) . To date, most characters created using these principles understood and spoke only English; one fairly limited character spoke Pashto, a language of Afghanistan (#REF) . To test language portability we chose Tamil, a Dravidian language spoken primarily in southern India. Tamil has close to 70 million speakers worldwide (#REF) , is the official language of Tamil Nadu and Puducherry in India (#REF) , and an official language in Sri Lanka and Singapore.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"These algorithms have been incorporated into a tool which has been used to create characters for a variety of applications (e.g. #TARGET_REF; #REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "We use accuracy as our success measure: the top ranked response to a test question is considered correct if it is identified as a correct response in the linked test data (there are up to 4 correct responses per question).",
                "This measure does not take into account non-understanding, that is the classifier's determination that the best response is not good enough #TARGET_REF , since this capability was not implemented; however, since all of our test questions are known to have at least one appropriate response, any non-understanding of a question would necessarily count against accuracy anyway."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "We use accuracy as our success measure: the top ranked response to a test question is considered correct if it is identified as a correct response in the linked test data (there are up to 4 correct responses per question). This measure does not take into account non-understanding, that is the classifier's determination that the best response is not good enough #TARGET_REF , since this capability was not implemented; however, since all of our test questions are known to have at least one appropriate response, any non-understanding of a question would necessarily count against accuracy anyway.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"This measure does not take into account non-understanding, that is the classifier's determination that the best response is not good enough #TARGET_REF , since this capability was not implemented; however, since all of our test questions are known to have at least one appropriate response, any non-understanding of a question would necessarily count against accuracy anyway.\"]}"
    },
    {
        "gold": {
            "text": [
                "We reimplemented parts of the response ranking algorithms of #TARGET_REF , including both the language modeling (LM) and cross-language modeling (CLM) approaches.",
                "The LM approach constructs language models for both questions and responses using the question vocabulary.",
                "For each training question S, a language model is estimated as the frequency distribution of tokens in S, smoothed by the distribution of tokens in the entire question corpus (eq. 1).",
                "The language model of a novel question Q is estimated as the probability of each token in the vocabulary coinciding with Q (eq. 2).",
                "Each available response R is associated with a pseudo-question Q R made up by the concatenation of all the questions linked to R in the training data."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We reimplemented parts of the response ranking algorithms of #TARGET_REF , including both the language modeling (LM) and cross-language modeling (CLM) approaches. The LM approach constructs language models for both questions and responses using the question vocabulary. For each training question S, a language model is estimated as the frequency distribution of tokens in S, smoothed by the distribution of tokens in the entire question corpus (eq. 1). The language model of a novel question Q is estimated as the probability of each token in the vocabulary coinciding with Q (eq. 2). Each available response R is associated with a pseudo-question Q R made up by the concatenation of all the questions linked to R in the training data.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"We reimplemented parts of the response ranking algorithms of #TARGET_REF , including both the language modeling (LM) and cross-language modeling (CLM) approaches.\"]}"
    },
    {
        "gold": {
            "text": [
                "The sum in eq. (5) is over all linked questionresponse pairs {S j , R j } in the training data, and the product is an estimate the probability of the question Q given the training question S j .",
                "In eq. (6), V R is the entire response vocabulary.",
                "We did not implement the parameter learning of #TARGET_REF ; instead we use a constant smoothing parameter λ π = λ φ = 0.1.",
                "We also do not use the response threshold parameter, which #REF use to determine whether the top-ranked response is good enough.",
                "Instead, we just check for the correctness of the top-ranked response."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The sum in eq. (5) is over all linked questionresponse pairs {S j , R j } in the training data, and the product is an estimate the probability of the question Q given the training question S j . In eq. (6), V R is the entire response vocabulary. We did not implement the parameter learning of #TARGET_REF ; instead we use a constant smoothing parameter λ π = λ φ = 0.1. We also do not use the response threshold parameter, which #REF use to determine whether the top-ranked response is good enough. Instead, we just check for the correctness of the top-ranked response.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We did not implement the parameter learning of #TARGET_REF ; instead we use a constant smoothing parameter \\u03bb \\u03c0 = \\u03bb \\u03c6 = 0.1.\"]}"
    },
    {
        "gold": {
            "text": [
                "The sum in eq. (5) is over all linked questionresponse pairs {S j , R j } in the training data, and the product is an estimate the probability of the question Q given the training question S j .",
                "In eq. (6), V R is the entire response vocabulary.",
                "We did not implement the parameter learning of #REF ; instead we use a constant smoothing parameter λ π = λ φ = 0.1.",
                "We also do not use the response threshold parameter, which #TARGET_REF use to determine whether the top-ranked response is good enough.",
                "Instead, we just check for the correctness of the top-ranked response."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The sum in eq. (5) is over all linked questionresponse pairs {S j , R j } in the training data, and the product is an estimate the probability of the question Q given the training question S j . In eq. (6), V R is the entire response vocabulary. We did not implement the parameter learning of #REF ; instead we use a constant smoothing parameter λ π = λ φ = 0.1. We also do not use the response threshold parameter, which #TARGET_REF use to determine whether the top-ranked response is good enough. Instead, we just check for the correctness of the top-ranked response.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We also do not use the response threshold parameter, which #TARGET_REF use to determine whether the top-ranked response is good enough.\"]}"
    },
    {
        "gold": {
            "text": [
                "The results of the experiments with matched question and response languages are reported in Table 1.",
                "The LM approach almost invariably produced better results than the CLM approach; this is the opposite of the findings of #TARGET_REF , where CLM fared consistently better.",
                "In most cases, the errors produced by the CLM approach were a superset of those of the LM approach; the only exceptions were Tamil with stemming.",
                "Accuracy of response selection on the Tamil data is about 10% lower than that of English, or twice the errors (6 errors rather than 3).",
                "The errors of automatically translated Tamil are a superset of the English errors; however, manually translated Tamil did get right some of the errors of English."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The results of the experiments with matched question and response languages are reported in Table 1. The LM approach almost invariably produced better results than the CLM approach; this is the opposite of the findings of #TARGET_REF , where CLM fared consistently better. In most cases, the errors produced by the CLM approach were a superset of those of the LM approach; the only exceptions were Tamil with stemming. Accuracy of response selection on the Tamil data is about 10% lower than that of English, or twice the errors (6 errors rather than 3). The errors of automatically translated Tamil are a superset of the English errors; however, manually translated Tamil did get right some of the errors of English.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The LM approach almost invariably produced better results than the CLM approach; this is the opposite of the findings of #TARGET_REF , where CLM fared consistently better.\"]}"
    },
    {
        "gold": {
            "text": [
                "Another alternative is to use both languages together for classification; the fact that the manual Tamil translation identified some responses missed by the English classifier suggests that there may be benefit to this approach.",
                "Another direction for future work is identifying bad responses by using the distance between question and response to plot the tradeoff curve between errors and return rates (#REF) .",
                "In our experiments the LM approach consistently outperforms the CLM approach, contra #TARGET_REF .",
                "Our data may not be quite natural: while the English data are well tested, our sampling method may introduce biases that affect the results. But even if we achieved full English-like performance using machine translation, the questions that Tamil speakers want to ask will likely be somewhat different than those of English speakers.",
                "A translated dialogue system is therefore only an initial step towards tailoring a system to a new population."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Another alternative is to use both languages together for classification; the fact that the manual Tamil translation identified some responses missed by the English classifier suggests that there may be benefit to this approach. Another direction for future work is identifying bad responses by using the distance between question and response to plot the tradeoff curve between errors and return rates (#REF) . In our experiments the LM approach consistently outperforms the CLM approach, contra #TARGET_REF . Our data may not be quite natural: while the English data are well tested, our sampling method may introduce biases that affect the results. But even if we achieved full English-like performance using machine translation, the questions that Tamil speakers want to ask will likely be somewhat different than those of English speakers. A translated dialogue system is therefore only an initial step towards tailoring a system to a new population.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"In our experiments the LM approach consistently outperforms the CLM approach, contra #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The supervision was either given in the form of meaning representations aligned with sentences (#REF; #REF; #REF) or in a somewhat more relaxed form, such as lists of candidate meanings for each sentence (#REF; #REF) or formal representations of the described world state for each text #TARGET_REF .",
                "Such annotated resources are scarce and expensive to create, motivating the need for unsupervised or semi-supervised techniques (#REF ).",
                "However, unsupervised methods have their own challenges: they are not always able to discover semantic equivalences of lexical entries or logical forms or, on the contrary, cluster semantically different or even opposite expressions (#REF ).",
                "Unsupervised approaches can only rely on distributional similarity of contexts (#REF) to decide on semantic relatedness of terms, but this information may be sparse and not reliable (#REF) .",
                "For example, when analyzing weather forecasts it is very hard to discover in an unsupervised way which of the expressions among \"south wind\", \"wind from west\" and \"southerly\" denote the same wind direction and which are not, as they all have a very similar distribution of their contexts."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The supervision was either given in the form of meaning representations aligned with sentences (#REF; #REF; #REF) or in a somewhat more relaxed form, such as lists of candidate meanings for each sentence (#REF; #REF) or formal representations of the described world state for each text #TARGET_REF . Such annotated resources are scarce and expensive to create, motivating the need for unsupervised or semi-supervised techniques (#REF ). However, unsupervised methods have their own challenges: they are not always able to discover semantic equivalences of lexical entries or logical forms or, on the contrary, cluster semantically different or even opposite expressions (#REF ). Unsupervised approaches can only rely on distributional similarity of contexts (#REF) to decide on semantic relatedness of terms, but this information may be sparse and not reliable (#REF) . For example, when analyzing weather forecasts it is very hard to discover in an unsupervised way which of the expressions among \"south wind\", \"wind from west\" and \"southerly\" denote the same wind direction and which are not, as they all have a very similar distribution of their contexts.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The supervision was either given in the form of meaning representations aligned with sentences (#REF; #REF; #REF) or in a somewhat more relaxed form, such as lists of candidate meanings for each sentence (#REF; #REF) or formal representations of the described world state for each text #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The rest of the paper is structured as follows.",
                "In section 2 we describe our inference algorithm for groups of non-contradictory documents.",
                "Section 3 redescribes the semantics-text correspondence model #TARGET_REF in the context of our learning scenario.",
                "In section 4 we provide an empirical evaluation of the proposed method.",
                "We conclude in section 5 with an examination of additional related work."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The rest of the paper is structured as follows. In section 2 we describe our inference algorithm for groups of non-contradictory documents. Section 3 redescribes the semantics-text correspondence model #TARGET_REF in the context of our learning scenario. In section 4 we provide an empirical evaluation of the proposed method. We conclude in section 5 with an examination of additional related work.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Section 3 redescribes the semantics-text correspondence model #TARGET_REF in the context of our learning scenario.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section we will describe our inference method on a higher conceptual level, not specifying the underlying meaning representation and the probabilistic model.",
                "An instantiation of the algorithm for the semantics-text correspondence model is given in section 3.2.",
                "Statistical models of parsing can often be regarded as defining the probability distribution of meaning m and its alignment a with the given text w, P (m, a, w) = P (a, w|m)P (m).",
                "The semantics m can be represented either as a logical formula (see, e.g., (#REF )) or as a set of field values if database records are used as a meaning representation #TARGET_REF ).",
                "The alignment a defines how semantics is verbalized in the text w, and it can be represented by a meaning derivation tree in case of full semantic parsing (#REF) or, e.g., by a hierarchical segmentation into utterances along with an utterance-field alignment in a more shallow variation of the problem."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "In this section we will describe our inference method on a higher conceptual level, not specifying the underlying meaning representation and the probabilistic model. An instantiation of the algorithm for the semantics-text correspondence model is given in section 3.2. Statistical models of parsing can often be regarded as defining the probability distribution of meaning m and its alignment a with the given text w, P (m, a, w) = P (a, w|m)P (m). The semantics m can be represented either as a logical formula (see, e.g., (#REF )) or as a set of field values if database records are used as a meaning representation #TARGET_REF ). The alignment a defines how semantics is verbalized in the text w, and it can be represented by a meaning derivation tree in case of full semantic parsing (#REF) or, e.g., by a hierarchical segmentation into utterances along with an utterance-field alignment in a more shallow variation of the problem.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The semantics m can be represented either as a logical formula (see, e.g., (#REF )) or as a set of field values if database records are used as a meaning representation #TARGET_REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "We select the model parameters θ by maximizing the marginal likelihood of the data, where the data D is given in the form of groups w = {w 1 ,..., w K } sharing the same latent state: 5 max θ w∈D s",
                "To estimate the parameters, we use the Expectation-Maximization algorithm (#REF) .",
                "When the world state is observable, learning does not require any approximations, as dynamic programming (a form of the forward-backward algorithm) can be used to infer the posterior distribution on the E-step #TARGET_REF .",
                "However, when the state is latent, dependencies are not local anymore, and approximate inference is required.",
                "We use the algorithm described in section 2 (figure 2) to infer the state."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We select the model parameters θ by maximizing the marginal likelihood of the data, where the data D is given in the form of groups w = {w 1 ,..., w K } sharing the same latent state: 5 max θ w∈D s To estimate the parameters, we use the Expectation-Maximization algorithm (#REF) . When the world state is observable, learning does not require any approximations, as dynamic programming (a form of the forward-backward algorithm) can be used to infer the posterior distribution on the E-step #TARGET_REF . However, when the state is latent, dependencies are not local anymore, and approximate inference is required. We use the algorithm described in section 2 (figure 2) to infer the state.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"When the world state is observable, learning does not require any approximations, as dynamic programming (a form of the forward-backward algorithm) can be used to infer the posterior distribution on the E-step #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Therefore, it is natural to evaluate their model on the database-text alignment problem (#REF) , i.e. measuring how well the model predicts the alignment between the text and the observable records describing the entire world state.",
                "We follow their set-up, but assume that instead of having access to the full semantic state for every training example, we have a very small amount of data annotated with semantic states and a larger number of unannotated texts with noncontradictory semantics.",
                "We study our set-up on the weather forecast data #TARGET_REF where the original textual weather forecasts were complemented by additional forecasts describing the same weather states (see figure 1 for an example).",
                "The average overlap between the verbalized fields in each group of noncontradictory forecasts was below 35%, and more than 60% of fields are mentioned only in a single forecast from a group.",
                "Our model, learned from 100 labeled forecasts and 259 groups of unannotated non-contradictory forecasts (750 texts in total), achieved 73.9% F 1 ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Therefore, it is natural to evaluate their model on the database-text alignment problem (#REF) , i.e. measuring how well the model predicts the alignment between the text and the observable records describing the entire world state. We follow their set-up, but assume that instead of having access to the full semantic state for every training example, we have a very small amount of data annotated with semantic states and a larger number of unannotated texts with noncontradictory semantics. We study our set-up on the weather forecast data #TARGET_REF where the original textual weather forecasts were complemented by additional forecasts describing the same weather states (see figure 1 for an example). The average overlap between the verbalized fields in each group of noncontradictory forecasts was below 35%, and more than 60% of fields are mentioned only in a single forecast from a group. Our model, learned from 100 labeled forecasts and 259 groups of unannotated non-contradictory forecasts (750 texts in total), achieved 73.9% F 1 .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We study our set-up on the weather forecast data #TARGET_REF where the original textual weather forecasts were complemented by additional forecasts describing the same weather states (see figure 1 for an example).\"]}"
    },
    {
        "gold": {
            "text": [
                "Though the semantics m k (k / ∈ n∪{j}) used in the estimates (line 6) can be inconsistent with each other, the final list of meanings m is guaranteed to be consistent.",
                "It holds because on each iteration we add a single meaningm n i to m (line 7), and m n i is guaranteed to be consistent with m , as the semanticsm n i was conditioned on the meaning m during inference (line 4).",
                "An important aspect of this algorithm is that unlike usual greedy inference, the remaining ('future') texts do affect the choice of meaning representations made on the earlier stages.",
                "As soon as semantics m k are inferred for every k, we find ourselves in the set-up of learning with unaligned semantic states considered in #TARGET_REF .",
                "The induced alignments a 1 ,..., a K of semantics m to texts w 1 ,..., w K at the same time induce alignments between the texts."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Though the semantics m k (k / ∈ n∪{j}) used in the estimates (line 6) can be inconsistent with each other, the final list of meanings m is guaranteed to be consistent. It holds because on each iteration we add a single meaningm n i to m (line 7), and m n i is guaranteed to be consistent with m , as the semanticsm n i was conditioned on the meaning m during inference (line 4). An important aspect of this algorithm is that unlike usual greedy inference, the remaining ('future') texts do affect the choice of meaning representations made on the earlier stages. As soon as semantics m k are inferred for every k, we find ourselves in the set-up of learning with unaligned semantic states considered in #TARGET_REF . The induced alignments a 1 ,..., a K of semantics m to texts w 1 ,..., w K at the same time induce alignments between the texts.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"As soon as semantics m k are inferred for every k, we find ourselves in the set-up of learning with unaligned semantic states considered in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Though the most likely alignmentâ j for a fixed semantic representationm j can be found efficiently using a Viterbi algorithm, computing the most probable pair (â j ,m j ) is still intractable.",
                "We use a modification of the beam search algorithm, where we keep a set of candidate meanings (partial semantic representations) and compute an alignment for each of them using a form of the Viterbi algorithm.",
                "As soon as the meaning representations m are inferred, we find ourselves in the set-up studied in #TARGET_REF ): the state s is no longer latent and we can run efficient inference on the E-step.",
                "Though some fields of the state s may still not be specified by m , we prohibit utterances from aligning to these non-specified fields.",
                "On the M-step of EM the parameters are estimated as proportional to the expected marginal counts computed on the E-step."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Though the most likely alignmentâ j for a fixed semantic representationm j can be found efficiently using a Viterbi algorithm, computing the most probable pair (â j ,m j ) is still intractable. We use a modification of the beam search algorithm, where we keep a set of candidate meanings (partial semantic representations) and compute an alignment for each of them using a form of the Viterbi algorithm. As soon as the meaning representations m are inferred, we find ourselves in the set-up studied in #TARGET_REF ): the state s is no longer latent and we can run efficient inference on the E-step. Though some fields of the state s may still not be specified by m , we prohibit utterances from aligning to these non-specified fields. On the M-step of EM the parameters are estimated as proportional to the expected marginal counts computed on the E-step.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"As soon as the meaning representations m are inferred, we find ourselves in the set-up studied in #TARGET_REF ): the state s is no longer latent and we can run efficient inference on the E-step.\"]}"
    },
    {
        "gold": {
            "text": [
                "To perform the experiments we used a subset of the weather dataset introduced in #TARGET_REF ).",
                "The original dataset contains 22,146 texts of 28.7 words on average, there are 12 types of records (predicates) and 36.0 records per forecast on average.",
                "We randomly chose 100 texts along with their world states to be used as the labeled data.",
                "6 To produce groups of noncontradictory texts we have randomly selected a subset of weather states, represented them in a visual form (icons accompanied by numerical and symbolic parameters) and then manually annotated these illustrations.",
                "These newly-produced forecasts, when combined with the original texts, resulted in 259 groups of non-contradictory texts (650 texts, 2.5 texts per group)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "To perform the experiments we used a subset of the weather dataset introduced in #TARGET_REF ). The original dataset contains 22,146 texts of 28.7 words on average, there are 12 types of records (predicates) and 36.0 records per forecast on average. We randomly chose 100 texts along with their world states to be used as the labeled data. 6 To produce groups of noncontradictory texts we have randomly selected a subset of weather states, represented them in a visual form (icons accompanied by numerical and symbolic parameters) and then manually annotated these illustrations. These newly-produced forecasts, when combined with the original texts, resulted in 259 groups of non-contradictory texts (650 texts, 2.5 texts per group).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"To perform the experiments we used a subset of the weather dataset introduced in #TARGET_REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "We aimed to preserve approximately the same proportion of new and original examples as we had in the training set, therefore, we combined 50 texts originally present in the weather dataset with additional 100 newly-produced texts.",
                "We annotated these 100 texts by aligning each line to one or more records, 7 whereas for the original texts the alignments were already present.",
                "Following #TARGET_REF we evaluate the models on how well they predict these alignments.",
                "When estimating the model parameters, we followed the training regime prescribed in (#REF) .",
                "Namely, 5 iterations of EM with a basic model (with no segmentation or coherence modeling), followed by 5 iterations of EM with the model which generates fields independently and, at last, 5 iterations with the full model."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We aimed to preserve approximately the same proportion of new and original examples as we had in the training set, therefore, we combined 50 texts originally present in the weather dataset with additional 100 newly-produced texts. We annotated these 100 texts by aligning each line to one or more records, 7 whereas for the original texts the alignments were already present. Following #TARGET_REF we evaluate the models on how well they predict these alignments. When estimating the model parameters, we followed the training regime prescribed in (#REF) . Namely, 5 iterations of EM with a basic model (with no segmentation or coherence modeling), followed by 5 iterations of EM with the model which generates fields independently and, at last, 5 iterations with the full model.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following #TARGET_REF we evaluate the models on how well they predict these alignments.\"]}"
    },
    {
        "gold": {
            "text": [
                "We aimed to preserve approximately the same proportion of new and original examples as we had in the training set, therefore, we combined 50 texts originally present in the weather dataset with additional 100 newly-produced texts.",
                "We annotated these 100 texts by aligning each line to one or more records, 7 whereas for the original texts the alignments were already present.",
                "Following #REF we evaluate the models on how well they predict these alignments.",
                "When estimating the model parameters, we followed the training regime prescribed in #TARGET_REF .",
                "Namely, 5 iterations of EM with a basic model (with no segmentation or coherence modeling), followed by 5 iterations of EM with the model which generates fields independently and, at last, 5 iterations with the full model."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We aimed to preserve approximately the same proportion of new and original examples as we had in the training set, therefore, we combined 50 texts originally present in the weather dataset with additional 100 newly-produced texts. We annotated these 100 texts by aligning each line to one or more records, 7 whereas for the original texts the alignments were already present. Following #REF we evaluate the models on how well they predict these alignments. When estimating the model parameters, we followed the training regime prescribed in #TARGET_REF . Namely, 5 iterations of EM with a basic model (with no segmentation or coherence modeling), followed by 5 iterations of EM with the model which generates fields independently and, at last, 5 iterations with the full model.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"When estimating the model parameters, we followed the training regime prescribed in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "However, exact inference for groups of documents with overlapping semantic representation is generally prohibitively expensive, as the shared latent semantics introduces nonlocal dependences between semantic representations of individual documents.",
                "To combat it, we proposed a simple iterative inference algorithm.",
                "We showed how it can be instantiated for the semantics-text correspondence model #TARGET_REF ) and evaluated it on a dataset of weather forecasts.",
                "Our approach resulted in an improvement over the scores of both the supervised baseline and of the traditional semi-supervised learning.",
                "There are many directions we plan on investigating in the future for the problem of learning semantics with non-contradictory relations."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "However, exact inference for groups of documents with overlapping semantic representation is generally prohibitively expensive, as the shared latent semantics introduces nonlocal dependences between semantic representations of individual documents. To combat it, we proposed a simple iterative inference algorithm. We showed how it can be instantiated for the semantics-text correspondence model #TARGET_REF ) and evaluated it on a dataset of weather forecasts. Our approach resulted in an improvement over the scores of both the supervised baseline and of the traditional semi-supervised learning. There are many directions we plan on investigating in the future for the problem of learning semantics with non-contradictory relations.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We showed how it can be instantiated for the semantics-text correspondence model #TARGET_REF ) and evaluated it on a dataset of weather forecasts.\"]}"
    },
    {
        "gold": {
            "text": [
                "The problem of predicting text-meaning alignments is interesting in itself, as the extracted alignments can be used in training of a statistical generation system or information extractors, but we also believe that evaluation on this problem is an appropriate test for the relative comparison of the semantic analyzers' performance.",
                "Additionally, note that the success of our weaklysupervised scenario indirectly suggests that the model is sufficiently accurate in predicting semantics of an unlabeled text, as otherwise there would be no useful information passed in between semantically overlapping documents during learning and, consequently, no improvement from sharing the state.",
                "8 To confirm that the model trained by our approach indeed assigns new words to correct fields and records, we visualize top words for the field characterizing sky cover (table 2).",
                "Note that the words \"sun\", \"cloudiness\" or \"gaps\" were not appearing in the labeled part of the data, but seem to be assigned to correct categories.",
                "However, correlation between rain and overcast, as also noted in #TARGET_REF , results in the wrong assignment of the rain-related words to the field value corresponding to very cloudy weather."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The problem of predicting text-meaning alignments is interesting in itself, as the extracted alignments can be used in training of a statistical generation system or information extractors, but we also believe that evaluation on this problem is an appropriate test for the relative comparison of the semantic analyzers' performance. Additionally, note that the success of our weaklysupervised scenario indirectly suggests that the model is sufficiently accurate in predicting semantics of an unlabeled text, as otherwise there would be no useful information passed in between semantically overlapping documents during learning and, consequently, no improvement from sharing the state. 8 To confirm that the model trained by our approach indeed assigns new words to correct fields and records, we visualize top words for the field characterizing sky cover (table 2). Note that the words \"sun\", \"cloudiness\" or \"gaps\" were not appearing in the labeled part of the data, but seem to be assigned to correct categories. However, correlation between rain and overcast, as also noted in #TARGET_REF , results in the wrong assignment of the rain-related words to the field value corresponding to very cloudy weather.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"However, correlation between rain and overcast, as also noted in #TARGET_REF , results in the wrong assignment of the rain-related words to the field value corresponding to very cloudy weather.\"]}"
    },
    {
        "gold": {
            "text": [
                "Namely, 5 iterations of EM with a basic model (with no segmentation or coherence modeling), followed by 5 iterations of EM with the model which generates fields independently and, at last, 5 iterations with the full model.",
                "Only then, in the semi-supervised learning scenarios, we added unlabeled data and ran 5 additional iterations of EM.",
                "Instead of prohibiting records from crossing punctuation, as suggested by #TARGET_REF , in our implementation we disregard the words not attached to specific fields (attached to the nullfield, see section 3.1) when computing spans of records.",
                "To speed-up training, only a single record of each type is allowed to be generated when running inference for unlabeled examples on the E- Table 1 : Results (precision, recall and F 1 ) on the weather forecast dataset.",
                "step of the EM algorithm, as it significantly reduces the search space."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Namely, 5 iterations of EM with a basic model (with no segmentation or coherence modeling), followed by 5 iterations of EM with the model which generates fields independently and, at last, 5 iterations with the full model. Only then, in the semi-supervised learning scenarios, we added unlabeled data and ran 5 additional iterations of EM. Instead of prohibiting records from crossing punctuation, as suggested by #TARGET_REF , in our implementation we disregard the words not attached to specific fields (attached to the nullfield, see section 3.1) when computing spans of records. To speed-up training, only a single record of each type is allowed to be generated when running inference for unlabeled examples on the E- Table 1 : Results (precision, recall and F 1 ) on the weather forecast dataset. step of the EM algorithm, as it significantly reduces the search space.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Instead of prohibiting records from crossing punctuation, as suggested by #TARGET_REF , in our implementation we disregard the words not attached to specific fields (attached to the nullfield, see section 3.1) when computing spans of records.\"]}"
    },
    {
        "gold": {
            "text": [
                "Distributed representations of multimodal embeddings (Feng & #REF) are receiving increasing attention recently in the machine learning literature, and techniques developed have found a wide spectrum of applications in the real world.",
                "These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary #TARGET_REF; Glenberg & #REF; ).",
                "As such, there has been development towards so-called multimodal distributional semantic models (Silberer & #REF; #REF; #REF; #REF; #REF) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.",
                "The work introduced in #REF sought to address many of the drawbacks of these models.",
                "In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Distributed representations of multimodal embeddings (Feng & #REF) are receiving increasing attention recently in the machine learning literature, and techniques developed have found a wide spectrum of applications in the real world. These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary #TARGET_REF; Glenberg & #REF; ). As such, there has been development towards so-called multimodal distributional semantic models (Silberer & #REF; #REF; #REF; #REF; #REF) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in #REF sought to address many of the drawbacks of these models. In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary #TARGET_REF; Glenberg & #REF; ).\"]}"
    },
    {
        "gold": {
            "text": [
                "Distributed representations of multimodal embeddings (Feng & #REF) are receiving increasing attention recently in the machine learning literature, and techniques developed have found a wide spectrum of applications in the real world.",
                "These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary (#REF; Glenberg & #REF; ).",
                "As such, there has been development towards so-called multimodal distributional semantic models (Silberer & #REF; #TARGET_REF; #REF; #REF; #REF) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.",
                "The work introduced in #REF sought to address many of the drawbacks of these models.",
                "In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Distributed representations of multimodal embeddings (Feng & #REF) are receiving increasing attention recently in the machine learning literature, and techniques developed have found a wide spectrum of applications in the real world. These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary (#REF; Glenberg & #REF; ). As such, there has been development towards so-called multimodal distributional semantic models (Silberer & #REF; #TARGET_REF; #REF; #REF; #REF) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in #REF sought to address many of the drawbacks of these models. In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"As such, there has been development towards so-called multimodal distributional semantic models (Silberer & #REF; #TARGET_REF; #REF; #REF; #REF) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.\"]}"
    },
    {
        "gold": {
            "text": [
                "These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary (#REF; Glenberg & #REF; ).",
                "As such, there has been development towards so-called multimodal distributional semantic models (Silberer & #REF; #REF; #REF; #REF; #REF) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.",
                "The work introduced in #TARGET_REF sought to address many of the drawbacks of these models.",
                "In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language.",
                "Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary (#REF; Glenberg & #REF; ). As such, there has been development towards so-called multimodal distributional semantic models (Silberer & #REF; #REF; #REF; #REF; #REF) , which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in #TARGET_REF sought to address many of the drawbacks of these models. In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language. Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The work introduced in #TARGET_REF sought to address many of the drawbacks of these models.\"]}"
    },
    {
        "gold": {
            "text": [
                "In the last few years, there has been a wealth of literature on multimodal representational models.",
                "As explained in #TARGET_REF , the majority of this literature focuses on constructing textual and visual representations independently and then combining them under some metrics.",
                "#REF utilize a direct approach to \"mixing\" the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition.",
                "The image vectors used here, though, are constructed using the bag-of-visual-words method.",
                "In Kiela & #REF , the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In the last few years, there has been a wealth of literature on multimodal representational models. As explained in #TARGET_REF , the majority of this literature focuses on constructing textual and visual representations independently and then combining them under some metrics. #REF utilize a direct approach to \"mixing\" the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition. The image vectors used here, though, are constructed using the bag-of-visual-words method. In Kiela & #REF , the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"As explained in #TARGET_REF , the majority of this literature focuses on constructing textual and visual representations independently and then combining them under some metrics.\"]}"
    },
    {
        "gold": {
            "text": [
                "In Kiela & #REF , the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text.",
                "Similarly, #REF also utilizes the skip-gram architecture and convolutional features; however the two modalities are then combined using a natural similarity metric.",
                "Other recent work has presented several methods for directly incorporating visual context in neural language models.",
                "In #REF , word context is enhanced by global visual context; i.e., a single image is used as the context for the whole sentence (conversely, the sentence acts as a caption for the image).",
                "The multimodal skip-gram architecture proposed by #TARGET_REF takes a more fine-grained approach by incorporating word-level visual context and concurrently training words to predict other text words in the window as well as their visual representation."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "In Kiela & #REF , the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text. Similarly, #REF also utilizes the skip-gram architecture and convolutional features; however the two modalities are then combined using a natural similarity metric. Other recent work has presented several methods for directly incorporating visual context in neural language models. In #REF , word context is enhanced by global visual context; i.e., a single image is used as the context for the whole sentence (conversely, the sentence acts as a caption for the image). The multimodal skip-gram architecture proposed by #TARGET_REF takes a more fine-grained approach by incorporating word-level visual context and concurrently training words to predict other text words in the window as well as their visual representation.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The multimodal skip-gram architecture proposed by #TARGET_REF takes a more fine-grained approach by incorporating word-level visual context and concurrently training words to predict other text words in the window as well as their visual representation.\"]}"
    },
    {
        "gold": {
            "text": [
                "Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words.",
                "As a result, the induced multimodal representations and multimodal mapping no longer rely on the assumption of full visual coverage of the vocabulary, so the results are able to generalize beyond the initial training set and to be applied to various representation-related tasks, such as image annotation or retrieval.",
                "In this work, we introduce a further refinement on the multimodal skip-gram architecture, building upon the approaches of Mikolov et al. (2013a; b) , , and #TARGET_REF .",
                "Rather than adding a visual term to the linguistic training objective, we directly situate terms in a visual context by replacing relevant words with multimodal pseudowords, derived by composing the textual representations with convolutional features projected into the multimodal space.",
                "In this way, we further address the grounding problem of Glenberg & #REF by incorpo-rating the word-level visual modality directly into the sentence context."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words. As a result, the induced multimodal representations and multimodal mapping no longer rely on the assumption of full visual coverage of the vocabulary, so the results are able to generalize beyond the initial training set and to be applied to various representation-related tasks, such as image annotation or retrieval. In this work, we introduce a further refinement on the multimodal skip-gram architecture, building upon the approaches of Mikolov et al. (2013a; b) , , and #TARGET_REF . Rather than adding a visual term to the linguistic training objective, we directly situate terms in a visual context by replacing relevant words with multimodal pseudowords, derived by composing the textual representations with convolutional features projected into the multimodal space. In this way, we further address the grounding problem of Glenberg & #REF by incorpo-rating the word-level visual modality directly into the sentence context.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"In this work, we introduce a further refinement on the multimodal skip-gram architecture, building upon the approaches of Mikolov et al. (2013a; b) , , and #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "For the visual data, we use the image data from ILSVRC 2012 (#REF) and the corresponding Wordnet hierarchy (#REF) to represent a word visually if the word or any of its hyponyms has an entry in Imagenet and occurs more than 500 times in the text corpus.",
                "This yields approximately 5,100 \"visual\" words.",
                "To construct the vectors for the visual representations, we follow a similar experimental set-up as that used by #TARGET_REF .",
                "In each of the cases described above-centroid and hypersphere-, we randomly sample 100 images from the corresponding synsets of Imagenet for each visual word and use a pre-trained convolutional neural network as described in #REF via the Caffe toolkit (#REF) to extract a 4096-dimensional vector representation of each image.",
                "We then treat the 100 vectors corresponding to each of the 5,100 visual words as clusters in the 4096-dimensional visual space."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "For the visual data, we use the image data from ILSVRC 2012 (#REF) and the corresponding Wordnet hierarchy (#REF) to represent a word visually if the word or any of its hyponyms has an entry in Imagenet and occurs more than 500 times in the text corpus. This yields approximately 5,100 \"visual\" words. To construct the vectors for the visual representations, we follow a similar experimental set-up as that used by #TARGET_REF . In each of the cases described above-centroid and hypersphere-, we randomly sample 100 images from the corresponding synsets of Imagenet for each visual word and use a pre-trained convolutional neural network as described in #REF via the Caffe toolkit (#REF) to extract a 4096-dimensional vector representation of each image. We then treat the 100 vectors corresponding to each of the 5,100 visual words as clusters in the 4096-dimensional visual space.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"To construct the vectors for the visual representations, we follow a similar experimental set-up as that used by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We evaluate three versions of our model on these benchmarks: pseudowords using the centroid method (PSUEDOWORDS-C), pseudowords using the hypersphere method (PSEUDOWORDS-H), and the centroid method with a randomly initialized mapping (PSEUDOWORDS-RAN), as explained below.",
                "Existing Multimodal Models We compare our results on these benchmarks against previously published results for other multimodal word embeddings .",
                "Using the results published by #TARGET_REF and a target word embedding of 300, we compare our results to their MMSKIP-GRAM-A and MMSKIP-GRAM-B, which maximize the similarity of the textual and visual representations Table 1 : Spearman correlation between the generated multimodal similarities and the benchmark human judgments.",
                "In all cases, results are reported on the full set of word similarity pairs.",
                "under a max-margin framework.; the former constrains the dimensionality of the visual features to be the same as the word embeddings, while the latter learns an explicit mapping between the textual and visual spaces."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "We evaluate three versions of our model on these benchmarks: pseudowords using the centroid method (PSUEDOWORDS-C), pseudowords using the hypersphere method (PSEUDOWORDS-H), and the centroid method with a randomly initialized mapping (PSEUDOWORDS-RAN), as explained below. Existing Multimodal Models We compare our results on these benchmarks against previously published results for other multimodal word embeddings . Using the results published by #TARGET_REF and a target word embedding of 300, we compare our results to their MMSKIP-GRAM-A and MMSKIP-GRAM-B, which maximize the similarity of the textual and visual representations Table 1 : Spearman correlation between the generated multimodal similarities and the benchmark human judgments. In all cases, results are reported on the full set of word similarity pairs. under a max-margin framework.; the former constrains the dimensionality of the visual features to be the same as the word embeddings, while the latter learns an explicit mapping between the textual and visual spaces.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Using the results published by #TARGET_REF and a target word embedding of 300, we compare our results to their MMSKIP-GRAM-A and MMSKIP-GRAM-B, which maximize the similarity of the textual and visual representations Table 1 : Spearman correlation between the generated multimodal similarities and the benchmark human judgments.\"]}"
    },
    {
        "gold": {
            "text": [
                "On the other hand, when the mapping is quickly pretrained on existing distributed word representations, the results are greatly improved.",
                "In the cases of capturing general relatedness and pure visual similarity, the multimodal model of #TARGET_REF performs better.",
                "However, in the case of capturing semantic word similarity, our model performs signficantly better than MMSKIP-GRAM-B (although it should be noted that these results are roughly on par with the benchmark authors (Silberer & #REF) and a point below the non-mapping MMSKIP-GRAM-A).",
                "Although further work is needed to examine this result, the performance of the model in this case can be visualized through an example.",
                "Table 2 provides some insights on the changes made to the word embeddings as a result of the inclusion of visual information in the learning process."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "On the other hand, when the mapping is quickly pretrained on existing distributed word representations, the results are greatly improved. In the cases of capturing general relatedness and pure visual similarity, the multimodal model of #TARGET_REF performs better. However, in the case of capturing semantic word similarity, our model performs signficantly better than MMSKIP-GRAM-B (although it should be noted that these results are roughly on par with the benchmark authors (Silberer & #REF) and a point below the non-mapping MMSKIP-GRAM-A). Although further work is needed to examine this result, the performance of the model in this case can be visualized through an example. Table 2 provides some insights on the changes made to the word embeddings as a result of the inclusion of visual information in the learning process.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"In the cases of capturing general relatedness and pure visual similarity, the multimodal model of #TARGET_REF performs better.\", \"However, in the case of capturing semantic word similarity, our model performs signficantly better than MMSKIP-GRAM-B (although it should be noted that these results are roughly on par with the benchmark authors (Silberer & #REF) and a point below the non-mapping MMSKIP-GRAM-A).\"]}"
    },
    {
        "gold": {
            "text": [
                "Research efforts in order to solve this issue have steadily increased (#REF; #REF; #REF; #REF; #REF) .",
                "In language modeling, interpretability often takes place via an attention mechanism in the neural network (#REF; #REF; #REF; #REF) .",
                "In this context, attention essentially allows a network to identify which words in a sentence are more relevant.",
                "Beyond interpretability, this often results in improved decision making by the network.",
                "Recently, #TARGET_REF proposed the Transformer architecture for machine translation."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Research efforts in order to solve this issue have steadily increased (#REF; #REF; #REF; #REF; #REF) . In language modeling, interpretability often takes place via an attention mechanism in the neural network (#REF; #REF; #REF; #REF) . In this context, attention essentially allows a network to identify which words in a sentence are more relevant. Beyond interpretability, this often results in improved decision making by the network. Recently, #TARGET_REF proposed the Transformer architecture for machine translation.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Recently, #TARGET_REF proposed the Transformer architecture for machine translation.\"]}"
    },
    {
        "gold": {
            "text": [
                ". . . ; x T n ] be the concatenation of a sequence of n vectors giving a matrix X ∈ R n×d such that x i ∈ R d .",
                "#TARGET_REF defined attention as a function with as input a triplet containing queries Q, keys K with associated values V .",
                "In the case of self-attention, Q, K and V are linear projections of X. Thus, we define the dot-product (#REF) .",
                "The self-attention block is repeated N times.",
                "self-attention mechanism as follows."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": ". . . ; x T n ] be the concatenation of a sequence of n vectors giving a matrix X ∈ R n×d such that x i ∈ R d . #TARGET_REF defined attention as a function with as input a triplet containing queries Q, keys K with associated values V . In the case of self-attention, Q, K and V are linear projections of X. Thus, we define the dot-product (#REF) . The self-attention block is repeated N times. self-attention mechanism as follows.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF defined attention as a function with as input a triplet containing queries Q, keys K with associated values V .\"]}"
    },
    {
        "gold": {
            "text": [
                ". . . ; x T n ] be the concatenation of a sequence of n vectors giving a matrix X ∈ R n×d such that x i ∈ R d .",
                "#REF defined attention as a function with as input a triplet containing queries Q, keys K with associated values V .",
                "In the case of self-attention, Q, K and V are linear projections of X. Thus, we define the dot-product #TARGET_REF .",
                "The self-attention block is repeated N times.",
                "self-attention mechanism as follows."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": ". . . ; x T n ] be the concatenation of a sequence of n vectors giving a matrix X ∈ R n×d such that x i ∈ R d . #REF defined attention as a function with as input a triplet containing queries Q, keys K with associated values V . In the case of self-attention, Q, K and V are linear projections of X. Thus, we define the dot-product #TARGET_REF . The self-attention block is repeated N times. self-attention mechanism as follows.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In the case of self-attention, Q, K and V are linear projections of X. Thus, we define the dot-product #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "They were introduced by #REF , where they achieved state-of-the-art in machine translation.",
                "Since then, attention mechanisms have been used in other language modeling tasks such as image captioning (#REF) , question answer-ing (#REF; #REF) , and text classification (#REF) .",
                "The concept of self-attention (#REF; #REF) , central to our proposed approach, has shown great promises in natural language processing; It produced state-of-the-art results for machine translation #TARGET_REF .",
                "In text classification, the focus on interpretability has thus far been limited.",
                "#REF used a convolutional neural network (CNN) with Class Activation Mapping (CAM) (#REF) to do sentiment analysis."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "They were introduced by #REF , where they achieved state-of-the-art in machine translation. Since then, attention mechanisms have been used in other language modeling tasks such as image captioning (#REF) , question answer-ing (#REF; #REF) , and text classification (#REF) . The concept of self-attention (#REF; #REF) , central to our proposed approach, has shown great promises in natural language processing; It produced state-of-the-art results for machine translation #TARGET_REF . In text classification, the focus on interpretability has thus far been limited. #REF used a convolutional neural network (CNN) with Class Activation Mapping (CAM) (#REF) to do sentiment analysis.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"The concept of self-attention (#REF; #REF) , central to our proposed approach, has shown great promises in natural language processing; It produced state-of-the-art results for machine translation #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Pre-trained embeddings, like GloVe (#REF) , may be used and fine-tuned during the learning process.",
                "Next, to inject information about the order of the words, the positional encoding layer adds location information to each word.",
                "We use the positional encoding vectors that were defined by #TARGET_REF as follows.",
                "A linear layer then performs dimensionality reduction/augmentation of the embedding space to a vector space of dimension d, which is kept constant throughout the network.",
                "It is followed by one or several \"self-attention blocks\" stacked one onto another."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Pre-trained embeddings, like GloVe (#REF) , may be used and fine-tuned during the learning process. Next, to inject information about the order of the words, the positional encoding layer adds location information to each word. We use the positional encoding vectors that were defined by #TARGET_REF as follows. A linear layer then performs dimensionality reduction/augmentation of the embedding space to a vector space of dimension d, which is kept constant throughout the network. It is followed by one or several \"self-attention blocks\" stacked one onto another.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We use the positional encoding vectors that were defined by #TARGET_REF as follows.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, their approach limits the range and acuteness of the interactions between the words in the text. and #REF both combined an attention mechanism with a recurrent neural network.",
                "The main difference with our work is, while being interpretable, these approaches do not perform true word-on-word attention across a whole sequence such as our self-attention layer.",
                "3 SANet: Self-Attention Network Inspired by the Transformer architecture #TARGET_REF which performed machine translation without recurrent or convolutional layers, we propose the Self-Attention Network (SANet) architecture targeting instead text classification.",
                "One key difference between our approach and #REF 's is that we only perform input-input attention with self-attention, as we do not have sequences as output but a text classification.",
                "Moreover, we employ global max pooling at the top, which enables our architecture to process input sequences of arbitrary length."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "However, their approach limits the range and acuteness of the interactions between the words in the text. and #REF both combined an attention mechanism with a recurrent neural network. The main difference with our work is, while being interpretable, these approaches do not perform true word-on-word attention across a whole sequence such as our self-attention layer. 3 SANet: Self-Attention Network Inspired by the Transformer architecture #TARGET_REF which performed machine translation without recurrent or convolutional layers, we propose the Self-Attention Network (SANet) architecture targeting instead text classification. One key difference between our approach and #REF 's is that we only perform input-input attention with self-attention, as we do not have sequences as output but a text classification. Moreover, we employ global max pooling at the top, which enables our architecture to process input sequences of arbitrary length.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"3 SANet: Self-Attention Network Inspired by the Transformer architecture #TARGET_REF which performed machine translation without recurrent or convolutional layers, we propose the Self-Attention Network (SANet) architecture targeting instead text classification.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, their approach limits the range and acuteness of the interactions between the words in the text. and #REF both combined an attention mechanism with a recurrent neural network.",
                "The main difference with our work is, while being interpretable, these approaches do not perform true word-on-word attention across a whole sequence such as our self-attention layer.",
                "3 SANet: Self-Attention Network Inspired by the Transformer architecture (#REF) which performed machine translation without recurrent or convolutional layers, we propose the Self-Attention Network (SANet) architecture targeting instead text classification.",
                "One key difference between our approach and #TARGET_REF 's is that we only perform input-input attention with self-attention, as we do not have sequences as output but a text classification.",
                "Moreover, we employ global max pooling at the top, which enables our architecture to process input sequences of arbitrary length."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "However, their approach limits the range and acuteness of the interactions between the words in the text. and #REF both combined an attention mechanism with a recurrent neural network. The main difference with our work is, while being interpretable, these approaches do not perform true word-on-word attention across a whole sequence such as our self-attention layer. 3 SANet: Self-Attention Network Inspired by the Transformer architecture (#REF) which performed machine translation without recurrent or convolutional layers, we propose the Self-Attention Network (SANet) architecture targeting instead text classification. One key difference between our approach and #TARGET_REF 's is that we only perform input-input attention with self-attention, as we do not have sequences as output but a text classification. Moreover, we employ global max pooling at the top, which enables our architecture to process input sequences of arbitrary length.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"One key difference between our approach and #TARGET_REF 's is that we only perform input-input attention with self-attention, as we do not have sequences as output but a text classification.\"]}"
    },
    {
        "gold": {
            "text": [
                "These blocks are comprised of a selfattention layer followed by a feed-forward network, both with residual connections.",
                "Contrary to #TARGET_REF , we only use a single attention head, with attention performed on the complete sequence with constant d-dimensional inputs.",
                "The feed-forward network consists of a single hidden layer with a ReLU.",
                "Where W 1 , W 2 ∈ R d×d are learned parameters.",
                "The \"Add & Norm\" layer is a residual connection defined by LayerNorm(x + SubLayer(x)), where SubLayer(x) is the output of the previous layer and LayerNorm is a layer normalization method introduced by #REF ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "These blocks are comprised of a selfattention layer followed by a feed-forward network, both with residual connections. Contrary to #TARGET_REF , we only use a single attention head, with attention performed on the complete sequence with constant d-dimensional inputs. The feed-forward network consists of a single hidden layer with a ReLU. Where W 1 , W 2 ∈ R d×d are learned parameters. The \"Add & Norm\" layer is a residual connection defined by LayerNorm(x + SubLayer(x)), where SubLayer(x) is the output of the previous layer and LayerNorm is a layer normalization method introduced by #REF .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Contrary to #TARGET_REF , we only use a single attention head, with attention performed on the complete sequence with constant d-dimensional inputs.\"]}"
    },
    {
        "gold": {
            "text": [
                "In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (#REF) .",
                "Lexical dialectology is (in part) the converse of user geolocation (#REF) : given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions.",
                "The complexity of the task is two-fold: (1) localised named entities (e.g. sporting team names) are not of interest; and (2) without semantic knowledge it is difficult to detect terms that are in general use but have a special meaning in a region.",
                "In this paper we propose a text-based geolocation method based on neural networks.",
                "Our contributions are as follows: (1) we achieve state-of-the-art results on benchmark Twitter geolocation datasets; (2) we show that the model is less sensitive to the specific location discretisation method; (3) we release the first broad-coverage dataset for evaluation of lexical dialectology models; (4) we incorporate our text-based model into a network-based model #TARGET_REF and improve the performance utilising both network and text; and (5) we use the model's embeddings for extraction of local terms and show that it outperforms two baselines."
            ],
            "label": [
                "BACK",
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (#REF) . Lexical dialectology is (in part) the converse of user geolocation (#REF) : given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions. The complexity of the task is two-fold: (1) localised named entities (e.g. sporting team names) are not of interest; and (2) without semantic knowledge it is difficult to detect terms that are in general use but have a special meaning in a region. In this paper we propose a text-based geolocation method based on neural networks. Our contributions are as follows: (1) we achieve state-of-the-art results on benchmark Twitter geolocation datasets; (2) we show that the model is less sensitive to the specific location discretisation method; (3) we release the first broad-coverage dataset for evaluation of lexical dialectology models; (4) we incorporate our text-based model into a network-based model #TARGET_REF and improve the performance utilising both network and text; and (5) we use the model's embeddings for extraction of local terms and show that it outperforms two baselines.",
        "output": "{\"label\": [\"BACK\", \"EXT\"], \"context\": [\"In this paper we propose a text-based geolocation method based on neural networks.\", \"Our contributions are as follows: (1) we achieve state-of-the-art results on benchmark Twitter geolocation datasets; (2) we show that the model is less sensitive to the specific location discretisation method; (3) we release the first broad-coverage dataset for evaluation of lexical dialectology models; (4) we incorporate our text-based model into a network-based model #TARGET_REF and improve the performance utilising both network and text; and (5) we use the model's embeddings for extraction of local terms and show that it outperforms two baselines.\"]}"
    },
    {
        "gold": {
            "text": [
                "The classification models often rely on less than 1% of geotagged tweets for supervision and discretise real-valued coordinates into equalsized grids (#REF ), administrative regions (#REF; #REF; #REF; #REF , or flat (#REF) or hierarchical k-d tree clusters (#REF) .",
                "Network-based methods also use either real-valued coordinates (#REF) or discretised regions #TARGET_REF as labels, and use label propagation over the interaction graph (e.g. @-mentions).",
                "More recent methods have focused on representation learning by using sparse coding (#REF) or neural networks (#REF) , utilising both text and network information (#REFa) .",
                "Dialect is a variety of language shared by a group of speakers (#REF) .",
                "Our focus here is on geographical dialects which are spoken (and written in social media) by people from particular areas."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The classification models often rely on less than 1% of geotagged tweets for supervision and discretise real-valued coordinates into equalsized grids (#REF ), administrative regions (#REF; #REF; #REF; #REF , or flat (#REF) or hierarchical k-d tree clusters (#REF) . Network-based methods also use either real-valued coordinates (#REF) or discretised regions #TARGET_REF as labels, and use label propagation over the interaction graph (e.g. @-mentions). More recent methods have focused on representation learning by using sparse coding (#REF) or neural networks (#REF) , utilising both text and network information (#REFa) . Dialect is a variety of language shared by a group of speakers (#REF) . Our focus here is on geographical dialects which are spoken (and written in social media) by people from particular areas.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Network-based methods also use either real-valued coordinates (#REF) or discretised regions #TARGET_REF as labels, and use label propagation over the interaction graph (e.g. @-mentions).\"]}"
    },
    {
        "gold": {
            "text": [
                "Network-based methods also use either real-valued coordinates (#REF) or discretised regions (#REFa) as labels, and use label propagation over the interaction graph (e.g. @-mentions).",
                "More recent methods have focused on representation learning by using sparse coding (#REF) or neural networks (#REF) , utilising both text and network information #TARGET_REF .",
                "Dialect is a variety of language shared by a group of speakers (#REF) .",
                "Our focus here is on geographical dialects which are spoken (and written in social media) by people from particular areas.",
                "The traditional approach to dialectology is to find the geographical distribution of known lexical alternatives (e.g. you, yall and yinz: (#REF; #REF; Gonçalves and Sánchez, 2014; #REF; #REF; #REF) ), the shortcoming of which is that the alternative lexical variables must be known beforehand."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Network-based methods also use either real-valued coordinates (#REF) or discretised regions (#REFa) as labels, and use label propagation over the interaction graph (e.g. @-mentions). More recent methods have focused on representation learning by using sparse coding (#REF) or neural networks (#REF) , utilising both text and network information #TARGET_REF . Dialect is a variety of language shared by a group of speakers (#REF) . Our focus here is on geographical dialects which are spoken (and written in social media) by people from particular areas. The traditional approach to dialectology is to find the geographical distribution of known lexical alternatives (e.g. you, yall and yinz: (#REF; #REF; Gonçalves and Sánchez, 2014; #REF; #REF; #REF) ), the shortcoming of which is that the alternative lexical variables must be known beforehand.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"More recent methods have focused on representation learning by using sparse coding (#REF) or neural networks (#REF) , utilising both text and network information #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Our method outperforms both the flat and hierarchical text-based models by a large margin.",
                "Comparing the two discretisation strategies, k-means outperforms k-d tree by a reasonable margin.",
                "We also incorporated the MLP predictions into a network-based model based on the method of #TARGET_REF , and improved upon their work.",
                "We analysed the Table 2 : Nearest neighbours of place names.",
                "in Figure 3 ."
            ],
            "label": [
                "BACK",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Our method outperforms both the flat and hierarchical text-based models by a large margin. Comparing the two discretisation strategies, k-means outperforms k-d tree by a reasonable margin. We also incorporated the MLP predictions into a network-based model based on the method of #TARGET_REF , and improved upon their work. We analysed the Table 2 : Nearest neighbours of place names. in Figure 3 .",
        "output": "{\"label\": [\"BACK\", \"EXT\"], \"context\": [\"We also incorporated the MLP predictions into a network-based model based on the method of #TARGET_REF , and improved upon their work.\"]}"
    },
    {
        "gold": {
            "text": [
                "4 The results reported in Rahimi et al. (2015b; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset.",
                "While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user #TARGET_REF .",
                "Note that it would, of course, be possible to combine text and network information in a joint deep learning model (#REF; #REF) , which we leave to future work (noting that scalability will potentially be a major issue for the larger datasets).",
                "To test the applicability of the model's embeddings in dialectology, we created DAREDS.",
                "The output of the hidden layer of the model is used as embeddings for both location names and dialect terms."
            ],
            "label": [
                "EXT",
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "4 The results reported in Rahimi et al. (2015b; 2015a) for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset. While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \"dongle\" nodes to each user node, providing a personalised geolocation prior for each user #TARGET_REF . Note that it would, of course, be possible to combine text and network information in a joint deep learning model (#REF; #REF) , which we leave to future work (noting that scalability will potentially be a major issue for the larger datasets). To test the applicability of the model's embeddings in dialectology, we created DAREDS. The output of the hidden layer of the model is used as embeddings for both location names and dialect terms.",
        "output": "{\"label\": [\"EXT\", \"DIFFER\"], \"context\": [\"While the focus of this paper is text-based user geolocation, state-of-the-art results for the three datasets have been achieved with hybrid text+network-based models, where the predictions of the text-based model are fed into a mention network as \\\"dongle\\\" nodes to each user node, providing a personalised geolocation prior for each user #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The number of regions, regularisation strength, hidden layer and mini-batch size are tuned over development data and set to (32, 10 −5 , 896, 100), (256, 10 −6 , 2048, 10000) and (930, 10 −6 , 3720, 10000) for GEOTEXT, TWITTER-US and TWITTER-WORLD, respectively.",
                "The parameters are optimised using Adamx (#REF) using Lasagne/Theano (Theano Development #REF) .",
                "Following #REF and #REF , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\").",
                "Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161.",
                "4 The results reported in Rahimi et al. (2015b; #TARGET_REF for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The number of regions, regularisation strength, hidden layer and mini-batch size are tuned over development data and set to (32, 10 −5 , 896, 100), (256, 10 −6 , 2048, 10000) and (930, 10 −6 , 3720, 10000) for GEOTEXT, TWITTER-US and TWITTER-WORLD, respectively. The parameters are optimised using Adamx (#REF) using Lasagne/Theano (Theano Development #REF) . Following #REF and #REF , we evaluated the geolocation model using mean and median error in km (\"Mean\" and \"Median\" resp.) and accuracy within 161km of the actual location (\"Acc@161\"). Note that lower numbers are better for Mean and Median, and higher numbers better for Acc@161. 4 The results reported in Rahimi et al. (2015b; #TARGET_REF for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"4 The results reported in Rahimi et al. (2015b; #TARGET_REF for TWITTER-WORLD were over a superset of the dataset; the results reported here are based on the actual dataset.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, these are not 'pure' neural approaches for QE.",
                "#TARGET_REF apply neural machine translation (NMT) models, based on recurrent neural network, to sentence-level QE.",
                "This is the first try of using NMT models for the translation quality estimation.",
                "This recurrent neural network based quality estimation model is a pure neural approach for QE and achieves a competitive performance in sentence-level QE (English-Spanish).",
                "In this paper, we extend the recurrent neural network based quality estimation model to word and phrase level."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "However, these are not 'pure' neural approaches for QE. #TARGET_REF apply neural machine translation (NMT) models, based on recurrent neural network, to sentence-level QE. This is the first try of using NMT models for the translation quality estimation. This recurrent neural network based quality estimation model is a pure neural approach for QE and achieves a competitive performance in sentence-level QE (English-Spanish). In this paper, we extend the recurrent neural network based quality estimation model to word and phrase level.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF apply neural machine translation (NMT) models, based on recurrent neural network, to sentence-level QE.\"]}"
    },
    {
        "gold": {
            "text": [
                "Each quality vector for each target word has, as not a number unit but a vector unit, the quality information about whether each target word is properly translated from source sentence.",
                "Each quality vector is generated by decomposing the probability of each target word from the modified NMT model.",
                "1 #TARGET_REF modify the NMT model to 1) use source and target A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output",
                "Figure 1: First part of recurrent neural network based quality estimation model for generating quality vectors (#REF) sentences as inputs, 2 2) apply bidirectional RNNs both on source and target sentences, which enable to fully utilize the bidirectional quality information, and 3) generate quality vectors for target words as outputs.",
                "In the second part ( Figure 2 , 3 and 4), the final quality of translation at various level (sentencelevel/word-level/phrase-level) is predicted by using the quality vectors as inputs."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Each quality vector for each target word has, as not a number unit but a vector unit, the quality information about whether each target word is properly translated from source sentence. Each quality vector is generated by decomposing the probability of each target word from the modified NMT model. 1 #TARGET_REF modify the NMT model to 1) use source and target A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output Figure 1: First part of recurrent neural network based quality estimation model for generating quality vectors (#REF) sentences as inputs, 2 2) apply bidirectional RNNs both on source and target sentences, which enable to fully utilize the bidirectional quality information, and 3) generate quality vectors for target words as outputs. In the second part ( Figure 2 , 3 and 4), the final quality of translation at various level (sentencelevel/word-level/phrase-level) is predicted by using the quality vectors as inputs.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"1 #TARGET_REF modify the NMT model to 1) use source and target A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output\"]}"
    },
    {
        "gold": {
            "text": [
                "1 #REF modify the NMT model to 1) use source and target A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output",
                "Figure 1: First part of recurrent neural network based quality estimation model for generating quality vectors #TARGET_REF sentences as inputs, 2 2) apply bidirectional RNNs both on source and target sentences, which enable to fully utilize the bidirectional quality information, and 3) generate quality vectors for target words as outputs.",
                "In the second part ( Figure 2 , 3 and 4), the final quality of translation at various level (sentencelevel/word-level/phrase-level) is predicted by using the quality vectors as inputs.",
                "#REF apply RNN based model to sentence-level QE and we extend this model to word and phraselevel QE.",
                "In subsection 2.1, 2.2 and 2.3, we describe the RNN based 3 (second part) sentence, word and phrase-level QE models."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "1 #REF modify the NMT model to 1) use source and target A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output Figure 1: First part of recurrent neural network based quality estimation model for generating quality vectors #TARGET_REF sentences as inputs, 2 2) apply bidirectional RNNs both on source and target sentences, which enable to fully utilize the bidirectional quality information, and 3) generate quality vectors for target words as outputs. In the second part ( Figure 2 , 3 and 4), the final quality of translation at various level (sentencelevel/word-level/phrase-level) is predicted by using the quality vectors as inputs. #REF apply RNN based model to sentence-level QE and we extend this model to word and phraselevel QE. In subsection 2.1, 2.2 and 2.3, we describe the RNN based 3 (second part) sentence, word and phrase-level QE models.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Figure 1: First part of recurrent neural network based quality estimation model for generating quality vectors #TARGET_REF sentences as inputs, 2 2) apply bidirectional RNNs both on source and target sentences, which enable to fully utilize the bidirectional quality information, and 3) generate quality vectors for target words as outputs.\"]}"
    },
    {
        "gold": {
            "text": [
                "In RNN based sentence-level QE model (Figure 2) , HTER (human-targeted translation edit rate) (#REF) in [0,1] for target sentence is predicted by using a logistic sigmoid func-A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output #TARGET_REF tion such that",
                "(1)",
                "W s is the weight matrix of sigmoid function 5 at sentence-level QE.",
                "s is a summary unit of the sequential quality vectors and is fixed to the last hidden state 6 h T y of RNN.",
                "The hidden state h j is computed by"
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In RNN based sentence-level QE model (Figure 2) , HTER (human-targeted translation edit rate) (#REF) in [0,1] for target sentence is predicted by using a logistic sigmoid func-A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output #TARGET_REF tion such that (1) W s is the weight matrix of sigmoid function 5 at sentence-level QE. s is a summary unit of the sequential quality vectors and is fixed to the last hidden state 6 h T y of RNN. The hidden state h j is computed by",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In RNN based sentence-level QE model (Figure 2) , HTER (human-targeted translation edit rate) (#REF) in [0,1] for target sentence is predicted by using a logistic sigmoid func-A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output #TARGET_REF tion such that\"]}"
    },
    {
        "gold": {
            "text": [
                "(1)",
                "W s is the weight matrix of sigmoid function 5 at sentence-level QE.",
                "s is a summary unit of the sequential quality vectors and is fixed to the last hidden state 6 h T y of RNN.",
                "The hidden state h j is computed by",
                "where f is the activation function of RNN #TARGET_REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "(1) W s is the weight matrix of sigmoid function 5 at sentence-level QE. s is a summary unit of the sequential quality vectors and is fixed to the last hidden state 6 h T y of RNN. The hidden state h j is computed by where f is the activation function of RNN #TARGET_REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The hidden state h j is computed by\", \"where f is the activation function of RNN #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 1: First part of recurrent neural network based quality estimation model for generating quality vectors (#REF) sentences as inputs, 2 2) apply bidirectional RNNs both on source and target sentences, which enable to fully utilize the bidirectional quality information, and 3) generate quality vectors for target words as outputs.",
                "In the second part ( Figure 2 , 3 and 4), the final quality of translation at various level (sentencelevel/word-level/phrase-level) is predicted by using the quality vectors as inputs.",
                "#TARGET_REF apply RNN based model to sentence-level QE and we extend this model to word and phraselevel QE.",
                "In subsection 2.1, 2.2 and 2.3, we describe the RNN based 3 (second part) sentence, word and phrase-level QE models.",
                "4 The cause of these separated parts of the QE model comes from the insufficiency of QE datasets to train the whole QE model."
            ],
            "label": [
                "USE",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Figure 1: First part of recurrent neural network based quality estimation model for generating quality vectors (#REF) sentences as inputs, 2 2) apply bidirectional RNNs both on source and target sentences, which enable to fully utilize the bidirectional quality information, and 3) generate quality vectors for target words as outputs. In the second part ( Figure 2 , 3 and 4), the final quality of translation at various level (sentencelevel/word-level/phrase-level) is predicted by using the quality vectors as inputs. #TARGET_REF apply RNN based model to sentence-level QE and we extend this model to word and phraselevel QE. In subsection 2.1, 2.2 and 2.3, we describe the RNN based 3 (second part) sentence, word and phrase-level QE models. 4 The cause of these separated parts of the QE model comes from the insufficiency of QE datasets to train the whole QE model.",
        "output": "{\"label\": [\"USE\", \"EXT\"], \"context\": [\"#TARGET_REF apply RNN based model to sentence-level QE and we extend this model to word and phraselevel QE.\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, POS tags can be projected via word alignments, and the projected POS is then used to train a model in the lowresource language #REF; #TARGET_REF .",
                "These methods overall have limited effectiveness due to errors in the alignment and fundamental differences between the languages.",
                "They also assume a large parallel corpus, which may not be available for many low-resource languages.",
                "To address these limitations, we propose a new technique for low resource tagging, with more modest resource requirements: 1) a bilingual dictionary; 2) monolingual corpora in the high and low resource languages; and 3) a small annotated corpus of around 1, 000 tokens in the low-resource language.",
                "The first two resources are used as a form of distant supervision through learning crosslingual word embeddings over the monolingual corpora and bilingual dictionary (#REF) ."
            ],
            "label": [
                "BACK",
                "DIFFER",
                "MOT"
            ],
            "context": [
                1,
                1,
                1,
                1,
                0
            ]
        },
        "input": "For example, POS tags can be projected via word alignments, and the projected POS is then used to train a model in the lowresource language #REF; #TARGET_REF . These methods overall have limited effectiveness due to errors in the alignment and fundamental differences between the languages. They also assume a large parallel corpus, which may not be available for many low-resource languages. To address these limitations, we propose a new technique for low resource tagging, with more modest resource requirements: 1) a bilingual dictionary; 2) monolingual corpora in the high and low resource languages; and 3) a small annotated corpus of around 1, 000 tokens in the low-resource language. The first two resources are used as a form of distant supervision through learning crosslingual word embeddings over the monolingual corpora and bilingual dictionary (#REF) .",
        "output": "{\"label\": [\"BACK\", \"DIFFER\", \"MOT\"], \"context\": [\"For example, POS tags can be projected via word alignments, and the projected POS is then used to train a model in the lowresource language #REF; #TARGET_REF .\", \"These methods overall have limited effectiveness due to errors in the alignment and fundamental differences between the languages.\", \"They also assume a large parallel corpus, which may not be available for many low-resource languages.\", \"To address these limitations, we propose a new technique for low resource tagging, with more modest resource requirements: 1) a bilingual dictionary; 2) monolingual corpora in the high and low resource languages; and 3) a small annotated corpus of around 1, 000 tokens in the low-resource language.\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, the CRFBiLSTM POS tagger obtained the state-of-theart performance on Penn Treebank WSJ corpus (#REF) .",
                "However, in low-resource languages, these models are seldom used because of limited labelled data.",
                "Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems in low-resource languages (#REF; Täckström et al., 2013; #TARGET_REF; #REF) .",
                "#REF pioneered the use of parallel data for projecting POS tag information from one language to another language.",
                "used parallel data and exploited graph-based label propagation to expand the coverage of labelled tokens."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "For example, the CRFBiLSTM POS tagger obtained the state-of-theart performance on Penn Treebank WSJ corpus (#REF) . However, in low-resource languages, these models are seldom used because of limited labelled data. Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems in low-resource languages (#REF; Täckström et al., 2013; #TARGET_REF; #REF) . #REF pioneered the use of parallel data for projecting POS tag information from one language to another language. used parallel data and exploited graph-based label propagation to expand the coverage of labelled tokens.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems in low-resource languages (#REF; T\\u00e4ckstr\\u00f6m et al., 2013; #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "To model this data we employ the same model structure as above but augmented with a second perceptron output layer, as illustrated in Figure 1 (right) .",
                "Formally,ỹ t ∼ Categorial(õ t ) whereõ t = MLP(o t ) is a single hidden layer perceptron with tanh activation and softmax output transformation.",
                "This component allows for a more expressive label mapping than #TARGET_REF's linear matrix translation.",
                "Joint multi-task learning To combine the two sources of information, we use a joint objective,",
                "where N and M index the token positions in the distant and ground truth corpora, respectively, and γ is a constant balancing the two components which we set for uniform weighting, γ = |M| |N | ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "To model this data we employ the same model structure as above but augmented with a second perceptron output layer, as illustrated in Figure 1 (right) . Formally,ỹ t ∼ Categorial(õ t ) whereõ t = MLP(o t ) is a single hidden layer perceptron with tanh activation and softmax output transformation. This component allows for a more expressive label mapping than #TARGET_REF's linear matrix translation. Joint multi-task learning To combine the two sources of information, we use a joint objective, where N and M index the token positions in the distant and ground truth corpora, respectively, and γ is a constant balancing the two components which we set for uniform weighting, γ = |M| |N | .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"This component allows for a more expressive label mapping than #TARGET_REF's linear matrix translation.\"]}"
    },
    {
        "gold": {
            "text": [
                "We used SGD with momentum to train models, with early stopping based on development performance.",
                "For benchmarks, we compare the proposed model against various state-of-the-art supervised learning methods, namely: a BILSTM tagger, BILSTM-CRF tagger (#REF) , and a state-of-the-art semi-supervised POS tagging algorithm, MINITAGGER (#REF) , which is also focusing on minimising the amount of labelled data.",
                "Note these methods do not use cross-lingual supervision.",
                "For a more direct comparison, we include BILSTM-DEBIAS #TARGET_REF , applied using our proposed cross-lingual supervision based on dictionaries, instead of parallel corpora; accordingly the key difference is their linear transformation for the distant data, versus our non-linear transformation to the gold data.",
                "Results Table 1 reports the tagging accuracy, showing that our models consistently outperform the baseline techniques."
            ],
            "label": [
                "DIFFER",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We used SGD with momentum to train models, with early stopping based on development performance. For benchmarks, we compare the proposed model against various state-of-the-art supervised learning methods, namely: a BILSTM tagger, BILSTM-CRF tagger (#REF) , and a state-of-the-art semi-supervised POS tagging algorithm, MINITAGGER (#REF) , which is also focusing on minimising the amount of labelled data. Note these methods do not use cross-lingual supervision. For a more direct comparison, we include BILSTM-DEBIAS #TARGET_REF , applied using our proposed cross-lingual supervision based on dictionaries, instead of parallel corpora; accordingly the key difference is their linear transformation for the distant data, versus our non-linear transformation to the gold data. Results Table 1 reports the tagging accuracy, showing that our models consistently outperform the baseline techniques.",
        "output": "{\"label\": [\"DIFFER\", \"USE\"], \"context\": [\"For a more direct comparison, we include BILSTM-DEBIAS #TARGET_REF , applied using our proposed cross-lingual supervision based on dictionaries, instead of parallel corpora; accordingly the key difference is their linear transformation for the distant data, versus our non-linear transformation to the gold data.\"]}"
    },
    {
        "gold": {
            "text": [
                "The poor performance of the supervised methods suggests they are overfitting the small training set, however this is much less of a problem for our approach (labelled Joint).",
                "Note that distant supervision alone gives reasonable performance (labelled DISTANT) however the joint modelling of the ground truth and distant data yields significant improvements in almost all cases.",
                "BILSTM-DEBIAS #TARGET_REF performs worse than our proposed method, indicating that a linear transformation is insufficient for modelling distant supervision.",
                "The accuracies are higher overall for the European cf.",
                "Turkic languages, presumably because these languages are Table 1 : POS tagging accuracy on over the ten target languages, showing first approaches using only the gold data; next methods using only distant cross-lingual supervision, and lastly joint multi-task learning."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The poor performance of the supervised methods suggests they are overfitting the small training set, however this is much less of a problem for our approach (labelled Joint). Note that distant supervision alone gives reasonable performance (labelled DISTANT) however the joint modelling of the ground truth and distant data yields significant improvements in almost all cases. BILSTM-DEBIAS #TARGET_REF performs worse than our proposed method, indicating that a linear transformation is insufficient for modelling distant supervision. The accuracies are higher overall for the European cf. Turkic languages, presumably because these languages are Table 1 : POS tagging accuracy on over the ten target languages, showing first approaches using only the gold data; next methods using only distant cross-lingual supervision, and lastly joint multi-task learning.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"BILSTM-DEBIAS #TARGET_REF performs worse than our proposed method, indicating that a linear transformation is insufficient for modelling distant supervision.\"]}"
    },
    {
        "gold": {
            "text": [
                "We now describe the modelling framework for POS tagging in a low-resource language, based on very limited linguistic resources.",
                "Our approach extends the work of #TARGET_REF , who present a model based on distant supervision in the form of cross-lingual projection and use projected tags generated from parallel corpora as distant annotations.",
                "There are three main differences between their work and ours: 1) We do not use parallel corpora, but instead use a bilingual dictionary for knowledge transfer.",
                "2) Our model uses a more expressive multi-layer perceptron when generating the gold standard tags.",
                "The multi-layer perceptron can capture both language-specific infor- mation and consistent tagging errors arising from this method of supervision."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We now describe the modelling framework for POS tagging in a low-resource language, based on very limited linguistic resources. Our approach extends the work of #TARGET_REF , who present a model based on distant supervision in the form of cross-lingual projection and use projected tags generated from parallel corpora as distant annotations. There are three main differences between their work and ours: 1) We do not use parallel corpora, but instead use a bilingual dictionary for knowledge transfer. 2) Our model uses a more expressive multi-layer perceptron when generating the gold standard tags. The multi-layer perceptron can capture both language-specific infor- mation and consistent tagging errors arising from this method of supervision.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"Our approach extends the work of #TARGET_REF , who present a model based on distant supervision in the form of cross-lingual projection and use projected tags generated from parallel corpora as distant annotations.\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular, we start with the gentle introduction to the fundamentals of reinforcement learning (#REF; #REF) .",
                "We further discuss We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL.",
                "To address the model design issue, we discuss several recent solutions (#REFb; #REF; #TARGET_REF .",
                "We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (#REFb) , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems.",
                "This tutorial aims at introducing deep reinforcement learning methods to researchers in the NLP community."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "In particular, we start with the gentle introduction to the fundamentals of reinforcement learning (#REF; #REF) . We further discuss We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (#REFb; #REF; #TARGET_REF . We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (#REFb) , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems. This tutorial aims at introducing deep reinforcement learning methods to researchers in the NLP community.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"To address the model design issue, we discuss several recent solutions (#REFb; #REF; #TARGET_REF .\", \"We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (#REFb) , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems.\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular, we start with the gentle introduction to the fundamentals of reinforcement learning (#REF; #REF) .",
                "We further discuss their modern deep learning extensions such as Deep QNetworks (#REF) , Policy Networks (#REF) , and Deep Hierarchical Reinforcement Learning (#REF) .",
                "We outline the applications of deep reinforcement learning in NLP, including dialog (#REF) , semi-supervised text classification (#REF) , coreference (#REF; #REF) , knowledge graph reasoning #TARGET_REF ), text games (#REF; #REFa) , social media (#REFb; #REF) , information extraction (#REF; #REF) , language and vision (#REF; #REF; #REFa,b,c; #REF) , etc.",
                "We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL.",
                "To address the model design issue, we discuss several recent solutions (#REFb; #REF; #REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In particular, we start with the gentle introduction to the fundamentals of reinforcement learning (#REF; #REF) . We further discuss their modern deep learning extensions such as Deep QNetworks (#REF) , Policy Networks (#REF) , and Deep Hierarchical Reinforcement Learning (#REF) . We outline the applications of deep reinforcement learning in NLP, including dialog (#REF) , semi-supervised text classification (#REF) , coreference (#REF; #REF) , knowledge graph reasoning #TARGET_REF ), text games (#REF; #REFa) , social media (#REFb; #REF) , information extraction (#REF; #REF) , language and vision (#REF; #REF; #REFa,b,c; #REF) , etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (#REFb; #REF; #REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"We outline the applications of deep reinforcement learning in NLP, including dialog (#REF) , semi-supervised text classification (#REF) , coreference (#REF; #REF) , knowledge graph reasoning #TARGET_REF ), text games (#REF; #REFa) , social media (#REFb; #REF) , information extraction (#REF; #REF) , language and vision (#REF; #REF; #REFa,b,c; #REF) , etc.\"]}"
    },
    {
        "gold": {
            "text": [
                "We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL.",
                "To address the model design issue, we discuss several recent solutions (#REFb; #REF; #TARGET_REF .",
                "We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (#REFb) , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems.",
                "This tutorial aims at introducing deep reinforcement learning methods to researchers in the NLP community.",
                "We do not assume any particular prior knowledge in reinforcement learning."
            ],
            "label": [
                "BACK",
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (#REFb; #REF; #TARGET_REF . We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (#REFb) , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems. This tutorial aims at introducing deep reinforcement learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in reinforcement learning.",
        "output": "{\"label\": [\"BACK\", \"DIFFER\"], \"context\": [\"To address the model design issue, we discuss several recent solutions (#REFb; #REF; #TARGET_REF .\", \"We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (#REFb) , discussing the techniques of leveraging hierarchies in DRL for NLP generation problems.\"]}"
    },
    {
        "gold": {
            "text": [
                "To address this shortcoming, (#REF) augmented the knowledge graph with paths obtained from an external corpus.",
                "The added paths consisted of unlexicalized dependency labels obtained from a dependency parsed external corpus.",
                "To improve the expressivity of the added paths, instead of the unlexicalized labels, #TARGET_REF augmented the KB graph with verbs (surface relations) from a corpus containing over 600 million Subject-Verb-Object (SVO) triples.",
                "These verbs act as edges that connect previously unconnected entities thereby increasing the connectivity of the KB graph which can potentially improve PRA performance.",
                "However, naïvely adding these edges increases the feature sparsity which degrades the discriminative ability of the logistic regression classifier used in PRA."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "To address this shortcoming, (#REF) augmented the knowledge graph with paths obtained from an external corpus. The added paths consisted of unlexicalized dependency labels obtained from a dependency parsed external corpus. To improve the expressivity of the added paths, instead of the unlexicalized labels, #TARGET_REF augmented the KB graph with verbs (surface relations) from a corpus containing over 600 million Subject-Verb-Object (SVO) triples. These verbs act as edges that connect previously unconnected entities thereby increasing the connectivity of the KB graph which can potentially improve PRA performance. However, naïvely adding these edges increases the feature sparsity which degrades the discriminative ability of the logistic regression classifier used in PRA.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"To improve the expressivity of the added paths, instead of the unlexicalized labels, #TARGET_REF augmented the KB graph with verbs (surface relations) from a corpus containing over 600 million Subject-Verb-Object (SVO) triples.\"]}"
    },
    {
        "gold": {
            "text": [
                "This can be addressed by adding latent relations obtained by clustering the surface relations, instead of directly adding the surface relations.",
                "This reduces feature sparsity and has been shown to improve PRA inference #TARGET_REF , (#REF) .",
                "In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus.",
                "We term these noun phrases as bridging entities since they bridge two KB relations to form a path.",
                "This is different from the scheme in (#REF) and (#REF) , which adds edges between KB nodes by mining surface relations from an external corpus."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "This can be addressed by adding latent relations obtained by clustering the surface relations, instead of directly adding the surface relations. This reduces feature sparsity and has been shown to improve PRA inference #TARGET_REF , (#REF) . In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus. We term these noun phrases as bridging entities since they bridge two KB relations to form a path. This is different from the scheme in (#REF) and (#REF) , which adds edges between KB nodes by mining surface relations from an external corpus.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"This reduces feature sparsity and has been shown to improve PRA inference #TARGET_REF , (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "It was extended by (#REF) , to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus.",
                "Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in #TARGET_REF .",
                "Instead of hard mapping of surface relations to latent embeddings, (#REF ) perform a 'soft' mapping using vector space random walks.",
                "This allows the random walker to traverse an edge semantically similar to the current edge type more frequently than other edges.",
                "Although, like others, we too use an external corpus to augment the KB, the crucial difference in our approach is that apart from adding surface relations, we also add bridging entities that enable us to create new paths in the KB."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "It was extended by (#REF) , to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in #TARGET_REF . Instead of hard mapping of surface relations to latent embeddings, (#REF ) perform a 'soft' mapping using vector space random walks. This allows the random walker to traverse an edge semantically similar to the current edge type more frequently than other edges. Although, like others, we too use an external corpus to augment the KB, the crucial difference in our approach is that apart from adding surface relations, we also add bridging entities that enable us to create new paths in the KB.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF and PRA-VS are the systems proposed in #TARGET_REF and (#REF) respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus.",
                "In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting.",
                "In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                1,
                1,
                1
            ]
        },
        "input": "#TARGET_REF and PRA-VS are the systems proposed in #TARGET_REF and (#REF) respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"#TARGET_REF and PRA-VS are the systems proposed in #TARGET_REF and (#REF) respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus.\", \"In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting.\", \"In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner.\"]}"
    },
    {
        "gold": {
            "text": [
                "Between the two top performing systems, i.e., PRA-ODA and PRA-VS, PRA-ODA is faster by a factor of 1.8.",
                "and journalistwritesforpublication).",
                "The hyperparameter values d max = 2, K = 10 reported the highest MRR and were used for the rest of the relations.",
                "For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in ( #TARGET_REF; #REF) , viz., L 1 = 0.005, and L 2 = 1.0.",
                "This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Between the two top performing systems, i.e., PRA-ODA and PRA-VS, PRA-ODA is faster by a factor of 1.8. and journalistwritesforpublication). The hyperparameter values d max = 2, K = 10 reported the highest MRR and were used for the rest of the relations. For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in ( #TARGET_REF; #REF) , viz., L 1 = 0.005, and L 2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the knowledge base was augmented.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"For the L 1 and L 2 regularization parameters in the logistic regression classifier, we used the same values as used in ( #TARGET_REF; #REF) , viz., L 1 = 0.005, and L 2 = 1.0.\"]}"
    },
    {
        "gold": {
            "text": [
                "Systems, such as treebank-based parsers #TARGET_REF; #REF) and semantic role labelers (#REF; #REF) , are trained and tested on hand-annotated data.",
                "Evaluation is based on differences between system output and test data.",
                "Other systems use these programs to perform tasks unrelated to the original annotation.",
                "For example, participating systems in CONLL (#REF; Hajič et al., 2009 ), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand.",
                "This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generating Grammatical and Logical Representation Framework (GLARF) for English, Chinese and Japanese sentences."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Systems, such as treebank-based parsers #TARGET_REF; #REF) and semantic role labelers (#REF; #REF) , are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (#REF; Hajič et al., 2009 ), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generating Grammatical and Logical Representation Framework (GLARF) for English, Chinese and Japanese sentences.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Systems, such as treebank-based parsers #TARGET_REF; #REF) and semantic role labelers (#REF; #REF) , are trained and tested on hand-annotated data.\"]}"
    },
    {
        "gold": {
            "text": [
                "While annotation guidelines may direct a human annotator to prefer, for example, high attachment, systems output may have other preferences, e.g., the probability that land is modified by a PP (headed by in) versus the probability that acres can be so modified.",
                "Even if the manual annotation for a particular corpus is consistent when it comes to other factors such as tokenization or part of speech, developers of parsers sometimes change these guidelines to suit their needs.",
                "For example, users of the Charniak parser #TARGET_REF should add the AUX category to the PTB parts of speech and adjust their systems to account for the conversion of the word ain't into the tokens IS and n't.",
                "Similarly, tokenization decisions with respect to hyphens vary among different versions of the Penn Treebank, as well as different parsers based on these treebanks.",
                "Thus if a system uses multiple parsers, such differences must be accounted for."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "While annotation guidelines may direct a human annotator to prefer, for example, high attachment, systems output may have other preferences, e.g., the probability that land is modified by a PP (headed by in) versus the probability that acres can be so modified. Even if the manual annotation for a particular corpus is consistent when it comes to other factors such as tokenization or part of speech, developers of parsers sometimes change these guidelines to suit their needs. For example, users of the Charniak parser #TARGET_REF should add the AUX category to the PTB parts of speech and adjust their systems to account for the conversion of the word ain't into the tokens IS and n't. Similarly, tokenization decisions with respect to hyphens vary among different versions of the Penn Treebank, as well as different parsers based on these treebanks. Thus if a system uses multiple parsers, such differences must be accounted for.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"For example, users of the Charniak parser #TARGET_REF should add the AUX category to the PTB parts of speech and adjust their systems to account for the conversion of the word ain't into the tokens IS and n't.\"]}"
    },
    {
        "gold": {
            "text": [
                "We use Charniak, UMD and KNP parsers #TARGET_REF; #REF; #REF) , JET Named Entity tagger (#REF; #REF) and other resources in conjunction with languagespecific GLARFers that incorporate hand-written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper.",
                "English GLARFer rules use Comlex (#REFa) and the various NomBank lexicons (http:// nlp.cs.nyu.edu/meyers/nombank/) for lexical lookup.",
                "The GLARF rules implemented vary by language as follows.",
                "English: correcting/standardizing phrase boundaries and part of speech (POS); recognizing multiword expressions; marking subconstituents; labeling relations; incorporating NEs; regularizing infinitival, passives, relatives, VP deletion, predicative and numerous other constructions.",
                "Chinese: correcting/standardizing phrase boundaries and POS, marking subconstituents, labeling relations; regularizing copula constructions; incorporating NEs; recognizing dates and number expressions."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We use Charniak, UMD and KNP parsers #TARGET_REF; #REF; #REF) , JET Named Entity tagger (#REF; #REF) and other resources in conjunction with languagespecific GLARFers that incorporate hand-written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper. English GLARFer rules use Comlex (#REFa) and the various NomBank lexicons (http:// nlp.cs.nyu.edu/meyers/nombank/) for lexical lookup. The GLARF rules implemented vary by language as follows. English: correcting/standardizing phrase boundaries and part of speech (POS); recognizing multiword expressions; marking subconstituents; labeling relations; incorporating NEs; regularizing infinitival, passives, relatives, VP deletion, predicative and numerous other constructions. Chinese: correcting/standardizing phrase boundaries and POS, marking subconstituents, labeling relations; regularizing copula constructions; incorporating NEs; recognizing dates and number expressions.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use Charniak, UMD and KNP parsers #TARGET_REF; #REF; #REF) , JET Named Entity tagger (#REF; #REF) and other resources in conjunction with languagespecific GLARFers that incorporate hand-written rules to convert output of these processors into a final representation, including logic1 structure, the focus of this paper.\"]}"
    },
    {
        "gold": {
            "text": [
                "General bitext mapping algorithms are a recent invention.",
                "So far, most researchers interested in co-occurrence of mutual translations have relied on bitexts where sentence boundaries (or other text unit boundaries) were easy to find (e.g. Gale & #REF; Kumano & #REF; #REF; #TARGET_REF .",
                "Aligned text segments suggest a boundary-based model of cooccurrence, illustrated in Figure 2 .",
                "For bitexts involving languages with similar word order, a more accurate combined model of co-occurrence can be built using both segment boundary information and the map-distance threshold.",
                "As shown in Figure 3 , each of these constraints eliminates the noise from a characteristic region of the bitext space."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "General bitext mapping algorithms are a recent invention. So far, most researchers interested in co-occurrence of mutual translations have relied on bitexts where sentence boundaries (or other text unit boundaries) were easy to find (e.g. Gale & #REF; Kumano & #REF; #REF; #TARGET_REF . Aligned text segments suggest a boundary-based model of cooccurrence, illustrated in Figure 2 . For bitexts involving languages with similar word order, a more accurate combined model of co-occurrence can be built using both segment boundary information and the map-distance threshold. As shown in Figure 3 , each of these constraints eliminates the noise from a characteristic region of the bitext space.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"So far, most researchers interested in co-occurrence of mutual translations have relied on bitexts where sentence boundaries (or other text unit boundaries) were easy to find (e.g. Gale & #REF; Kumano & #REF; #REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "It is somewhat surprising that this is a question at all, and most authors ignore it.",
                "However, when authors specify their algorithms in sufficient detail to answer this question, the most common answer (given, e.g., by #REF; #REF; #REF; #TARGET_REF turns out to be unsound.",
                "The problem is easiest to illustrate under the boundary-based model of co-occurrence.",
                "Given two aligned text segments, the naive way to count co-occurrences is",
                "( 1) where e(u) and f (v) are the frequencies of occurrence of u and v in their respective segments."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "It is somewhat surprising that this is a question at all, and most authors ignore it. However, when authors specify their algorithms in sufficient detail to answer this question, the most common answer (given, e.g., by #REF; #REF; #REF; #TARGET_REF turns out to be unsound. The problem is easiest to illustrate under the boundary-based model of co-occurrence. Given two aligned text segments, the naive way to count co-occurrences is ( 1) where e(u) and f (v) are the frequencies of occurrence of u and v in their respective segments.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"However, when authors specify their algorithms in sufficient detail to answer this question, the most common answer (given, e.g., by #REF; #REF; #REF; #TARGET_REF turns out to be unsound.\"]}"
    },
    {
        "gold": {
            "text": [
                "Co-occurrence is a universal precondition for translational equivalence among word tokens in bitexts.",
                "Other preconditions may be imposed if certain language-specific resources are available #TARGET_REF .",
                "For example, parts of speech tend to be preserved in translation (#REF) .",
                "If part-of-speech taggers are available for both languages in a bitext, and if cases where one part of speech is translated to another are not important for the intended application, then we can rule out the possibility of translational equivalence for all token pairs involving different parts of speech.",
                "A more obvious source of language-specific information is a machine-readable bilingual dictionary (MRBD)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Co-occurrence is a universal precondition for translational equivalence among word tokens in bitexts. Other preconditions may be imposed if certain language-specific resources are available #TARGET_REF . For example, parts of speech tend to be preserved in translation (#REF) . If part-of-speech taggers are available for both languages in a bitext, and if cases where one part of speech is translated to another are not important for the intended application, then we can rule out the possibility of translational equivalence for all token pairs involving different parts of speech. A more obvious source of language-specific information is a machine-readable bilingual dictionary (MRBD).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Other preconditions may be imposed if certain language-specific resources are available #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Similarly exclusive candidacy can be granted to cognate token pairs (#REF) .",
                "Most published translation models treat co-occurrence counts as counts of potential link tokens (#REF) .",
                "More accurate models may result if the co-occurrence counts are biased with language-specific knowledge.",
                "Without loss of generality, whenever translation models refer to cooccurrence counts, they can refer to co-occurrence counts that have been filtered using whatever language-specific resources happen to be available.",
                "It does not matter if there are dependencies among the different knowledge sources, as long as each is used as a simple filter on the co-occurrence relation #TARGET_REF ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Similarly exclusive candidacy can be granted to cognate token pairs (#REF) . Most published translation models treat co-occurrence counts as counts of potential link tokens (#REF) . More accurate models may result if the co-occurrence counts are biased with language-specific knowledge. Without loss of generality, whenever translation models refer to cooccurrence counts, they can refer to co-occurrence counts that have been filtered using whatever language-specific resources happen to be available. It does not matter if there are dependencies among the different knowledge sources, as long as each is used as a simple filter on the co-occurrence relation #TARGET_REF .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"It does not matter if there are dependencies among the different knowledge sources, as long as each is used as a simple filter on the co-occurrence relation #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The last years have witnessed a continuous progress in statistical multilingual models for syntax, thanks to shared tasks such as CoNLL 2006-7 (#REF; #REF) and, more recently, SPMRL 2013-14 (#REF; #REF) .",
                "As a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate #REF; #REF; #REF #TARGET_REF .",
                "The same rationale applies to semantic dependency parsing, also a structured prediction problem, but where the output variable is a semantic graph, rather than a syntactic tree.",
                "Indeed, the best performing systems in last year shared task on broad-coverage semantic dependency parsing follow this principle (#REF) .",
                "This year, a new challenge was put forth: how to handle multiple languages and out-ofdomain data?"
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The last years have witnessed a continuous progress in statistical multilingual models for syntax, thanks to shared tasks such as CoNLL 2006-7 (#REF; #REF) and, more recently, SPMRL 2013-14 (#REF; #REF) . As a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate #REF; #REF; #REF #TARGET_REF . The same rationale applies to semantic dependency parsing, also a structured prediction problem, but where the output variable is a semantic graph, rather than a syntactic tree. Indeed, the best performing systems in last year shared task on broad-coverage semantic dependency parsing follow this principle (#REF) . This year, a new challenge was put forth: how to handle multiple languages and out-ofdomain data?",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"As a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate #REF; #REF; #REF #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Our second-order model looks at some pairs of arcs: arcs bearing a grandparent relationship, arguments of the same predicate, predicates sharing the same argument, and consecutive versions of these two.",
                "#REF for further details.",
                "The parser was built as an extension of a recent dependency parser, TurboParser (#REF #TARGET_REF , with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD).",
                "We have followed prior work in semantic role labeling (#REF; #REF; #REF; #REF) , by adding constraints and modeling interactions among arguments within the same frame; however, we went beyond such sibling interactions to consider more complex grandparent and co-parent structures, effectively correlating different predicates.",
                "The overall set of parts used by our parser is illustrated in Figure 1 ; note that by using only a subset of the parts (predicate, arc, labeled arc, and sibling parts), the semantic parser decodes each predicate frame independently from other predicates; it is the co-parent and grandparent parts that have the effect of creating inter-dependence among predicates; we will analyze the effect of these dependencies in the experimental section ( §3)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Our second-order model looks at some pairs of arcs: arcs bearing a grandparent relationship, arguments of the same predicate, predicates sharing the same argument, and consecutive versions of these two. #REF for further details. The parser was built as an extension of a recent dependency parser, TurboParser (#REF #TARGET_REF , with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD). We have followed prior work in semantic role labeling (#REF; #REF; #REF; #REF) , by adding constraints and modeling interactions among arguments within the same frame; however, we went beyond such sibling interactions to consider more complex grandparent and co-parent structures, effectively correlating different predicates. The overall set of parts used by our parser is illustrated in Figure 1 ; note that by using only a subset of the parts (predicate, arc, labeled arc, and sibling parts), the semantic parser decodes each predicate frame independently from other predicates; it is the co-parent and grandparent parts that have the effect of creating inter-dependence among predicates; we will analyze the effect of these dependencies in the experimental section ( §3).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The parser was built as an extension of a recent dependency parser, TurboParser (#REF #TARGET_REF , with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD).\"]}"
    },
    {
        "gold": {
            "text": [
                "For each part in our model (shown in Figure 1 ), we computed binary features based on various combination of lexical forms, lemmas, POS tags and syntactic dependency relations of words related to the corresponding predicates and arguments.",
                "Most of these features were taken from TurboParser #TARGET_REF , and others were inspired by the semantic parser of #REF .",
                "To tackle all the parts, we formulate parsing as a global optimization problem and solve a relaxation through AD 3 (#REF), a fast dual decomposition algorithm in which several simple local subproblems are solved iteratively.",
                "Through a rich set of features, we arrive at top accuracies at parsing speeds around 1,000 tokens per second.",
                "See #REF for details on the model, features and decoding process that were used."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "For each part in our model (shown in Figure 1 ), we computed binary features based on various combination of lexical forms, lemmas, POS tags and syntactic dependency relations of words related to the corresponding predicates and arguments. Most of these features were taken from TurboParser #TARGET_REF , and others were inspired by the semantic parser of #REF . To tackle all the parts, we formulate parsing as a global optimization problem and solve a relaxation through AD 3 (#REF), a fast dual decomposition algorithm in which several simple local subproblems are solved iteratively. Through a rich set of features, we arrive at top accuracies at parsing speeds around 1,000 tokens per second. See #REF for details on the model, features and decoding process that were used.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Most of these features were taken from TurboParser #TARGET_REF , and others were inspired by the semantic parser of #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Previous works have shown that supervised approaches to Word Sense Disambiguation which rely on sense annotated corpora (#REF; #REF) outperform unsupervised (#REF ) and knowledge based approaches (#REF) .",
                "However, creation of sense marked corpora has always remained a costly proposition, especially for some of the resource deprived languages.",
                "To circumvent this problem, #TARGET_REF proposed a WSD method that can be applied to a language even when no sense tagged corpus for that language is available.",
                "This is achieved by projecting Wordnet and corpus parameters from another language to the language in question.",
                "The approach is centered on a novel synset based multilingual dictionary (#REF) where the synsets of different languages are aligned and thereafter the words within the synsets are manually cross-linked."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Previous works have shown that supervised approaches to Word Sense Disambiguation which rely on sense annotated corpora (#REF; #REF) outperform unsupervised (#REF ) and knowledge based approaches (#REF) . However, creation of sense marked corpora has always remained a costly proposition, especially for some of the resource deprived languages. To circumvent this problem, #TARGET_REF proposed a WSD method that can be applied to a language even when no sense tagged corpus for that language is available. This is achieved by projecting Wordnet and corpus parameters from another language to the language in question. The approach is centered on a novel synset based multilingual dictionary (#REF) where the synsets of different languages are aligned and thereafter the words within the synsets are manually cross-linked.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"To circumvent this problem, #TARGET_REF proposed a WSD method that can be applied to a language even when no sense tagged corpus for that language is available.\"]}"
    },
    {
        "gold": {
            "text": [
                "In section 2 we present related work.",
                "In section 3 we describe the Synset based multilingual dictionary which enables parameter projection.",
                "In section 4 we discuss the work of #TARGET_REF on parameter projection for multilingual WSD.",
                "Section 5 is on the economics of multilingual WSD.",
                "In section 6 we propose a probabilistic model for representing the cross-linkage of words within synsets."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In section 2 we present related work. In section 3 we describe the Synset based multilingual dictionary which enables parameter projection. In section 4 we discuss the work of #TARGET_REF on parameter projection for multilingual WSD. Section 5 is on the economics of multilingual WSD. In section 6 we propose a probabilistic model for representing the cross-linkage of words within synsets.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In section 4 we discuss the work of #TARGET_REF on parameter projection for multilingual WSD.\"]}"
    },
    {
        "gold": {
            "text": [
                "Supervised approaches like SVM (#REF) and k-NN (#REF) , on the other hand, give better accuracies, but the requirement of large annotated corpora renders them unsuitable for resource scarce languages.",
                "Recent work by #TARGET_REF has shown that it is possible to project the parameters learnt from the annotation work of one language to another language provided aligned Wordnets for two languages are available.",
                "However, their work does not address the question of further improving the accuracy of WSD by using a small amount of training data from the target language.",
                "Some similar work has been done in the area of domain adaptation where #REF showed that adding just 30% of the target data to the source data achieved the same performance as that obtained by taking the entire source and target data.",
                "Similarly, Agirre and de #REF reported a 22% error reduction when source and target data were combined for training a classifier, compared to the case when only the target data was used for training the classifier."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Supervised approaches like SVM (#REF) and k-NN (#REF) , on the other hand, give better accuracies, but the requirement of large annotated corpora renders them unsuitable for resource scarce languages. Recent work by #TARGET_REF has shown that it is possible to project the parameters learnt from the annotation work of one language to another language provided aligned Wordnets for two languages are available. However, their work does not address the question of further improving the accuracy of WSD by using a small amount of training data from the target language. Some similar work has been done in the area of domain adaptation where #REF showed that adding just 30% of the target data to the source data achieved the same performance as that obtained by taking the entire source and target data. Similarly, Agirre and de #REF reported a 22% error reduction when source and target data were combined for training a classifier, compared to the case when only the target data was used for training the classifier.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Recent work by #TARGET_REF has shown that it is possible to project the parameters learnt from the annotation work of one language to another language provided aligned Wordnets for two languages are available.\"]}"
    },
    {
        "gold": {
            "text": [
                "The first factor is the cost of manually cross-linking the words in a synsets of the target language to the words in the corresponding synset in the pivot language.",
                "The second factor is the cost of sense annotated data from the target language.",
                "The third factor is the accuracy of WSD The first two factors in some sense relate to the cost of purchasing a commodity and the third factor relates to the commodity itself.",
                "The work of #TARGET_REF as described above does not attempt to reach an optimal costbenefit point in this economic system.",
                "They place their bets on manual cross-linking only and settle for the accuracy achieved thereof."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The first factor is the cost of manually cross-linking the words in a synsets of the target language to the words in the corresponding synset in the pivot language. The second factor is the cost of sense annotated data from the target language. The third factor is the accuracy of WSD The first two factors in some sense relate to the cost of purchasing a commodity and the third factor relates to the commodity itself. The work of #TARGET_REF as described above does not attempt to reach an optimal costbenefit point in this economic system. They place their bets on manual cross-linking only and settle for the accuracy achieved thereof.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The work of #TARGET_REF as described above does not attempt to reach an optimal costbenefit point in this economic system.\"]}"
    },
    {
        "gold": {
            "text": [
                "For such situations, we propose that only a small number of words, comprising of the most frequently appearing ones should be manually cross linked and the rest of the words should be cross-linked using a probabilistic model.",
                "The rationale here is simple: invest money in words which are bound to occur frequently in the test data and achieve maximum impact on the accuracy.",
                "In the following paragraphs, we explain our probabilistic cross linking model.",
                "The model proposed by #TARGET_REF ) is a deterministic model where the expected count for (Sense S, Marathi Word W ), i.e., the number of times the word W appears in sense S is approximated by the count for the corresponding cross linked Hindi word.",
                "Such a model assumes that each Marathi word links to appropriate Hindi word(s) as identified manually by a lexicographer."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "For such situations, we propose that only a small number of words, comprising of the most frequently appearing ones should be manually cross linked and the rest of the words should be cross-linked using a probabilistic model. The rationale here is simple: invest money in words which are bound to occur frequently in the test data and achieve maximum impact on the accuracy. In the following paragraphs, we explain our probabilistic cross linking model. The model proposed by #TARGET_REF ) is a deterministic model where the expected count for (Sense S, Marathi Word W ), i.e., the number of times the word W appears in sense S is approximated by the count for the corresponding cross linked Hindi word. Such a model assumes that each Marathi word links to appropriate Hindi word(s) as identified manually by a lexicographer.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The model proposed by #TARGET_REF ) is a deterministic model where the expected count for (Sense S, Marathi Word W ), i.e., the number of times the word W appears in sense S is approximated by the count for the corresponding cross linked Hindi word.\"]}"
    },
    {
        "gold": {
            "text": [
                "There it makes sense to place fewer bets on manual crosslinking and more on collecting annotated corpora.",
                "On the other hand if manual cross-linking is cheap then a very small amount of annotated corpora can be used in conjunction with full manual crosslinking to boost the accuracy.",
                "Based on the above discussion, if k a is the cost of sense annotating one word, k c is the cost of manually cross-linking a word and A is the accuracy desired then the problem of multilingual WSD can be cast as an optimization problem:",
                "Accuracy ≥ A where, w c and w a are the number of words to be manually cross linked and annotated respectively.",
                "Ours is thus a 3-factor economic model (crosslinking, annotation and accuracy) as opposed to the 2-factor model (cross-linking, accuracy) proposed by #TARGET_REF ."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "There it makes sense to place fewer bets on manual crosslinking and more on collecting annotated corpora. On the other hand if manual cross-linking is cheap then a very small amount of annotated corpora can be used in conjunction with full manual crosslinking to boost the accuracy. Based on the above discussion, if k a is the cost of sense annotating one word, k c is the cost of manually cross-linking a word and A is the accuracy desired then the problem of multilingual WSD can be cast as an optimization problem: Accuracy ≥ A where, w c and w a are the number of words to be manually cross linked and annotated respectively. Ours is thus a 3-factor economic model (crosslinking, annotation and accuracy) as opposed to the 2-factor model (cross-linking, accuracy) proposed by #TARGET_REF .",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"Ours is thus a 3-factor economic model (crosslinking, annotation and accuracy) as opposed to the 2-factor model (cross-linking, accuracy) proposed by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The parameters thus learnt were then projected using the MultiDict (refer section 3 and 4) to build a resource conscious Marathi (T L ) WSD engine.",
                "We used the same dataset as described in #TARGET_REF for all our experiments.",
                "The data was collected from two domains, viz., Tourism and Health.",
                "The data for Tourism domain was collected by manually translating English documents downloaded from Indian Tourism websites into Hindi and Marathi.",
                "Similarly, English documents for Health domain were obtained from two doctors and were manually translated into Hindi and Marathi."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The parameters thus learnt were then projected using the MultiDict (refer section 3 and 4) to build a resource conscious Marathi (T L ) WSD engine. We used the same dataset as described in #TARGET_REF for all our experiments. The data was collected from two domains, viz., Tourism and Health. The data for Tourism domain was collected by manually translating English documents downloaded from Indian Tourism websites into Hindi and Marathi. Similarly, English documents for Health domain were obtained from two doctors and were manually translated into Hindi and Marathi.",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"We used the same dataset as described in #TARGET_REF for all our experiments.\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, the extraction rules used by #REF and #REF match text in which syntactic chunks have been identified.",
                "More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (#REF; #REF; #TARGET_REF; #REF) .",
                "In these approaches extraction patterns are essentially parts of the dependency tree.",
                "To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern.",
                "Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "For example, the extraction rules used by #REF and #REF match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (#REF; #REF; #TARGET_REF; #REF) . In these approaches extraction patterns are essentially parts of the dependency tree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (#REF; #REF; #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns.",
                "A variety of pattern models have been proposed.",
                "For example the patterns used by #REF are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) while #TARGET_REF allow any subtree within the dependency parse to act as an extraction pattern.",
                "#REF showed that the choice of pattern model has important implications for IE algorithms including significant differences between the various models in terms of their ability to identify information of interest in text.",
                "However, there has been little comparison between the various pattern models."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed. For example the patterns used by #REF are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) while #TARGET_REF allow any subtree within the dependency parse to act as an extraction pattern. #REF showed that the choice of pattern model has important implications for IE algorithms including significant differences between the various models in terms of their ability to identify information of interest in text. However, there has been little comparison between the various pattern models.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"For example the patterns used by #REF are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) while #TARGET_REF allow any subtree within the dependency parse to act as an extraction pattern.\"]}"
    },
    {
        "gold": {
            "text": [
                "An example dependency analysis for the sentence \"Acme hired Smith as their new CEO, replacing Bloggs.\" is shown The remainder of this section outlines four models for representing extraction patterns which can be derived from dependency trees.",
                "Predicate-Argument Model (SVO): A simple approach, used by #REF , #REF and , is to use subject-verb-object tuples from the dependency parse as extraction patterns.",
                "These consist of a verb and its subject and/or direct object.",
                "Figure  2 shows the two SVO patterns 1 which are produced for the dependency tree shown in Figure 1 .",
                "This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1 The formalism used for representing dependency patterns is similar to the one introduced by #TARGET_REF ."
            ],
            "label": [
                "BACK",
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                1
            ]
        },
        "input": "An example dependency analysis for the sentence \"Acme hired Smith as their new CEO, replacing Bloggs.\" is shown The remainder of this section outlines four models for representing extraction patterns which can be derived from dependency trees. Predicate-Argument Model (SVO): A simple approach, used by #REF , #REF and , is to use subject-verb-object tuples from the dependency parse as extraction patterns. These consist of a verb and its subject and/or direct object. Figure  2 shows the two SVO patterns 1 which are produced for the dependency tree shown in Figure 1 . This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1 The formalism used for representing dependency patterns is similar to the one introduced by #TARGET_REF .",
        "output": "{\"label\": [\"BACK\", \"USE\"], \"context\": [\"Predicate-Argument Model (SVO): A simple approach, used by #REF , #REF and , is to use subject-verb-object tuples from the dependency parse as extraction patterns.\", \"This model can identify information which is expressed using simple predicate-argument constructions such as the relation between Acme and Smith 1 The formalism used for representing dependency patterns is similar to the one introduced by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "This pattern representation encodes most of the information in the sentence with the advantage of being able to link together event participants which neither of the SVO or chain model can, for example the relation between \"Smith\" and \"Bloggs\" in Figure 1 .",
                "Subtrees: The final model to be considered is the subtree model #TARGET_REF .",
                "In this model any subtree of a dependency tree can be used as an extraction pattern, where a subtree is any set of nodes in the tree which are connected to one another.",
                "Single nodes are not considered to be subtrees.",
                "The subtree model is a richer representation than those discussed so far and can represent any part of a dependency tree."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "This pattern representation encodes most of the information in the sentence with the advantage of being able to link together event participants which neither of the SVO or chain model can, for example the relation between \"Smith\" and \"Bloggs\" in Figure 1 . Subtrees: The final model to be considered is the subtree model #TARGET_REF . In this model any subtree of a dependency tree can be used as an extraction pattern, where a subtree is any set of nodes in the tree which are connected to one another. Single nodes are not considered to be subtrees. The subtree model is a richer representation than those discussed so far and can represent any part of a dependency tree.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Subtrees: The final model to be considered is the subtree model #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "There have been few direct comparisons of the various pattern models.",
                "#TARGET_REF compared three models (SVO, chains and subtrees) on two IE scenarios using a entity extraction task.",
                "Models were evaluated in terms of their ability to identify entities taking part in events and distinguish them from those which did not.",
                "They found the SVO model performed poorly in comparison with the other two models and that the performance of the subtree model was generally the same as, or better than, the chain model.",
                "However, they did not attempt to determine whether the models could identify the relations between these entities, simply whether they could identify the entities participating in relevant events."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "There have been few direct comparisons of the various pattern models. #TARGET_REF compared three models (SVO, chains and subtrees) on two IE scenarios using a entity extraction task. Models were evaluated in terms of their ability to identify entities taking part in events and distinguish them from those which did not. They found the SVO model performed poorly in comparison with the other two models and that the performance of the subtree model was generally the same as, or better than, the chain model. However, they did not attempt to determine whether the models could identify the relations between these entities, simply whether they could identify the entities participating in relevant events.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF compared three models (SVO, chains and subtrees) on two IE scenarios using a entity extraction task.\"]}"
    },
    {
        "gold": {
            "text": [
                "The subtree model could represent more of the relations than any other model but that there was no statistical difference between those relations and the ones covered by the linked chain model.",
                "They concluded that the linked chain model was optional since it is expressive enough to represent the information of interest without introducing a potentially unwieldy number of patterns.",
                "There is some agreement between these two studies, for example that the SVO model performs poorly in comparison with other models.",
                "However, #REF also found that the coverage of the chain model was significantly worse than the subtree model, although #TARGET_REF found that in some cases their performance could not be distinguished.",
                "In addition to these disagreements, these studies are also limited by the fact that they are indirect; they do not evaluate the various pattern models on an IE task."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The subtree model could represent more of the relations than any other model but that there was no statistical difference between those relations and the ones covered by the linked chain model. They concluded that the linked chain model was optional since it is expressive enough to represent the information of interest without introducing a potentially unwieldy number of patterns. There is some agreement between these two studies, for example that the SVO model performs poorly in comparison with other models. However, #REF also found that the coverage of the chain model was significantly worse than the subtree model, although #TARGET_REF found that in some cases their performance could not be distinguished. In addition to these disagreements, these studies are also limited by the fact that they are indirect; they do not evaluate the various pattern models on an IE task.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"However, #REF also found that the coverage of the chain model was significantly worse than the subtree model, although #TARGET_REF found that in some cases their performance could not be distinguished.\"]}"
    },
    {
        "gold": {
            "text": [
                "Equation 1 combines two factors: the term frequency (in relevant documents) and inverse document frequency (across the corpus).",
                "Patterns which occur frequently in relevant documents without being too prevalent in the corpus are preferred.",
                "#TARGET_REF found that it was important to find the appropriate balance between these two factors.",
                "They introduced the β parameter as a way of controlling the relative contribution of the inverse document frequency.",
                "β is tuned for each extraction task and pattern model combination."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Equation 1 combines two factors: the term frequency (in relevant documents) and inverse document frequency (across the corpus). Patterns which occur frequently in relevant documents without being too prevalent in the corpus are preferred. #TARGET_REF found that it was important to find the appropriate balance between these two factors. They introduced the β parameter as a way of controlling the relative contribution of the inverse document frequency. β is tuned for each extraction task and pattern model combination.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Patterns which occur frequently in relevant documents without being too prevalent in the corpus are preferred.\", \"#TARGET_REF found that it was important to find the appropriate balance between these two factors.\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, #REF used subtrees for parse ranking but could only generate subtrees which appear at least ten times in a 40,000 sentence corpus.",
                "They comment that the size of their data set meant that it would have been difficult to complete the experiments with less restrictive parameters.",
                "In addition, #TARGET_REF only generated subtrees which appeared in at least three documents.",
                "#REF and #REF both used the rightmost extension algorithm to generate subtrees.",
                "To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "For example, #REF used subtrees for parse ranking but could only generate subtrees which appear at least ten times in a 40,000 sentence corpus. They comment that the size of their data set meant that it would have been difficult to complete the experiments with less restrictive parameters. In addition, #TARGET_REF only generated subtrees which appeared in at least three documents. #REF and #REF both used the rightmost extension algorithm to generate subtrees. To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In addition, #TARGET_REF only generated subtrees which appeared in at least three documents.\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, #REF used subtrees for parse ranking but could only generate subtrees which appear at least ten times in a 40,000 sentence corpus.",
                "They comment that the size of their data set meant that it would have been difficult to complete the experiments with less restrictive parameters.",
                "In addition, #REF only generated subtrees which appeared in at least three documents.",
                "#REF and #TARGET_REF both used the rightmost extension algorithm to generate subtrees.",
                "To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "For example, #REF used subtrees for parse ranking but could only generate subtrees which appear at least ten times in a 40,000 sentence corpus. They comment that the size of their data set meant that it would have been difficult to complete the experiments with less restrictive parameters. In addition, #REF only generated subtrees which appeared in at least three documents. #REF and #TARGET_REF both used the rightmost extension algorithm to generate subtrees. To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted for the SVO, chain and linked chain models in which patterns which occurred fewer than four times were removed.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#REF and #TARGET_REF both used the rightmost extension algorithm to generate subtrees.\"]}"
    },
    {
        "gold": {
            "text": [
                "We compared each of the patterns models described in Section 2 using an unsupervised IE experiment similar to one described by #TARGET_REF .",
                "Let D be a corpus of documents and R a set of documents which are relevant to a particular extraction task.",
                "In this context \"relevant\" means that the document contains the information we are interested in identifying.",
                "D and R are such that D = R ∪R and R∩R = ∅. As assumption behind this approach is that useful patterns will be far more likely to occur in R than D overall."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "We compared each of the patterns models described in Section 2 using an unsupervised IE experiment similar to one described by #TARGET_REF . Let D be a corpus of documents and R a set of documents which are relevant to a particular extraction task. In this context \"relevant\" means that the document contains the information we are interested in identifying. D and R are such that D = R ∪R and R∩R = ∅. As assumption behind this approach is that useful patterns will be far more likely to occur in R than D overall.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We compared each of the patterns models described in Section 2 using an unsupervised IE experiment similar to one described by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The value of β in equation 1 was set using a separate corpus from which the patterns were generated, a methodology suggested by #TARGET_REF .",
                "To generate this additional text we used the Reuters Corpus (#REF ) which consists of a year's worth of newswire output.",
                "Each document in the Reuters corpus has been manually annotated with topic codes indicating its general subject area(s).",
                "One of these topic codes (C411) refers to management succession events and was used to identify documents which are relevant to the MUC6 IE scenario.",
                "A corpus consisting of 348 documents annotated with code C411 and 250 documents without that code, representing irrelevant documents, were taken from the Reuters corpus to create a corpus with the same distribution of relevant and irrelevant documents as found in the MUC-6 corpus."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The value of β in equation 1 was set using a separate corpus from which the patterns were generated, a methodology suggested by #TARGET_REF . To generate this additional text we used the Reuters Corpus (#REF ) which consists of a year's worth of newswire output. Each document in the Reuters corpus has been manually annotated with topic codes indicating its general subject area(s). One of these topic codes (C411) refers to management succession events and was used to identify documents which are relevant to the MUC6 IE scenario. A corpus consisting of 348 documents annotated with code C411 and 250 documents without that code, representing irrelevant documents, were taken from the Reuters corpus to create a corpus with the same distribution of relevant and irrelevant documents as found in the MUC-6 corpus.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The value of \\u03b2 in equation 1 was set using a separate corpus from which the patterns were generated, a methodology suggested by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Relevant information describes an executive entering or leaving a position within a company, for example \"Last month Smith resigned as CEO of Rooter Ltd.\".",
                "This sentence described as event involving three items: a person (Smith), position (CEO) and company (Rooter Ltd).",
                "We made use of a version of the MUC-6 corpus described by #REF which consists of 598 documents.",
                "For these experiments relevant documents were identified using annotations in the corpus.",
                "However, this is not necessary since #TARGET_REF showed that adequate knowledge about document relevance could be obtained automatically using an IR system."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "Relevant information describes an executive entering or leaving a position within a company, for example \"Last month Smith resigned as CEO of Rooter Ltd.\". This sentence described as event involving three items: a person (Smith), position (CEO) and company (Rooter Ltd). We made use of a version of the MUC-6 corpus described by #REF which consists of 598 documents. For these experiments relevant documents were identified using annotations in the corpus. However, this is not necessary since #TARGET_REF showed that adequate knowledge about document relevance could be obtained automatically using an IR system.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"For these experiments relevant documents were identified using annotations in the corpus.\", \"However, this is not necessary since #TARGET_REF showed that adequate knowledge about document relevance could be obtained automatically using an IR system.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our intuitive explanation for this result is that lazy learning techniques keep all training items, whereas greedy approaches lose useful information by forgetting low-frequency or exceptional instances of the task, not covered by the extracted rules or models (#REF) .",
                "Apart from the empirical work in Tilburg and Antwerp, a number of recent studies on statistical natural language processing (e.g. Dagan & #REF; #TARGET_REF also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low-frequency events, is harmful to generalization accuracy.",
                "After reviewing this empirical work briefly, I will report on new results (work in progress in collaboration with van den Bosch and Zavrel), systematically comparing greedy and lazy learning techniques on a number of benehrnark natural language processing tasks: tagging, grapheme-to-phoneme conversion, and pp-attachment.",
                "The results show that forgetting individual training items, however \"improbable' they may be, is indeed harmful.",
                "Furthermore, they show that combining lazy learning with training set editing techniques (based on typicality and other regularity criteria) also leads to worse generalization results."
            ],
            "label": [
                "BACK",
                "SIM"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "Our intuitive explanation for this result is that lazy learning techniques keep all training items, whereas greedy approaches lose useful information by forgetting low-frequency or exceptional instances of the task, not covered by the extracted rules or models (#REF) . Apart from the empirical work in Tilburg and Antwerp, a number of recent studies on statistical natural language processing (e.g. Dagan & #REF; #TARGET_REF also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low-frequency events, is harmful to generalization accuracy. After reviewing this empirical work briefly, I will report on new results (work in progress in collaboration with van den Bosch and Zavrel), systematically comparing greedy and lazy learning techniques on a number of benehrnark natural language processing tasks: tagging, grapheme-to-phoneme conversion, and pp-attachment. The results show that forgetting individual training items, however \"improbable' they may be, is indeed harmful. Furthermore, they show that combining lazy learning with training set editing techniques (based on typicality and other regularity criteria) also leads to worse generalization results.",
        "output": "{\"label\": [\"BACK\", \"SIM\"], \"context\": [\"Apart from the empirical work in Tilburg and Antwerp, a number of recent studies on statistical natural language processing (e.g. Dagan & #REF; #TARGET_REF also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low-frequency events, is harmful to generalization accuracy.\", \"After reviewing this empirical work briefly, I will report on new results (work in progress in collaboration with van den Bosch and Zavrel), systematically comparing greedy and lazy learning techniques on a number of benehrnark natural language processing tasks: tagging, grapheme-to-phoneme conversion, and pp-attachment.\", \"The results show that forgetting individual training items, however \\\"improbable' they may be, is indeed harmful.\"]}"
    },
    {
        "gold": {
            "text": [
                "Lazy Learning with a simple similarity metric based on information entropy (IB I-IG, Daelemarts & van den #REF consistently outperforms abstracting (greedy) learning techniques such as C5.0 or backprop learning on a broad selection of natural language processing tasks ranging from phonology to semantics.",
                "Our intuitive explanation for this result is that lazy learning techniques keep all training items, whereas greedy approaches lose useful information by forgetting low-frequency or exceptional instances of the task, not covered by the extracted rules or models (#REF) .",
                "Apart from the empirical work in Tilburg and Antwerp, a number of recent studies on statistical natural language processing (e.g. Dagan & #REF; #TARGET_REF also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low-frequency events, is harmful to generalization accuracy.",
                "After reviewing this empirical work briefly, I will report on new results (work in progress in collaboration with van den Bosch and Zavrel), systematically comparing greedy and lazy learning techniques on a number of benehrnark natural language processing tasks: tagging, grapheme-to-phoneme conversion, and pp-attachment.",
                "The results show that forgetting individual training items, however \"improbable' they may be, is indeed harmful."
            ],
            "label": [
                "BACK",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Lazy Learning with a simple similarity metric based on information entropy (IB I-IG, Daelemarts & van den #REF consistently outperforms abstracting (greedy) learning techniques such as C5.0 or backprop learning on a broad selection of natural language processing tasks ranging from phonology to semantics. Our intuitive explanation for this result is that lazy learning techniques keep all training items, whereas greedy approaches lose useful information by forgetting low-frequency or exceptional instances of the task, not covered by the extracted rules or models (#REF) . Apart from the empirical work in Tilburg and Antwerp, a number of recent studies on statistical natural language processing (e.g. Dagan & #REF; #TARGET_REF also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low-frequency events, is harmful to generalization accuracy. After reviewing this empirical work briefly, I will report on new results (work in progress in collaboration with van den Bosch and Zavrel), systematically comparing greedy and lazy learning techniques on a number of benehrnark natural language processing tasks: tagging, grapheme-to-phoneme conversion, and pp-attachment. The results show that forgetting individual training items, however \"improbable' they may be, is indeed harmful.",
        "output": "{\"label\": [\"BACK\", \"EXT\"], \"context\": [\"Apart from the empirical work in Tilburg and Antwerp, a number of recent studies on statistical natural language processing (e.g. Dagan & #REF; #TARGET_REF also suggest that, contrary to common wisdom, forgetting specific training items, even when they represent extremely low-frequency events, is harmful to generalization accuracy.\", \"After reviewing this empirical work briefly, I will report on new results (work in progress in collaboration with van den Bosch and Zavrel), systematically comparing greedy and lazy learning techniques on a number of benehrnark natural language processing tasks: tagging, grapheme-to-phoneme conversion, and pp-attachment.\"]}"
    },
    {
        "gold": {
            "text": [
                "LSTM-based language models have been shown effective in Word Sense Disambiguation (WSD).",
                "In particular, the technique proposed by #TARGET_REF returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released.",
                "This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow).",
                "Our study showed that similar results can be obtained with much less data than hinted at by #REF .",
                "Detailed analyses shed light on the strengths and weaknesses of this method."
            ],
            "label": [
                "BACK",
                "USE"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "LSTM-based language models have been shown effective in Word Sense Disambiguation (WSD). In particular, the technique proposed by #TARGET_REF returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released. This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow). Our study showed that similar results can be obtained with much less data than hinted at by #REF . Detailed analyses shed light on the strengths and weaknesses of this method.",
        "output": "{\"label\": [\"BACK\", \"USE\"], \"context\": [\"In particular, the technique proposed by #TARGET_REF returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released.\", \"This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow).\"]}"
    },
    {
        "gold": {
            "text": [
                "In recent years, there has been a surge in interest in using Long short-term memory (LSTM) (#REF) to perform WSD (#REFb; #REF) .",
                "These approaches are characterized by their high performance, simplicity and their ability to extract a lot of information from raw text.",
                "Among the best-performing ones is the approach by #TARGET_REF , in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small sense-annotated datasets to achieve state-of-the-art performance in all-words WSD.",
                "Even though the results obtained by #REF outperform the previous state-of-the-art, neither the used datasets nor the constructed models are available to the community.",
                "This is unfortunate because this makes the re-application of this technique a non-trivial process, and it hinders further studies for understanding which limitations prevent even higher accuracies."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In recent years, there has been a surge in interest in using Long short-term memory (LSTM) (#REF) to perform WSD (#REFb; #REF) . These approaches are characterized by their high performance, simplicity and their ability to extract a lot of information from raw text. Among the best-performing ones is the approach by #TARGET_REF , in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small sense-annotated datasets to achieve state-of-the-art performance in all-words WSD. Even though the results obtained by #REF outperform the previous state-of-the-art, neither the used datasets nor the constructed models are available to the community. This is unfortunate because this makes the re-application of this technique a non-trivial process, and it hinders further studies for understanding which limitations prevent even higher accuracies.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Among the best-performing ones is the approach by #TARGET_REF , in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small sense-annotated datasets to achieve state-of-the-art performance in all-words WSD.\"]}"
    },
    {
        "gold": {
            "text": [
                "The method proposed by #TARGET_REF performs WSD by annotating each lemma in a text with one WordNet synset that is associated with its meaning.",
                "Broadly speaking, the disambiguation is done by: 1) constructing a language model from a large unannotated dataset; 2) extracting sense embeddings from this model using a much smaller annotated dataset; 3) relying on the sense embeddings to make predictions on the lemmas in unseen sentences.",
                "Each operation is described below.",
                "Constructing Language Models.",
                "Long Short-Term Memory (LSTM) (#REF ) is a celebrated recurrent neural network architecture that has proven to be effective in many natural language processing tasks (#REF; #REF; #REF, among others) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "The method proposed by #TARGET_REF performs WSD by annotating each lemma in a text with one WordNet synset that is associated with its meaning. Broadly speaking, the disambiguation is done by: 1) constructing a language model from a large unannotated dataset; 2) extracting sense embeddings from this model using a much smaller annotated dataset; 3) relying on the sense embeddings to make predictions on the lemmas in unseen sentences. Each operation is described below. Constructing Language Models. Long Short-Term Memory (LSTM) (#REF ) is a celebrated recurrent neural network architecture that has proven to be effective in many natural language processing tasks (#REF; #REF; #REF, among others) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The method proposed by #TARGET_REF performs WSD by annotating each lemma in a text with one WordNet synset that is associated with its meaning.\"]}"
    },
    {
        "gold": {
            "text": [
                "Different from previous architectures, LSTM is equipped with trainable gates that control the flow of information, allowing the neural networks to learn both short-and long-range dependencies.",
                "In #TARGET_REF , the first operation consists of constructing an LSTM language model to capture the meaning of words in context.",
                "They use an LSTM network with a single hidden layer of h nodes.",
                "Given a sentence s = (w 1 , w 2 , . . . , w n ), they replace word w k (1 ≤ k ≤ n) by a special token $.",
                "The model takes this new sentence as input and produces a context vector c of dimensionality p (see Figure 1 )."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Different from previous architectures, LSTM is equipped with trainable gates that control the flow of information, allowing the neural networks to learn both short-and long-range dependencies. In #TARGET_REF , the first operation consists of constructing an LSTM language model to capture the meaning of words in context. They use an LSTM network with a single hidden layer of h nodes. Given a sentence s = (w 1 , w 2 , . . . , w n ), they replace word w k (1 ≤ k ≤ n) by a special token $. The model takes this new sentence as input and produces a context vector c of dimensionality p (see Figure 1 ).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In #TARGET_REF , the first operation consists of constructing an LSTM language model to capture the meaning of words in context.\"]}"
    },
    {
        "gold": {
            "text": [
                "It selects the sense whose vector is closest to c t using cosine as the similarity function.",
                "Label Propagation.",
                "#TARGET_REF argue that the averaging procedure is suboptimal because of two reasons.",
                "First, the distribution of occurrences of senses is unknown whereas averaging is only suitable for spherical clusters.",
                "Second, averaging reduces the representation of occurrences of each sense to a single vector and therefore ignores sense prior."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "It selects the sense whose vector is closest to c t using cosine as the similarity function. Label Propagation. #TARGET_REF argue that the averaging procedure is suboptimal because of two reasons. First, the distribution of occurrences of senses is unknown whereas averaging is only suitable for spherical clusters. Second, averaging reduces the representation of occurrences of each sense to a single vector and therefore ignores sense prior.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF argue that the averaging procedure is suboptimal because of two reasons.\"]}"
    },
    {
        "gold": {
            "text": [
                "We anticipate some conclusions.",
                "First, a positive result is that we were able to reproduce the method from #TARGET_REF and obtain similar results to the ones originally published.",
                "However, to our surprise, these results were obtained using a much smaller corpus of 1.8 billion tokens (Gigaword), which is less than 2% of the data used in the original study.",
                "In addition, we observe that the amount of unannotated data is important, but that the relationship between its size and the improvement is not linear, meaning that exponentially more unannotated data is needed in order to improve the performance.",
                "Moreover, we show that the percentage of correct sense assignments is more balanced w.r.t sense popularity, meaning that the system has a less-strong bias towards the most-frequent sense (MFS) and is better at recognizing both popular and unpopular meanings."
            ],
            "label": [
                "USE",
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "We anticipate some conclusions. First, a positive result is that we were able to reproduce the method from #TARGET_REF and obtain similar results to the ones originally published. However, to our surprise, these results were obtained using a much smaller corpus of 1.8 billion tokens (Gigaword), which is less than 2% of the data used in the original study. In addition, we observe that the amount of unannotated data is important, but that the relationship between its size and the improvement is not linear, meaning that exponentially more unannotated data is needed in order to improve the performance. Moreover, we show that the percentage of correct sense assignments is more balanced w.r.t sense popularity, meaning that the system has a less-strong bias towards the most-frequent sense (MFS) and is better at recognizing both popular and unpopular meanings.",
        "output": "{\"label\": [\"USE\", \"DIFFER\"], \"context\": [\"First, a positive result is that we were able to reproduce the method from #TARGET_REF and obtain similar results to the ones originally published.\", \"However, to our surprise, these results were obtained using a much smaller corpus of 1.8 billion tokens (Gigaword), which is less than 2% of the data used in the original study.\"]}"
    },
    {
        "gold": {
            "text": [
                "The best performing one is based on a bidirectional Long Short-Term Memory (BLSTM) with attention and two auxiliary loss functions (part-of-speech and the WordNet coarse-grained semantic labels).",
                "#REF also make use of unannotated data to train a BLSTM.",
                "The work by #TARGET_REF , which we consider in this paper, belongs to this last category.",
                "Different from #REF , it uses significantly more unannotated data, the model contains more hidden units (2048 vs. 600), and the sense assignment is more elaborated.",
                "We describe this approach in more detail in the following section."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The best performing one is based on a bidirectional Long Short-Term Memory (BLSTM) with attention and two auxiliary loss functions (part-of-speech and the WordNet coarse-grained semantic labels). #REF also make use of unannotated data to train a BLSTM. The work by #TARGET_REF , which we consider in this paper, belongs to this last category. Different from #REF , it uses significantly more unannotated data, the model contains more hidden units (2048 vs. 600), and the sense assignment is more elaborated. We describe this approach in more detail in the following section.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The work by #TARGET_REF , which we consider in this paper, belongs to this last category.\"]}"
    },
    {
        "gold": {
            "text": [
                "The corpus consists of 1.8 billion tokens in 4.1 million documents, originated from four major news agencies.",
                "We leave the study of bigger corpora for future work.",
                "For the training of the sense embeddings, we use the same two corpora used by #TARGET_REF: 1. SemCor (#REF ) is a corpus containing approximately 240,000 sense annotated words.",
                "The tagged documents originate from the Brown corpus (#REF) and cover various genres.",
                "2. OMSTI (#REF) contains one million sense annotations automatically tagged by exploiting the English-Chinese part of the parallel MultiUN corpus (#REF) ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The corpus consists of 1.8 billion tokens in 4.1 million documents, originated from four major news agencies. We leave the study of bigger corpora for future work. For the training of the sense embeddings, we use the same two corpora used by #TARGET_REF: 1. SemCor (#REF ) is a corpus containing approximately 240,000 sense annotated words. The tagged documents originate from the Brown corpus (#REF) and cover various genres. 2. OMSTI (#REF) contains one million sense annotations automatically tagged by exploiting the English-Chinese part of the parallel MultiUN corpus (#REF) .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For the training of the sense embeddings, we use the same two corpora used by #TARGET_REF: 1. SemCor (#REF ) is a corpus containing approximately 240,000 sense annotated words.\"]}"
    },
    {
        "gold": {
            "text": [
                "The datasets following T: indicate the annotated corpus used to represent the senses while U:OMSTI stands for using OMSTI as unlabeled sentences in case label propagation is used.",
                "P: SemCor indicates that sense distributions from SemCor are used in the system architecture.",
                "Three scorers are used: \"framework\" refers to the WSD evaluation framework from Raganato et al. (2017a) ; \"mapping to WN3.0\" refers to the evaluation used by #TARGET_REF while \"competition\" refers to the scorer provided by the competition itself (e.g., semeval2013).",
                "1,644 test instances in total, which are all nouns.",
                "The application of the MFS baseline on this dataset yields an F 1 score of 63.0%."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The datasets following T: indicate the annotated corpus used to represent the senses while U:OMSTI stands for using OMSTI as unlabeled sentences in case label propagation is used. P: SemCor indicates that sense distributions from SemCor are used in the system architecture. Three scorers are used: \"framework\" refers to the WSD evaluation framework from Raganato et al. (2017a) ; \"mapping to WN3.0\" refers to the evaluation used by #TARGET_REF while \"competition\" refers to the scorer provided by the competition itself (e.g., semeval2013). 1,644 test instances in total, which are all nouns. The application of the MFS baseline on this dataset yields an F 1 score of 63.0%.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Three scorers are used: \\\"framework\\\" refers to the WSD evaluation framework from Raganato et al. (2017a) ; \\\"mapping to WN3.0\\\" refers to the evaluation used by #TARGET_REF while \\\"competition\\\" refers to the scorer provided by the competition itself (e.g., semeval2013).\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section, we report our reproduction of the results of #TARGET_REF and additional experiments to gain a deeper insight into the strengths and weaknesses of the approach.",
                "These experiments focus on the performance on the most-and less-frequent senses, coverage of the annotated dataset and the consequent impact on the overall predictions, the granularity of the sense representation, and the impact of the unannotated data and model complexity on the accuracy of WSD.",
                "Reproduction results.",
                "We trained the LSTM model with the best reported settings in #REF (hidden layer size h = 2048, embedding dimensionality p = 512) using a machine equipped with an Intel Xeon E5-2650, 256GB of RAM, 8TB of disk space, and two nVIDIA GeForce GTX 1080 Ti GPUs.",
                "During our training, one epoch took about one day to finish with TensorFlow fully utilizing one GPU."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "In this section, we report our reproduction of the results of #TARGET_REF and additional experiments to gain a deeper insight into the strengths and weaknesses of the approach. These experiments focus on the performance on the most-and less-frequent senses, coverage of the annotated dataset and the consequent impact on the overall predictions, the granularity of the sense representation, and the impact of the unannotated data and model complexity on the accuracy of WSD. Reproduction results. We trained the LSTM model with the best reported settings in #REF (hidden layer size h = 2048, embedding dimensionality p = 512) using a machine equipped with an Intel Xeon E5-2650, 256GB of RAM, 8TB of disk space, and two nVIDIA GeForce GTX 1080 Ti GPUs. During our training, one epoch took about one day to finish with TensorFlow fully utilizing one GPU.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In this section, we report our reproduction of the results of #TARGET_REF and additional experiments to gain a deeper insight into the strengths and weaknesses of the approach.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this section, we report our reproduction of the results of #REF and additional experiments to gain a deeper insight into the strengths and weaknesses of the approach.",
                "These experiments focus on the performance on the most-and less-frequent senses, coverage of the annotated dataset and the consequent impact on the overall predictions, the granularity of the sense representation, and the impact of the unannotated data and model complexity on the accuracy of WSD.",
                "Reproduction results.",
                "We trained the LSTM model with the best reported settings in #TARGET_REF (hidden layer size h = 2048, embedding dimensionality p = 512) using a machine equipped with an Intel Xeon E5-2650, 256GB of RAM, 8TB of disk space, and two nVIDIA GeForce GTX 1080 Ti GPUs.",
                "During our training, one epoch took about one day to finish with TensorFlow fully utilizing one GPU."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "In this section, we report our reproduction of the results of #REF and additional experiments to gain a deeper insight into the strengths and weaknesses of the approach. These experiments focus on the performance on the most-and less-frequent senses, coverage of the annotated dataset and the consequent impact on the overall predictions, the granularity of the sense representation, and the impact of the unannotated data and model complexity on the accuracy of WSD. Reproduction results. We trained the LSTM model with the best reported settings in #TARGET_REF (hidden layer size h = 2048, embedding dimensionality p = 512) using a machine equipped with an Intel Xeon E5-2650, 256GB of RAM, 8TB of disk space, and two nVIDIA GeForce GTX 1080 Ti GPUs. During our training, one epoch took about one day to finish with TensorFlow fully utilizing one GPU.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We trained the LSTM model with the best reported settings in #TARGET_REF (hidden layer size h = 2048, embedding dimensionality p = 512) using a machine equipped with an Intel Xeon E5-2650, 256GB of RAM, 8TB of disk space, and two nVIDIA GeForce GTX 1080 Ti GPUs.\"]}"
    },
    {
        "gold": {
            "text": [
                "We tested the performance of the downstream WSD task three times during the training and observed that the best performance is obtained at the 65 th epoch, despite a later model producing a lower negative log-likelihood.",
                "Thus, we used the model produced at the 65 th epoch for our experiments below.",
                "Table 1 presents the results using the test sets senseval2 and semeval2013, respectively.",
                "The top part of the table presents our reproduction results, the middle part reports the results from #TARGET_REF , while the bottom part reports a representative sample of the other state-of-the-art approaches.",
                "It should be noted that with the test set semeval2013, all scorers use WordNet 3.0, therefore the performance of the various methods can be directly compared."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "We tested the performance of the downstream WSD task three times during the training and observed that the best performance is obtained at the 65 th epoch, despite a later model producing a lower negative log-likelihood. Thus, we used the model produced at the 65 th epoch for our experiments below. Table 1 presents the results using the test sets senseval2 and semeval2013, respectively. The top part of the table presents our reproduction results, the middle part reports the results from #TARGET_REF , while the bottom part reports a representative sample of the other state-of-the-art approaches. It should be noted that with the test set semeval2013, all scorers use WordNet 3.0, therefore the performance of the various methods can be directly compared.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"The top part of the table presents our reproduction results, the middle part reports the results from #TARGET_REF , while the bottom part reports a representative sample of the other state-of-the-art approaches.\", \"It should be noted that with the test set semeval2013, all scorers use WordNet 3.0, therefore the performance of the various methods can be directly compared.\"]}"
    },
    {
        "gold": {
            "text": [
                "The original paper only analyses the performance on the whole test sets.",
                "We extend this analysis by looking at the performance for disambiguating the most frequent-sense (MFS) and less-frequent-sense (LFS) instances.",
                "The first type of instances are the ones for which the correct link is the most-frequent sense, whereas the second subset consists of the remaining ones.",
                "This analysis is important because it is well-known that the simple strategy of always choosing the MFS is a strong baseline in WSD, thus there is a tendency for WSD systems to overfit towards the MFS (#REF) .",
                "Table 2 shows that the method by #TARGET_REF does not overfit towards the MFS to the same extent as other supervised systems since the recall on LFS instances is still quite high 0.41 (a lower recall on LFS instances than on MFS ones is expected due to the reduced training data for them)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "The original paper only analyses the performance on the whole test sets. We extend this analysis by looking at the performance for disambiguating the most frequent-sense (MFS) and less-frequent-sense (LFS) instances. The first type of instances are the ones for which the correct link is the most-frequent sense, whereas the second subset consists of the remaining ones. This analysis is important because it is well-known that the simple strategy of always choosing the MFS is a strong baseline in WSD, thus there is a tendency for WSD systems to overfit towards the MFS (#REF) . Table 2 shows that the method by #TARGET_REF does not overfit towards the MFS to the same extent as other supervised systems since the recall on LFS instances is still quite high 0.41 (a lower recall on LFS instances than on MFS ones is expected due to the reduced training data for them).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Table 2 shows that the method by #TARGET_REF does not overfit towards the MFS to the same extent as other supervised systems since the recall on LFS instances is still quite high 0.41 (a lower recall on LFS instances than on MFS ones is expected due to the reduced training data for them).\"]}"
    },
    {
        "gold": {
            "text": [
                "Figure 2a shows the effect of unannotated data volume on WSD performance.",
                "The data points at 100 billion (10 11 ) tokens correspond to #TARGET_REF 's reported results.",
                "As might be expected, a bigger corpus leads to more meaningful context vectors and therefore higher performance on WSD.",
                "However, the amount of data needed for 1% of improvement in F 1 grows exponentially fast (notice that the horizontal axis is in log scale).",
                "Extrapolating from this graph, to get a performance of 0.8 F 1 by adding more unannotated data, one would need a corpus of 10 12 tokens."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Figure 2a shows the effect of unannotated data volume on WSD performance. The data points at 100 billion (10 11 ) tokens correspond to #TARGET_REF 's reported results. As might be expected, a bigger corpus leads to more meaningful context vectors and therefore higher performance on WSD. However, the amount of data needed for 1% of improvement in F 1 grows exponentially fast (notice that the horizontal axis is in log scale). Extrapolating from this graph, to get a performance of 0.8 F 1 by adding more unannotated data, one would need a corpus of 10 12 tokens.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"The data points at 100 billion (10 11 ) tokens correspond to #TARGET_REF 's reported results.\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper reports the results of a reproduction study of the model proposed by #TARGET_REF and an additional analysis to gain a deeper understanding of the impact of various factors on its performance.",
                "A number of interesting conclusions can be drawn from our results.",
                "First, we observed that we do not need a very large unannotated dataset to achieve state-of-the-art all-words WSD performance since we used the Gigaword corpus, which is two orders of magnitude smaller than #REF 's proprietary corpus, and got similar performance on senseval2 and semeval2013.",
                "A more detailed analysis hints that adding more unannotated data and increasing model capacity are subject to diminishing returns.",
                "Moreover, we observed that this approach has a more balanced sense assignment than other techniques, as shown by the relatively good performance on less-frequent-sense instances."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "This paper reports the results of a reproduction study of the model proposed by #TARGET_REF and an additional analysis to gain a deeper understanding of the impact of various factors on its performance. A number of interesting conclusions can be drawn from our results. First, we observed that we do not need a very large unannotated dataset to achieve state-of-the-art all-words WSD performance since we used the Gigaword corpus, which is two orders of magnitude smaller than #REF 's proprietary corpus, and got similar performance on senseval2 and semeval2013. A more detailed analysis hints that adding more unannotated data and increasing model capacity are subject to diminishing returns. Moreover, we observed that this approach has a more balanced sense assignment than other techniques, as shown by the relatively good performance on less-frequent-sense instances.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"This paper reports the results of a reproduction study of the model proposed by #TARGET_REF and an additional analysis to gain a deeper understanding of the impact of various factors on its performance.\"]}"
    },
    {
        "gold": {
            "text": [
                "In particular, the technique proposed by #REF returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released.",
                "This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow).",
                "Our study showed that similar results can be obtained with much less data than hinted at by #TARGET_REF .",
                "Detailed analyses shed light on the strengths and weaknesses of this method.",
                "First, adding more unannotated training data is useful, but is subject to diminishing returns."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In particular, the technique proposed by #REF returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released. This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow). Our study showed that similar results can be obtained with much less data than hinted at by #TARGET_REF . Detailed analyses shed light on the strengths and weaknesses of this method. First, adding more unannotated training data is useful, but is subject to diminishing returns.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Our study showed that similar results can be obtained with much less data than hinted at by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "However, the gap with the graph-based approach of #REF is still significant.",
                "When we use both SemCor and OMSTI for the annotated data, our results drop 0.02 point for senseval2, whereas they increase by almost 0.01 for semeval2013.",
                "Different from #TARGET_REF, we did not observe improvement by using label propagation (comparing T: SemCor, U: OMSTI against T:SemCor without propagation).",
                "However, the performance of the label propagation strategy is still competitive on both test sets.",
                "Most-vs."
            ],
            "label": [
                "DIFFER",
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "However, the gap with the graph-based approach of #REF is still significant. When we use both SemCor and OMSTI for the annotated data, our results drop 0.02 point for senseval2, whereas they increase by almost 0.01 for semeval2013. Different from #TARGET_REF, we did not observe improvement by using label propagation (comparing T: SemCor, U: OMSTI against T:SemCor without propagation). However, the performance of the label propagation strategy is still competitive on both test sets. Most-vs.",
        "output": "{\"label\": [\"DIFFER\", \"SIM\"], \"context\": [\"Different from #TARGET_REF, we did not observe improvement by using label propagation (comparing T: SemCor, U: OMSTI against T:SemCor without propagation).\", \"However, the performance of the label propagation strategy is still competitive on both test sets.\"]}"
    },
    {
        "gold": {
            "text": [
                "A number of interesting conclusions can be drawn from our results.",
                "First, we observed that we do not need a very large unannotated dataset to achieve state-of-the-art all-words WSD performance since we used the Gigaword corpus, which is two orders of magnitude smaller than #TARGET_REF 's proprietary corpus, and got similar performance on senseval2 and semeval2013.",
                "A more detailed analysis hints that adding more unannotated data and increasing model capacity are subject to diminishing returns.",
                "Moreover, we observed that this approach has a more balanced sense assignment than other techniques, as shown by the relatively good performance on less-frequent-sense instances.",
                "In addition, we identified that the limited sense coverage in annotated dataset places a potentially upper bound for the overall performance."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "A number of interesting conclusions can be drawn from our results. First, we observed that we do not need a very large unannotated dataset to achieve state-of-the-art all-words WSD performance since we used the Gigaword corpus, which is two orders of magnitude smaller than #TARGET_REF 's proprietary corpus, and got similar performance on senseval2 and semeval2013. A more detailed analysis hints that adding more unannotated data and increasing model capacity are subject to diminishing returns. Moreover, we observed that this approach has a more balanced sense assignment than other techniques, as shown by the relatively good performance on less-frequent-sense instances. In addition, we identified that the limited sense coverage in annotated dataset places a potentially upper bound for the overall performance.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"First, we observed that we do not need a very large unannotated dataset to achieve state-of-the-art all-words WSD performance since we used the Gigaword corpus, which is two orders of magnitude smaller than #TARGET_REF 's proprietary corpus, and got similar performance on senseval2 and semeval2013.\"]}"
    },
    {
        "gold": {
            "text": [
                "In addition, some details are not reported, and this could prevent other attempts from replicating the results.",
                "To address these issues, we reimplemented #TARGET_REF 's method with the goal of: 1) reproducing and making available the code, trained models, and results and 2) understanding which are the main factors that constitute the strengths and weaknesses of this method.",
                "While a full replication is not possible due to the unavailability of the original data, we nevertheless managed to reproduce their approach with other public text corpora, and this allowed us to perform a deeper investigation on the performance of this technique.",
                "This investigation aimed at understanding how sensitive the WSD approach is w.r.t.",
                "the amount of unannotated data (i.e., raw text) used for training, model complexity, how biased the method is towards the choice of the most frequent senses (MFS), and identifying limitations that cannot be overcome with bigger unannotated datasets."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In addition, some details are not reported, and this could prevent other attempts from replicating the results. To address these issues, we reimplemented #TARGET_REF 's method with the goal of: 1) reproducing and making available the code, trained models, and results and 2) understanding which are the main factors that constitute the strengths and weaknesses of this method. While a full replication is not possible due to the unavailability of the original data, we nevertheless managed to reproduce their approach with other public text corpora, and this allowed us to perform a deeper investigation on the performance of this technique. This investigation aimed at understanding how sensitive the WSD approach is w.r.t. the amount of unannotated data (i.e., raw text) used for training, model complexity, how biased the method is towards the choice of the most frequent senses (MFS), and identifying limitations that cannot be overcome with bigger unannotated datasets.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"To address these issues, we reimplemented #TARGET_REF 's method with the goal of: 1) reproducing and making available the code, trained models, and results and 2) understanding which are the main factors that constitute the strengths and weaknesses of this method.\"]}"
    },
    {
        "gold": {
            "text": [
                "First, we design and test multiple game-inspired novel interfaces for collecting human attention maps of where humans choose to look to answer questions from the large-scale VQA dataset (#REF) ; this VQA-HAT (Human ATtention) dataset will be released publicly.",
                "Second, we perform qualitative and quantitative comparison of the maps generated by state-of-the-art attention-based VQA models (#REF; #TARGET_REF ) and a task-independent saliency baseline (#REF ) against our human attention maps through visualizations and rank-order correlation.",
                "We find that machine-generated attention maps from the most accurate VQA model have a mean rank-correlation of 0.26 with human attention maps, which is worse than task-independent saliency maps that have a mean rank-correlation of 0.49.",
                "It is well understood that task-independent saliency maps have a 'center bias' (#REF; #REF ).",
                "After we control for this center bias in our human attention maps, we find that the correlation of task-independent saliency is poor (as expected), while trends for machine-generated VQA-attention maps remain the same (which is promising)."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "First, we design and test multiple game-inspired novel interfaces for collecting human attention maps of where humans choose to look to answer questions from the large-scale VQA dataset (#REF) ; this VQA-HAT (Human ATtention) dataset will be released publicly. Second, we perform qualitative and quantitative comparison of the maps generated by state-of-the-art attention-based VQA models (#REF; #TARGET_REF ) and a task-independent saliency baseline (#REF ) against our human attention maps through visualizations and rank-order correlation. We find that machine-generated attention maps from the most accurate VQA model have a mean rank-correlation of 0.26 with human attention maps, which is worse than task-independent saliency maps that have a mean rank-correlation of 0.49. It is well understood that task-independent saliency maps have a 'center bias' (#REF; #REF ). After we control for this center bias in our human attention maps, we find that the correlation of task-independent saliency is poor (as expected), while trends for machine-generated VQA-attention maps remain the same (which is promising).",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"Second, we perform qualitative and quantitative comparison of the maps generated by state-of-the-art attention-based VQA models (#REF; #TARGET_REF ) and a task-independent saliency baseline (#REF ) against our human attention maps through visualizations and rank-order correlation.\"]}"
    },
    {
        "gold": {
            "text": [
                "Stacked Attention Networks (SAN) proposed in (#REF) use LSTM encodings of question words to produce a spatial attention distribution over the convolutional layer features of the image.",
                "Hierarchical Co-Attention Network #TARGET_REF generates multiple levels of image attention based on words, phrases and complete questions, and is the top entry on the VQA Challenge 1 as of the time of this submission.",
                "Another interesting approach uses question parsing to compose the neural network from modules, attention being one of the sub-tasks addressed by these modules (#REF) .",
                "Note that all these works are unsupervised attention models, where \"attention\" is simply an intermediate variable (a spatial distribution) that is produced by the model to optimize downstream loss (VQA cross-entropy).",
                "The fact that some (it's unclear how many) of these spatial distributions end up being interpretable is simply fortuitous."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Stacked Attention Networks (SAN) proposed in (#REF) use LSTM encodings of question words to produce a spatial attention distribution over the convolutional layer features of the image. Hierarchical Co-Attention Network #TARGET_REF generates multiple levels of image attention based on words, phrases and complete questions, and is the top entry on the VQA Challenge 1 as of the time of this submission. Another interesting approach uses question parsing to compose the neural network from modules, attention being one of the sub-tasks addressed by these modules (#REF) . Note that all these works are unsupervised attention models, where \"attention\" is simply an intermediate variable (a spatial distribution) that is produced by the model to optimize downstream loss (VQA cross-entropy). The fact that some (it's unclear how many) of these spatial distributions end up being interpretable is simply fortuitous.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Hierarchical Co-Attention Network #TARGET_REF generates multiple levels of image attention based on words, phrases and complete questions, and is the top entry on the VQA Challenge 1 as of the time of this submission.\"]}"
    },
    {
        "gold": {
            "text": [
                "• Stacked Attention Network (SAN) (#REF) with two attention layers (SAN-2) 2 .",
                "• Hierarchical Co-Attention Network (HieCoAtt) #TARGET_REF with word-level (HieCoAtt-W), phrase-level (HieCoAtt-P) and question-level (HieCoAtt-Q) attention maps; we evaluate all three maps 3 .",
                "Comparison Metric: Rank Correlation.",
                "We first scale both the machine-generated and human attention maps to 14x14, rank the pixels according to their spatial attention and then compute correlation between these two ranked lists.",
                "We choose an orderbased metric so as to make the evaluation invariant to absolute spatial probability values which can be made peaky or diffuse by tweaking a 'temperature' parameter."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "• Stacked Attention Network (SAN) (#REF) with two attention layers (SAN-2) 2 . • Hierarchical Co-Attention Network (HieCoAtt) #TARGET_REF with word-level (HieCoAtt-W), phrase-level (HieCoAtt-P) and question-level (HieCoAtt-Q) attention maps; we evaluate all three maps 3 . Comparison Metric: Rank Correlation. We first scale both the machine-generated and human attention maps to 14x14, rank the pixels according to their spatial attention and then compute correlation between these two ranked lists. We choose an orderbased metric so as to make the evaluation invariant to absolute spatial probability values which can be made peaky or diffuse by tweaking a 'temperature' parameter.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"\\u2022 Hierarchical Co-Attention Network (HieCoAtt) #TARGET_REF with word-level (HieCoAtt-W), phrase-level (HieCoAtt-P) and question-level (HieCoAtt-Q) attention maps; we evaluate all three maps 3 .\"]}"
    },
    {
        "gold": {
            "text": [
                "Finally, there is am emerging line of research in the topic of unsupervised neural MT (#REF; #REF) .",
                "This study designs and details an experiment for testing the standard cascade pivot architecture which has been employed in standard statistical machine translation (Costajussà et al., 2012) .",
                "The system that we propose builds on top of one of the latest neural MT architectures called the Transformer #TARGET_REF .",
                "This architecture is an encoderdecoder structure which uses attention-based mechanisms as an alternative to recurrent neural networks proposed in initial architectures (#REF; #REF) .",
                "This new architecture has been proven more efficient and better than all previous proposed so far (#REF) ."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Finally, there is am emerging line of research in the topic of unsupervised neural MT (#REF; #REF) . This study designs and details an experiment for testing the standard cascade pivot architecture which has been employed in standard statistical machine translation (Costajussà et al., 2012) . The system that we propose builds on top of one of the latest neural MT architectures called the Transformer #TARGET_REF . This architecture is an encoderdecoder structure which uses attention-based mechanisms as an alternative to recurrent neural networks proposed in initial architectures (#REF; #REF) . This new architecture has been proven more efficient and better than all previous proposed so far (#REF) .",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"The system that we propose builds on top of one of the latest neural MT architectures called the Transformer #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "This section provides a brief high-level explanation of the neural MT approach that we are using as a baseline system, which is one of the strongest systems presented recently #TARGET_REF , as well as a glance of its differences with other popular neural machine translation architectures.",
                "Sequence-to-sequence recurrent models (#REF; #REF) have been the standard approach for neural machine translation, especially since the incorporation of attention mechanisms #REF) , which enables the system to learn to identify the information which is relevant for producing each word in the translation.",
                "Convolutional networks (#REF) were the second paradigm to effectively approach sequence transduction tasks like machine translation.",
                "In this paper we make use of the third paradigm for neural machine translation, proposed in (#REF) , namely the Transformer architecture, which is based on a feed-forward encoder-decoder scheme with attention mechanisms.",
                "The type of attention mechanism used in the system, referred to as multi-head attention, allows to train several attention modules in parallel, combining also self-attention with standard attention."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "This section provides a brief high-level explanation of the neural MT approach that we are using as a baseline system, which is one of the strongest systems presented recently #TARGET_REF , as well as a glance of its differences with other popular neural machine translation architectures. Sequence-to-sequence recurrent models (#REF; #REF) have been the standard approach for neural machine translation, especially since the incorporation of attention mechanisms #REF) , which enables the system to learn to identify the information which is relevant for producing each word in the translation. Convolutional networks (#REF) were the second paradigm to effectively approach sequence transduction tasks like machine translation. In this paper we make use of the third paradigm for neural machine translation, proposed in (#REF) , namely the Transformer architecture, which is based on a feed-forward encoder-decoder scheme with attention mechanisms. The type of attention mechanism used in the system, referred to as multi-head attention, allows to train several attention modules in parallel, combining also self-attention with standard attention.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"This section provides a brief high-level explanation of the neural MT approach that we are using as a baseline system, which is one of the strongest systems presented recently #TARGET_REF , as well as a glance of its differences with other popular neural machine translation architectures.\"]}"
    },
    {
        "gold": {
            "text": [
                "The type of attention mechanism used in the system, referred to as multi-head attention, allows to train several attention modules in parallel, combining also self-attention with standard attention.",
                "Self-attention differs from standard attention in the use of the same sentence as input and trains over it allowing to solve issues as coreference resolution.",
                "Equations and details about the transformer system can be found in the original paper #TARGET_REF and are out of the scope of this paper.",
                "For the definition of the vocabulary to be used as input for the neural network, we used the sub-word mechanism from tensor2tensor package, which is similar to BytePair Encoding (BPE) from (#REF) .",
                "For the English-Spanish language pair, two separate 32K sub-word vocabularies where extracted, while for Spanish-Catalan we extracted a single shared 32K sub-word vocabulary for both languages."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The type of attention mechanism used in the system, referred to as multi-head attention, allows to train several attention modules in parallel, combining also self-attention with standard attention. Self-attention differs from standard attention in the use of the same sentence as input and trains over it allowing to solve issues as coreference resolution. Equations and details about the transformer system can be found in the original paper #TARGET_REF and are out of the scope of this paper. For the definition of the vocabulary to be used as input for the neural network, we used the sub-word mechanism from tensor2tensor package, which is similar to BytePair Encoding (BPE) from (#REF) . For the English-Spanish language pair, two separate 32K sub-word vocabularies where extracted, while for Spanish-Catalan we extracted a single shared 32K sub-word vocabulary for both languages.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Equations and details about the transformer system can be found in the original paper #TARGET_REF and are out of the scope of this paper.\"]}"
    },
    {
        "gold": {
            "text": [
                "This section provides a brief high-level explanation of the neural MT approach that we are using as a baseline system, which is one of the strongest systems presented recently (#REF) , as well as a glance of its differences with other popular neural machine translation architectures.",
                "Sequence-to-sequence recurrent models (#REF; #REF) have been the standard approach for neural machine translation, especially since the incorporation of attention mechanisms #REF) , which enables the system to learn to identify the information which is relevant for producing each word in the translation.",
                "Convolutional networks (#REF) were the second paradigm to effectively approach sequence transduction tasks like machine translation.",
                "In this paper we make use of the third paradigm for neural machine translation, proposed in #TARGET_REF , namely the Transformer architecture, which is based on a feed-forward encoder-decoder scheme with attention mechanisms.",
                "The type of attention mechanism used in the system, referred to as multi-head attention, allows to train several attention modules in parallel, combining also self-attention with standard attention."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "This section provides a brief high-level explanation of the neural MT approach that we are using as a baseline system, which is one of the strongest systems presented recently (#REF) , as well as a glance of its differences with other popular neural machine translation architectures. Sequence-to-sequence recurrent models (#REF; #REF) have been the standard approach for neural machine translation, especially since the incorporation of attention mechanisms #REF) , which enables the system to learn to identify the information which is relevant for producing each word in the translation. Convolutional networks (#REF) were the second paradigm to effectively approach sequence transduction tasks like machine translation. In this paper we make use of the third paradigm for neural machine translation, proposed in #TARGET_REF , namely the Transformer architecture, which is based on a feed-forward encoder-decoder scheme with attention mechanisms. The type of attention mechanism used in the system, referred to as multi-head attention, allows to train several attention modules in parallel, combining also self-attention with standard attention.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In this paper we make use of the third paradigm for neural machine translation, proposed in #TARGET_REF , namely the Transformer architecture, which is based on a feed-forward encoder-decoder scheme with attention mechanisms.\"]}"
    },
    {
        "gold": {
            "text": [
                "Even though other measures have been proposed in the literature (#REF ), Vector Cosine is still by far the most popular one (#REF) .",
                "However, in a recent paper of Santus et al. (2016b) , the authors have claimed that Vector Cosine is outperformed by APSyn (Average Precision for Synonymy), a metric based on the extent of the intersection between the most salient contexts of two target words.",
                "The measure, tested on a window-based DSM, outperformed Vector Cosine on the ESL and on the TOEFL datasets.",
                "In the present work, we perform a systematic evaluation of APSyn, testing it on the most popular test sets for similarity estimation -namely WordSim-353 (#REF) , MEN (#REF) and #TARGET_REF ( #TARGET_REF) .",
                "For comparison, Vector Cosine is also calculated on several countbased DSMs."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Even though other measures have been proposed in the literature (#REF ), Vector Cosine is still by far the most popular one (#REF) . However, in a recent paper of Santus et al. (2016b) , the authors have claimed that Vector Cosine is outperformed by APSyn (Average Precision for Synonymy), a metric based on the extent of the intersection between the most salient contexts of two target words. The measure, tested on a window-based DSM, outperformed Vector Cosine on the ESL and on the TOEFL datasets. In the present work, we perform a systematic evaluation of APSyn, testing it on the most popular test sets for similarity estimation -namely WordSim-353 (#REF) , MEN (#REF) and #TARGET_REF ( #TARGET_REF) . For comparison, Vector Cosine is also calculated on several countbased DSMs.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In the present work, we perform a systematic evaluation of APSyn, testing it on the most popular test sets for similarity estimation -namely WordSim-353 (#REF) , MEN (#REF) and #TARGET_REF ( #TARGET_REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "The new metric is shown to outperform Vector Cosine in most settings, except when the latter metric is applied on a PPMI-SVD reduced matrix (#REF) , against which APSyn still obtains competitive performances.",
                "The results are also discussed in relation to the state-of-the-art DSMs, as reported in #TARGET_REF .",
                "In such comparison, the best settings of our models outperform the word embeddings in almost all datasets.",
                "A pilot study was also carried out to investigate whether APSyn is scalable.",
                "Results prove its high performance also when calculated on large corpora, such as those used by ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The new metric is shown to outperform Vector Cosine in most settings, except when the latter metric is applied on a PPMI-SVD reduced matrix (#REF) , against which APSyn still obtains competitive performances. The results are also discussed in relation to the state-of-the-art DSMs, as reported in #TARGET_REF . In such comparison, the best settings of our models outperform the word embeddings in almost all datasets. A pilot study was also carried out to investigate whether APSyn is scalable. Results prove its high performance also when calculated on large corpora, such as those used by .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The results are also discussed in relation to the state-of-the-art DSMs, as reported in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "For our evaluation, we used three widely popular datasets: WordSim-353 (#REF) , MEN (#REF) , #TARGET_REF ( #TARGET_REF) .",
                "These datasets have a different history, but all of them consist in word pairs with an associated score, that should either represent word association or word similarity.",
                "WordSim-353 (#REF ) was proposed as a word similarity dataset containing 353 pairs annotated with scores between 0 and 10.",
                "However, #REF claimed that the instructions to the annotators were ambiguous with respect to similarity and association, so that the subjects assigned high similarity scores to entities that are only related by virtue of frequent association (e.g. coffee and cup; movie and theater).",
                "On top of it, WordSim-353 does not provide the POS-tags for the 439 words that it contains, forcing the users to decide which POS to assign to the ambiguous words (e.g. [white, rabbit] and [run, marathon] )."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "For our evaluation, we used three widely popular datasets: WordSim-353 (#REF) , MEN (#REF) , #TARGET_REF ( #TARGET_REF) . These datasets have a different history, but all of them consist in word pairs with an associated score, that should either represent word association or word similarity. WordSim-353 (#REF ) was proposed as a word similarity dataset containing 353 pairs annotated with scores between 0 and 10. However, #REF claimed that the instructions to the annotators were ambiguous with respect to similarity and association, so that the subjects assigned high similarity scores to entities that are only related by virtue of frequent association (e.g. coffee and cup; movie and theater). On top of it, WordSim-353 does not provide the POS-tags for the 439 words that it contains, forcing the users to decide which POS to assign to the ambiguous words (e.g. [white, rabbit] and [run, marathon] ).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For our evaluation, we used three widely popular datasets: WordSim-353 (#REF) , MEN (#REF) , #TARGET_REF ( #TARGET_REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by #TARGET_REF , who used the code (or directly the embeddings) shared by the original authors.",
                "As we trained our models on almost the same corpora used by Hill and colleagues, the results are perfectly comparable.",
                "The three models we compare our results to are: i) the convolutional neural network of #REF , which was trained on 852 million words of Wikipedia; ii) the neural network of #REF , which was trained on 990 million words of Wikipedia; and iii) the word2vec of #REF , which was trained on 1000 million words of Wikipedia and on the RCV Vol.",
                "1 Corpus (#REF #REF , as reported in #REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by #TARGET_REF , who used the code (or directly the embeddings) shared by the original authors. As we trained our models on almost the same corpora used by Hill and colleagues, the results are perfectly comparable. The three models we compare our results to are: i) the convolutional neural network of #REF , which was trained on 852 million words of Wikipedia; ii) the neural network of #REF , which was trained on 990 million words of Wikipedia; and iii) the word2vec of #REF , which was trained on 1000 million words of Wikipedia and on the RCV Vol. 1 Corpus (#REF #REF , as reported in #REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by #TARGET_REF , who used the code (or directly the embeddings) shared by the original authors.\"]}"
    },
    {
        "gold": {
            "text": [
                "For our experiments, we implemented twenty-eight DSMs, but for reasons of space only sixteen of them are reported in the tables.",
                "All of them include the pos-tagged target words used in the three datasets (i.e. MEN, WordSim-353 and #TARGET_REF) and the pos-tagged contexts having frequency above 100 in the two corpora.",
                "We considered as contexts the content words (i.e. nouns, verbs and adjectives) within a window of 2, 3 and 5, even though the latter was given up for its poor performances.",
                "As for SVD factorization, we found out that the best results were always achieved when the number of latent dimensions was between 300 and 500.",
                "We report here only the scores for k = 300, since 300 is one of the most common choices for the dimensionality of SVD-reduced spaces and it is always close to be an optimal value for the parameter."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "For our experiments, we implemented twenty-eight DSMs, but for reasons of space only sixteen of them are reported in the tables. All of them include the pos-tagged target words used in the three datasets (i.e. MEN, WordSim-353 and #TARGET_REF) and the pos-tagged contexts having frequency above 100 in the two corpora. We considered as contexts the content words (i.e. nouns, verbs and adjectives) within a window of 2, 3 and 5, even though the latter was given up for its poor performances. As for SVD factorization, we found out that the best results were always achieved when the number of latent dimensions was between 300 and 500. We report here only the scores for k = 300, since 300 is one of the most common choices for the dimensionality of SVD-reduced spaces and it is always close to be an optimal value for the parameter.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For our experiments, we implemented twenty-eight DSMs, but for reasons of space only sixteen of them are reported in the tables.\", \"All of them include the pos-tagged target words used in the three datasets (i.e. MEN, WordSim-353 and #TARGET_REF) and the pos-tagged contexts having frequency above 100 in the two corpora.\"]}"
    },
    {
        "gold": {
            "text": [
                "Given the twenty-eight DSMs, for each dataset we have measured the Vector Cosine and APSyn between the words in the test pairs.",
                "Table 2 : Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets #TARGET_REF, WordSim-353 and MEN.",
                "In the bottom the performance of the state-of-the-art models of #REF , #REF, #REF , as reported in #REF .",
                "The Spearman correlation between our scores and the gold standard was then computed for every model and it is reported in Table 1 and Table 2 .",
                "In particular, Table 1 describes the performances on #REF9, WordSim-353 and MEN for the measures applied on RCV Vol."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Given the twenty-eight DSMs, for each dataset we have measured the Vector Cosine and APSyn between the words in the test pairs. Table 2 : Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets #TARGET_REF, WordSim-353 and MEN. In the bottom the performance of the state-of-the-art models of #REF , #REF, #REF , as reported in #REF . The Spearman correlation between our scores and the gold standard was then computed for every model and it is reported in Table 1 and Table 2 . In particular, Table 1 describes the performances on #REF9, WordSim-353 and MEN for the measures applied on RCV Vol.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Table 2 : Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets #TARGET_REF, WordSim-353 and MEN.\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 2 : Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets #REF9, WordSim-353 and MEN.",
                "In the bottom the performance of the state-of-the-art models of #REF , #REF, #REF , as reported in #TARGET_REF .",
                "The Spearman correlation between our scores and the gold standard was then computed for every model and it is reported in Table 1 and Table 2 .",
                "In particular, Table 1 describes the performances on #REF9, WordSim-353 and MEN for the measures applied on RCV Vol.",
                "1 models."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Table 2 : Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets #REF9, WordSim-353 and MEN. In the bottom the performance of the state-of-the-art models of #REF , #REF, #REF , as reported in #TARGET_REF . The Spearman correlation between our scores and the gold standard was then computed for every model and it is reported in Table 1 and Table 2 . In particular, Table 1 describes the performances on #REF9, WordSim-353 and MEN for the measures applied on RCV Vol. 1 models.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In the bottom the performance of the state-of-the-art models of #REF , #REF, #REF , as reported in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In the bottom the performance of the state-of-the-art models of #REF , #REF, #REF , as reported in #REF .",
                "The Spearman correlation between our scores and the gold standard was then computed for every model and it is reported in Table 1 and Table 2 .",
                "In particular, Table 1 describes the performances on #TARGET_REF, WordSim-353 and MEN for the measures applied on RCV Vol.",
                "1 models.",
                "Table 2 , instead, describes the performances of the measures on the three datasets for the Wikipedia models."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In the bottom the performance of the state-of-the-art models of #REF , #REF, #REF , as reported in #REF . The Spearman correlation between our scores and the gold standard was then computed for every model and it is reported in Table 1 and Table 2 . In particular, Table 1 describes the performances on #TARGET_REF, WordSim-353 and MEN for the measures applied on RCV Vol. 1 models. Table 2 , instead, describes the performances of the measures on the three datasets for the Wikipedia models.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In particular, Table 1 describes the performances on #TARGET_REF, WordSim-353 and MEN for the measures applied on RCV Vol.\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 2 , instead, describes the performances of the measures on the three datasets for the Wikipedia models.",
                "Concurrently, Table 3 and Table 4 describe the performances of the measures respectively on the RCV Vol.",
                "1 and Wikipedia models, tested on the subsets of WordSim-353 extracted by #REF .",
                "Table 1 shows the Spearman correlation scores for Vector Cosine and APSyn on the three datasets for the eight most representative DSMs built using RCV Vol. 1. Table 2 does the same for the DSMs built using Wikipedia.",
                "For the sake of comparison, we also report the results of the state-of-the-art DSMs mentioned in #TARGET_REF (see Section 2.5)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Table 2 , instead, describes the performances of the measures on the three datasets for the Wikipedia models. Concurrently, Table 3 and Table 4 describe the performances of the measures respectively on the RCV Vol. 1 and Wikipedia models, tested on the subsets of WordSim-353 extracted by #REF . Table 1 shows the Spearman correlation scores for Vector Cosine and APSyn on the three datasets for the eight most representative DSMs built using RCV Vol. 1. Table 2 does the same for the DSMs built using Wikipedia. For the sake of comparison, we also report the results of the state-of-the-art DSMs mentioned in #TARGET_REF (see Section 2.5).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For the sake of comparison, we also report the results of the state-of-the-art DSMs mentioned in #TARGET_REF (see Section 2.5).\"]}"
    },
    {
        "gold": {
            "text": [
                "Also, we can notice that the smaller window (i.e. 2) does not always perform better than the larger one (i.e. 3).",
                "The former appears to perform better on #REF9, while the latter seems to have some advantages on the other datasets.",
                "This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in #TARGET_REF (i.e. genuine similarity).",
                "On top of it, despite #REF 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (#REF; #REF) , we need to mention that window 5 was abandoned because of its low performance.",
                "With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by #REF , using the words of the #REF9 dataset as query words and collecting for each of them the top 1000 nearest neighbors."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Also, we can notice that the smaller window (i.e. 2) does not always perform better than the larger one (i.e. 3). The former appears to perform better on #REF9, while the latter seems to have some advantages on the other datasets. This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in #TARGET_REF (i.e. genuine similarity). On top of it, despite #REF 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (#REF; #REF) , we need to mention that window 5 was abandoned because of its low performance. With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by #REF , using the words of the #REF9 dataset as query words and collecting for each of them the top 1000 nearest neighbors.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in #TARGET_REF (i.e. genuine similarity).\"]}"
    },
    {
        "gold": {
            "text": [
                "On top of it, despite #REF 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (#REF; #REF) , we need to mention that window 5 was abandoned because of its low performance.",
                "With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by #REF , using the words of the #TARGET_REF dataset as query words and collecting for each of them the top 1000 nearest neighbors.",
                "Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora.",
                "Figure 1 shows the relation between the rank in the nearest neighbor list and the rank in the frequency list.",
                "It can be easily noticed that the highest ranked nearest neighbors tend to have higher rank also in the frequency list, supporting the idea that frequent words are more likely to be nearest neighbors."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "On top of it, despite #REF 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (#REF; #REF) , we need to mention that window 5 was abandoned because of its low performance. With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by #REF , using the words of the #TARGET_REF dataset as query words and collecting for each of them the top 1000 nearest neighbors. Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora. Figure 1 shows the relation between the rank in the nearest neighbor list and the rank in the frequency list. It can be easily noticed that the highest ranked nearest neighbors tend to have higher rank also in the frequency list, supporting the idea that frequent words are more likely to be nearest neighbors.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by #REF , using the words of the #TARGET_REF dataset as query words and collecting for each of them the top 1000 nearest neighbors.\"]}"
    },
    {
        "gold": {
            "text": [
                "APSyn does not seem to be able to overcome such bias, which seems to be in fact an inherent property of the DSMs (#REF) .",
                "Further investigation is needed to see whether variations of APSyn can tackle this problem.",
                "Finally, few words need to be spent with regard to the ability of calculating genuine similarity, as distinguished from word relatedness (#REF; #REF; #TARGET_REF) .",
                "Table 3 and  Table 4 show the Spearman correlation scores for the two measures calculated on the models respectively trained on RCV1 and Wikipedia, tested on the subsets of WordSim-353 extracted by #REF .",
                "It can be easily noticed that our best models work better on the similarity subset."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "APSyn does not seem to be able to overcome such bias, which seems to be in fact an inherent property of the DSMs (#REF) . Further investigation is needed to see whether variations of APSyn can tackle this problem. Finally, few words need to be spent with regard to the ability of calculating genuine similarity, as distinguished from word relatedness (#REF; #REF; #TARGET_REF) . Table 3 and  Table 4 show the Spearman correlation scores for the two measures calculated on the models respectively trained on RCV1 and Wikipedia, tested on the subsets of WordSim-353 extracted by #REF . It can be easily noticed that our best models work better on the similarity subset.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Finally, few words need to be spent with regard to the ability of calculating genuine similarity, as distinguished from word relatedness (#REF; #REF; #TARGET_REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "On top of it, APSyn does not suffer from some of the problems reported for the Vector Cosine, such as the inability of identifying the number of shared features.",
                "It still however seems to be affected by the hubness issue, and more research should be carried out to tackle it.",
                "Concerning the discrimination between similarity and association, the good performance of APSyn on #TARGET_REF (which was built with a specific attention to genuine similarity) and the large difference in performance between the two subsets of WordSim-353 described in Table 3 and Table 4 make us conclude that APSyn is indeed efficient in quantifying genuine similarity.",
                "To conclude, being a linguistically and cognitively grounded metric, APSyn offers the possibility for further improvements, by simply combining it to other properties that were not yet considered in its definition.",
                "A natural extension would be to verify whether APSyn hypothesis and implementation holds on SVD reduced matrices and word embeddings."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "On top of it, APSyn does not suffer from some of the problems reported for the Vector Cosine, such as the inability of identifying the number of shared features. It still however seems to be affected by the hubness issue, and more research should be carried out to tackle it. Concerning the discrimination between similarity and association, the good performance of APSyn on #TARGET_REF (which was built with a specific attention to genuine similarity) and the large difference in performance between the two subsets of WordSim-353 described in Table 3 and Table 4 make us conclude that APSyn is indeed efficient in quantifying genuine similarity. To conclude, being a linguistically and cognitively grounded metric, APSyn offers the possibility for further improvements, by simply combining it to other properties that were not yet considered in its definition. A natural extension would be to verify whether APSyn hypothesis and implementation holds on SVD reduced matrices and word embeddings.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Concerning the discrimination between similarity and association, the good performance of APSyn on #TARGET_REF (which was built with a specific attention to genuine similarity) and the large difference in performance between the two subsets of WordSim-353 described in Table 3 and Table 4 make us conclude that APSyn is indeed efficient in quantifying genuine similarity.\"]}"
    },
    {
        "gold": {
            "text": [
                "In such comparison, the best settings of our models outperform the word embeddings in almost all datasets.",
                "A pilot study was also carried out to investigate whether APSyn is scalable.",
                "Results prove its high performance also when calculated on large corpora, such as those used by .",
                "On top of the performance, APSyn seems not to be subject to some of the biases that affect Vector Cosine.",
                "Finally, considering the debate about the ability of DSMs to calculate genuine similarity as opposed to word relatedness (#REF; #REF; #TARGET_REF) , we test the ability of the models to quantify genuine semantic similarity."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "In such comparison, the best settings of our models outperform the word embeddings in almost all datasets. A pilot study was also carried out to investigate whether APSyn is scalable. Results prove its high performance also when calculated on large corpora, such as those used by . On top of the performance, APSyn seems not to be subject to some of the biases that affect Vector Cosine. Finally, considering the debate about the ability of DSMs to calculate genuine similarity as opposed to word relatedness (#REF; #REF; #TARGET_REF) , we test the ability of the models to quantify genuine semantic similarity.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Finally, considering the debate about the ability of DSMs to calculate genuine similarity as opposed to word relatedness (#REF; #REF; #TARGET_REF) , we test the ability of the models to quantify genuine semantic similarity.\"]}"
    },
    {
        "gold": {
            "text": [
                "These datasets have a different history, but all of them consist in word pairs with an associated score, that should either represent word association or word similarity.",
                "WordSim-353 (#REF ) was proposed as a word similarity dataset containing 353 pairs annotated with scores between 0 and 10.",
                "However, #TARGET_REF claimed that the instructions to the annotators were ambiguous with respect to similarity and association, so that the subjects assigned high similarity scores to entities that are only related by virtue of frequent association (e.g. coffee and cup; movie and theater).",
                "On top of it, WordSim-353 does not provide the POS-tags for the 439 words that it contains, forcing the users to decide which POS to assign to the ambiguous words (e.g. [white, rabbit] and [run, marathon] ).",
                "An extension of this dataset resulted from the subclassification carried out by #REF , which discriminated between similar and associated word pairs."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "These datasets have a different history, but all of them consist in word pairs with an associated score, that should either represent word association or word similarity. WordSim-353 (#REF ) was proposed as a word similarity dataset containing 353 pairs annotated with scores between 0 and 10. However, #TARGET_REF claimed that the instructions to the annotators were ambiguous with respect to similarity and association, so that the subjects assigned high similarity scores to entities that are only related by virtue of frequent association (e.g. coffee and cup; movie and theater). On top of it, WordSim-353 does not provide the POS-tags for the 439 words that it contains, forcing the users to decide which POS to assign to the ambiguous words (e.g. [white, rabbit] and [run, marathon] ). An extension of this dataset resulted from the subclassification carried out by #REF , which discriminated between similar and associated word pairs.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"However, #TARGET_REF claimed that the instructions to the annotators were ambiguous with respect to similarity and association, so that the subjects assigned high similarity scores to entities that are only related by virtue of frequent association (e.g. coffee and cup; movie and theater).\"]}"
    },
    {
        "gold": {
            "text": [
                "Such discrimination was done by asking annotators to classify all pairs according to the semantic relation they hold (i.e. identical, synonymy, antonymy, hypernymy, meronymy and none-of-the-above).",
                "The annotation was then used to group the pairs in three categories: similar pairs (those classified as identical, synonyms, antonyms and hypernyms), associated pairs (those classified as meronyms and none-of-the-above, with an average similarity greater than 5), and non-associated pairs (those classified as none-of-the-above, with an average similarity below or equal to 5).",
                "Two gold standard were finally produced: i) one for similarity, containing 203 word pairs resulting from the union of similar and non-associated pairs; ii) one for relatedness, containing 252 word pairs resulting from the union of associated and non-associated pairs.",
                "Even though such a classification made a clear distinction between the two types of relations (i.e. similarity and association), #TARGET_REF argue that these gold standards still carry the scores they had in WordSim-353, which are known to be ambiguous in this regard.",
                "The MEN Test Collection (#REF) includes 3,000 word pairs divided in two sets (one for training and one for testing) together with human judgments, obtained through Amazon Mechanical Turk."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Such discrimination was done by asking annotators to classify all pairs according to the semantic relation they hold (i.e. identical, synonymy, antonymy, hypernymy, meronymy and none-of-the-above). The annotation was then used to group the pairs in three categories: similar pairs (those classified as identical, synonyms, antonyms and hypernyms), associated pairs (those classified as meronyms and none-of-the-above, with an average similarity greater than 5), and non-associated pairs (those classified as none-of-the-above, with an average similarity below or equal to 5). Two gold standard were finally produced: i) one for similarity, containing 203 word pairs resulting from the union of similar and non-associated pairs; ii) one for relatedness, containing 252 word pairs resulting from the union of associated and non-associated pairs. Even though such a classification made a clear distinction between the two types of relations (i.e. similarity and association), #TARGET_REF argue that these gold standards still carry the scores they had in WordSim-353, which are known to be ambiguous in this regard. The MEN Test Collection (#REF) includes 3,000 word pairs divided in two sets (one for training and one for testing) together with human judgments, obtained through Amazon Mechanical Turk.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Even though such a classification made a clear distinction between the two types of relations (i.e. similarity and association), #TARGET_REF argue that these gold standards still carry the scores they had in WordSim-353, which are known to be ambiguous in this regard.\"]}"
    },
    {
        "gold": {
            "text": [
                "Every pairs-couple was proposed only once, and a final score out of 50 was attributed to each pair, according to how many times it was rated as the most related.",
                "According to #TARGET_REF , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association.",
                "#REF9 is the dataset introduced by #REF to address the above mentioned criticisms of confusion between similarity and association.",
                "The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness.",
                "The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Every pairs-couple was proposed only once, and a final score out of 50 was attributed to each pair, according to how many times it was rated as the most related. According to #TARGET_REF , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association. #REF9 is the dataset introduced by #REF to address the above mentioned criticisms of confusion between similarity and association. The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness. The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"According to #TARGET_REF , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association.\"]}"
    },
    {
        "gold": {
            "text": [
                "The construction was performed by asking subjects to rate which pair -among two of themwas the more related one (i.e. the most associated).",
                "Every pairs-couple was proposed only once, and a final score out of 50 was attributed to each pair, according to how many times it was rated as the most related.",
                "According to #REF , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association.",
                "#TARGET_REF is the dataset introduced by #TARGET_REF to address the above mentioned criticisms of confusion between similarity and association.",
                "The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The construction was performed by asking subjects to rate which pair -among two of themwas the more related one (i.e. the most associated). Every pairs-couple was proposed only once, and a final score out of 50 was attributed to each pair, according to how many times it was rated as the most related. According to #REF , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association. #TARGET_REF is the dataset introduced by #TARGET_REF to address the above mentioned criticisms of confusion between similarity and association. The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF is the dataset introduced by #TARGET_REF to address the above mentioned criticisms of confusion between similarity and association.\"]}"
    },
    {
        "gold": {
            "text": [
                "According to #REF , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association.",
                "#REF9 is the dataset introduced by #REF to address the above mentioned criticisms of confusion between similarity and association.",
                "The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness.",
                "The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association.",
                "#TARGET_REF claim that differently from other datasets, #TARGET_REF interannotator agreement has not been surpassed by any automatic approach."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "According to #REF , the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association. #REF9 is the dataset introduced by #REF to address the above mentioned criticisms of confusion between similarity and association. The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness. The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association. #TARGET_REF claim that differently from other datasets, #TARGET_REF interannotator agreement has not been surpassed by any automatic approach.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF claim that differently from other datasets, #TARGET_REF interannotator agreement has not been surpassed by any automatic approach.\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by #REF , who used the code (or directly the embeddings) shared by the original authors.",
                "As we trained our models on almost the same corpora used by #TARGET_REF, the results are perfectly comparable.",
                "The three models we compare our results to are: i) the convolutional neural network of #REF , which was trained on 852 million words of Wikipedia; ii) the neural network of #REF , which was trained on 990 million words of Wikipedia; and iii) the word2vec of #REF , which was trained on 1000 million words of Wikipedia and on the RCV Vol.",
                "1 Corpus (#REF #REF , as reported in #REF ."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0
            ]
        },
        "input": "In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by #REF , who used the code (or directly the embeddings) shared by the original authors. As we trained our models on almost the same corpora used by #TARGET_REF, the results are perfectly comparable. The three models we compare our results to are: i) the convolutional neural network of #REF , which was trained on 852 million words of Wikipedia; ii) the neural network of #REF , which was trained on 990 million words of Wikipedia; and iii) the word2vec of #REF , which was trained on 1000 million words of Wikipedia and on the RCV Vol. 1 Corpus (#REF #REF , as reported in #REF .",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"As we trained our models on almost the same corpora used by #TARGET_REF, the results are perfectly comparable.\"]}"
    },
    {
        "gold": {
            "text": [
                "It outperforms the Vector Cosine in almost all settings, except when the latter is used on a PPMI-weighed SVD-reduced DSM.",
                "Even in this case, anyway, its performance is very competitive.",
                "Interestingly, our best models achieve results that are comparable to -or even better than -those reported by #TARGET_REF for the stateof-the-art word embeddings models.",
                "In Section 3.5 we show that APSyn is scalable, outperforming the state-of-the-art count-based models reported in .",
                "On top of it, APSyn does not suffer from some of the problems reported for the Vector Cosine, such as the inability of identifying the number of shared features."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "It outperforms the Vector Cosine in almost all settings, except when the latter is used on a PPMI-weighed SVD-reduced DSM. Even in this case, anyway, its performance is very competitive. Interestingly, our best models achieve results that are comparable to -or even better than -those reported by #TARGET_REF for the stateof-the-art word embeddings models. In Section 3.5 we show that APSyn is scalable, outperforming the state-of-the-art count-based models reported in . On top of it, APSyn does not suffer from some of the problems reported for the Vector Cosine, such as the inability of identifying the number of shared features.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Interestingly, our best models achieve results that are comparable to -or even better than -those reported by #TARGET_REF for the stateof-the-art word embeddings models.\"]}"
    },
    {
        "gold": {
            "text": [
                "Some further observations are: i) corpus size strongly affects the results; ii) PPMI strongly outperforms LMI for both Vector Cosine and APSyn; iii) SVD boosts the Vector Cosine, especially when it is combined with PPMI; iv) N has some impact on the performance of APSyn, which generally achieves the best results for N=500.",
                "As a note about iii), the results of using SVD jointly with LMI spaces are less predictable than when combining it with PPMI.",
                "Also, we can notice that the smaller window (i.e. 2) does not always perform better than the larger one (i.e. 3).",
                "The former appears to perform better on #TARGET_REF, while the latter seems to have some advantages on the other datasets.",
                "This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in #REF9 (i.e. genuine similarity)."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "Some further observations are: i) corpus size strongly affects the results; ii) PPMI strongly outperforms LMI for both Vector Cosine and APSyn; iii) SVD boosts the Vector Cosine, especially when it is combined with PPMI; iv) N has some impact on the performance of APSyn, which generally achieves the best results for N=500. As a note about iii), the results of using SVD jointly with LMI spaces are less predictable than when combining it with PPMI. Also, we can notice that the smaller window (i.e. 2) does not always perform better than the larger one (i.e. 3). The former appears to perform better on #TARGET_REF, while the latter seems to have some advantages on the other datasets. This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in #REF9 (i.e. genuine similarity).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"As a note about iii), the results of using SVD jointly with LMI spaces are less predictable than when combining it with PPMI.\", \"Also, we can notice that the smaller window (i.e. 2) does not always perform better than the larger one (i.e. 3).\", \"The former appears to perform better on #TARGET_REF, while the latter seems to have some advantages on the other datasets.\"]}"
    },
    {
        "gold": {
            "text": [
                "This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in #REF9 (i.e. genuine similarity).",
                "On top of it, despite #TARGET_REF 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (#REF; #REF) , we need to mention that window 5 was abandoned because of its low performance.",
                "With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by #REF , using the words of the #REF9 dataset as query words and collecting for each of them the top 1000 nearest neighbors.",
                "Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora.",
                "Figure 1 shows the relation between the rank in the nearest neighbor list and the rank in the frequency list."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "This Table 3 : Spearman correlation scores for our eight models trained on RCV1, in the two subsets of might depend on the different type of similarity encoded in #REF9 (i.e. genuine similarity). On top of it, despite #TARGET_REF 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (#REF; #REF) , we need to mention that window 5 was abandoned because of its low performance. With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by #REF , using the words of the #REF9 dataset as query words and collecting for each of them the top 1000 nearest neighbors. Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora. Figure 1 shows the relation between the rank in the nearest neighbor list and the rank in the frequency list.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"On top of it, despite #TARGET_REF 's claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (#REF; #REF) , we need to mention that window 5 was abandoned because of its low performance.\"]}"
    },
    {
        "gold": {
            "text": [
                "We experiment with using much less training data than recent systems based on recurrent neural networks (RNN) (#REFb; #REF) , and we find that our generator learns successfully to produce both strings and deep syntax trees on the BAGEL restaurant information dataset .",
                "It is able to surpass n-gram-based scores achieved previously by #TARGET_REF , offering a simpler setup and more relevant outputs.",
                "We introduce the generation setting in Section 2 and describe our generator architecture in Section 3.",
                "Section 4 details our experiments, Section 5 analyzes the results.",
                "We summarize related work in Section 6 and offer conclusions in Section 7."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "We experiment with using much less training data than recent systems based on recurrent neural networks (RNN) (#REFb; #REF) , and we find that our generator learns successfully to produce both strings and deep syntax trees on the BAGEL restaurant information dataset . It is able to surpass n-gram-based scores achieved previously by #TARGET_REF , offering a simpler setup and more relevant outputs. We introduce the generation setting in Section 2 and describe our generator architecture in Section 3. Section 4 details our experiments, Section 5 analyzes the results. We summarize related work in Section 6 and offer conclusions in Section 7.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"It is able to surpass n-gram-based scores achieved previously by #TARGET_REF , offering a simpler setup and more relevant outputs.\"]}"
    },
    {
        "gold": {
            "text": [
                "A larger beam leads to a small BLEU decrease even though the sentences contain less errors; here, NIST reflects the situation more accurately.",
                "A comparison of the two approaches goes in favor of the joint setup: Without the reranker, models generating trees produce less semantic errors and gain higher BLEU/NIST scores.",
                "However, with the reranker, the string-based model is able to reduce the number of semantic errors while producing outputs significantly better in terms of BLEU/NIST.",
                "11 In addition, the joint setup does not need an external surface realizer.",
                "The best results of both setups surpass the best results on this dataset using training data without manual alignments #TARGET_REF in both automatic metrics 12 and the number of semantic errors."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "A larger beam leads to a small BLEU decrease even though the sentences contain less errors; here, NIST reflects the situation more accurately. A comparison of the two approaches goes in favor of the joint setup: Without the reranker, models generating trees produce less semantic errors and gain higher BLEU/NIST scores. However, with the reranker, the string-based model is able to reduce the number of semantic errors while producing outputs significantly better in terms of BLEU/NIST. 11 In addition, the joint setup does not need an external surface realizer. The best results of both setups surpass the best results on this dataset using training data without manual alignments #TARGET_REF in both automatic metrics 12 and the number of semantic errors.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The best results of both setups surpass the best results on this dataset using training data without manual alignments #TARGET_REF in both automatic metrics 12 and the number of semantic errors.\"]}"
    },
    {
        "gold": {
            "text": [
                "The results show the direct approach as more favorable, with significantly higher n-gram based scores and a similar number of semantic errors in the output.",
                "We also showed that our generator can learn to produce meaningful utterances using a much smaller amount of training data than what is typically used for RNN-based approaches.",
                "The resulting models had virtually no problems with produc-ing fluent, coherent sentences or with generating valid structure of bracketed deep syntax trees.",
                "Our generator was able to surpass the best BLEU/NIST scores on the same dataset previously achieved by a perceptron-based generator of #TARGET_REF while reducing the amount of irrelevant information on the output.",
                "Our generator is released on GitHub at the following URL:"
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The results show the direct approach as more favorable, with significantly higher n-gram based scores and a similar number of semantic errors in the output. We also showed that our generator can learn to produce meaningful utterances using a much smaller amount of training data than what is typically used for RNN-based approaches. The resulting models had virtually no problems with produc-ing fluent, coherent sentences or with generating valid structure of bracketed deep syntax trees. Our generator was able to surpass the best BLEU/NIST scores on the same dataset previously achieved by a perceptron-based generator of #TARGET_REF while reducing the amount of irrelevant information on the output. Our generator is released on GitHub at the following URL:",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Our generator was able to surpass the best BLEU/NIST scores on the same dataset previously achieved by a perceptron-based generator of #TARGET_REF while reducing the amount of irrelevant information on the output.\"]}"
    },
    {
        "gold": {
            "text": [
                "Our generator operates in two modes, producing either deep syntax trees (Dušek et al., 2012) or natural language strings (see Fig. 1 ).",
                "The first mode corresponds to the sentence planning NLG stage as it decides the syntactic shape of the output sentence; the resulting deep syntax tree involves content words (lemmas) and their syntactic form (formemes, purple in Fig. 1 ).",
                "The trees are linearized to strings using a surface realizer from the TectoMT translation system .",
                "The second generator mode joins sentence planning and surface realization into one step, producing natural language sentences directly.",
                "Both modes offer their advantages: The twostep mode simplifies generation by abstracting away from complex surface syntax and morphology, which can be handled by a handcrafted, domain-independent module to ensure grammatical correctness at all times #TARGET_REF , and the joint mode does not need to model structure explicitly and avoids accumulating errors along the pipeline (#REF) ."
            ],
            "label": [
                "BACK",
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Our generator operates in two modes, producing either deep syntax trees (Dušek et al., 2012) or natural language strings (see Fig. 1 ). The first mode corresponds to the sentence planning NLG stage as it decides the syntactic shape of the output sentence; the resulting deep syntax tree involves content words (lemmas) and their syntactic form (formemes, purple in Fig. 1 ). The trees are linearized to strings using a surface realizer from the TectoMT translation system . The second generator mode joins sentence planning and surface realization into one step, producing natural language sentences directly. Both modes offer their advantages: The twostep mode simplifies generation by abstracting away from complex surface syntax and morphology, which can be handled by a handcrafted, domain-independent module to ensure grammatical correctness at all times #TARGET_REF , and the joint mode does not need to model structure explicitly and avoids accumulating errors along the pipeline (#REF) .",
        "output": "{\"label\": [\"BACK\", \"USE\"], \"context\": [\"Our generator operates in two modes, producing either deep syntax trees (Du\\u0161ek et al., 2012) or natural language strings (see Fig. 1 ).\", \"Both modes offer their advantages: The twostep mode simplifies generation by abstracting away from complex surface syntax and morphology, which can be handled by a handcrafted, domain-independent module to ensure grammatical correctness at all times #TARGET_REF , and the joint mode does not need to model structure explicitly and avoids accumulating errors along the pipeline (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Using a larger beam increases the effect of the reranker as expected, resulting in slightly improved outputs.",
                "Models generating deep syntax trees are also able to learn the domain style, and they have virtually no problems producing valid trees.",
                "10 The surface realizer works almost flawlessly on this lim-ited domain #TARGET_REF , leaving the seq2seq generator as the major error source.",
                "The syntax-generating models tend to make different kinds of errors than the string-based models: Some outputs are valid trees but not entirely syntactically fluent; missing, incorrect, or repeated information is more frequent than a confusion of semantically similar items (see Table 2 ).",
                "Semantic error rates of greedy and beam-search decoding are lower than for string-based models, partly because confusion of two similar items counts as two errors."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Using a larger beam increases the effect of the reranker as expected, resulting in slightly improved outputs. Models generating deep syntax trees are also able to learn the domain style, and they have virtually no problems producing valid trees. 10 The surface realizer works almost flawlessly on this lim-ited domain #TARGET_REF , leaving the seq2seq generator as the major error source. The syntax-generating models tend to make different kinds of errors than the string-based models: Some outputs are valid trees but not entirely syntactically fluent; missing, incorrect, or repeated information is more frequent than a confusion of semantically similar items (see Table 2 ). Semantic error rates of greedy and beam-search decoding are lower than for string-based models, partly because confusion of two similar items counts as two errors.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"10 The surface realizer works almost flawlessly on this lim-ited domain #TARGET_REF , leaving the seq2seq generator as the major error source.\"]}"
    },
    {
        "gold": {
            "text": [
                "In the case of words, this means detecting which words need to be deleted or replaced, while in the case of phrases this means detecting which phrases contain words translated inadequately, but also if there are missing words, or the order of the words in the phrase is not correct.",
                "The systems participating in the task are required to apply the labels BAD and OK, either to words or phrases.",
                "In this paper we describe the approach behind the submissions of the Universitat d'Alacant team to these sub-tasks.",
                "For our word-level submissions we have applied the approach proposed by #TARGET_REF , where we used black-box bilingual on-line resources.",
                "The new task tackles MTQE for translating English into German."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "In the case of words, this means detecting which words need to be deleted or replaced, while in the case of phrases this means detecting which phrases contain words translated inadequately, but also if there are missing words, or the order of the words in the phrase is not correct. The systems participating in the task are required to apply the labels BAD and OK, either to words or phrases. In this paper we describe the approach behind the submissions of the Universitat d'Alacant team to these sub-tasks. For our word-level submissions we have applied the approach proposed by #TARGET_REF , where we used black-box bilingual on-line resources. The new task tackles MTQE for translating English into German.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"For our word-level submissions we have applied the approach proposed by #TARGET_REF , where we used black-box bilingual on-line resources.\"]}"
    },
    {
        "gold": {
            "text": [
                "The new task tackles MTQE for translating English into German.",
                "For this task we have combined two on-line-available MT systems, 1 Lucy LT KWIK Translator 2 and Google Translate, 3 and the bilingual concordancer Reverso Context 4 to spot sub-segment correspondences between a sentence S in the source language (SL) and a given translation hypothesis T in the target language (TL).",
                "As described by #TARGET_REF , a collection of features is obtained from these correspondences and then used by a binary classifier to determine the final word-level MTQE labels.",
                "We have repeated the approach proposed in WMT 2015 for word-level sub-tasks, and have proposed a new one for phrase-level MTQE that builds upon the system trained for word-level MTQE.",
                "The rest of the paper is organised as follows."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The new task tackles MTQE for translating English into German. For this task we have combined two on-line-available MT systems, 1 Lucy LT KWIK Translator 2 and Google Translate, 3 and the bilingual concordancer Reverso Context 4 to spot sub-segment correspondences between a sentence S in the source language (SL) and a given translation hypothesis T in the target language (TL). As described by #TARGET_REF , a collection of features is obtained from these correspondences and then used by a binary classifier to determine the final word-level MTQE labels. We have repeated the approach proposed in WMT 2015 for word-level sub-tasks, and have proposed a new one for phrase-level MTQE that builds upon the system trained for word-level MTQE. The rest of the paper is organised as follows.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"As described by #TARGET_REF , a collection of features is obtained from these correspondences and then used by a binary classifier to determine the final word-level MTQE labels.\"]}"
    },
    {
        "gold": {
            "text": [
                "A multilayer perceptron (#REF , Section 6) was used for classification, as implemented in Weka 3.7 (#REF) .",
                "Following the approach by #TARGET_REF , the perceptron was built with a single hidden layer containing the same number of nodes as the number of features; this was the best performing architecture in the preliminary experiments.",
                "7 The training sets 5 The list of features can be found in the file features list in the package http://www.quest.",
                "dcs.shef.ac.uk/wmt16_files_qe/task2_ en-de_test.tar.gz 6 The list of features can be found in the file features list in the package http://www.quest.",
                "dcs.shef.ac.uk/wmt16_files_qe/task2p_ en-de_test.tar.gz 7 The rest of parameters of the classifiers were also kept as in the approach by Esplà-#REF ."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "A multilayer perceptron (#REF , Section 6) was used for classification, as implemented in Weka 3.7 (#REF) . Following the approach by #TARGET_REF , the perceptron was built with a single hidden layer containing the same number of nodes as the number of features; this was the best performing architecture in the preliminary experiments. 7 The training sets 5 The list of features can be found in the file features list in the package http://www.quest. dcs.shef.ac.uk/wmt16_files_qe/task2_ en-de_test.tar.gz 6 The list of features can be found in the file features list in the package http://www.quest. dcs.shef.ac.uk/wmt16_files_qe/task2p_ en-de_test.tar.gz 7 The rest of parameters of the classifiers were also kept as in the approach by Esplà-#REF .",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"Following the approach by #TARGET_REF , the perceptron was built with a single hidden layer containing the same number of nodes as the number of features; this was the best performing architecture in the preliminary experiments.\"]}"
    },
    {
        "gold": {
            "text": [
                "The approach employed is aimed at being system-independent, since it only uses resources produced by external systems, which makes the addition of new sources of bilingual information straightforward.",
                "In fact, one of the sources of bilingual information used in the previous edition of the shared task, Apertium, has been replaced by a new one: Lucy LT.",
                "The results obtained confirm the conclusion by #TARGET_REF that combining the baseline features with those obtained from external sources of bilingual information provide a noticeable improvement, in this case, not only for word-level MTQE, but also for phrase-level MTQE.",
                "Some future work may be interesting, specially as regards the approach to phrase-level MTQE.",
                "As already mentioned, it would be interesting to use binary classifiers that support sparse features, in order to be able to directly train a single binary classifier capable to deal with phrases of any length."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The approach employed is aimed at being system-independent, since it only uses resources produced by external systems, which makes the addition of new sources of bilingual information straightforward. In fact, one of the sources of bilingual information used in the previous edition of the shared task, Apertium, has been replaced by a new one: Lucy LT. The results obtained confirm the conclusion by #TARGET_REF that combining the baseline features with those obtained from external sources of bilingual information provide a noticeable improvement, in this case, not only for word-level MTQE, but also for phrase-level MTQE. Some future work may be interesting, specially as regards the approach to phrase-level MTQE. As already mentioned, it would be interesting to use binary classifiers that support sparse features, in order to be able to directly train a single binary classifier capable to deal with phrases of any length.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"The results obtained confirm the conclusion by #TARGET_REF that combining the baseline features with those obtained from external sources of bilingual information provide a noticeable improvement, in this case, not only for word-level MTQE, but also for phrase-level MTQE.\"]}"
    },
    {
        "gold": {
            "text": [
                "dcs.shef.ac.uk/wmt16_files_qe/task2_ en-de_test.tar.gz 6 The list of features can be found in the file features list in the package http://www.quest.",
                "dcs.shef.ac.uk/wmt16_files_qe/task2p_ en-de_test.tar.gz 7 The rest of parameters of the classifiers were also kept as in the approach by #TARGET_REF .",
                "provided by the organisation were used to train the binary classifiers, both for word and phrase levels, while the development sets were used as validation sets on which the training error was computed, in order to minimise the risk of overfitting.",
                "The binary classifiers for the sub-task on phrase-level MTQE was trained to optimise the main comparison metric: F BAD 1 ·F OK 1 , while the classifier for word-level MTQE was trained to optimise the F BAD 1 metric, which was the main comparison metric in WMT 2015.",
                "8 Given that the binary classifier used for the phrase-level sub-task depends on the output of the binary classifier for word-level MTQE, the training process was incremental, training first the wordlevel MTQE binary classifiers and then the phraselevel ones."
            ],
            "label": [
                "USE",
                "SIM"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "dcs.shef.ac.uk/wmt16_files_qe/task2_ en-de_test.tar.gz 6 The list of features can be found in the file features list in the package http://www.quest. dcs.shef.ac.uk/wmt16_files_qe/task2p_ en-de_test.tar.gz 7 The rest of parameters of the classifiers were also kept as in the approach by #TARGET_REF . provided by the organisation were used to train the binary classifiers, both for word and phrase levels, while the development sets were used as validation sets on which the training error was computed, in order to minimise the risk of overfitting. The binary classifiers for the sub-task on phrase-level MTQE was trained to optimise the main comparison metric: F BAD 1 ·F OK 1 , while the classifier for word-level MTQE was trained to optimise the F BAD 1 metric, which was the main comparison metric in WMT 2015. 8 Given that the binary classifier used for the phrase-level sub-task depends on the output of the binary classifier for word-level MTQE, the training process was incremental, training first the wordlevel MTQE binary classifiers and then the phraselevel ones.",
        "output": "{\"label\": [\"USE\", \"SIM\"], \"context\": [\"dcs.shef.ac.uk/wmt16_files_qe/task2p_ en-de_test.tar.gz 7 The rest of parameters of the classifiers were also kept as in the approach by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "It is worth mentioning that the binary classifiers for phrase-level MTQE use the probabilities provided by the best performing system for word-level MTQE: the one that combines the features obtained from on-line sources of bilingual information with the baseline features.",
                "However, the phrase-level baseline features are only used in one of the systems submitted.",
                "Table 1 shows the results obtained by the systems submitted to the shared task on MTQE, both at the level of words and at the level of phrases.",
                "The table also includes the results obtained with a binary classifier trained only on the baseline features (baseline), in order to estimate the contribution of the features described in this work on the performance of the system.",
                "Incidentally, and in spite of the changes in languages and machine translation systems, the results obtained for word-level MTQE are very similar to those obtained by #TARGET_REF for the translation from English into Spanish."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "It is worth mentioning that the binary classifiers for phrase-level MTQE use the probabilities provided by the best performing system for word-level MTQE: the one that combines the features obtained from on-line sources of bilingual information with the baseline features. However, the phrase-level baseline features are only used in one of the systems submitted. Table 1 shows the results obtained by the systems submitted to the shared task on MTQE, both at the level of words and at the level of phrases. The table also includes the results obtained with a binary classifier trained only on the baseline features (baseline), in order to estimate the contribution of the features described in this work on the performance of the system. Incidentally, and in spite of the changes in languages and machine translation systems, the results obtained for word-level MTQE are very similar to those obtained by #TARGET_REF for the translation from English into Spanish.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Incidentally, and in spite of the changes in languages and machine translation systems, the results obtained for word-level MTQE are very similar to those obtained by #TARGET_REF for the translation from English into Spanish.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, approaches based on distributional semantics are limited by the grounding problem [7] , which calls for techniques to ground certain conceptual knowledge in perceptual information.",
                "Both NLP and vision communities have proposed various multi-modal learning methods to bridge the gap between language and vision.",
                "However, how general sentence representations can be benefited from visual grounding has not been fully explored yet.",
                "Very recently, #TARGET_REF proposed a multi-modal #TARGET_REF that, given an image caption, jointly predicts another caption and the features of associated image.",
                "The work showed promising results for further improving general sentence representations by grounding them visually."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "However, approaches based on distributional semantics are limited by the grounding problem [7] , which calls for techniques to ground certain conceptual knowledge in perceptual information. Both NLP and vision communities have proposed various multi-modal learning methods to bridge the gap between language and vision. However, how general sentence representations can be benefited from visual grounding has not been fully explored yet. Very recently, #TARGET_REF proposed a multi-modal #TARGET_REF that, given an image caption, jointly predicts another caption and the features of associated image. The work showed promising results for further improving general sentence representations by grounding them visually.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Very recently, #TARGET_REF proposed a multi-modal #TARGET_REF that, given an image caption, jointly predicts another caption and the features of associated image.\"]}"
    },
    {
        "gold": {
            "text": [
                "Image captioning [11] [12] [13] [14] and image synthesis [15] are two common tasks.",
                "There have been significant studies focusing on improving word embeddings [16, 17] , phrase embeddings [18] , sentence embeddings #TARGET_REF, 19] , language models [20] through multi-modal learning of vision and language.",
                "Among all studies, [8] is the first to apply skip-gram-like intuition (predicting multiple modalities from langauge) to joint learning of language and vision in the perspective of general sentence representations.",
                "Attention Mechanism in Multi-Modal Semantics.",
                "Attention mechanism was first introduced in [21] for neural machine translation."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Image captioning [11] [12] [13] [14] and image synthesis [15] are two common tasks. There have been significant studies focusing on improving word embeddings [16, 17] , phrase embeddings [18] , sentence embeddings #TARGET_REF, 19] , language models [20] through multi-modal learning of vision and language. Among all studies, [8] is the first to apply skip-gram-like intuition (predicting multiple modalities from langauge) to joint learning of language and vision in the perspective of general sentence representations. Attention Mechanism in Multi-Modal Semantics. Attention mechanism was first introduced in [21] for neural machine translation.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"There have been significant studies focusing on improving word embeddings [16, 17] , phrase embeddings [18] , sentence embeddings #TARGET_REF, 19] , language models [20] through multi-modal learning of vision and language.\"]}"
    },
    {
        "gold": {
            "text": [
                "We base our model on the #TARGET_REF introduced in #TARGET_REF .",
                "A bidirectional Long Short-Term Memory (LSTM) [24] encodes an input sentence and produces a sentence representation for the input.",
                "A pair of LSTM cells encodes the input sequence in both directions and produce two final hidden states: h t and h t .",
                "The hidden representation of the entire sequence is produced by selecting maximum elements between the two hidden states: h S = max( h t , h t ).",
                "The decoder calculates the probability of a target word y t at each time step t, conditional to the sentence representation h S and all target words before t. P (y t | y <t , h S )."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We base our model on the #TARGET_REF introduced in #TARGET_REF . A bidirectional Long Short-Term Memory (LSTM) [24] encodes an input sentence and produces a sentence representation for the input. A pair of LSTM cells encodes the input sequence in both directions and produce two final hidden states: h t and h t . The hidden representation of the entire sequence is produced by selecting maximum elements between the two hidden states: h S = max( h t , h t ). The decoder calculates the probability of a target word y t at each time step t, conditional to the sentence representation h S and all target words before t. P (y t | y <t , h S ).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We base our model on the #TARGET_REF introduced in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Following the experimental design of #TARGET_REF , we conduct experiments on three different learning objectives: CAP2ALL, CAP2CAP, CAP2IMG.",
                "Under CAP2ALL, the model is trained to predict both the target caption and the associated image: L = L C + L V G .",
                "Under CAP2CAP, the model is trained to predict only the target caption (L = L C ) and, under CAP2IMG, only the associated image (L = L V G )."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "Following the experimental design of #TARGET_REF , we conduct experiments on three different learning objectives: CAP2ALL, CAP2CAP, CAP2IMG. Under CAP2ALL, the model is trained to predict both the target caption and the associated image: L = L C + L V G . Under CAP2CAP, the model is trained to predict only the target caption (L = L C ) and, under CAP2IMG, only the associated image (L = L V G ).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following the experimental design of #TARGET_REF , we conduct experiments on three different learning objectives: CAP2ALL, CAP2CAP, CAP2IMG.\"]}"
    },
    {
        "gold": {
            "text": [
                "We employ orthogonal initialization [30] for recurrent weights and xavier initialization [31] for all others.",
                "For the datasets, we use Karpathy and Fei-Fei's split for MS-COCO dataset [13] .",
                "Image features are prepared by extracting hidden representations at the final layer of ResNet-101 [32] .",
                "We evaluate sentence representation quality using SentEval 2 #TARGET_REF, 10] scripts.",
                "Mini-batch size is 128 and negative samples are prepared from remaining data samples in the same mini-batch."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We employ orthogonal initialization [30] for recurrent weights and xavier initialization [31] for all others. For the datasets, we use Karpathy and Fei-Fei's split for MS-COCO dataset [13] . Image features are prepared by extracting hidden representations at the final layer of ResNet-101 [32] . We evaluate sentence representation quality using SentEval 2 #TARGET_REF, 10] scripts. Mini-batch size is 128 and negative samples are prepared from remaining data samples in the same mini-batch.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We evaluate sentence representation quality using SentEval 2 #TARGET_REF, 10] scripts.\"]}"
    },
    {
        "gold": {
            "text": [
                "Adhering to the experimental settings of #TARGET_REF , we concatenate sentence representations produced from our model with those obtained from the state-of-the-art unsupervised learning model (Layer Normalized Skip-Thoughts, ST-LN) [33] .",
                "We evaluate the quality of sentence representations produced from different variants of our encoders on well-known transfer tasks: movie review sentiment (MR) [34] , customer reviews (CR) [35] , subjectivity (SUBJ) [36] , opinion polarity (MPQA) [37] , paraphrase identification (MSRP) [38] , binary sentiment classification (SST) [39] , SICK entailment and SICK relatedness [40] ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0
            ]
        },
        "input": "Adhering to the experimental settings of #TARGET_REF , we concatenate sentence representations produced from our model with those obtained from the state-of-the-art unsupervised learning model (Layer Normalized Skip-Thoughts, ST-LN) [33] . We evaluate the quality of sentence representations produced from different variants of our encoders on well-known transfer tasks: movie review sentiment (MR) [34] , customer reviews (CR) [35] , subjectivity (SUBJ) [36] , opinion polarity (MPQA) [37] , paraphrase identification (MSRP) [38] , binary sentiment classification (SST) [39] , SICK entailment and SICK relatedness [40] .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Adhering to the experimental settings of #TARGET_REF , we concatenate sentence representations produced from our model with those obtained from the state-of-the-art unsupervised learning model (Layer Normalized Skip-Thoughts, ST-LN) [33] .\"]}"
    },
    {
        "gold": {
            "text": [
                "Given the source caption representation h S and the relevant image representation h I , we associate the two representations by projecting h S into image feature space.",
                "We train the model to rank the similarity between predicted image featuresh I and the target image features h I higher than other pairs, which is achieved by ranking loss functions.",
                "Although margin ranking loss has been the dominant choice for training cross-modal feature matching #TARGET_REF, 20, 25] , we find that log-exp-sum pairwise ranking [26] yields better results in terms of evaluation performance and efficiency.",
                "Thus, the objective for ranking",
                "where N is the set of negative examples and sim is cosine similarity."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Given the source caption representation h S and the relevant image representation h I , we associate the two representations by projecting h S into image feature space. We train the model to rank the similarity between predicted image featuresh I and the target image features h I higher than other pairs, which is achieved by ranking loss functions. Although margin ranking loss has been the dominant choice for training cross-modal feature matching #TARGET_REF, 20, 25] , we find that log-exp-sum pairwise ranking [26] yields better results in terms of evaluation performance and efficiency. Thus, the objective for ranking where N is the set of negative examples and sim is cosine similarity.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Although margin ranking loss has been the dominant choice for training cross-modal feature matching #TARGET_REF, 20, 25] , we find that log-exp-sum pairwise ranking [26] yields better results in terms of evaluation performance and efficiency.\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to study the effects of incorporating self-attention mechanism in joint prediction of image and language features, we examine attention vectors for selected samples from MS-COCO dataset and compare them to associated images ( Figure 1 ).",
                "For example, given the sentence \"man in black shirt is playing guitar\", our model identifies words that have association with strong visual imagery, such as \"man\", \"black\" and \"guitar\".",
                "Given the second sentence, our model learned to attend to visually significant words such as \"cat\" and \"bowl\".",
                "These findings show that visually grounding self-attended sentence representations helps to expose word-level visual features onto sentence representations #TARGET_REF .",
                "Figure 1: Activated attention weights on two samples from MS-COCO dataset."
            ],
            "label": [
                "DIFFER",
                "EXT"
            ],
            "context": [
                1,
                0,
                0,
                1,
                0
            ]
        },
        "input": "In order to study the effects of incorporating self-attention mechanism in joint prediction of image and language features, we examine attention vectors for selected samples from MS-COCO dataset and compare them to associated images ( Figure 1 ). For example, given the sentence \"man in black shirt is playing guitar\", our model identifies words that have association with strong visual imagery, such as \"man\", \"black\" and \"guitar\". Given the second sentence, our model learned to attend to visually significant words such as \"cat\" and \"bowl\". These findings show that visually grounding self-attended sentence representations helps to expose word-level visual features onto sentence representations #TARGET_REF . Figure 1: Activated attention weights on two samples from MS-COCO dataset.",
        "output": "{\"label\": [\"DIFFER\", \"EXT\"], \"context\": [\"In order to study the effects of incorporating self-attention mechanism in joint prediction of image and language features, we examine attention vectors for selected samples from MS-COCO dataset and compare them to associated images ( Figure 1 ).\", \"These findings show that visually grounding self-attended sentence representations helps to expose word-level visual features onto sentence representations #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "This paper proposes dynamic chunk reader (DCR), an end-toend neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions.",
                "DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities.",
                "DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer.",
                "Experimental results show that DCR achieves stateof-the-art exact match and F1 scores on the SQuAD dataset #TARGET_REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1
            ]
        },
        "input": "This paper proposes dynamic chunk reader (DCR), an end-toend neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer. Experimental results show that DCR achieves stateof-the-art exact match and F1 scores on the SQuAD dataset #TARGET_REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Experimental results show that DCR achieves stateof-the-art exact match and F1 scores on the SQuAD dataset #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The contributions of this paper are three-fold.",
                "(1) We pro- We also propose several simple but effective features to strengthen the attention mechanism, which fundamentally improves candidate ranking, with the by-product of higher exact boundary match accuracy.",
                "The experiments on the Stanford Question Answering Dataset (SQuAD) #TARGET_REF , which contains a variety of human-generated factoid and non-factoid questions, have shown the effectiveness of above three contributions.",
                "Our paper is organized as follows.",
                "We formally define the RCQA problem first."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The contributions of this paper are three-fold. (1) We pro- We also propose several simple but effective features to strengthen the attention mechanism, which fundamentally improves candidate ranking, with the by-product of higher exact boundary match accuracy. The experiments on the Stanford Question Answering Dataset (SQuAD) #TARGET_REF , which contains a variety of human-generated factoid and non-factoid questions, have shown the effectiveness of above three contributions. Our paper is organized as follows. We formally define the RCQA problem first.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The experiments on the Stanford Question Answering Dataset (SQuAD) #TARGET_REF , which contains a variety of human-generated factoid and non-factoid questions, have shown the effectiveness of above three contributions.\"]}"
    },
    {
        "gold": {
            "text": [
                "Remark: Categories of RC Tasks Other simpler variants of the aforementioned RC task were explored in the past.",
                "For example, quiz-style datasets (e.g., MCTest (Richardson, Burges, and #REF), MovieQA (#REF) ) have multiple-choice questions with answer options.",
                "Cloze-style datesets #REF; #REF) , usually automatically generated, have factoid \"question\"s created by replacing the answer in a sentence from the text with blank.",
                "For the answer selection task this paper focuses on, several datasets exist, e.g. TREC-QA for factoid answer extraction from multiple given passages, bAbI (Weston, Chopra, and #REF) designed for inference purpose, and the SQuAD dataset #TARGET_REF used in this paper.",
                "To the best of our knowledge, the SQuAD dataset is the only one for both factoid and nonfactoid answer extraction with a question distribution more close to real-world applications."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Remark: Categories of RC Tasks Other simpler variants of the aforementioned RC task were explored in the past. For example, quiz-style datasets (e.g., MCTest (Richardson, Burges, and #REF), MovieQA (#REF) ) have multiple-choice questions with answer options. Cloze-style datesets #REF; #REF) , usually automatically generated, have factoid \"question\"s created by replacing the answer in a sentence from the text with blank. For the answer selection task this paper focuses on, several datasets exist, e.g. TREC-QA for factoid answer extraction from multiple given passages, bAbI (Weston, Chopra, and #REF) designed for inference purpose, and the SQuAD dataset #TARGET_REF used in this paper. To the best of our knowledge, the SQuAD dataset is the only one for both factoid and nonfactoid answer extraction with a question distribution more close to real-world applications.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"For the answer selection task this paper focuses on, several datasets exist, e.g. TREC-QA for factoid answer extraction from multiple given passages, bAbI (Weston, Chopra, and #REF) designed for inference purpose, and the SQuAD dataset #TARGET_REF used in this paper.\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to make the cloze-style RC system to make chunk-level decision, we use the RC model to generate features for chunks, which are further used in a feature-based ranker like in (#REF) .",
                "As a result, this baseline can be viewed as a deep learning based counterpart of the system in (#REF ).",
                "It has two main components: 1) a standalone answer chunker, which is trained to produce overlapping candidate chunks, and 2) a neural RC model, which is used to score each word in a given passage to be used thereafter for generating chunk scores.",
                "Answer Chunking To reduce the errors generated by the rule-based chunker in #TARGET_REF , first, we capture the part-of-speech (POS) pattern of all answer subsequences in the training dataset to form a POS pattern trie tree, and then apply the answer POS patterns to passage P i to acquire a collection of all subsequences (chunk candidates) C i whose POS patterns can be matched to the POS pattern trie.",
                "This is equivalent to putting an constraint subj(m, n, P i ) to candidate answer chunk generation process that only choose the chunk with a POS pattern seen for answers in the training data."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "In order to make the cloze-style RC system to make chunk-level decision, we use the RC model to generate features for chunks, which are further used in a feature-based ranker like in (#REF) . As a result, this baseline can be viewed as a deep learning based counterpart of the system in (#REF ). It has two main components: 1) a standalone answer chunker, which is trained to produce overlapping candidate chunks, and 2) a neural RC model, which is used to score each word in a given passage to be used thereafter for generating chunk scores. Answer Chunking To reduce the errors generated by the rule-based chunker in #TARGET_REF , first, we capture the part-of-speech (POS) pattern of all answer subsequences in the training dataset to form a POS pattern trie tree, and then apply the answer POS patterns to passage P i to acquire a collection of all subsequences (chunk candidates) C i whose POS patterns can be matched to the POS pattern trie. This is equivalent to putting an constraint subj(m, n, P i ) to candidate answer chunk generation process that only choose the chunk with a POS pattern seen for answers in the training data.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Answer Chunking To reduce the errors generated by the rule-based chunker in #TARGET_REF , first, we capture the part-of-speech (POS) pattern of all answer subsequences in the training dataset to form a POS pattern trie tree, and then apply the answer POS patterns to passage P i to acquire a collection of all subsequences (chunk candidates) C i whose POS patterns can be matched to the POS pattern trie.\"]}"
    },
    {
        "gold": {
            "text": [
                "Dataset We used the Stanford Question Answering Dataset (SQuAD) #TARGET_REF for the experiment.",
                "SQuAD came into our sight because it is a mix of factoid and non-factoid questions, a real-world data (crowd-sourced), and of large scale (over 100K question-answer pairs collected from 536 Wikipedia articles).",
                "Answers range from single words to long, variable-length phrase/clauses.",
                "It is a relaxation of assumptions by the cloze-style and quiz-style RC datasets in the Problem Definition section.",
                "Features The input vector representation of each word w to encoder RNNs has six parts including a pre-trained 300-dimensional GloVe embedding (Pennington, Socher, and and five features (see Figure 1) : (1) a onehot encoding (46 dimensions) for the part-of-speech (POS) tag of w; (2) a one-hot encoding (14 dimensions) for named entity (NE) tag of w; (3) a binary value indicating whether w's surface form is the same to any word in the quesiton; (4) if the lemma form of w is the same to any word in the question; and (5) if w is caplitalized."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "Dataset We used the Stanford Question Answering Dataset (SQuAD) #TARGET_REF for the experiment. SQuAD came into our sight because it is a mix of factoid and non-factoid questions, a real-world data (crowd-sourced), and of large scale (over 100K question-answer pairs collected from 536 Wikipedia articles). Answers range from single words to long, variable-length phrase/clauses. It is a relaxation of assumptions by the cloze-style and quiz-style RC datasets in the Problem Definition section. Features The input vector representation of each word w to encoder RNNs has six parts including a pre-trained 300-dimensional GloVe embedding (Pennington, Socher, and and five features (see Figure 1) : (1) a onehot encoding (46 dimensions) for the part-of-speech (POS) tag of w; (2) a one-hot encoding (14 dimensions) for named entity (NE) tag of w; (3) a binary value indicating whether w's surface form is the same to any word in the quesiton; (4) if the lemma form of w is the same to any word in the question; and (5) if w is caplitalized.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Dataset We used the Stanford Question Answering Dataset (SQuAD) #TARGET_REF for the experiment.\"]}"
    },
    {
        "gold": {
            "text": [
                "This is because RCQA can exploit the textual evidences to ensure increased answer coverage, which is particularly helpful for non-factoid answers.",
                "However, it is also challenging for RCQA to identify answer in arbitrary position in the passage with arbitrary length, especially for nonfactoid answers which might be clauses or sentences.",
                "As a result, apart from a few exceptions #TARGET_REF; #REF) , this research direction has not been fully explored yet.",
                "Compared to the relatively easier RC task of predicting single tokens/entities 1 , predicting answers of arbitrary lengths and positions significantly increase the search space complexity: the number of possible candidates to consider is in the order of O(n 2 ), where n is the number of passage words.",
                "In contrast, for previous works in which answers are single tokens/entities or from candidate lists, the complexity is in O(n) or the size of candidate lists l (usually l ≤5), respectively."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "This is because RCQA can exploit the textual evidences to ensure increased answer coverage, which is particularly helpful for non-factoid answers. However, it is also challenging for RCQA to identify answer in arbitrary position in the passage with arbitrary length, especially for nonfactoid answers which might be clauses or sentences. As a result, apart from a few exceptions #TARGET_REF; #REF) , this research direction has not been fully explored yet. Compared to the relatively easier RC task of predicting single tokens/entities 1 , predicting answers of arbitrary lengths and positions significantly increase the search space complexity: the number of possible candidates to consider is in the order of O(n 2 ), where n is the number of passage words. In contrast, for previous works in which answers are single tokens/entities or from candidate lists, the complexity is in O(n) or the size of candidate lists l (usually l ≤5), respectively.",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"However, it is also challenging for RCQA to identify answer in arbitrary position in the passage with arbitrary length, especially for nonfactoid answers which might be clauses or sentences.\", \"As a result, apart from a few exceptions #TARGET_REF; #REF) , this research direction has not been fully explored yet.\"]}"
    },
    {
        "gold": {
            "text": [
                "The rule-based chunking approach suffered from low coverage (≈ 70% recall of answer chunks) that cannot be improved during training; and candidate ranking performance depends greatly on the quality of the hand-crafted features.",
                "More recently, #REF proposed two endto-end neural network models, one of which chunks a candidate answer by predicting the answer's two boundary indices and the other classifies each passage word into answer/notanswer.",
                "Both models improved significantly over the method proposed by #TARGET_REF .",
                "Our proposed model, called dynamic chunk reader (DCR), not only significantly differs from both the above systems in the way that answer candidates are generated and ranked, but also shares merits with both works.",
                "First, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in (#REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "The rule-based chunking approach suffered from low coverage (≈ 70% recall of answer chunks) that cannot be improved during training; and candidate ranking performance depends greatly on the quality of the hand-crafted features. More recently, #REF proposed two endto-end neural network models, one of which chunks a candidate answer by predicting the answer's two boundary indices and the other classifies each passage word into answer/notanswer. Both models improved significantly over the method proposed by #TARGET_REF . Our proposed model, called dynamic chunk reader (DCR), not only significantly differs from both the above systems in the way that answer candidates are generated and ranked, but also shares merits with both works. First, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in (#REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"More recently, #REF proposed two endto-end neural network models, one of which chunks a candidate answer by predicting the answer's two boundary indices and the other classifies each passage word into answer/notanswer.\", \"Both models improved significantly over the method proposed by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "More recently, #REF proposed two endto-end neural network models, one of which chunks a candidate answer by predicting the answer's two boundary indices and the other classifies each passage word into answer/notanswer.",
                "Both models improved significantly over the method proposed by #REF .",
                "Our proposed model, called dynamic chunk reader (DCR), not only significantly differs from both the above systems in the way that answer candidates are generated and ranked, but also shares merits with both works.",
                "First, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in #TARGET_REF .",
                "Second, it represents answer candidates as chunks, as in (#REF ), instead of word-level representations (#REF) , to make the model aware of the subtle differences among candidates (importantly, overlapping candidates)."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "More recently, #REF proposed two endto-end neural network models, one of which chunks a candidate answer by predicting the answer's two boundary indices and the other classifies each passage word into answer/notanswer. Both models improved significantly over the method proposed by #REF . Our proposed model, called dynamic chunk reader (DCR), not only significantly differs from both the above systems in the way that answer candidates are generated and ranked, but also shares merits with both works. First, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in #TARGET_REF . Second, it represents answer candidates as chunks, as in (#REF ), instead of word-level representations (#REF) , to make the model aware of the subtle differences among candidates (importantly, overlapping candidates).",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"First, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 3 shows the details as well as the results of the baseline ranker.",
                "As the first row of Table 3 shows, our baseline system improves 10% (EM) over #TARGET_REF (Table 2 , row 1), the feature-based ranking system.",
                "However when compared to our DCR model (Table 3 , row 2), the baseline (row 1) is more than 12% (EM) behind even though it is based on the state-of-the-art model for cloze-style RC tasks.",
                "This can be attributed to the advanced model structure and end-to-end manner of DCR.",
                "We also did ablation tests on our DCR model."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Table 3 shows the details as well as the results of the baseline ranker. As the first row of Table 3 shows, our baseline system improves 10% (EM) over #TARGET_REF (Table 2 , row 1), the feature-based ranking system. However when compared to our DCR model (Table 3 , row 2), the baseline (row 1) is more than 12% (EM) behind even though it is based on the state-of-the-art model for cloze-style RC tasks. This can be attributed to the advanced model structure and end-to-end manner of DCR. We also did ablation tests on our DCR model.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"As the first row of Table 3 shows, our baseline system improves 10% (EM) over #TARGET_REF (Table 2 , row 1), the feature-based ranking system.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, none of these have been explored in previous studies.",
                "The second aspect is that on which most variations between existing approaches occur.",
                "Multiple Deep Neural Network (DNN) architectures have been explored to combine the token representations into a single segment representation that captures relevant information for the task.",
                "Of the two state-of-the-art approaches on dialog act recognition, one uses a deep stack of Recurrent Neural Networks (RNNs) (#REF) to capture long distance relations between tokens (#REF) , while the other uses multiple parallel temporal Convolutional Neural Networks (CNNs) (#REF) to capture relevant functional patterns with different length #TARGET_REF .",
                "Although these approaches focus on capturing different information, both have been proved successful on the task."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "However, none of these have been explored in previous studies. The second aspect is that on which most variations between existing approaches occur. Multiple Deep Neural Network (DNN) architectures have been explored to combine the token representations into a single segment representation that captures relevant information for the task. Of the two state-of-the-art approaches on dialog act recognition, one uses a deep stack of Recurrent Neural Networks (RNNs) (#REF) to capture long distance relations between tokens (#REF) , while the other uses multiple parallel temporal Convolutional Neural Networks (CNNs) (#REF) to capture relevant functional patterns with different length #TARGET_REF . Although these approaches focus on capturing different information, both have been proved successful on the task.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Of the two state-of-the-art approaches on dialog act recognition, one uses a deep stack of Recurrent Neural Networks (RNNs) (#REF) to capture long distance relations between tokens (#REF) , while the other uses multiple parallel temporal Convolutional Neural Networks (CNNs) (#REF) to capture relevant functional patterns with different length #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The latter are hard to capture and are usually not available for a dialog system.",
                "Thus, only speaker information that is directly related to the dialog, such as turn-taking #TARGET_REF , is typically considered.",
                "Concerning information from the surrounding segments, its influence, especially that of preceding segments, has been thoroughly explored in at least two studies (#REF; #REF) .",
                "However, in both cases, although it is one of its most important characteristics, that information was represented in ways that are not appropriate to capture its sequentiality.",
                "In this article we use the Switchboard Dialog Act Corpus (#REF) , which is the most explored corpus for dialog act recognition, to study and compare different solutions concerning the three aspects."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "The latter are hard to capture and are usually not available for a dialog system. Thus, only speaker information that is directly related to the dialog, such as turn-taking #TARGET_REF , is typically considered. Concerning information from the surrounding segments, its influence, especially that of preceding segments, has been thoroughly explored in at least two studies (#REF; #REF) . However, in both cases, although it is one of its most important characteristics, that information was represented in ways that are not appropriate to capture its sequentiality. In this article we use the Switchboard Dialog Act Corpus (#REF) , which is the most explored corpus for dialog act recognition, to study and compare different solutions concerning the three aspects.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Thus, only speaker information that is directly related to the dialog, such as turn-taking #TARGET_REF , is typically considered.\"]}"
    },
    {
        "gold": {
            "text": [
                "The latter are hard to capture and are usually not available for a dialog system.",
                "Thus, only speaker information that is directly related to the dialog, such as turn-taking (#REF) , is typically considered.",
                "Concerning information from the surrounding segments, its influence, especially that of preceding segments, has been thoroughly explored in at least two studies (#REF; #TARGET_REF .",
                "However, in both cases, although it is one of its most important characteristics, that information was represented in ways that are not appropriate to capture its sequentiality.",
                "In this article we use the Switchboard Dialog Act Corpus (#REF) , which is the most explored corpus for dialog act recognition, to study and compare different solutions concerning the three aspects."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "The latter are hard to capture and are usually not available for a dialog system. Thus, only speaker information that is directly related to the dialog, such as turn-taking (#REF) , is typically considered. Concerning information from the surrounding segments, its influence, especially that of preceding segments, has been thoroughly explored in at least two studies (#REF; #TARGET_REF . However, in both cases, although it is one of its most important characteristics, that information was represented in ways that are not appropriate to capture its sequentiality. In this article we use the Switchboard Dialog Act Corpus (#REF) , which is the most explored corpus for dialog act recognition, to study and compare different solutions concerning the three aspects.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Concerning information from the surrounding segments, its influence, especially that of preceding segments, has been thoroughly explored in at least two studies (#REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "On the recurrent side, #REF achieved their best results using a segment representation generated by concatenating the outputs of a stack of 10 LSTM units at the last time step.",
                "This way, the model is able to capture long distance relations between tokens.",
                "On the convolutional side, #TARGET_REF generated the segment representation by combining the outputs of three parallel CNNs with different context window sizes, in order to capture different functional patterns.",
                "In both cases, pre-trained word-embeddings were used as input to the network.",
                "#REF compared the performance of embeddings with different dimensionality trained on multiple corpora using GloVe and Word2Vec (#REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "On the recurrent side, #REF achieved their best results using a segment representation generated by concatenating the outputs of a stack of 10 LSTM units at the last time step. This way, the model is able to capture long distance relations between tokens. On the convolutional side, #TARGET_REF generated the segment representation by combining the outputs of three parallel CNNs with different context window sizes, in order to capture different functional patterns. In both cases, pre-trained word-embeddings were used as input to the network. #REF compared the performance of embeddings with different dimensionality trained on multiple corpora using GloVe and Word2Vec (#REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"On the convolutional side, #TARGET_REF generated the segment representation by combining the outputs of three parallel CNNs with different context window sizes, in order to capture different functional patterns.\"]}"
    },
    {
        "gold": {
            "text": [
                "#TARGET_REF used 200-dimensional Word2Vec embeddings trained on Facebook data.",
                "Overall, from the reported results, it is not possible to state which is the top performing segment representation approach since the evaluation was performed on different subsets of the corpus.",
                "Still, #REF reported 73.9% accuracy on the validation set and 80.1% on the test set, while #REF reported 74.5% and 76.9% accuracy on the two sets used to evaluate their experiments.",
                "Additionally, #REF explored the use of context information concerning speaker changes and from the surrounding segments.",
                "The first was provided as a flag and concatenated to the segment representation."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "#TARGET_REF used 200-dimensional Word2Vec embeddings trained on Facebook data. Overall, from the reported results, it is not possible to state which is the top performing segment representation approach since the evaluation was performed on different subsets of the corpus. Still, #REF reported 73.9% accuracy on the validation set and 80.1% on the test set, while #REF reported 74.5% and 76.9% accuracy on the two sets used to evaluate their experiments. Additionally, #REF explored the use of context information concerning speaker changes and from the surrounding segments. The first was provided as a flag and concatenated to the segment representation.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF used 200-dimensional Word2Vec embeddings trained on Facebook data.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF compared the performance of embeddings with different dimensionality trained on multiple corpora using GloVe and Word2Vec (#REF) .",
                "The best results were achieved when using 150-dimensional embeddings trained on Wikipedia data using Word2Vec.",
                "#REF used 200-dimensional Word2Vec embeddings trained on Facebook data.",
                "Overall, from the reported results, it is not possible to state which is the top performing segment representation approach since the evaluation was performed on different subsets of the corpus.",
                "Still, #REF reported 73.9% accuracy on the validation set and 80.1% on the test set, while #TARGET_REF reported 74.5% and 76.9% accuracy on the two sets used to evaluate #TARGET_REF."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "#REF compared the performance of embeddings with different dimensionality trained on multiple corpora using GloVe and Word2Vec (#REF) . The best results were achieved when using 150-dimensional embeddings trained on Wikipedia data using Word2Vec. #REF used 200-dimensional Word2Vec embeddings trained on Facebook data. Overall, from the reported results, it is not possible to state which is the top performing segment representation approach since the evaluation was performed on different subsets of the corpus. Still, #REF reported 73.9% accuracy on the validation set and 80.1% on the test set, while #TARGET_REF reported 74.5% and 76.9% accuracy on the two sets used to evaluate #TARGET_REF.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Still, #REF reported 73.9% accuracy on the validation set and 80.1% on the test set, while #TARGET_REF reported 74.5% and 76.9% accuracy on the two sets used to evaluate #TARGET_REF.\"]}"
    },
    {
        "gold": {
            "text": [
                "Still, #REF reported 73.9% accuracy on the validation set and 80.1% on the test set, while #REF reported 74.5% and 76.9% accuracy on the two sets used to evaluate their experiments.",
                "Additionally, #TARGET_REF explored the use of context information concerning speaker changes and from the surrounding segments.",
                "The first was provided as a flag and concatenated to the segment representation.",
                "Concerning the latter, they explored the use of discourse models, as well as of approaches that concatenated the context information directly to the segment representation.",
                "The discourse models transform the model into a hierarchical one by generating a sequence of dialog act classifications from the sequence of segment representations."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Still, #REF reported 73.9% accuracy on the validation set and 80.1% on the test set, while #REF reported 74.5% and 76.9% accuracy on the two sets used to evaluate their experiments. Additionally, #TARGET_REF explored the use of context information concerning speaker changes and from the surrounding segments. The first was provided as a flag and concatenated to the segment representation. Concerning the latter, they explored the use of discourse models, as well as of approaches that concatenated the context information directly to the segment representation. The discourse models transform the model into a hierarchical one by generating a sequence of dialog act classifications from the sequence of segment representations.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Additionally, #TARGET_REF explored the use of context information concerning speaker changes and from the surrounding segments.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, when the discourse model is based on a CNN or a bidirectional LSTM unit, it considers information from future segments, which is not available for a dialog system.",
                "Still, even when relying on future information, the approaches based on discourse models performed worse than those that concatenated the context information directly to the segment representation.",
                "In this sense, similarly to our previous study using SVMs (#REF) , #TARGET_REF concluded that providing that information in the form of the classification of the surrounding segments leads to better results than using #TARGET_REF, even when those classifications are obtained automatically.",
                "Furthermore, both studies have shown that the first preceding segment is the most important and that the influence decays with the distance.",
                "Using the setup with gold standard labels from three preceding segments, #REF achieved 79.6% and 81.8% on the two sets used to evaluate the approach."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "However, when the discourse model is based on a CNN or a bidirectional LSTM unit, it considers information from future segments, which is not available for a dialog system. Still, even when relying on future information, the approaches based on discourse models performed worse than those that concatenated the context information directly to the segment representation. In this sense, similarly to our previous study using SVMs (#REF) , #TARGET_REF concluded that providing that information in the form of the classification of the surrounding segments leads to better results than using #TARGET_REF, even when those classifications are obtained automatically. Furthermore, both studies have shown that the first preceding segment is the most important and that the influence decays with the distance. Using the setup with gold standard labels from three preceding segments, #REF achieved 79.6% and 81.8% on the two sets used to evaluate the approach.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In this sense, similarly to our previous study using SVMs (#REF) , #TARGET_REF concluded that providing that information in the form of the classification of the surrounding segments leads to better results than using #TARGET_REF, even when those classifications are obtained automatically.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, when the discourse model is based on a CNN or a bidirectional LSTM unit, it considers information from future segments, which is not available for a dialog system.",
                "Still, even when relying on future information, the approaches based on discourse models performed worse than those that concatenated the context information directly to the segment representation.",
                "In this sense, similarly to our previous study using SVMs (#REF) , #REF concluded that providing that information in the form of the classification of the surrounding segments leads to better results than using their words, even when those classifications are obtained automatically.",
                "Furthermore, both studies have shown that the first preceding segment is the most important and that the influence decays with the distance.",
                "Using the setup with gold standard labels from three preceding segments, #TARGET_REF achieved 79.6% and 81.8% on the two sets used to evaluate the approach."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "However, when the discourse model is based on a CNN or a bidirectional LSTM unit, it considers information from future segments, which is not available for a dialog system. Still, even when relying on future information, the approaches based on discourse models performed worse than those that concatenated the context information directly to the segment representation. In this sense, similarly to our previous study using SVMs (#REF) , #REF concluded that providing that information in the form of the classification of the surrounding segments leads to better results than using their words, even when those classifications are obtained automatically. Furthermore, both studies have shown that the first preceding segment is the most important and that the influence decays with the distance. Using the setup with gold standard labels from three preceding segments, #TARGET_REF achieved 79.6% and 81.8% on the two sets used to evaluate the approach.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Using the setup with gold standard labels from three preceding segments, #TARGET_REF achieved 79.6% and 81.8% on the two sets used to evaluate the approach.\"]}"
    },
    {
        "gold": {
            "text": [
                "As stated in the previous section, the segment representation step is the one which introduces higher variability in the network.",
                "Consequently, it is where the main differences between previous dialog act recognition approaches occur.",
                "Thus, we start our study by exploring different approaches for this step.",
                "As stated in Section 3, of the two state-of-the-art approaches on dialog act recognition, one uses a RNN-based approach (#REF) for segment representation, while the other uses one based on CNNs #TARGET_REF .",
                "Both have their own advantages, as while the first focuses on capturing information from relevant sequences of tokens, the latter focuses on the context surrounding each token and, thus, captures information concerning neighboring tokens."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "As stated in the previous section, the segment representation step is the one which introduces higher variability in the network. Consequently, it is where the main differences between previous dialog act recognition approaches occur. Thus, we start our study by exploring different approaches for this step. As stated in Section 3, of the two state-of-the-art approaches on dialog act recognition, one uses a RNN-based approach (#REF) for segment representation, while the other uses one based on CNNs #TARGET_REF . Both have their own advantages, as while the first focuses on capturing information from relevant sequences of tokens, the latter focuses on the context surrounding each token and, thus, captures information concerning neighboring tokens.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"As stated in Section 3, of the two state-of-the-art approaches on dialog act recognition, one uses a RNN-based approach (#REF) for segment representation, while the other uses one based on CNNs #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "As described in Section 3, the convolutional approach by #TARGET_REF uses a set of parallel temporal CNNs with different window size, each followed by a max pooling operation.",
                "The segment representation is given by the concatenation of the results of the pooling operations.",
                "This way, the representation contains information concerning groups of tokens with different sizes.",
                "To achieve the results presented in their paper, #REF used three CNNs with 100 filters and 1, 2, and 3 as context window sizes.",
                "In a previous study using the same architecture for different tasks, #REF used 3, 4, and 5 as window sizes."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "As described in Section 3, the convolutional approach by #TARGET_REF uses a set of parallel temporal CNNs with different window size, each followed by a max pooling operation. The segment representation is given by the concatenation of the results of the pooling operations. This way, the representation contains information concerning groups of tokens with different sizes. To achieve the results presented in their paper, #REF used three CNNs with 100 filters and 1, 2, and 3 as context window sizes. In a previous study using the same architecture for different tasks, #REF used 3, 4, and 5 as window sizes.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"As described in Section 3, the convolutional approach by #TARGET_REF uses a set of parallel temporal CNNs with different window size, each followed by a max pooling operation.\"]}"
    },
    {
        "gold": {
            "text": [
                "This way, the representation contains information concerning groups of tokens with different sizes.",
                "To achieve the results presented in #TARGET_REF, #TARGET_REF used three CNNs with 100 filters and 1, 2, and 3 as context window sizes.",
                "In a previous study using the same architecture for different tasks, #REF used 3, 4, and 5 as window sizes.",
                "Both setups performed similarly in our experiments.",
                "However, their combination, that is, five parallel CNNs with window sizes between 1 and 5, led to better results, which means that both small and large groups of tokens provide relevant information."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "This way, the representation contains information concerning groups of tokens with different sizes. To achieve the results presented in #TARGET_REF, #TARGET_REF used three CNNs with 100 filters and 1, 2, and 3 as context window sizes. In a previous study using the same architecture for different tasks, #REF used 3, 4, and 5 as window sizes. Both setups performed similarly in our experiments. However, their combination, that is, five parallel CNNs with window sizes between 1 and 5, led to better results, which means that both small and large groups of tokens provide relevant information.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"To achieve the results presented in #TARGET_REF, #TARGET_REF used three CNNs with 100 filters and 1, 2, and 3 as context window sizes.\"]}"
    },
    {
        "gold": {
            "text": [
                "However, up to a certain level, ambiguity is not necessarily harmful.",
                "As stated in Section 3, #REF explored embedding spaces with dimensionality 75, 150, and 300 together with different embedding approaches.",
                "In every case, the embedding space with dimensionality 150 led to the best results.",
                "#TARGET_REF used a different dimensionality value, 200, in #TARGET_REF.",
                "In our experiments we explored the use of the four different dimensionality values."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "However, up to a certain level, ambiguity is not necessarily harmful. As stated in Section 3, #REF explored embedding spaces with dimensionality 75, 150, and 300 together with different embedding approaches. In every case, the embedding space with dimensionality 150 led to the best results. #TARGET_REF used a different dimensionality value, 200, in #TARGET_REF. In our experiments we explored the use of the four different dimensionality values.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"As stated in Section 3, #REF explored embedding spaces with dimensionality 75, 150, and 300 together with different embedding approaches.\", \"In every case, the embedding space with dimensionality 150 led to the best results.\", \"#TARGET_REF used a different dimensionality value, 200, in #TARGET_REF.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF used pre-trained embeddings using both approaches in their study and achieved their best results using Word2Vec embeddings trained on Wikipedia data.",
                "#TARGET_REF also used Word2Vec embeddings, but trained on Facebook data.",
                "Since we have access to the embeddings trained on Wikipedia data, but not to those trained on Facebook data, we used the first in our experiments.",
                "The CBOW model generates word representations based on the co-occurrence of adjacent words.",
                "However, as previously stated, many dialog acts are related to the structure of the segment and not sequences of specific words."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "#REF used pre-trained embeddings using both approaches in their study and achieved their best results using Word2Vec embeddings trained on Wikipedia data. #TARGET_REF also used Word2Vec embeddings, but trained on Facebook data. Since we have access to the embeddings trained on Wikipedia data, but not to those trained on Facebook data, we used the first in our experiments. The CBOW model generates word representations based on the co-occurrence of adjacent words. However, as previously stated, many dialog acts are related to the structure of the segment and not sequences of specific words.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#REF used pre-trained embeddings using both approaches in their study and achieved their best results using Word2Vec embeddings trained on Wikipedia data.\", \"#TARGET_REF also used Word2Vec embeddings, but trained on Facebook data.\"]}"
    },
    {
        "gold": {
            "text": [
                "Although a dialog act represents the intention behind a set of words, that intention is not constrained to a specific segment and its context provides relevant cues.",
                "As described in Section 3, previous studies have shown that the most important source of context information Table 8 : Accuracy results using POS tags.",
                "for dialog act recognition is the dialog history, with influence decaying with distance (#REF; Lee & #REF; #TARGET_REF .",
                "However, information concerning the speakers and, more specifically, turn-taking has also been proved important (#REF) .",
                "Thus, in our study, we explore both the surrounding segments and speaker information as sources of context information."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Although a dialog act represents the intention behind a set of words, that intention is not constrained to a specific segment and its context provides relevant cues. As described in Section 3, previous studies have shown that the most important source of context information Table 8 : Accuracy results using POS tags. for dialog act recognition is the dialog history, with influence decaying with distance (#REF; Lee & #REF; #TARGET_REF . However, information concerning the speakers and, more specifically, turn-taking has also been proved important (#REF) . Thus, in our study, we explore both the surrounding segments and speaker information as sources of context information.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"for dialog act recognition is the dialog history, with influence decaying with distance (#REF; Lee & #REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Although a dialog act represents the intention behind a set of words, that intention is not constrained to a specific segment and its context provides relevant cues.",
                "As described in Section 3, previous studies have shown that the most important source of context information Table 8 : Accuracy results using POS tags.",
                "for dialog act recognition is the dialog history, with influence decaying with distance (#REF; Lee & #REF; #REF) .",
                "However, information concerning the speakers and, more specifically, turn-taking has also been proved important #TARGET_REF .",
                "Thus, in our study, we explore both the surrounding segments and speaker information as sources of context information."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Although a dialog act represents the intention behind a set of words, that intention is not constrained to a specific segment and its context provides relevant cues. As described in Section 3, previous studies have shown that the most important source of context information Table 8 : Accuracy results using POS tags. for dialog act recognition is the dialog history, with influence decaying with distance (#REF; Lee & #REF; #REF) . However, information concerning the speakers and, more specifically, turn-taking has also been proved important #TARGET_REF . Thus, in our study, we explore both the surrounding segments and speaker information as sources of context information.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"However, information concerning the speakers and, more specifically, turn-taking has also been proved important #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "As stated in Section 3, considering the preceding segments, we have shown in a previous study (#REF) that providing information in the form of segment classifications leads to better results than in the form of words.",
                "#TARGET_REF further showed that using a single label per segment is better than using the probability of each class.",
                "Furthermore, both studies showed that using automatic predictions leads to a decrease in performance around 2 percentage points in comparison to using the manual annotations.",
                "Thus, in order to simplify the experiments and obtain an upper bound for the approach, in this study we just use the manual annotations.",
                "In our previous study, we have used up to five preceding segments and showed that the gain becomes smaller as the number of preceding segments increases, which supports the claim that the closest segments are the most relevant."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "As stated in Section 3, considering the preceding segments, we have shown in a previous study (#REF) that providing information in the form of segment classifications leads to better results than in the form of words. #TARGET_REF further showed that using a single label per segment is better than using the probability of each class. Furthermore, both studies showed that using automatic predictions leads to a decrease in performance around 2 percentage points in comparison to using the manual annotations. Thus, in order to simplify the experiments and obtain an upper bound for the approach, in this study we just use the manual annotations. In our previous study, we have used up to five preceding segments and showed that the gain becomes smaller as the number of preceding segments increases, which supports the claim that the closest segments are the most relevant.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF further showed that using a single label per segment is better than using the probability of each class.\"]}"
    },
    {
        "gold": {
            "text": [
                "Using pre-trained embeddings typically leads to results that generalize better, since they are trained on large amounts of data and not only on a reduced set focused on a particular domain.",
                "However, this also means that their generation does not take their future use on a specific task into account.",
                "In #TARGET_REF, #TARGET_REF used pre-trained embeddings but let them adapt to the task during the training phase.",
                "However, they did not perform a comparison with the case where the embeddings are not adaptable.",
                "Thus, in our study we experimented with both fixed and adaptable embeddings."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                1
            ]
        },
        "input": "Using pre-trained embeddings typically leads to results that generalize better, since they are trained on large amounts of data and not only on a reduced set focused on a particular domain. However, this also means that their generation does not take their future use on a specific task into account. In #TARGET_REF, #TARGET_REF used pre-trained embeddings but let them adapt to the task during the training phase. However, they did not perform a comparison with the case where the embeddings are not adaptable. Thus, in our study we experimented with both fixed and adaptable embeddings.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"In #TARGET_REF, #TARGET_REF used pre-trained embeddings but let them adapt to the task during the training phase.\", \"However, they did not perform a comparison with the case where the embeddings are not adaptable.\", \"Thus, in our study we experimented with both fixed and adaptable embeddings.\"]}"
    },
    {
        "gold": {
            "text": [
                "#REF stopped at three preceding segments, but noticed a similar pattern.",
                "In this study we explore up to five preceding segments, as well as the entire dialog history.",
                "Although both our previous study and that by #TARGET_REF used the classifications of preceding segments as context information, none of them took into account that those segments have a sequential nature and simply flattened the sequence before appending it to the segment representation.",
                "However, as previously stated, both studies have shown that each label in that sequence is related to those that precede it.",
                "Thus, an approach that captures that information is expected to lead to better performance."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "#REF stopped at three preceding segments, but noticed a similar pattern. In this study we explore up to five preceding segments, as well as the entire dialog history. Although both our previous study and that by #TARGET_REF used the classifications of preceding segments as context information, none of them took into account that those segments have a sequential nature and simply flattened the sequence before appending it to the segment representation. However, as previously stated, both studies have shown that each label in that sequence is related to those that precede it. Thus, an approach that captures that information is expected to lead to better performance.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Although both our previous study and that by #TARGET_REF used the classifications of preceding segments as context information, none of them took into account that those segments have a sequential nature and simply flattened the sequence before appending it to the segment representation.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this sense, intentions may vary if two sequential segments are uttered by the same or different speakers.",
                "Thus, turn-taking information is relevant for dialog act recognition.",
                "In fact, this has been confirmed in the study by #TARGET_REF .",
                "Thus, we also use turn-taking information in this study.",
                "It is provided as a flag that states whether the speaker is different from that of the preceding segment."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "In this sense, intentions may vary if two sequential segments are uttered by the same or different speakers. Thus, turn-taking information is relevant for dialog act recognition. In fact, this has been confirmed in the study by #TARGET_REF . Thus, we also use turn-taking information in this study. It is provided as a flag that states whether the speaker is different from that of the preceding segment.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Thus, turn-taking information is relevant for dialog act recognition.\", \"In fact, this has been confirmed in the study by #TARGET_REF .\", \"Thus, we also use turn-taking information in this study.\"]}"
    },
    {
        "gold": {
            "text": [
                "All the experiments were performed on the Switchboard Dialog Act Corpus (#REF) , which is the most explored for the task.",
                "We started with the segment representation approaches, since that is the step with higher variation among the previous studies described in Section 3 and that which introduces more changes in the overall architecture.",
                "We used adaptations of the approaches with top performance in previous studies, namely the RNN-based approach by #REF and the CNN-based approach by #TARGET_REF .",
                "However, those approaches focus on capturing different kinds of information, both of which are relevant for the task.",
                "Thus, we introduced the use of the RCNN-based approach by #REF and adapted it to capture relevant relations between distant tokens."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "All the experiments were performed on the Switchboard Dialog Act Corpus (#REF) , which is the most explored for the task. We started with the segment representation approaches, since that is the step with higher variation among the previous studies described in Section 3 and that which introduces more changes in the overall architecture. We used adaptations of the approaches with top performance in previous studies, namely the RNN-based approach by #REF and the CNN-based approach by #TARGET_REF . However, those approaches focus on capturing different kinds of information, both of which are relevant for the task. Thus, we introduced the use of the RCNN-based approach by #REF and adapted it to capture relevant relations between distant tokens.",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"We used adaptations of the approaches with top performance in previous studies, namely the RNN-based approach by #REF and the CNN-based approach by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "In terms of token embedding, we have explored approaches at the character, word, and functional levels.",
                "Starting with the typically used word-level, we have shown that using an embedding space with 200 dimensions as used by #TARGET_REF in #TARGET_REF leads to better results than any of the dimensionality values used by #REF .",
                "Furthermore, we have shown that, since the dialogs in the Switchboard Dialog Act Corpus have multiple domains, using fixed pre-trained word embeddings leads to better results than letting them be trained along with the network.",
                "In this sense, we have shown that dependency-based embeddings outperform those generated by Word2Vec, which is the most used embedding approach.",
                "This is in accordance with the task, since many dialog acts are related to the structure of the segment and, thus, the dependencies between tokens."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In terms of token embedding, we have explored approaches at the character, word, and functional levels. Starting with the typically used word-level, we have shown that using an embedding space with 200 dimensions as used by #TARGET_REF in #TARGET_REF leads to better results than any of the dimensionality values used by #REF . Furthermore, we have shown that, since the dialogs in the Switchboard Dialog Act Corpus have multiple domains, using fixed pre-trained word embeddings leads to better results than letting them be trained along with the network. In this sense, we have shown that dependency-based embeddings outperform those generated by Word2Vec, which is the most used embedding approach. This is in accordance with the task, since many dialog acts are related to the structure of the segment and, thus, the dependencies between tokens.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Starting with the typically used word-level, we have shown that using an embedding space with 200 dimensions as used by #TARGET_REF in #TARGET_REF leads to better results than any of the dimensionality values used by #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Thus, we assume that the discrepancy of over 6 percentage points between the results presented for the two sets in their paper was due to the fact they considered the outcome of a single run, with a specific initialization.",
                "Furthermore, our study has shown that their approach can be improved in many aspects.",
                "In the case of #TARGET_REF by #TARGET_REF , direct result comparison with those reported is not possible since they were obtained on different sets.",
                "However, the result differences between overlapping steps in our experiments are consistent with those described in their paper.",
                "Thus, we can safely state that their approach can be improved by using five parallel CNNs, dependency-based word embeddings, and the summary representation of context information."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Thus, we assume that the discrepancy of over 6 percentage points between the results presented for the two sets in their paper was due to the fact they considered the outcome of a single run, with a specific initialization. Furthermore, our study has shown that their approach can be improved in many aspects. In the case of #TARGET_REF by #TARGET_REF , direct result comparison with those reported is not possible since they were obtained on different sets. However, the result differences between overlapping steps in our experiments are consistent with those described in their paper. Thus, we can safely state that their approach can be improved by using five parallel CNNs, dependency-based word embeddings, and the summary representation of context information.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"In the case of #TARGET_REF by #TARGET_REF , direct result comparison with those reported is not possible since they were obtained on different sets.\"]}"
    },
    {
        "gold": {
            "text": [
                "Adding pictures as input can provide information for guiding story construction by offering visual illustrations of the storyline.",
                "In the related task of image captioning, most methods try to generate descriptions only for individual images or for short videos depicting a single activity.",
                "Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (#REF; #REF; #REF; #TARGET_REF .",
                "The type of data we consider in this paper provides input illustrations for story generation in the form of photo albums, sampled over a few minutes to a few days of time.",
                "For this type of data, generating textual descriptions involves telling a temporally consistent story about the depicted visual information, where stories must be coherent and take into account the temporal context of the images."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Adding pictures as input can provide information for guiding story construction by offering visual illustrations of the storyline. In the related task of image captioning, most methods try to generate descriptions only for individual images or for short videos depicting a single activity. Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (#REF; #REF; #REF; #TARGET_REF . The type of data we consider in this paper provides input illustrations for story generation in the form of photo albums, sampled over a few minutes to a few days of time. For this type of data, generating textual descriptions involves telling a temporally consistent story about the depicted visual information, where stories must be coherent and take into account the temporal context of the images.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Very recently, datasets have been introduced that extend this task to longer temporal sequences such as movies or photo albums (#REF; #REF; #REF; #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "However, this kind of data often contains contextual information or loosely related language.",
                "A more direct dataset was recently released #TARGET_REF , where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk.",
                "In this paper, we make use of the Visual Storytelling Dataset (#REF) .",
                "While the authors provide a seq2seq baseline, they only deal with the task of generating stories given 5-representative (summary) photos hand-selected by people from an album.",
                "Instead, we focus on the more challenging and realistic problem of end-toend generation of stories from entire albums."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "However, this kind of data often contains contextual information or loosely related language. A more direct dataset was recently released #TARGET_REF , where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk. In this paper, we make use of the Visual Storytelling Dataset (#REF) . While the authors provide a seq2seq baseline, they only deal with the task of generating stories given 5-representative (summary) photos hand-selected by people from an album. Instead, we focus on the more challenging and realistic problem of end-toend generation of stories from entire albums.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"A more direct dataset was recently released #TARGET_REF , where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk.\"]}"
    },
    {
        "gold": {
            "text": [
                "Video Summarization: Similar to documentation summarization (#REF; #REF; #REF) which extracts key sentences and words, video summarization selects key frames or shots.",
                "While some approaches use unsupervised learning (#REF; #REF) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (#REF; #REFb,a; #REF) .",
                "Recently, to better exploit semantics, (#REF) proposed textually customized summaries.",
                "Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story about an image set.",
                "Previous works include storyline graph modeling #REF), unsupervised mining (#REF) , blog-photo alignment , and language retelling #TARGET_REF; #REF) ."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Video Summarization: Similar to documentation summarization (#REF; #REF; #REF) which extracts key sentences and words, video summarization selects key frames or shots. While some approaches use unsupervised learning (#REF; #REF) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (#REF; #REFb,a; #REF) . Recently, to better exploit semantics, (#REF) proposed textually customized summaries. Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story about an image set. Previous works include storyline graph modeling #REF), unsupervised mining (#REF) , blog-photo alignment , and language retelling #TARGET_REF; #REF) .",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Previous works include storyline graph modeling #REF), unsupervised mining (#REF) , blog-photo alignment , and language retelling #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "While some approaches use unsupervised learning (#REF; #REF) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (#REF; #REFb,a; #REF) .",
                "Recently, to better exploit semantics, (#REF) proposed textually customized summaries.",
                "Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story about an image set.",
                "Previous works include storyline graph modeling #REF), unsupervised mining (#REF) , blog-photo alignment , and language retelling (#REF; #REF) .",
                "While (Park and collects data by mining Blog Posts, #TARGET_REF collects stories using Mechanical Turk, providing more directly relevant stories."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "While some approaches use unsupervised learning (#REF; #REF) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (#REF; #REFb,a; #REF) . Recently, to better exploit semantics, (#REF) proposed textually customized summaries. Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story about an image set. Previous works include storyline graph modeling #REF), unsupervised mining (#REF) , blog-photo alignment , and language retelling (#REF; #REF) . While (Park and collects data by mining Blog Posts, #TARGET_REF collects stories using Mechanical Turk, providing more directly relevant stories.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"While (Park and collects data by mining Blog Posts, #TARGET_REF collects stories using Mechanical Turk, providing more directly relevant stories.\"]}"
    },
    {
        "gold": {
            "text": [
                "To drive this work (Park and collected a dataset mined from Blog Posts.",
                "However, this kind of data often contains contextual information or loosely related language.",
                "A more direct dataset was recently released (#REF) , where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk.",
                "In this paper, we make use of the Visual Storytelling Dataset #TARGET_REF .",
                "While the authors provide a seq2seq baseline, they only deal with the task of generating stories given 5-representative (summary) photos hand-selected by people from an album."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "To drive this work (Park and collected a dataset mined from Blog Posts. However, this kind of data often contains contextual information or loosely related language. A more direct dataset was recently released (#REF) , where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk. In this paper, we make use of the Visual Storytelling Dataset #TARGET_REF . While the authors provide a seq2seq baseline, they only deal with the task of generating stories given 5-representative (summary) photos hand-selected by people from an album.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"In this paper, we make use of the Visual Storytelling Dataset #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Given an album A = {a 1 , a 2 , ..., a n }, composed of a set of photos, we use a bi-directional RNN to encode the local album context for each photo.",
                "We first extract the 2048-dimensional visual representation f i ∈ R k for each photo using ResNet101 , then a bi-directional RNN is applied to encode the full album.",
                "Following #TARGET_REF , we choose a Gated Recurrent Unit (GRU) as the RNN unit to encode the photo sequence.",
                "The sequence output at each time step encodes the local album context for each photo (from both directions).",
                "Fused with the visual representation followed by ReLU, our final photo representation is (top module in Fig. 1 ):"
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Given an album A = {a 1 , a 2 , ..., a n }, composed of a set of photos, we use a bi-directional RNN to encode the local album context for each photo. We first extract the 2048-dimensional visual representation f i ∈ R k for each photo using ResNet101 , then a bi-directional RNN is applied to encode the full album. Following #TARGET_REF , we choose a Gated Recurrent Unit (GRU) as the RNN unit to encode the photo sequence. The sequence output at each time step encodes the local album context for each photo (from both directions). Fused with the visual representation followed by ReLU, our final photo representation is (top module in Fig. 1 ):",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Following #TARGET_REF , we choose a Gated Recurrent Unit (GRU) as the RNN unit to encode the photo sequence.\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the Visual Storytelling Dataset #TARGET_REF , consisting of 10,000 albums with 200,000 photos.",
                "Each album contains 10-50 photos taken within a 48-hour span with two annotations: 1) 2 album summarizations, each with 5 selected representative photos, and 2) 5 stories describing the selected photos."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0
            ]
        },
        "input": "We use the Visual Storytelling Dataset #TARGET_REF , consisting of 10,000 albums with 200,000 photos. Each album contains 10-50 photos taken within a 48-hour span with two annotations: 1) 2 album summarizations, each with 5 selected representative photos, and 2) 5 stories describing the selected photos.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use the Visual Storytelling Dataset #TARGET_REF , consisting of 10,000 albums with 200,000 photos.\"]}"
    },
    {
        "gold": {
            "text": [
                "Pre-trained word embeddings are used in many NLP downstream tasks, such as natural language inference (NLI), machine translation (MT) or question answering (QA).",
                "Recent progress in word embedding techniques has been achieved with contextualized word embeddings (#REF) which provide different vector representations for the same word in different contexts.",
                "While gender bias has been studied, detected and partially addressed for standard word embeddings techniques #TARGET_REF; #REFa; #REF) , it is not the case for the latest techniques of contextualized word embeddings.",
                "Only just recently, #REF present a first analysis on the topic based on the proposed methods in #REF .",
                "In this paper, we further analyse the presence of gender biases in contextualized word embeddings by means of the proposed methods in #REF ."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "Pre-trained word embeddings are used in many NLP downstream tasks, such as natural language inference (NLI), machine translation (MT) or question answering (QA). Recent progress in word embedding techniques has been achieved with contextualized word embeddings (#REF) which provide different vector representations for the same word in different contexts. While gender bias has been studied, detected and partially addressed for standard word embeddings techniques #TARGET_REF; #REFa; #REF) , it is not the case for the latest techniques of contextualized word embeddings. Only just recently, #REF present a first analysis on the topic based on the proposed methods in #REF . In this paper, we further analyse the presence of gender biases in contextualized word embeddings by means of the proposed methods in #REF .",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Recent progress in word embedding techniques has been achieved with contextualized word embeddings (#REF) which provide different vector representations for the same word in different contexts.\", \"While gender bias has been studied, detected and partially addressed for standard word embeddings techniques #TARGET_REF; #REFa; #REF) , it is not the case for the latest techniques of contextualized word embeddings.\"]}"
    },
    {
        "gold": {
            "text": [
                "Human-generated corpora suffer from social biases.",
                "Those biases are reflected in the cooccurrence statistics, and therefore learned into word embeddings trained in those corpora, amplifying them #TARGET_REF; #REF) .",
                "#REF studied from a geometrical point of view the presence of gender bias in word embeddings.",
                "For this, they compute the subspace where the gender information concentrates by computing the principal components of the difference of vector representations of male and female gender-defining word pairs.",
                "With the gender subspace, the authors identify direct and indirect biases in profession words."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                1,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Human-generated corpora suffer from social biases. Those biases are reflected in the cooccurrence statistics, and therefore learned into word embeddings trained in those corpora, amplifying them #TARGET_REF; #REF) . #REF studied from a geometrical point of view the presence of gender bias in word embeddings. For this, they compute the subspace where the gender information concentrates by computing the principal components of the difference of vector representations of male and female gender-defining word pairs. With the gender subspace, the authors identify direct and indirect biases in profession words.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Human-generated corpora suffer from social biases.\", \"Those biases are reflected in the cooccurrence statistics, and therefore learned into word embeddings trained in those corpora, amplifying them #TARGET_REF; #REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "Zhao et al. (2018b) proposed an extension to GloVe embeddings (#REF) where the loss function used to train the embeddings is enriched with terms that confine the gender information to a specific portion of the embedded vector.",
                "The authors refer to these pieces of information as protected attributes.",
                "Once the embeddings are trained, the gender protected attribute can be simply removed from the vector representation, therefore eliminating any gender bias present in it.",
                "The transformations proposed by both #TARGET_REF and Zhao et al. (2018b) are downstream task-agnostic.",
                "This fact is used in the work of #REF to showcase that, while apparently the embedding information is removed, there is still gender information remaining in the vector representations."
            ],
            "label": [
                "MOT"
            ],
            "context": [
                0,
                0,
                1,
                1,
                1
            ]
        },
        "input": "Zhao et al. (2018b) proposed an extension to GloVe embeddings (#REF) where the loss function used to train the embeddings is enriched with terms that confine the gender information to a specific portion of the embedded vector. The authors refer to these pieces of information as protected attributes. Once the embeddings are trained, the gender protected attribute can be simply removed from the vector representation, therefore eliminating any gender bias present in it. The transformations proposed by both #TARGET_REF and Zhao et al. (2018b) are downstream task-agnostic. This fact is used in the work of #REF to showcase that, while apparently the embedding information is removed, there is still gender information remaining in the vector representations.",
        "output": "{\"label\": [\"MOT\"], \"context\": [\"Once the embeddings are trained, the gender protected attribute can be simply removed from the vector representation, therefore eliminating any gender bias present in it.\", \"The transformations proposed by both #TARGET_REF and Zhao et al. (2018b) are downstream task-agnostic.\", \"This fact is used in the work of #REF to showcase that, while apparently the embedding information is removed, there is still gender information remaining in the vector representations.\"]}"
    },
    {
        "gold": {
            "text": [
                "Given the high impact of contextualized word embeddings in the area of NLP and the social consequences of having biases in such embeddings, in this work we analyse the presence of bias in these contextualized word embeddings.",
                "In particular, we focus on gender biases, and specifically on the following questions:",
                "• Do contextualized word embeddings exhibit gender bias and how does this bias compare to standard and debiased word embeddings?",
                "• Do different evaluation techniques identify bias similarly and what would be the best measure to use for gender bias detection in contextualized embeddings?",
                "To address these questions, we adapt and contrast with the evaluation measures proposed by #TARGET_REF and #REF ."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Given the high impact of contextualized word embeddings in the area of NLP and the social consequences of having biases in such embeddings, in this work we analyse the presence of bias in these contextualized word embeddings. In particular, we focus on gender biases, and specifically on the following questions: • Do contextualized word embeddings exhibit gender bias and how does this bias compare to standard and debiased word embeddings? • Do different evaluation techniques identify bias similarly and what would be the best measure to use for gender bias detection in contextualized embeddings? To address these questions, we adapt and contrast with the evaluation measures proposed by #TARGET_REF and #REF .",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"To address these questions, we adapt and contrast with the evaluation measures proposed by #TARGET_REF and #REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "where N is the amount of gender neutral words, g the gender direction, and w the word vector of each profession.",
                "We got direct bias of 0.03, compared to 0.08 from standard word2vec embeddings described in #TARGET_REF .",
                "This reduction on the direct bias confirms that the substantial component along the gender direction that is present in standard word embeddings is less for the contextualized word embeddings.",
                "Probably, this reduction comes from the fact that we are using different word embeddings for the same profession depending on the sentence which is a direct consequence and advantage of using contextualized embeddings.",
                "Male and female-biased words clustering."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "where N is the amount of gender neutral words, g the gender direction, and w the word vector of each profession. We got direct bias of 0.03, compared to 0.08 from standard word2vec embeddings described in #TARGET_REF . This reduction on the direct bias confirms that the substantial component along the gender direction that is present in standard word embeddings is less for the contextualized word embeddings. Probably, this reduction comes from the fact that we are using different word embeddings for the same profession depending on the sentence which is a direct consequence and advantage of using contextualized embeddings. Male and female-biased words clustering.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"We got direct bias of 0.03, compared to 0.08 from standard word2vec embeddings described in #TARGET_REF .\", \"This reduction on the direct bias confirms that the substantial component along the gender direction that is present in standard word embeddings is less for the contextualized word embeddings.\", \"Probably, this reduction comes from the fact that we are using different word embeddings for the same profession depending on the sentence which is a direct consequence and advantage of using contextualized embeddings.\"]}"
    },
    {
        "gold": {
            "text": [
                "The 'Biased List' is the list used in the clustering experiment and it consists of biased male and female words (500 female biased tokens and 500 male biased token).",
                "This list is generated by taking the most biased words, where the bias of a word is computed by taking its projection on the gender direction ( − → he-−→ she) (e.g. breastfeeding, bridal and diet for female and hero, cigar and teammates for male).",
                "The 'Extended Biased List' is the list used in classification experiment , which contains 5000 male and female biased tokens, 2500 for each gender, generated in the same way of the Biased List 4 .",
                "A note to be considered, is that the lists we used in our experiments (and obtained from #TARGET_REF and #REF ) may contain words that are missing in our corpus and so we can not obtain contextualized embeddings for them.",
                "Among different approaches to contextualized word embeddings (mentioned in section 2), we choose ELMo (#REF) as contextualized word embedding approach."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The 'Biased List' is the list used in the clustering experiment and it consists of biased male and female words (500 female biased tokens and 500 male biased token). This list is generated by taking the most biased words, where the bias of a word is computed by taking its projection on the gender direction ( − → he-−→ she) (e.g. breastfeeding, bridal and diet for female and hero, cigar and teammates for male). The 'Extended Biased List' is the list used in classification experiment , which contains 5000 male and female biased tokens, 2500 for each gender, generated in the same way of the Biased List 4 . A note to be considered, is that the lists we used in our experiments (and obtained from #TARGET_REF and #REF ) may contain words that are missing in our corpus and so we can not obtain contextualized embeddings for them. Among different approaches to contextualized word embeddings (mentioned in section 2), we choose ELMo (#REF) as contextualized word embedding approach.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"A note to be considered, is that the lists we used in our experiments (and obtained from #TARGET_REF and #REF ) may contain words that are missing in our corpus and so we can not obtain contextualized embeddings for them.\"]}"
    },
    {
        "gold": {
            "text": [
                "There is no standard measure for gender bias, and even less for such the recently proposed contextualized word embeddings.",
                "In this section, we adapt gender bias measures for word embedding methods from previous work #TARGET_REF and (#REF) to be applicable to contextualized word embeddings.",
                "This way, we first compute the gender subspace from the ELMo vector representations of genderdefining words, then identify the presence of direct bias in the contextualized representations.",
                "We then proceed to identify gender information by means of clustering and classifications techniques.",
                "We compare our results to previous results from debiased and non-debiased word embeddings (#REF) ."
            ],
            "label": [
                "EXT"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "There is no standard measure for gender bias, and even less for such the recently proposed contextualized word embeddings. In this section, we adapt gender bias measures for word embedding methods from previous work #TARGET_REF and (#REF) to be applicable to contextualized word embeddings. This way, we first compute the gender subspace from the ELMo vector representations of genderdefining words, then identify the presence of direct bias in the contextualized representations. We then proceed to identify gender information by means of clustering and classifications techniques. We compare our results to previous results from debiased and non-debiased word embeddings (#REF) .",
        "output": "{\"label\": [\"EXT\"], \"context\": [\"In this section, we adapt gender bias measures for word embedding methods from previous work #TARGET_REF and (#REF) to be applicable to contextualized word embeddings.\"]}"
    },
    {
        "gold": {
            "text": [
                "In order to have a reference, we computed the principal components of representation of random words.",
                "Similarly to #TARGET_REF , figure 1 shows that the first eigenvalue is significantly larger than the rest and that there is also a single direction describing the majority of variance in these vectors, still the difference between the percentage of variances is less in case of contextualized embeddings, which may refer that there is less bias in such embeddings.",
                "We can easily note the difference in the case of random, where there is a smooth and gradual decrease in eigenvalues, and hence the variance percentage.",
                "A similar conclusion was stated in the recent work (#REF) where the authors applied the same approach, but for gender swapped variants of sentences with professions.",
                "They computed the difference between the vectors of occupation words in corresponding sentences and got a skewed graph where the first component represent the gender information while the second component groups the male and female related words."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                1,
                1,
                0,
                0
            ]
        },
        "input": "In order to have a reference, we computed the principal components of representation of random words. Similarly to #TARGET_REF , figure 1 shows that the first eigenvalue is significantly larger than the rest and that there is also a single direction describing the majority of variance in these vectors, still the difference between the percentage of variances is less in case of contextualized embeddings, which may refer that there is less bias in such embeddings. We can easily note the difference in the case of random, where there is a smooth and gradual decrease in eigenvalues, and hence the variance percentage. A similar conclusion was stated in the recent work (#REF) where the authors applied the same approach, but for gender swapped variants of sentences with professions. They computed the difference between the vectors of occupation words in corresponding sentences and got a skewed graph where the first component represent the gender information while the second component groups the male and female related words.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"Similarly to #TARGET_REF , figure 1 shows that the first eigenvalue is significantly larger than the rest and that there is also a single direction describing the majority of variance in these vectors, still the difference between the percentage of variances is less in case of contextualized embeddings, which may refer that there is less bias in such embeddings.\", \"We can easily note the difference in the case of random, where there is a smooth and gradual decrease in eigenvalues, and hence the variance percentage.\"]}"
    },
    {
        "gold": {
            "text": [
                "Direct Bias Direct Bias is a measure of how close a certain set of words are to the gender vector.",
                "To compute it, we extracted from the training data the sentences that contain words in the Professional List.",
                "We excluded the sentences that have both a professional token and definitional gender word to avoid the influence of the latter over the presence of bias in the former.",
                "We applied the definition of direct bias from #TARGET_REF on the ELMo representations of the professional words in these sentences.",
                "where N is the amount of gender neutral words, g the gender direction, and w the word vector of each profession."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                1
            ]
        },
        "input": "Direct Bias Direct Bias is a measure of how close a certain set of words are to the gender vector. To compute it, we extracted from the training data the sentences that contain words in the Professional List. We excluded the sentences that have both a professional token and definitional gender word to avoid the influence of the latter over the presence of bias in the former. We applied the definition of direct bias from #TARGET_REF on the ELMo representations of the professional words in these sentences. where N is the amount of gender neutral words, g the gender direction, and w the word vector of each profession.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We applied the definition of direct bias from #TARGET_REF on the ELMo representations of the professional words in these sentences.\", \"where N is the amount of gender neutral words, g the gender direction, and w the word vector of each profession.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we limit ourselves to word-by-word normalization, ignoring the syntactic differences between modern-day languages and their historic predecessors.",
                "Resources for historical text normalization are scarce.",
                "Even for major languages like English and German, we have very little training data for inducing normalization models, and the models we induce may be very specific to these datasets and not scale to writings from other historic periodsor even just writings from another monastery or by another author.",
                "Bollmann and Søgaard (2016) and #TARGET_REF recently showed that we can obtain more robust historical text normalization models by exploiting synergies across historical text normalization datasets and with related tasks.",
                "Specifically, #REF showed that multitask learning with German grapheme-to-phoneme translation as an auxiliary task improves a stateof-the-art sequence-to-sequence model for historical text normalization of medieval German manuscripts."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "In this paper, we limit ourselves to word-by-word normalization, ignoring the syntactic differences between modern-day languages and their historic predecessors. Resources for historical text normalization are scarce. Even for major languages like English and German, we have very little training data for inducing normalization models, and the models we induce may be very specific to these datasets and not scale to writings from other historic periodsor even just writings from another monastery or by another author. Bollmann and Søgaard (2016) and #TARGET_REF recently showed that we can obtain more robust historical text normalization models by exploiting synergies across historical text normalization datasets and with related tasks. Specifically, #REF showed that multitask learning with German grapheme-to-phoneme translation as an auxiliary task improves a stateof-the-art sequence-to-sequence model for historical text normalization of medieval German manuscripts.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Bollmann and S\\u00f8gaard (2016) and #TARGET_REF recently showed that we can obtain more robust historical text normalization models by exploiting synergies across historical text normalization datasets and with related tasks.\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper, we limit ourselves to word-by-word normalization, ignoring the syntactic differences between modern-day languages and their historic predecessors.",
                "Resources for historical text normalization are scarce.",
                "Even for major languages like English and German, we have very little training data for inducing normalization models, and the models we induce may be very specific to these datasets and not scale to writings from other historic periodsor even just writings from another monastery or by another author.",
                "Bollmann and Søgaard (2016) and #REF recently showed that we can obtain more robust historical text normalization models by exploiting synergies across historical text normalization datasets and with related tasks.",
                "Specifically, #TARGET_REF showed that multitask learning with German grapheme-to-phoneme translation as an auxiliary task improves a stateof-the-art sequence-to-sequence model for historical text normalization of medieval German manuscripts."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "In this paper, we limit ourselves to word-by-word normalization, ignoring the syntactic differences between modern-day languages and their historic predecessors. Resources for historical text normalization are scarce. Even for major languages like English and German, we have very little training data for inducing normalization models, and the models we induce may be very specific to these datasets and not scale to writings from other historic periodsor even just writings from another monastery or by another author. Bollmann and Søgaard (2016) and #REF recently showed that we can obtain more robust historical text normalization models by exploiting synergies across historical text normalization datasets and with related tasks. Specifically, #TARGET_REF showed that multitask learning with German grapheme-to-phoneme translation as an auxiliary task improves a stateof-the-art sequence-to-sequence model for historical text normalization of medieval German manuscripts.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Specifically, #TARGET_REF showed that multitask learning with German grapheme-to-phoneme translation as an auxiliary task improves a stateof-the-art sequence-to-sequence model for historical text normalization of medieval German manuscripts.\"]}"
    },
    {
        "gold": {
            "text": [
                "Training is done on mini-batches of 50 samples with early stopping based on validation on the individual development sets.",
                "The hyperparameters were set on a randomly selected subset of 50,000 tokens from each of the following datasets: English, German (Anselm), Hungarian, Icelandic, and Slovene (Gaj).",
                "#TARGET_REF also describe a multi-task learning (MTL) scenario where the encoder-decoder model is trained on two datasets in parallel.",
                "We perform similar experiments on pairwise combinations of our datasets.",
                "Table 2 : Examples of input tokens (first line) and reference normalization (second line) for each of the historical datasets."
            ],
            "label": [
                "BACK",
                "SIM"
            ],
            "context": [
                0,
                0,
                1,
                1,
                0
            ]
        },
        "input": "Training is done on mini-batches of 50 samples with early stopping based on validation on the individual development sets. The hyperparameters were set on a randomly selected subset of 50,000 tokens from each of the following datasets: English, German (Anselm), Hungarian, Icelandic, and Slovene (Gaj). #TARGET_REF also describe a multi-task learning (MTL) scenario where the encoder-decoder model is trained on two datasets in parallel. We perform similar experiments on pairwise combinations of our datasets. Table 2 : Examples of input tokens (first line) and reference normalization (second line) for each of the historical datasets.",
        "output": "{\"label\": [\"BACK\", \"SIM\"], \"context\": [\"#TARGET_REF also describe a multi-task learning (MTL) scenario where the encoder-decoder model is trained on two datasets in parallel.\", \"We perform similar experiments on pairwise combinations of our datasets.\"]}"
    },
    {
        "gold": {
            "text": [
                "We study when multi-task learning leads to improvements in historical text normalization.",
                "Specifically, we evaluate a state-ofthe-art approach to historical text normalization #TARGET_REF with and without various auxiliary tasks, across 10 historical text normalization datasets.",
                "We also include an experiment in English historical text normalization using data from Twitter and a grammatical error correction corpus (FCE) as auxiliary datasets.",
                "Across the board, we find that, unlike what has been observed for other NLP tasks, multi-task learning only helps when target task data is scarce."
            ],
            "label": [
                "USE",
                "MOT"
            ],
            "context": [
                0,
                1,
                0,
                0
            ]
        },
        "input": "We study when multi-task learning leads to improvements in historical text normalization. Specifically, we evaluate a state-ofthe-art approach to historical text normalization #TARGET_REF with and without various auxiliary tasks, across 10 historical text normalization datasets. We also include an experiment in English historical text normalization using data from Twitter and a grammatical error correction corpus (FCE) as auxiliary datasets. Across the board, we find that, unlike what has been observed for other NLP tasks, multi-task learning only helps when target task data is scarce.",
        "output": "{\"label\": [\"USE\", \"MOT\"], \"context\": [\"Specifically, we evaluate a state-ofthe-art approach to historical text normalization #TARGET_REF with and without various auxiliary tasks, across 10 historical text normalization datasets.\"]}"
    },
    {
        "gold": {
            "text": [
                "We consider 10 datasets from 8 different languages: German, using the #TARGET_REF (taken from #TARGET_REF) and texts from the RIDGES corpus (#REF) #TARGET_REF to obtain a single dataset.",
                "For RIDGES, we use 16 texts and randomly sample 70% of all sentences from each text for the training set, and 15% for the dev/test sets.",
                "The Spanish and Portuguese datasets consist of manually normalized subsets of the Post Scriptum corpus; here, we randomly sample 80% (train) and 10% (dev/test) of all sentences per century represented in the corpus.",
                "Dataset splits for the other languages are taken from #REF and Ljubešić et al. (2016) .",
                "We preprocessed all datasets to remove punctuation, perform Unicode normalization, replace digits that do not require normalization with a dummy symbol, and lowercase all tokens."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0,
                0
            ]
        },
        "input": "We consider 10 datasets from 8 different languages: German, using the #TARGET_REF (taken from #TARGET_REF) and texts from the RIDGES corpus (#REF) #TARGET_REF to obtain a single dataset. For RIDGES, we use 16 texts and randomly sample 70% of all sentences from each text for the training set, and 15% for the dev/test sets. The Spanish and Portuguese datasets consist of manually normalized subsets of the Post Scriptum corpus; here, we randomly sample 80% (train) and 10% (dev/test) of all sentences per century represented in the corpus. Dataset splits for the other languages are taken from #REF and Ljubešić et al. (2016) . We preprocessed all datasets to remove punctuation, perform Unicode normalization, replace digits that do not require normalization with a dummy symbol, and lowercase all tokens.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We consider 10 datasets from 8 different languages: German, using the #TARGET_REF (taken from #TARGET_REF) and texts from the RIDGES corpus (#REF) #TARGET_REF to obtain a single dataset.\"]}"
    },
    {
        "gold": {
            "text": [
                "Model We use the same encoder-decoder architecture with attention as described in #TARGET_REF .",
                "4 This is a fairly standard model consisting of one bidirectional LSTM unit in the encoder and one (unidirectional) LSTM unit in the decoder.",
                "The input for the encoder is a single historical word form represented as a sequence of characters and padded with word boundary symbols; i.e., we only input single tokens in isolation, not full sentences.",
                "The decoder attends over the encoder's outputs and generates the normalized output characters."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0,
                0
            ]
        },
        "input": "Model We use the same encoder-decoder architecture with attention as described in #TARGET_REF . 4 This is a fairly standard model consisting of one bidirectional LSTM unit in the encoder and one (unidirectional) LSTM unit in the decoder. The input for the encoder is a single historical word form represented as a sequence of characters and padded with word boundary symbols; i.e., we only input single tokens in isolation, not full sentences. The decoder attends over the encoder's outputs and generates the normalized output characters.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Model We use the same encoder-decoder architecture with attention as described in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We use the same hyperparameters across all our experiments: The dimensionality of the embedding layer is 60, the size of the LSTM layers is set to 300, and we use a dropout rate of 0.2.",
                "We use the Adam optimizer (#REF) with a character-wise cross-entropy loss.",
                "Training is done on mini-batches of 50 samples with early stopping based on validation on the individual development sets.",
                "The hyperparameters were set on a randomly selected subset of 50,000 tokens from each of the following datasets: English, German ( #TARGET_REF), Hungarian, Icelandic, and Slovene (Gaj).",
                "#REF also describe a multi-task learning (MTL) scenario where the encoder-decoder model is trained on two datasets in parallel."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "We use the same hyperparameters across all our experiments: The dimensionality of the embedding layer is 60, the size of the LSTM layers is set to 300, and we use a dropout rate of 0.2. We use the Adam optimizer (#REF) with a character-wise cross-entropy loss. Training is done on mini-batches of 50 samples with early stopping based on validation on the individual development sets. The hyperparameters were set on a randomly selected subset of 50,000 tokens from each of the following datasets: English, German ( #TARGET_REF), Hungarian, Icelandic, and Slovene (Gaj). #REF also describe a multi-task learning (MTL) scenario where the encoder-decoder model is trained on two datasets in parallel.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"The hyperparameters were set on a randomly selected subset of 50,000 tokens from each of the following datasets: English, German ( #TARGET_REF), Hungarian, Icelandic, and Slovene (Gaj).\"]}"
    },
    {
        "gold": {
            "text": [
                "There has been considerable work on multitask sequence-to-sequence models for other tasks (#REF; #REF; Elliott and Kádár, 2017) .",
                "There is a wide range of design questions and sharing strategies that we ignore here, focusing instead on under what circumstances the approach advocated in #TARGET_REF works.",
                "Our main observation-that the size of the target dataset is most predictive of multi-task learning gains-runs counter previous findings for other NLP tasks (Martínez #REF; Bingel and Søgaard, 2017) .",
                "Martínez #REF find that the label entropy of the auxiliary dataset is more predictive; Bingel and Sø-gaard (2017) find that the relative differences in the steepness of the two single-task loss curves is more predictive.",
                "Both papers consider sequence tagging problems with a small number of labels; and it is probably not a surprise that their findings do not seem to scale to the case of historical text normalization."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "There has been considerable work on multitask sequence-to-sequence models for other tasks (#REF; #REF; Elliott and Kádár, 2017) . There is a wide range of design questions and sharing strategies that we ignore here, focusing instead on under what circumstances the approach advocated in #TARGET_REF works. Our main observation-that the size of the target dataset is most predictive of multi-task learning gains-runs counter previous findings for other NLP tasks (Martínez #REF; Bingel and Søgaard, 2017) . Martínez #REF find that the label entropy of the auxiliary dataset is more predictive; Bingel and Sø-gaard (2017) find that the relative differences in the steepness of the two single-task loss curves is more predictive. Both papers consider sequence tagging problems with a small number of labels; and it is probably not a surprise that their findings do not seem to scale to the case of historical text normalization.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"There is a wide range of design questions and sharing strategies that we ignore here, focusing instead on under what circumstances the approach advocated in #TARGET_REF works.\"]}"
    },
    {
        "gold": {
            "text": [
                "Text available in the social media are typically written in a more casual style, are opinionated and speculative (#REF) .",
                "Because of this, techniques developed for formal texts, such as news articles, often do not behave as well when dealing with informal documents.",
                "In particular, news articles are more uniform in style and structure; whereas blogs often do not exhibit a stereotypical discourse structure.",
                "As a result, for blogs, it is usually more difficult to identify and rank relevant units for summarization compared to news articles.",
                "Several work have shown that discourse relations can improve the results of summarization in the case of factual texts or news articles (e.g. #TARGET_REF )."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Text available in the social media are typically written in a more casual style, are opinionated and speculative (#REF) . Because of this, techniques developed for formal texts, such as news articles, often do not behave as well when dealing with informal documents. In particular, news articles are more uniform in style and structure; whereas blogs often do not exhibit a stereotypical discourse structure. As a result, for blogs, it is usually more difficult to identify and rank relevant units for summarization compared to news articles. Several work have shown that discourse relations can improve the results of summarization in the case of factual texts or news articles (e.g. #TARGET_REF ).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Several work have shown that discourse relations can improve the results of summarization in the case of factual texts or news articles (e.g. #TARGET_REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "In some work (e.g. (#REF; #REF) ), discourse relations have been exploited successfully for multi-document summarization.",
                "In particular, #TARGET_REF experimentally showed that discourse relations can improve the coherence of multi-document summaries.",
                "(#REF) showed how discourse relations can be used effectively to incorporate additional contextual information for a given question in a query-based summarization.",
                "(#REF) used discourse relations for content selection and organization of automatic summaries and achieved an improvement in both cases.",
                "Discourse relations were also used successfully by (#REF) for news summarization."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "In some work (e.g. (#REF; #REF) ), discourse relations have been exploited successfully for multi-document summarization. In particular, #TARGET_REF experimentally showed that discourse relations can improve the coherence of multi-document summaries. (#REF) showed how discourse relations can be used effectively to incorporate additional contextual information for a given question in a query-based summarization. (#REF) used discourse relations for content selection and organization of automatic summaries and achieved an improvement in both cases. Discourse relations were also used successfully by (#REF) for news summarization.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"In particular, #TARGET_REF experimentally showed that discourse relations can improve the coherence of multi-document summaries.\"]}"
    },
    {
        "gold": {
            "text": [
                "These discourse relations are also considered in Grimes' (#REF) and Williams' predicate lists.",
                "From our corpus analysis, we have identified the six most prevalent discourse relations in this blog dataset, namely comparison, contingency, illustration, attribution, topic-opinion, and attributive.",
                "The comparison, contingency, and illustration relations are also considered by most of the work in the field of discourse analysis such as the PDTB: Penn Discourse TreeBank research group #TARGET_REF and the RST Discourse Treebank research group (#REF ).",
                "We considered three additional classes of relations: attributive, attribution, and topic-opinion.",
                "These discourse relations are summarized in Figure 1 while a description of these relations is given below."
            ],
            "label": [
                "BACK",
                "SIM",
                "EXT",
                "DIFFER"
            ],
            "context": [
                0,
                1,
                1,
                1,
                0
            ]
        },
        "input": "These discourse relations are also considered in Grimes' (#REF) and Williams' predicate lists. From our corpus analysis, we have identified the six most prevalent discourse relations in this blog dataset, namely comparison, contingency, illustration, attribution, topic-opinion, and attributive. The comparison, contingency, and illustration relations are also considered by most of the work in the field of discourse analysis such as the PDTB: Penn Discourse TreeBank research group #TARGET_REF and the RST Discourse Treebank research group (#REF ). We considered three additional classes of relations: attributive, attribution, and topic-opinion. These discourse relations are summarized in Figure 1 while a description of these relations is given below.",
        "output": "{\"label\": [\"BACK\", \"SIM\", \"EXT\", \"DIFFER\"], \"context\": [\"From our corpus analysis, we have identified the six most prevalent discourse relations in this blog dataset, namely comparison, contingency, illustration, attribution, topic-opinion, and attributive.\", \"The comparison, contingency, and illustration relations are also considered by most of the work in the field of discourse analysis such as the PDTB: Penn Discourse TreeBank research group #TARGET_REF and the RST Discourse Treebank research group (#REF ).\", \"We considered three additional classes of relations: attributive, attribution, and topic-opinion.\"]}"
    },
    {
        "gold": {
            "text": [
                "These discourse relations are summarized in Figure 1 while a description of these relations is given below.",
                "Illustration: Is used to provide additional information or detail about a situation.",
                "For example: \"Allied Capital is a closed-end management investment company that will operate as a business development concern.\" As shown in Figure 1 , illustration relations can be sub-divided into sub-categories: joint, list, disjoint, and elaboration relations according to the RST Discourse Treebank (#REF ) and the Penn Discourse TreeBank #TARGET_REF .",
                "Contingency: Provides cause, condition, reason or evidence for a situation, result or claim.",
                "For example: \"The meat is good because they slice it right in front of you.\""
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "These discourse relations are summarized in Figure 1 while a description of these relations is given below. Illustration: Is used to provide additional information or detail about a situation. For example: \"Allied Capital is a closed-end management investment company that will operate as a business development concern.\" As shown in Figure 1 , illustration relations can be sub-divided into sub-categories: joint, list, disjoint, and elaboration relations according to the RST Discourse Treebank (#REF ) and the Penn Discourse TreeBank #TARGET_REF . Contingency: Provides cause, condition, reason or evidence for a situation, result or claim. For example: \"The meat is good because they slice it right in front of you.\"",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"For example: \\\"Allied Capital is a closed-end management investment company that will operate as a business development concern.\\\" As shown in Figure 1 , illustration relations can be sub-divided into sub-categories: joint, list, disjoint, and elaboration relations according to the RST Discourse Treebank (#REF ) and the Penn Discourse TreeBank #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Contingency: Provides cause, condition, reason or evidence for a situation, result or claim.",
                "For example: \"The meat is good because they slice it right in front of you.\"",
                "As shown in Figure 1 , the contingency relation subsumes several more specific relations: explanation, evidence, reason, cause, result, consequence, background, condition, hypothetical, enablement, and purpose relations according to the Penn Discourse TreeBank #TARGET_REF .",
                "Comparison: Gives a comparison and contrast among different situations.",
                "For example, \"Its fastforward and rewind work much more smoothly and consistently than those of other models I've had.\""
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Contingency: Provides cause, condition, reason or evidence for a situation, result or claim. For example: \"The meat is good because they slice it right in front of you.\" As shown in Figure 1 , the contingency relation subsumes several more specific relations: explanation, evidence, reason, cause, result, consequence, background, condition, hypothetical, enablement, and purpose relations according to the Penn Discourse TreeBank #TARGET_REF . Comparison: Gives a comparison and contrast among different situations. For example, \"Its fastforward and rewind work much more smoothly and consistently than those of other models I've had.\"",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"As shown in Figure 1 , the contingency relation subsumes several more specific relations: explanation, evidence, reason, cause, result, consequence, background, condition, hypothetical, enablement, and purpose relations according to the Penn Discourse TreeBank #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "As shown in Figure 1 , the contingency relation subsumes several more specific relations: explanation, evidence, reason, cause, result, consequence, background, condition, hypothetical, enablement, and purpose relations according to the Penn Discourse TreeBank (#REF) .",
                "Comparison: Gives a comparison and contrast among different situations.",
                "For example, \"Its fastforward and rewind work much more smoothly and consistently than those of other models I've had.\"",
                "The comparison relation subsumes the contrast relation according to the Penn Discourse TreeBank #TARGET_REF and the analogy and preference relations according to the RST Discourse Treebank (#REF) .",
                "Attributive: Relation provides details about an entity or an event -e.g."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "As shown in Figure 1 , the contingency relation subsumes several more specific relations: explanation, evidence, reason, cause, result, consequence, background, condition, hypothetical, enablement, and purpose relations according to the Penn Discourse TreeBank (#REF) . Comparison: Gives a comparison and contrast among different situations. For example, \"Its fastforward and rewind work much more smoothly and consistently than those of other models I've had.\" The comparison relation subsumes the contrast relation according to the Penn Discourse TreeBank #TARGET_REF and the analogy and preference relations according to the RST Discourse Treebank (#REF) . Attributive: Relation provides details about an entity or an event -e.g.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The comparison relation subsumes the contrast relation according to the Penn Discourse TreeBank #TARGET_REF and the analogy and preference relations according to the RST Discourse Treebank (#REF) .\"]}"
    },
    {
        "gold": {
            "text": [
                "However, at the time this research was done, the only publicly available discourse parser was SPADE (#REF) which operates on individual sentences.",
                "To identify illustration, contingency, comparison, and attribution relations, we have used SPADE discourse parser.",
                "However, we have complemented this parser with three other approaches: (#REF )'s approach is used to identify intra-sentence comparison relations; we have designed a tagger based on (#REF) 's approach to identify topic-opinion relations; and we have proposed a new approach to tag attributive relations (#REF) .",
                "A description and evaluation of these approaches can be found in #TARGET_REF .",
                "By combining these approaches, a sentence is tagged with all possible discourse relations that it contains."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "However, at the time this research was done, the only publicly available discourse parser was SPADE (#REF) which operates on individual sentences. To identify illustration, contingency, comparison, and attribution relations, we have used SPADE discourse parser. However, we have complemented this parser with three other approaches: (#REF )'s approach is used to identify intra-sentence comparison relations; we have designed a tagger based on (#REF) 's approach to identify topic-opinion relations; and we have proposed a new approach to tag attributive relations (#REF) . A description and evaluation of these approaches can be found in #TARGET_REF . By combining these approaches, a sentence is tagged with all possible discourse relations that it contains.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"A description and evaluation of these approaches can be found in #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Each schema is designed to give priority to its associated question type and subjective sentences as summaries for opinionated texts are generated.",
                "Each schema specifies the types of discourse relations and the order in which they should appear in the output summary for a particular question type.",
                "Figure 2 shows a sample schema that is used to answer reason questions (e.g. \"Why do people like Picasa?\").",
                "According to this schema 8 , one or more sentences containing a topic-opinion or attribution relation followed by zero or many sentences containing a contingency or comparison relation followed by zero or many sentences containing a attributive relation should be used.",
                "Finally the most appropriate schema is selected based on a given question type; and candidate sentences fill particular slots in the selected schema based on which discourse relations they contain in order to create the final summary (details of BlogSum can be found in #TARGET_REF )."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "Each schema is designed to give priority to its associated question type and subjective sentences as summaries for opinionated texts are generated. Each schema specifies the types of discourse relations and the order in which they should appear in the output summary for a particular question type. Figure 2 shows a sample schema that is used to answer reason questions (e.g. \"Why do people like Picasa?\"). According to this schema 8 , one or more sentences containing a topic-opinion or attribution relation followed by zero or many sentences containing a contingency or comparison relation followed by zero or many sentences containing a attributive relation should be used. Finally the most appropriate schema is selected based on a given question type; and candidate sentences fill particular slots in the selected schema based on which discourse relations they contain in order to create the final summary (details of BlogSum can be found in #TARGET_REF ).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Finally the most appropriate schema is selected based on a given question type; and candidate sentences fill particular slots in the selected schema based on which discourse relations they contain in order to create the final summary (details of BlogSum can be found in #TARGET_REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "However, at the time this research was done, the only publicly available discourse parser was SPADE (#REF) which operates on individual sentences.",
                "To identify illustration, contingency, comparison, and attribution relations, we have used SPADE discourse parser.",
                "However, we have complemented this parser with three other approaches: (#REF )'s approach is used to identify intra-sentence comparison relations; we have designed a tagger based on (#REF) 's approach to identify topic-opinion relations; and we have proposed a new approach to tag attributive relations #TARGET_REF .",
                "A description and evaluation of these approaches can be found in (#REF) .",
                "By combining these approaches, a sentence is tagged with all possible discourse relations that it contains."
            ],
            "label": [
                "EXT",
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "However, at the time this research was done, the only publicly available discourse parser was SPADE (#REF) which operates on individual sentences. To identify illustration, contingency, comparison, and attribution relations, we have used SPADE discourse parser. However, we have complemented this parser with three other approaches: (#REF )'s approach is used to identify intra-sentence comparison relations; we have designed a tagger based on (#REF) 's approach to identify topic-opinion relations; and we have proposed a new approach to tag attributive relations #TARGET_REF . A description and evaluation of these approaches can be found in (#REF) . By combining these approaches, a sentence is tagged with all possible discourse relations that it contains.",
        "output": "{\"label\": [\"EXT\", \"USE\"], \"context\": [\"However, we have complemented this parser with three other approaches: (#REF )'s approach is used to identify intra-sentence comparison relations; we have designed a tagger based on (#REF) 's approach to identify topic-opinion relations; and we have proposed a new approach to tag attributive relations #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "To measure the usefulness of discourse relations for the summarization of informal texts, we have tested the effect of each relation with four different summarizers: BlogSum #TARGET_REF , MEAD #TARGET_REF , the best scoring system at TAC 2008 5 and the best scoring system at DUC 2007 6 .",
                "We have evaluated the effect of each discourse relation on the summaries generated and compared the results.",
                "Let us first describe the BlogSum summarizer."
            ],
            "label": [
                "USE"
            ],
            "context": [
                1,
                0,
                0
            ]
        },
        "input": "To measure the usefulness of discourse relations for the summarization of informal texts, we have tested the effect of each relation with four different summarizers: BlogSum #TARGET_REF , MEAD #TARGET_REF , the best scoring system at TAC 2008 5 and the best scoring system at DUC 2007 6 . We have evaluated the effect of each discourse relation on the summaries generated and compared the results. Let us first describe the BlogSum summarizer.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"To measure the usefulness of discourse relations for the summarization of informal texts, we have tested the effect of each relation with four different summarizers: BlogSum #TARGET_REF , MEAD #TARGET_REF , the best scoring system at TAC 2008 5 and the best scoring system at DUC 2007 6 .\"]}"
    },
    {
        "gold": {
            "text": [
                "For example, the baseline setup of BlogSum performed significantly lower for both R-2 and R-SU4 compared to BlogSum with all relations.",
                "This result indicates that the use of discourse relations as a whole helps to include more question relevant sentences and improve the summary content.",
                "To ensure that the results were not specific to our summarizer, we performed the same experiments with two other systems: the MEAD summarizer #TARGET_REF , a publicly available and a widely used summarizer, and with the output of the TAC best-scoring system.",
                "For MEAD, we first generated candidate sentences using MEAD, then these candidate sentences were tagged using discourse relation taggers used under BlogSum.",
                "Then these tagged sentences were filtered using BlogSum so that no sentence with a specific relation is used in summary generation for a particular experiment."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "For example, the baseline setup of BlogSum performed significantly lower for both R-2 and R-SU4 compared to BlogSum with all relations. This result indicates that the use of discourse relations as a whole helps to include more question relevant sentences and improve the summary content. To ensure that the results were not specific to our summarizer, we performed the same experiments with two other systems: the MEAD summarizer #TARGET_REF , a publicly available and a widely used summarizer, and with the output of the TAC best-scoring system. For MEAD, we first generated candidate sentences using MEAD, then these candidate sentences were tagged using discourse relation taggers used under BlogSum. Then these tagged sentences were filtered using BlogSum so that no sentence with a specific relation is used in summary generation for a particular experiment.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"To ensure that the results were not specific to our summarizer, we performed the same experiments with two other systems: the MEAD summarizer #TARGET_REF , a publicly available and a widely used summarizer, and with the output of the TAC best-scoring system.\"]}"
    },
    {
        "gold": {
            "text": [
                "It can be applied in many areas, like word sense disambiguation, information retrieval, information extraction and others.",
                "It has long history of improvements, starting with simple models, like bag-of-words (often weighted by TF-IDF score), continuing with more complex ones, like LSA [6] , which attempts to find \"latent\" meanings of words and phrases, and even more abstract models, like NNLM [3] .",
                "Latest results are based on neural network experience, but are far more simple: various versions of Word2Vec, Skip-gram and CBOW models [8] , which currently show the State-of-the-Art results and have proven success with morphologically complex languages like #REF , #TARGET_REF .",
                "These are corpus-based approaches, where one computes or trains the model from a large corpus.",
                "They usually consider some word context, like in bag-ofwords, where model is simple count of how often can some word be seen in context of a word being described."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "It can be applied in many areas, like word sense disambiguation, information retrieval, information extraction and others. It has long history of improvements, starting with simple models, like bag-of-words (often weighted by TF-IDF score), continuing with more complex ones, like LSA [6] , which attempts to find \"latent\" meanings of words and phrases, and even more abstract models, like NNLM [3] . Latest results are based on neural network experience, but are far more simple: various versions of Word2Vec, Skip-gram and CBOW models [8] , which currently show the State-of-the-Art results and have proven success with morphologically complex languages like #REF , #TARGET_REF . These are corpus-based approaches, where one computes or trains the model from a large corpus. They usually consider some word context, like in bag-ofwords, where model is simple count of how often can some word be seen in context of a word being described.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Latest results are based on neural network experience, but are far more simple: various versions of Word2Vec, Skip-gram and CBOW models [8] , which currently show the State-of-the-Art results and have proven success with morphologically complex languages like #REF , #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "The primary goal of this paper is to prove usefulness of Russian Twitter stream as word semantic similarity resource.",
                "Twitter is a popular social network 1 , or also called \"microblogging service\", which enables users to share and interact with short messages instantly and publicly (although private accounts are also available).",
                "Users all over the world generate hundreds of millions of tweets per day, all over the world, in many languages, generating enormous amount of verbal data.",
                "Traditional corpora for the word semantic similarity task are News, Wikipedia, electronic libraries and others (e.g. RUSSE workshop #TARGET_REF ).",
                "It was shown that type of corpus used for training affects the resulting accuracy."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "The primary goal of this paper is to prove usefulness of Russian Twitter stream as word semantic similarity resource. Twitter is a popular social network 1 , or also called \"microblogging service\", which enables users to share and interact with short messages instantly and publicly (although private accounts are also available). Users all over the world generate hundreds of millions of tweets per day, all over the world, in many languages, generating enormous amount of verbal data. Traditional corpora for the word semantic similarity task are News, Wikipedia, electronic libraries and others (e.g. RUSSE workshop #TARGET_REF ). It was shown that type of corpus used for training affects the resulting accuracy.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"Traditional corpora for the word semantic similarity task are News, Wikipedia, electronic libraries and others (e.g. RUSSE workshop #TARGET_REF ).\"]}"
    },
    {
        "gold": {
            "text": [
                "Until recent days there was no such dataset for Russian language.",
                "To mitigate this the \"RUSSE: The First Workshop on Russian Semantic Similarity\" #TARGET_REF was conducted, producing RUSSE Human-Judgements evaluation dataset (we will refer to it as HJ-dataset).",
                "RUSSE dataset was constructed the following way.",
                "Firstly, datasets WordSim353, MC [9] and RG-65 were combined and translated.",
                "Then human judgements were obtained by crowdsourcing (using custom implementation)."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Until recent days there was no such dataset for Russian language. To mitigate this the \"RUSSE: The First Workshop on Russian Semantic Similarity\" #TARGET_REF was conducted, producing RUSSE Human-Judgements evaluation dataset (we will refer to it as HJ-dataset). RUSSE dataset was constructed the following way. Firstly, datasets WordSim353, MC [9] and RG-65 were combined and translated. Then human judgements were obtained by crowdsourcing (using custom implementation).",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"To mitigate this the \\\"RUSSE: The First Workshop on Russian Semantic Similarity\\\" #TARGET_REF was conducted, producing RUSSE Human-Judgements evaluation dataset (we will refer to it as HJ-dataset).\"]}"
    },
    {
        "gold": {
            "text": [
                "Firstly, datasets WordSim353, MC [9] and RG-65 were combined and translated.",
                "Then human judgements were obtained by crowdsourcing (using custom implementation).",
                "Final size of the dataset is 333 word pairs, it is available online 5 .",
                "The RUSSE contest was followed by paper from its organizers #TARGET_REF and several participators [1] , [5] , thus filling the gap in word semantic similarity task for Russian language.",
                "In this paper we evaluate a Word2Vec model, trained on Russian Twitter corpus against RUSSE HJ-dataset, and show results comparable to top results of other RUSSE competitors."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Firstly, datasets WordSim353, MC [9] and RG-65 were combined and translated. Then human judgements were obtained by crowdsourcing (using custom implementation). Final size of the dataset is 333 word pairs, it is available online 5 . The RUSSE contest was followed by paper from its organizers #TARGET_REF and several participators [1] , [5] , thus filling the gap in word semantic similarity task for Russian language. In this paper we evaluate a Word2Vec model, trained on Russian Twitter corpus against RUSSE HJ-dataset, and show results comparable to top results of other RUSSE competitors.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"The RUSSE contest was followed by paper from its organizers #TARGET_REF and several participators [1] , [5] , thus filling the gap in word semantic similarity task for Russian language.\"]}"
    },
    {
        "gold": {
            "text": [
                "There are several implementations of Word2Vec available, including original C utility 10 and a Python library gensim 11 .",
                "We use the latter one as we find it more convenient.",
                "Output of Tomita Parser is fed directly line-by-line to the model.",
                "It produces the set of vectors, which we then query to obtain similarity between word vectors, in order to compute the correlation with HJ-dataset.",
                "To compute correlation we use Spearman coefficient, since it was used as accuracy measure in RUSSE #TARGET_REF ."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "There are several implementations of Word2Vec available, including original C utility 10 and a Python library gensim 11 . We use the latter one as we find it more convenient. Output of Tomita Parser is fed directly line-by-line to the model. It produces the set of vectors, which we then query to obtain similarity between word vectors, in order to compute the correlation with HJ-dataset. To compute correlation we use Spearman coefficient, since it was used as accuracy measure in RUSSE #TARGET_REF .",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"To compute correlation we use Spearman coefficient, since it was used as accuracy measure in RUSSE #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "We have an intuition that further improvements are possible with larger corpus.",
                "Comparing our results to ones reported by RUSSE participants, we conclude that our best result of 0.598 is comparable to other results, as it (virtually) encloses the top-10 of results.",
                "However, best submission of RUSSE has huge gap in accuracy of 0.16, compared to our Twitter corpus.",
                "Having in mind that best results in RUSSE combine several corpora, it is reasonable to compare Twitter results to other single-corpus results.",
                "For convenience we replicate results for these corpora, originally presented in #TARGET_REF , alongside with our result in Table 5 ."
            ],
            "label": [
                "SIM",
                "USE"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "We have an intuition that further improvements are possible with larger corpus. Comparing our results to ones reported by RUSSE participants, we conclude that our best result of 0.598 is comparable to other results, as it (virtually) encloses the top-10 of results. However, best submission of RUSSE has huge gap in accuracy of 0.16, compared to our Twitter corpus. Having in mind that best results in RUSSE combine several corpora, it is reasonable to compare Twitter results to other single-corpus results. For convenience we replicate results for these corpora, originally presented in #TARGET_REF , alongside with our result in Table 5 .",
        "output": "{\"label\": [\"SIM\", \"USE\"], \"context\": [\"For convenience we replicate results for these corpora, originally presented in #TARGET_REF , alongside with our result in Table 5 .\"]}"
    },
    {
        "gold": {
            "text": [
                "In this paper we investigated the use of Twitter corpus for training Word2Vec model for task of word semantic similarity.",
                "We described a method to obtain stream of Twitter messages and prepare them for training.",
                "We use HJ-dataset, which was created for RUSSE contest #TARGET_REF to measure correlation between similarity of word vectors and human judgements on word pairs similarity.",
                "We achieve results comparable with results obtained while training Word2Vec on traditional corpora, like Wikipedia and Web pages [1] , [5] .",
                "This is especially important because Twitter data is highly dynamic, and traditional sources are mostly static (rarely change over time)."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "In this paper we investigated the use of Twitter corpus for training Word2Vec model for task of word semantic similarity. We described a method to obtain stream of Twitter messages and prepare them for training. We use HJ-dataset, which was created for RUSSE contest #TARGET_REF to measure correlation between similarity of word vectors and human judgements on word pairs similarity. We achieve results comparable with results obtained while training Word2Vec on traditional corpora, like Wikipedia and Web pages [1] , [5] . This is especially important because Twitter data is highly dynamic, and traditional sources are mostly static (rarely change over time).",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We use HJ-dataset, which was created for RUSSE contest #TARGET_REF to measure correlation between similarity of word vectors and human judgements on word pairs similarity.\"]}"
    },
    {
        "gold": {
            "text": [
                "To date, standard approaches to named entity classification rely on supervised models, that typically require a large-scale annotated corpus and a widecoverage dictionary.",
                "However, since new named entities arise regularly, it becomes increasingly difficult to maintain an up-to-date dictionary and/or adapt a named entity classifier to a new domain; for example, sequence labeling techniques that use feature templates (#REF; #REF) are not robust for unknown named entities because their feature space is very sparse #TARGET_REF .",
                "This problem worsens when we attempt to use a combination of features for sparse named entity classification.",
                "Therefore, in this paper, we propose the use of matrix factorization for named entity classification to consider the relationships between sparse features.",
                "Through our experiments, we achieved competitive accuracy to models developed in previous works in terms of using fewer features and compactness using factorization machines (#REF) ."
            ],
            "label": [
                "BACK",
                "MOT"
            ],
            "context": [
                0,
                1,
                0,
                1,
                0
            ]
        },
        "input": "To date, standard approaches to named entity classification rely on supervised models, that typically require a large-scale annotated corpus and a widecoverage dictionary. However, since new named entities arise regularly, it becomes increasingly difficult to maintain an up-to-date dictionary and/or adapt a named entity classifier to a new domain; for example, sequence labeling techniques that use feature templates (#REF; #REF) are not robust for unknown named entities because their feature space is very sparse #TARGET_REF . This problem worsens when we attempt to use a combination of features for sparse named entity classification. Therefore, in this paper, we propose the use of matrix factorization for named entity classification to consider the relationships between sparse features. Through our experiments, we achieved competitive accuracy to models developed in previous works in terms of using fewer features and compactness using factorization machines (#REF) .",
        "output": "{\"label\": [\"BACK\", \"MOT\"], \"context\": [\"However, since new named entities arise regularly, it becomes increasingly difficult to maintain an up-to-date dictionary and/or adapt a named entity classifier to a new domain; for example, sequence labeling techniques that use feature templates (#REF; #REF) are not robust for unknown named entities because their feature space is very sparse #TARGET_REF .\", \"Therefore, in this paper, we propose the use of matrix factorization for named entity classification to consider the relationships between sparse features.\"]}"
    },
    {
        "gold": {
            "text": [
                "A standard approach to named entity classification is to formulate a task as a sequence labeling problem and use a supervised method, such as conditional random fields (#REF; #REF; #REF) .",
                "These studies heavily rely on feature templates for learning combinations of features; however, since combinations of features in conventional supervised learning are treated independently, this approach is not robust for named entities that do not appear in the training data.",
                "To address the task of unknown named entity classification, #TARGET_REF explored the use of sparse combinatorial features.",
                "They proposed a log-bilinear model that defines a score function considering interactions between features; the score function is regularized via a nuclear norm on a feature weight matrix.",
                "Further, heir method employs singular value decomposition (SVD)-based regularization to handle the combination of features."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "A standard approach to named entity classification is to formulate a task as a sequence labeling problem and use a supervised method, such as conditional random fields (#REF; #REF; #REF) . These studies heavily rely on feature templates for learning combinations of features; however, since combinations of features in conventional supervised learning are treated independently, this approach is not robust for named entities that do not appear in the training data. To address the task of unknown named entity classification, #TARGET_REF explored the use of sparse combinatorial features. They proposed a log-bilinear model that defines a score function considering interactions between features; the score function is regularized via a nuclear norm on a feature weight matrix. Further, heir method employs singular value decomposition (SVD)-based regularization to handle the combination of features.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"To address the task of unknown named entity classification, #TARGET_REF explored the use of sparse combinatorial features.\"]}"
    },
    {
        "gold": {
            "text": [
                "These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus.",
                "It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points.",
                "This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization.",
                "Both our approach and the methods of #REF address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized.",
                "#TARGET_REF use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines."
            ],
            "label": [
                "BACK"
            ],
            "context": [
                0,
                0,
                0,
                0,
                1
            ]
        },
        "input": "These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus. It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points. This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization. Both our approach and the methods of #REF address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized. #TARGET_REF use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines.",
        "output": "{\"label\": [\"BACK\"], \"context\": [\"#TARGET_REF use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines.\"]}"
    },
    {
        "gold": {
            "text": [
                "As described above, we aim to classify named entities that rarely appear in a given training corpus.",
                "We compared factorization machines with a log-linear model, a polynomial-kernel SVM, and a state-ofthe-art log-bilinear model using nuclear norm for regularization #TARGET_REF ."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1
            ]
        },
        "input": "As described above, we aim to classify named entities that rarely appear in a given training corpus. We compared factorization machines with a log-linear model, a polynomial-kernel SVM, and a state-ofthe-art log-bilinear model using nuclear norm for regularization #TARGET_REF .",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We compared factorization machines with a log-linear model, a polynomial-kernel SVM, and a state-ofthe-art log-bilinear model using nuclear norm for regularization #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Data.",
                "We used the dataset provided by #TARGET_REF ; this dataset was created for evaluating unknown named entity classification and is context features: Right and left contexts of the candidate in a sentence (do not take the order into account).",
                "cap=1, cap=0: Whether the first letter of the candidate is uppercase, or not. all-low=1, all-low=0: Whether all letters of the candidate are lowercase, or not.",
                "all-cap1=1, all-cap1=0: Whether all letters of the candidate are uppercase, or not.",
                "all-cap2=1, all-cap2=0: Whether all letters of the candidate are uppercase and periods, or not."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Data. We used the dataset provided by #TARGET_REF ; this dataset was created for evaluating unknown named entity classification and is context features: Right and left contexts of the candidate in a sentence (do not take the order into account). cap=1, cap=0: Whether the first letter of the candidate is uppercase, or not. all-low=1, all-low=0: Whether all letters of the candidate are lowercase, or not. all-cap1=1, all-cap1=0: Whether all letters of the candidate are uppercase, or not. all-cap2=1, all-cap2=0: Whether all letters of the candidate are uppercase and periods, or not.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We used the dataset provided by #TARGET_REF ; this dataset was created for evaluating unknown named entity classification and is context features: Right and left contexts of the candidate in a sentence (do not take the order into account).\"]}"
    },
    {
        "gold": {
            "text": [
                "Features.",
                "We used a subset of features from experiments performed by #TARGET_REF .",
                "Table 3 summarizes the features used in our experiment, including context and entity features.",
                "Tools.",
                "In terms of tools, we used scikit-learn 0.17 to implement a log-linear model and polynomial kernel in an SVM."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Features. We used a subset of features from experiments performed by #TARGET_REF . Table 3 summarizes the features used in our experiment, including context and entity features. Tools. In terms of tools, we used scikit-learn 0.17 to implement a log-linear model and polynomial kernel in an SVM.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"We used a subset of features from experiments performed by #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Upon investigating the confusion matrix, we found that the LOC tag was often misclassified as PER.",
                "We therefore conclude here that clustering and POS features are necessary to distinguish these tags.",
                "Figure 1 plots the F1-score of our proposed method as dimension k changes for matrix factor- ization using the same development data as that of #TARGET_REF .",
                "Our method yielded the best F1-score (i.e., 57.1) at k = 5, whereas the log-bilinear model achieved the best F1-score (i.e., 61.73) at k = 40.",
                "These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus."
            ],
            "label": [
                "USE"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Upon investigating the confusion matrix, we found that the LOC tag was often misclassified as PER. We therefore conclude here that clustering and POS features are necessary to distinguish these tags. Figure 1 plots the F1-score of our proposed method as dimension k changes for matrix factor- ization using the same development data as that of #TARGET_REF . Our method yielded the best F1-score (i.e., 57.1) at k = 5, whereas the log-bilinear model achieved the best F1-score (i.e., 61.73) at k = 40. These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus.",
        "output": "{\"label\": [\"USE\"], \"context\": [\"Figure 1 plots the F1-score of our proposed method as dimension k changes for matrix factor- ization using the same development data as that of #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 2 presents results of our experiments.",
                "Note that #TARGET_REF used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use.",
                "Table 4 and Figure 2 show the performance and precision-recall curves of named entity classification for each tag, respectively.",
                "We observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by #REF with fewer features.",
                "Overall, the microaveraged F1 score improved by 1.4 points."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                1,
                0,
                0,
                0
            ]
        },
        "input": "Table 2 presents results of our experiments. Note that #TARGET_REF used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use. Table 4 and Figure 2 show the performance and precision-recall curves of named entity classification for each tag, respectively. We observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by #REF with fewer features. Overall, the microaveraged F1 score improved by 1.4 points.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"Note that #TARGET_REF used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use.\"]}"
    },
    {
        "gold": {
            "text": [
                "Experimental results show that performance on ORG was improved.",
                "For example, the term \"VicePresident\" appears in both contexts of ORG and O, and our method correctly handled this sparse combination of context and entity features.",
                "The accuracy of LOC, however, was lower than that of the log-bilinear model #TARGET_REF .",
                "Upon investigating the confusion matrix, we found that the LOC tag was often misclassified as PER.",
                "We therefore conclude here that clustering and POS features are necessary to distinguish these tags."
            ],
            "label": [
                "DIFFER"
            ],
            "context": [
                0,
                0,
                1,
                0,
                0
            ]
        },
        "input": "Experimental results show that performance on ORG was improved. For example, the term \"VicePresident\" appears in both contexts of ORG and O, and our method correctly handled this sparse combination of context and entity features. The accuracy of LOC, however, was lower than that of the log-bilinear model #TARGET_REF . Upon investigating the confusion matrix, we found that the LOC tag was often misclassified as PER. We therefore conclude here that clustering and POS features are necessary to distinguish these tags.",
        "output": "{\"label\": [\"DIFFER\"], \"context\": [\"The accuracy of LOC, however, was lower than that of the log-bilinear model #TARGET_REF .\"]}"
    },
    {
        "gold": {
            "text": [
                "These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus.",
                "It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points.",
                "This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization.",
                "Both our approach and the methods of #TARGET_REF address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized.",
                "#REF use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines."
            ],
            "label": [
                "DIFFER",
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "These results show that factorization machines require a compact model to achieve state-of-the-art results on the test set of this corpus. It would be interesting to point out that the performance of our factorization machines approach on the development dataset was lower than that of the log-bilinear model by 4.6 points. This phenomenon may occur because the log-bilinear model overfits to sparse combinatorial features even with nuclear norm regularization; further factorization machines typically have better generalization abilities than those of nuclear norm regularization. Both our approach and the methods of #TARGET_REF address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized. #REF use maximum likelihood estimation as an objective function; whereas other objective functions such as hinge loss can be used in factorization machines.",
        "output": "{\"label\": [\"DIFFER\", \"SIM\"], \"context\": [\"Both our approach and the methods of #TARGET_REF address the problem of incorporating sparse combinatorial features by dimension reduction (i.e., matrix factorization); however, they differ in terms of the objective function to be optimized.\"]}"
    },
    {
        "gold": {
            "text": [
                "Table 2 presents results of our experiments.",
                "Note that #REF used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use.",
                "Table 4 and Figure 2 show the performance and precision-recall curves of named entity classification for each tag, respectively.",
                "We observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by #TARGET_REF with fewer features.",
                "Overall, the microaveraged F1 score improved by 1.4 points."
            ],
            "label": [
                "SIM"
            ],
            "context": [
                0,
                0,
                0,
                1,
                0
            ]
        },
        "input": "Table 2 presents results of our experiments. Note that #REF used additional features such as Brown clustering and parts-of-speech (POS) features, which we did not use. Table 4 and Figure 2 show the performance and precision-recall curves of named entity classification for each tag, respectively. We observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by #TARGET_REF with fewer features. Overall, the microaveraged F1 score improved by 1.4 points.",
        "output": "{\"label\": [\"SIM\"], \"context\": [\"We observed here that, aside from LOC, we obtained competitive results to the state-of-the-art named entity classifier proposed by #TARGET_REF with fewer features.\"]}"
    }
]